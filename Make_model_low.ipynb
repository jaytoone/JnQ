{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(308345, 54, 11)\n",
      "(308345, 1)\n",
      "104066.0\n",
      "(215841, 54, 11, 1)\n",
      "(46251, 54, 11, 1)\n",
      "(46253, 54, 11, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/lab23/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/lab23/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/lab23/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/lab23/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/lab23/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/lab23/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/lab23/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/lab23/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/lab23/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/lab23/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/lab23/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/lab23/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(215841, 3)\n",
      "(46251, 3)\n",
      "(46253, 3)\n",
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4070: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input (InputLayer)           (None, 54, 2, 1)          0         \n",
      "_________________________________________________________________\n",
      "conv1_1 (Conv2D)             (None, 54, 2, 100)        1000      \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 54, 2, 100)        400       \n",
      "_________________________________________________________________\n",
      "pool1_1 (MaxPooling2D)       (None, 27, 1, 100)        0         \n",
      "_________________________________________________________________\n",
      "drop1_1 (Dropout)            (None, 27, 1, 100)        0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 2700)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 100)               270100    \n",
      "_________________________________________________________________\n",
      "output (Dense)               (None, 3)                 303       \n",
      "=================================================================\n",
      "Total params: 271,803\n",
      "Trainable params: 271,603\n",
      "Non-trainable params: 200\n",
      "_________________________________________________________________\n",
      "None\n",
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/keras/callbacks/tensorboard_v1.py:200: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/keras/callbacks/tensorboard_v1.py:203: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n",
      "\n",
      "Epoch 1/100\n",
      " - 35s - loss: 0.7107 - accuracy: 0.7724 - val_loss: 0.7933 - val_accuracy: 0.7820\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.79326, saving model to model/rapid_ascending_vol 54_27.hdf5\n",
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/keras/callbacks/tensorboard_v1.py:343: The name tf.Summary is deprecated. Please use tf.compat.v1.Summary instead.\n",
      "\n",
      "Epoch 2/100\n",
      " - 33s - loss: 0.6912 - accuracy: 0.7751 - val_loss: 0.5906 - val_accuracy: 0.7820\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.79326 to 0.59061, saving model to model/rapid_ascending_vol 54_27.hdf5\n",
      "Epoch 3/100\n",
      " - 33s - loss: 0.6896 - accuracy: 0.7751 - val_loss: 0.7057 - val_accuracy: 0.7820\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.59061\n",
      "Epoch 4/100\n",
      " - 33s - loss: 0.6892 - accuracy: 0.7751 - val_loss: 0.5125 - val_accuracy: 0.7820\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.59061 to 0.51252, saving model to model/rapid_ascending_vol 54_27.hdf5\n",
      "Epoch 5/100\n",
      " - 33s - loss: 0.6888 - accuracy: 0.7751 - val_loss: 0.7000 - val_accuracy: 0.7820\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.51252\n",
      "Epoch 6/100\n",
      " - 33s - loss: 0.6884 - accuracy: 0.7751 - val_loss: 0.7116 - val_accuracy: 0.7820\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.51252\n",
      "Epoch 7/100\n",
      " - 33s - loss: 0.6886 - accuracy: 0.7751 - val_loss: 0.8989 - val_accuracy: 0.7820\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.51252\n",
      "Epoch 8/100\n",
      " - 33s - loss: 0.6885 - accuracy: 0.7751 - val_loss: 0.6557 - val_accuracy: 0.7820\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.51252\n",
      "Epoch 9/100\n",
      " - 34s - loss: 0.6881 - accuracy: 0.7751 - val_loss: 0.7422 - val_accuracy: 0.7820\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.51252\n",
      "Epoch 10/100\n",
      " - 34s - loss: 0.6878 - accuracy: 0.7751 - val_loss: 0.5299 - val_accuracy: 0.7820\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.51252\n",
      "Epoch 11/100\n",
      " - 33s - loss: 0.6879 - accuracy: 0.7751 - val_loss: 0.6953 - val_accuracy: 0.7820\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.51252\n",
      "Epoch 12/100\n",
      " - 33s - loss: 0.6878 - accuracy: 0.7751 - val_loss: 0.8513 - val_accuracy: 0.7820\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.51252\n",
      "Epoch 13/100\n",
      " - 33s - loss: 0.6878 - accuracy: 0.7751 - val_loss: 0.5344 - val_accuracy: 0.7820\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.51252\n",
      "Epoch 14/100\n",
      " - 33s - loss: 0.6877 - accuracy: 0.7751 - val_loss: 0.6207 - val_accuracy: 0.7820\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.51252\n",
      "Epoch 15/100\n",
      " - 33s - loss: 0.6876 - accuracy: 0.7751 - val_loss: 0.6235 - val_accuracy: 0.7820\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.51252\n",
      "Epoch 16/100\n",
      " - 33s - loss: 0.6876 - accuracy: 0.7751 - val_loss: 0.6183 - val_accuracy: 0.7820\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.51252\n",
      "Epoch 17/100\n",
      " - 33s - loss: 0.6875 - accuracy: 0.7751 - val_loss: 0.7083 - val_accuracy: 0.7820\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.51252\n",
      "Epoch 18/100\n",
      " - 33s - loss: 0.6874 - accuracy: 0.7751 - val_loss: 0.5856 - val_accuracy: 0.7820\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.51252\n",
      "Epoch 19/100\n",
      " - 33s - loss: 0.6876 - accuracy: 0.7751 - val_loss: 0.6101 - val_accuracy: 0.7820\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.51252\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import pyplot\n",
    "import scipy.misc\n",
    "from math import sqrt\n",
    "import itertools\n",
    "from IPython.display import display\n",
    "from PIL import Image\n",
    "\n",
    "input_data_length = 54\n",
    "model_num = 27\n",
    "\n",
    "Made_X = np.load('Made_X/Made_X %s_%s.npy' % (input_data_length, model_num))\n",
    "Made_Y = np.load('Made_X/Made_Y %s_%s.npy' % (input_data_length, model_num))\n",
    "\n",
    "print(Made_X.shape)\n",
    "print(Made_Y.shape)\n",
    "print(np.sum(Made_Y))\n",
    "\n",
    "# pyplot.figure(figsize=(10, 5))\n",
    "# pyplot.plot(Made_Y)\n",
    "# pyplot.show()\n",
    "\n",
    "row = Made_X.shape[1]\n",
    "col = Made_X.shape[2]\n",
    "\n",
    "total_len = len(Made_X)\n",
    "train_len = int(total_len * 0.7)\n",
    "val_len = int(total_len * 0.15)\n",
    "test_len = total_len - (train_len + val_len)\n",
    "\n",
    "X_train = Made_X[:train_len].astype('float32').reshape(-1, input_data_length, col, 1)\n",
    "X_val = Made_X[train_len:train_len + val_len].astype('float32').reshape(-1, input_data_length, col, 1)\n",
    "X_test = Made_X[train_len + val_len:].astype('float32').reshape(-1, input_data_length, col, 1)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_val.shape)\n",
    "print(X_test.shape)\n",
    "\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "Y_train = Made_Y[:train_len].astype('float32')\n",
    "Y_val = Made_Y[train_len:train_len + val_len].astype('float32')\n",
    "Y_test = Made_Y[train_len + val_len:].astype('float32')\n",
    "num_classes = 3\n",
    "Y_train = np_utils.to_categorical(Y_train, num_classes)\n",
    "Y_val = np_utils.to_categorical(Y_val, num_classes)\n",
    "Y_test = np_utils.to_categorical(Y_test, num_classes)\n",
    "print(Y_train.shape)\n",
    "print(Y_val.shape)\n",
    "print(Y_test.shape)\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=60,\n",
    "    horizontal_flip=True,\n",
    "    width_shift_range=0.6,\n",
    "    height_shift_range=0.6,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "testgen = ImageDataGenerator(\n",
    ")\n",
    "batch_size = 128\n",
    "\n",
    "# for X_batch, _ in datagen.flow(X_train, Y_train, batch_size=9):\n",
    "#     for i in range(0, 9):\n",
    "#         pyplot.axis('off')\n",
    "#         pyplot.subplot(330 + 1 + i)\n",
    "#         pyplot.imshow(X_batch[i].reshape(input_data_length, col), cmap=pyplot.get_cmap('gray'))\n",
    "#     pyplot.axis('off')\n",
    "#     pyplot.show()\n",
    "#     break\n",
    "\n",
    "#       dataset 분리      #\n",
    "def data_split_flow(dataX, dataY):\n",
    "\n",
    "    split_data = [dataX[:, :, :4], dataX[:, :, [4, -5]], dataX[:, :, [-6, -4]], dataX[:, :, -3:]]\n",
    "    \n",
    "    flow_list = [] * len(split_data)\n",
    "    for i in range(len(split_data)):\n",
    "        datagen.fit(split_data[i])\n",
    "        flow_data = datagen.flow(split_data[i], dataY, batch_size=batch_size)\n",
    "        flow_list.append(flow_data)\n",
    "        \n",
    "    return flow_list\n",
    "\n",
    "def test_split_flow(dataX, dataY):\n",
    "\n",
    "    split_data = [dataX[:, :, :4], dataX[:, :, [4, -5]], dataX[:, :, [-6, -4]], dataX[:, :, -3:]]\n",
    "    \n",
    "    flow_list = [] * len(split_data)\n",
    "    for i in range(len(split_data)):\n",
    "        datagen.fit(split_data[i])\n",
    "        flow_data = testgen.flow(split_data[i], dataY, batch_size=batch_size)\n",
    "        flow_list.append(flow_data)\n",
    "        \n",
    "    return flow_list\n",
    "\n",
    "\n",
    "#       list 순서 : price, vol, sto, macd     #\n",
    "train_flow_list = data_split_flow(X_train, Y_train)\n",
    "val_flow_list = test_split_flow(X_val, Y_val)\n",
    "test_flow_list = test_split_flow(X_test, Y_test)\n",
    "\n",
    "\n",
    "from keras.utils import plot_model\n",
    "import keras.backend as K\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import LSTM, TimeDistributed, Input, Dense, Flatten, Dropout, BatchNormalization, Conv1D\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.pooling import MaxPooling2D, MaxPooling1D\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.optimizers import Adam, SGD\n",
    "from keras.regularizers import l1, l2\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "def ohlc_Model(input_shape=(input_data_length, 4, 1)):\n",
    "    # first input model\n",
    "    visible = Input(shape=input_shape, name='input')\n",
    "    conv1_fit = 100\n",
    "    conv2_fit = 100\n",
    "    conv3_fit = 128\n",
    "    # conv4_fit = 256\n",
    "    # conv5_fit = 256\n",
    "\n",
    "    # the 1-st block\n",
    "    conv1_1 = Conv2D(conv1_fit, kernel_size=3, activation='relu', padding='same', name='conv1_1')(visible)\n",
    "    conv1_1 = BatchNormalization()(conv1_1)\n",
    "    # conv1_2 = Conv2D(conv1_fit, kernel_size=3, activation='relu', padding='same', name = 'conv1_2')(conv1_1)\n",
    "    # conv1_2 = BatchNormalization()(conv1_2)\n",
    "    pool1_1 = MaxPooling2D(pool_size=(2, 2), name='pool1_1')(conv1_1)\n",
    "    # drop1_1 = Dropout(0.3, name = 'drop1_1')(pool1_1)\n",
    "\n",
    "    # the 2-nd block\n",
    "    conv2_1 = Conv2D(conv2_fit, kernel_size=3, activation='relu', padding='same', name='conv2_1')(pool1_1)\n",
    "    conv2_1 = BatchNormalization()(conv2_1)\n",
    "    # conv2_2 = Conv2D(conv2_fit, kernel_size=3, activation='relu', padding='same', name = 'conv2_2')(conv2_1)\n",
    "    # conv2_2 = BatchNormalization()(conv2_2)\n",
    "    # conv2_3 = Conv2D(conv2_fit, kernel_size=3, activation='relu', padding='same', name = 'conv2_3')(conv2_2)\n",
    "    # conv2_3 = BatchNormalization()(conv2_3)\n",
    "    pool2_1 = MaxPooling2D(pool_size=(2, 2), name='pool2_1')(conv2_1)\n",
    "    drop2_1 = Dropout(0.3, name='drop2_1')(pool2_1)\n",
    "\n",
    "    # Flatten and output\n",
    "    flatten = Flatten(name='flatten')(drop2_1)\n",
    "    dense = Dense(100, activation='relu', name='dense')(flatten)\n",
    "    output = Dense(num_classes, activation='softmax', name='output')(dense)\n",
    "\n",
    "    # create model\n",
    "    model = Model(inputs=visible, outputs=output)\n",
    "    # summary layers\n",
    "    print(model.summary())\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def rest_Model(input_shape=(input_data_length, 2, 1)):\n",
    "    # first input model\n",
    "    visible = Input(shape=input_shape, name='input')\n",
    "    conv1_fit = 100\n",
    "    conv2_fit = 100\n",
    "    conv3_fit = 128\n",
    "    # conv4_fit = 256\n",
    "    # conv5_fit = 256\n",
    "\n",
    "    # the 1-st block\n",
    "    conv1_1 = Conv2D(conv1_fit, kernel_size=3, activation='relu', padding='same', name='conv1_1')(visible)\n",
    "    conv1_1 = BatchNormalization()(conv1_1)\n",
    "    # conv1_2 = Conv2D(conv1_fit, kernel_size=3, activation='relu', padding='same', name = 'conv1_2')(conv1_1)\n",
    "    # conv1_2 = BatchNormalization()(conv1_2)\n",
    "    pool1_1 = MaxPooling2D(pool_size=(2, 2), name='pool1_1')(conv1_1)\n",
    "    drop1_1 = Dropout(0.3, name = 'drop1_1')(pool1_1)\n",
    "\n",
    "    # the 2-nd block\n",
    "    # conv2_1 = Conv2D(conv2_fit, kernel_size=3, activation='relu', padding='same', name='conv2_1')(pool1_1)\n",
    "    # conv2_1 = BatchNormalization()(conv2_1)\n",
    "    # # conv2_2 = Conv2D(conv2_fit, kernel_size=3, activation='relu', padding='same', name = 'conv2_2')(conv2_1)\n",
    "    # # conv2_2 = BatchNormalization()(conv2_2)\n",
    "    # # conv2_3 = Conv2D(conv2_fit, kernel_size=3, activation='relu', padding='same', name = 'conv2_3')(conv2_2)\n",
    "    # # conv2_3 = BatchNormalization()(conv2_3)\n",
    "    # pool2_1 = MaxPooling2D(pool_size=(2, 2), name='pool2_1')(conv2_1)\n",
    "    # drop2_1 = Dropout(0.3, name='drop2_1')(pool2_1)\n",
    "\n",
    "    # Flatten and output\n",
    "    flatten = Flatten(name='flatten')(drop1_1)\n",
    "    dense = Dense(100, activation='relu', name='dense')(flatten)\n",
    "    output = Dense(num_classes, activation='softmax', name='output')(dense)\n",
    "\n",
    "    # create model\n",
    "    model = Model(inputs=visible, outputs=output)\n",
    "    # summary layers\n",
    "    print(model.summary())\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "model = rest_Model()\n",
    "opt = Adam(lr=0.0001, decay=1e-6)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "\n",
    "# callbacks log 를 저장시키는 방법 예시\n",
    "from keras.callbacks import Callback\n",
    "import pickle\n",
    "\n",
    "\n",
    "class Checkpoint_History(Callback):\n",
    "\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.loss = []\n",
    "        self.val_loss = []\n",
    "        self.acc = []\n",
    "        self.val_acc = []\n",
    "\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        self.loss.append(logs.get('loss'))\n",
    "        self.val_loss.append(logs.get('val_loss'))\n",
    "        self.acc.append(logs.get('acc'))\n",
    "        self.val_acc.append(logs.get('val_acc'))\n",
    "\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, TensorBoard\n",
    "\n",
    "filepath = \"model/rapid_ascending_vol %s_%s.hdf5\" % (input_data_length, model_num)\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "checkpoint2 = TensorBoard(log_dir='Tensorboard_graph',\n",
    "                          histogram_freq=0,\n",
    "                          write_graph=True,\n",
    "                          write_images=True)\n",
    "checkpoint3 = EarlyStopping(monitor='val_loss', patience=15)\n",
    "callbacks_list = [checkpoint, checkpoint2, checkpoint3]\n",
    "\n",
    "# keras.callbacks.Callback 로 부터 log 를 받아와 history log 를 작성할 수 있다.\n",
    "\n",
    "# we iterate 200 times over the entire training set\n",
    "num_epochs = 100\n",
    "history = model.fit_generator(train_flow_list[1],\n",
    "                              steps_per_epoch=len(X_train) / batch_size,\n",
    "                              epochs=num_epochs,\n",
    "                              verbose=2,\n",
    "                              callbacks=callbacks_list,\n",
    "                              validation_data=val_flow_list[1],\n",
    "                              validation_steps=len(X_val) / batch_size,\n",
    "                              shuffle=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "model = load_model('model/rapid_ascending %s_%s.hdf5' % (input_data_length, model_num))\n",
    "# model = load_model('/content/gdrive/My Drive/Colab Notebooks/model/rapid_ascending %s.hdf5' % input_data_length)\n",
    "loss = model.evaluate_generator(test_flow, steps=len(X_test) / batch_size) \n",
    "print(\"Test Loss \" + str(loss[0]))\n",
    "print(\"Test Acc: \" + str(loss[1]))\n",
    "\n",
    "loss = model.evaluate(X_val, Y_val) \n",
    "print(X_val.shape)\n",
    "print(Y_val.shape)\n",
    "\n",
    "print(\"Val Loss \" + str(loss[0]))\n",
    "print(\"Val Acc: \" + str(loss[1]))\n",
    "\n",
    "#     Prediction    #\n",
    "Y_pred_ = model.predict(X_test, verbose=1)\n",
    "\n",
    "# Y_pred = Y_pred_[:,[-1]]\n",
    "# print(Y_pred.shape)\n",
    "# print(Y_test.shape)\n",
    "Y_pred = np.argmax(Y_pred_, axis=1)\n",
    "t_te = np.argmax(Y_test, axis=1)\n",
    "\n",
    "#     Manual processing     #\n",
    "Y_pred_one = Y_pred_[:, [-1]]\n",
    "max_value = np.max(Y_pred_one)\n",
    "print(max_value)\n",
    "\n",
    "limit_line = 0.9\n",
    "Y_pred_one = np.where(Y_pred_one > max_value * limit_line, 1, 0)\n",
    "\n",
    "# print(Y_pred_one)\n",
    "Y_pred_one = Y_pred_one.reshape(-1,)\n",
    "# print(Y_pred_1)\n",
    "# print(Y_pred.shape)\n",
    "# print(t_te.shape)\n",
    "\n",
    "# fail = 0\n",
    "# fail2 = 0\n",
    "# for i in range(len(Y_pred)):\n",
    "#   if Y_pred_1[i] != t_te[i]:\n",
    "#     fail += 1\n",
    "\n",
    "#   if Y_pred[i] != t_te[i]:\n",
    "#     fail2 += 1\n",
    "\n",
    "# print(1 - fail / len(Y_pred))\n",
    "# print(1 - fail2 / len(Y_pred))\n",
    "\n",
    "# print(np.sum(Y_pred), np.sum(t_te))\n",
    "# print('Y_pred / Y_test :', np.sum(Y_pred) / np.sum(t_te))\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(100,10))\n",
    "plt.subplot(211)\n",
    "plt.plot(Y_test, 'purple', label='test')\n",
    "\n",
    "plt.subplot(212)\n",
    "plt.plot(Y_pred_one, 'y', label='pred')\n",
    "plt.show()\n",
    "# plt.savefig('/content/gdrive/My Drive/Colab Notebooks/model/rapid_ascending %s.png' % input_data_length)\n",
    "# plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow2_p36]",
   "language": "python",
   "name": "conda-env-tensorflow2_p36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
