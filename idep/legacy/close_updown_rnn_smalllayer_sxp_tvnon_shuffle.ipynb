{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python [conda env:tensorflow2_p36]","language":"python","name":"conda-env-tensorflow2_p36-py"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.5"},"colab":{"name":"close_updown_rnn_smalllayer_sxp_tvnon_shuffle.ipynb","provenance":[{"file_id":"1z4z_KLPzc6RWsxo3X_dHbpByxUAgjoMJ","timestamp":1583754134002}],"collapsed_sections":["y7bVjhlwPI_-","6XibtKgphXyQ"],"toc_visible":true,"machine_shape":"hm"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AK9FjWwLOyay","executionInfo":{"status":"ok","timestamp":1621947747036,"user_tz":-540,"elapsed":831,"user":{"displayName":"7th June","photoUrl":"","userId":"08178289703395036410"}},"outputId":"37a47918-93a8-42c7-8d5a-8f88aec44700"},"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","import os, sys\n","\n","current_path = '/content/drive/My Drive/Colab Notebooks/300/'\n","\n","os.chdir(current_path)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"8uqYv5StTazo"},"source":["### **Requirements**"]},{"cell_type":"code","metadata":{"id":"9qGt60DKTZmf"},"source":["# !pip install statsmodels==0.12.2\n","\n","# import statsmodels\n","# statsmodels.__version__"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"y7bVjhlwPI_-"},"source":["### **ARIMA**"]},{"cell_type":"code","metadata":{"id":"NvdpArctN_6l","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1621942420711,"user_tz":-540,"elapsed":5593,"user":{"displayName":"7th June","photoUrl":"","userId":"08178289703395036410"}},"outputId":"77ddcef9-ef75-49c2-a900-802451f3ada3"},"source":["from statsmodels.tsa.arima_model import ARIMA\n","# from statsmodels.tsa.arima.model import ARIMA\n","\n","from datetime import datetime\n","\n","\n","def arima_test(target, use_rows=None):\n","\n","  size = int(len(target) * 0.66)\n","  train, test = target[:size].values, target[size:]\n","  test_shift = test.shift(1).values\n","  test = test.values\n","  # break\n","\n","  history = list(train)\n","  predictions = list()\n","  err_ranges = list()\n","  for t in range(len(test)):\n","    \n","      if use_rows is not None:\n","        history = history[-use_rows:]\n","        \n","      model = ARIMA(history, order=(0, 2, 4))\n","      model_fit = model.fit()\n","      output = model_fit.forecast()\n","      # print(output)\n","      # break\n","\n","      predictions.append(output[0])\n","      err_ranges.append(output[1])\n","      obs = test[t]\n","      # print('obs :', obs)\n","      history.append(obs)\n","      # break\n","      print('\\r %.2f%%' % (t / len(test) * 100), end='')\n","\n","  print(len(test), len(predictions))\n","\n","  return predictions, err_ranges\n","\n","\n","# print(high)\n","\n","\n","def get_back_result(ohlcv, predictions, err_ranges, tp=0.04, sl=None, leverage=1, show_detail=False, show_plot=False, return_pr=False, cumsum=False, \n","                    close_ver=False, reverse_short=False):\n","\n","  \n","  high, low, test = np.split(ohlcv.values[-len(predictions):, [1, 2, 3]], 3, axis=1)\n","\n","  if close_ver:\n","    predictions = ohlcv['close'].shift(1).values[-len(test):]\n","\n","  fee = 0.0006\n","  long_profits = []\n","  short_profits = []\n","  liquidations = []\n","  win_cnt = 0\n","  for i in range(len(test)):\n","\n","    long_ep = predictions[i]\n","    if sl is not None:\n","      long_sl = long_ep * (1 / (sl + 1))\n","\n","    # assert long_ep < long_exit, 'long_exit < long_ep !, %s, %s' % (long_exit, long_ep)\n","    \n","    short_ep = (predictions[i] + err_ranges[i]) * (1 + tp)\n","    # short_ep = (predictions[i] + err_ranges[i]) * (1 / (1 - tp))\n","    if sl is not None:\n","      short_sl = short_ep * (1 / (1 - sl))\n","\n","    # print((low[i]))\n","\n","    #    long 우선   # <-- long & short 둘다 체결된 상황에서는 long 체결을 우선으로 한다.\n","    if low[i] < long_ep:\n","      \n","      liquidation = low[i] / long_ep - fee\n","      l_liquidation = 1 + (liquidation - 1) * leverage\n","      liquidations.append(l_liquidation)\n","\n","      if max(l_liquidation, 0) == 0:\n","        l_profit = 0\n","        # print('low[i], long_ep, l_liquidation :', low[i], long_ep, l_liquidation)\n","      else:\n","\n","        if sl is not None:\n","          if low[i] < long_sl:\n","            profit = long_sl / long_ep - fee\n","          else:\n","            profit = test[i] / long_ep - fee\n","\n","        else:\n","          profit = test[i] / long_ep - fee\n","\n","        l_profit = 1 + (profit - 1) * leverage\n","        l_profit = max(l_profit, 0)\n","        \n","        if profit >= 1:\n","          win_cnt += 1\n","\n","      long_profits.append(l_profit)\n","      short_profits.append(1.0)\n","\n","      if show_detail:\n","        print(test[i], predictions[i], long_ep)\n","\n","    # if high[i] > short_ep > low[i]: # 지정 대기가 아니라, 해당 price 가 지나면, long 한다.\n","\n","    #   if not reverse_short:\n","    #     liquidation = short_ep / high[i]  - fee\n","    #   else:\n","    #     liquidation = low[i] / short_ep  - fee\n","    #   l_liquidation = 1 + (liquidation - 1) * leverage\n","\n","    #   if max(l_liquidation, 0) == 0:\n","    #     l_profit = 0\n","    #   else:\n","\n","    #     if sl is not None:\n","    #       if high[i] > short_sl:\n","\n","    #         if not reverse_short:\n","    #           profit = short_ep / short_sl - fee\n","    #         else:\n","    #           profit = short_sl / short_ep - fee\n","\n","    #       else:\n","    #         if not reverse_short:\n","    #           profit = short_ep / test[i] - fee\n","    #         else:\n","    #           profit = test[i] / short_ep - fee\n","\n","    #     else:\n","\n","    #       if not reverse_short:\n","    #         profit = short_ep / test[i] - fee\n","    #       else:\n","    #         profit = test[i] / short_ep - fee\n","\n","    #     l_profit = 1 + (profit - 1) * leverage\n","    #     l_profit = max(l_profit, 0)\n","\n","    #     if profit >= 1:\n","    #       win_cnt += 1\n","\n","    #   short_profits.append(l_profit)\n","    #   long_profits.append(1.0)\n","\n","    #   if show_detail:\n","    #     print(test[i], predictions[i], short_ep)\n","    \n","    else:\n","      long_profits.append(1.0)\n","      short_profits.append(1.0)\n","      liquidations.append(1.0)\n","\n","\n","  long_win_ratio = sum(np.array(long_profits) > 1.0) / sum(np.array(long_profits) != 1.0)\n","  short_win_ratio = sum(np.array(short_profits) > 1.0) / sum(np.array(short_profits) != 1.0)\n","  long_frequency = sum(np.array(long_profits) != 1.0) / len(test)\n","  short_frequency = sum(np.array(short_profits) != 1.0) / len(test)\n","  if not cumsum:\n","    long_accum_profit = np.array(long_profits).cumprod()\n","    short_accum_profit = np.array(short_profits).cumprod()\n","  else:\n","    long_accum_profit = (np.array(long_profits) - 1.0).cumsum()\n","    short_accum_profit = (np.array(short_profits) - 1.0).cumsum()\n","\n","  # print(win_ratio)\n","\n","  if show_plot:\n","\n","    plt.figure(figsize=(10, 5))\n","    plt.suptitle('tp=%.4f, lvrg=%d' % (tp, leverage))\n","\n","    plt.subplot(151)\n","    plt.plot(liquidations)\n","    plt.title('liquidations')\n","\n","    plt.subplot(152)\n","    plt.plot(long_profits)\n","    plt.title('Win Ratio : %.2f %%\\nrequency : %.2f %%' % (long_win_ratio * 100, long_frequency * 100), color='black')\n","    # plt.show()\n","\n","    # print()\n","    plt.subplot(153)\n","    plt.plot(long_accum_profit)\n","    plt.title('Accum_profit : %.2f' % long_accum_profit[-1], color='black')\n","\n","    plt.subplot(154)\n","    plt.plot(short_profits)\n","    plt.title('Win Ratio : %.2f %%\\nrequency : %.2f %%' % (short_win_ratio * 100, short_frequency * 100), color='black')\n","    # plt.show()\n","\n","    # print()\n","    plt.subplot(155)\n","    plt.plot(short_accum_profit)\n","    plt.title('Accum_profit : %.2f' % short_accum_profit[-1], color='black')\n","    plt.show()\n","\n","  return [long_win_ratio, short_win_ratio], [long_frequency, short_frequency], [long_accum_profit[-1], short_accum_profit[-1]], [long_profits, short_profits]\n","\n","\n","# get_back_result(tp=0.04, leverage=1, show_plot=True)\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n","  import pandas.util.testing as tm\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"aDkU3tMiM2lO","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1621942422118,"user_tz":-540,"elapsed":1410,"user":{"displayName":"7th June","photoUrl":"","userId":"08178289703395036410"}},"outputId":"777c805d-3989-430b-8690-57a4bb9e689a"},"source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","interval = '30m'\n","date_path = './candlestick_concated/%s/2021-04-27/' % interval\n","file_list = os.listdir(date_path)\n","\n","print((file_list))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["['2021-04-27 BTCUSDT.xlsx', '2021-04-27 ETHUSDT.xlsx', '2021-04-27 BCHUSDT.xlsx', '2021-04-27 XRPUSDT.xlsx', '2021-04-27 EOSUSDT.xlsx', '2021-04-27 LTCUSDT.xlsx', '2021-04-27 ETCUSDT.xlsx', '2021-04-27 LINKUSDT.xlsx', '2021-04-27 XLMUSDT.xlsx', '2021-04-27 ADAUSDT.xlsx', '2021-04-27 XMRUSDT.xlsx', '2021-04-27 SXPUSDT.xlsx', '2021-04-27 KAVAUSDT.xlsx', '2021-04-27 BANDUSDT.xlsx', '2021-04-27 DASHUSDT.xlsx', '2021-04-27 ZECUSDT.xlsx', '2021-04-27 XTZUSDT.xlsx', '2021-04-27 BNBUSDT.xlsx', '2021-04-27 ATOMUSDT.xlsx', '2021-04-27 ONTUSDT.xlsx', '2021-04-27 IOTAUSDT.xlsx', '2021-04-27 BATUSDT.xlsx', '2021-04-27 NEOUSDT.xlsx', '2021-04-27 QTUMUSDT.xlsx', '2021-04-27 WAVESUSDT.xlsx', '2021-04-27 MKRUSDT.xlsx', '2021-04-27 SNXUSDT.xlsx', '2021-04-27 DOTUSDT.xlsx', '2021-04-27 THETAUSDT.xlsx', '2021-04-27 ALGOUSDT.xlsx', '2021-04-27 KNCUSDT.xlsx', '2021-04-27 ZRXUSDT.xlsx', '2021-04-27 COMPUSDT.xlsx', '2021-04-27 OMGUSDT.xlsx']\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"GmmgsEUMqUjN"},"source":["### **Model**"]},{"cell_type":"code","metadata":{"id":"mcDUjgQzqUSr"},"source":["import os\n","# os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   \n","# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","\n","%tensorflow_version 1.x\n","\n","import keras\n","import tensorflow as tf\n","\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import scipy.misc \n","from math import sqrt \n","import itertools\n","from IPython.display import display\n","\n","%matplotlib inline\n","\n","from keras.utils import plot_model\n","import keras.backend as K\n","from keras.models import Model, Sequential\n","import keras.layers as layers\n","from keras.optimizers import Adam, SGD\n","from keras.regularizers import l1, l2\n","\n","from sklearn.cluster import KMeans\n","from sklearn.metrics import silhouette_score\n","from sklearn.metrics import precision_score, recall_score, precision_recall_curve\n","from sklearn.metrics import confusion_matrix\n","from sklearn.metrics import plot_confusion_matrix\n","\n","\n","gdrive_path = current_path\n","\n","num_classes = 2\n","\n","def FER_Model(input_shape):\n","    # first input model\n","    visible = layers.Input(shape=input_shape, name='input')\n","    \n","    # net = layers.LSTM(32, return_sequences=False)(visible)\n","    net = layers.LSTM(128, return_sequences=False)(visible)\n","\n","    net = layers.Dense(num_classes, activation='softmax')(net)\n","\n","    # create model \n","    model = Model(inputs=visible, outputs=net)\n","    # summary layers\n","    print(model.summary())\n","    \n","    return model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OgZyYJPg3RJa"},"source":["def resize_npy(x):\n","\n","  temp_x = []\n","\n","  for d_i, data in enumerate(x):\n","    # resized_data = cv2.resize(data, (row * 2, col * 2)) --> input image 홰손된다\n","    # resized_data = data.repeat(2, axis=0).repeat(2, axis=1)\n","    data = data.repeat(2, axis=0).repeat(2, axis=1)\n","    # resized_data = data.repeat(1, axis=0).repeat(1, axis=1)\n","    # cmapped = plt.cm.Set1(resized_data)[:, :, :3]  # Drop Alpha Channel\n","    \n","    if d_i == 0:\n","      plt.imshow(data)\n","      plt.show()\n","      # plt.imshow(resized_data)\n","      # plt.show()\n","    # print('resized_data.shape :', resized_data.shape)\n","    # break\n","    temp_x.append(data)\n","\n","  return temp_x"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZZtZP5tqenWh"},"source":["def min_max_scale(npy_x):\n","\n","  return (npy_x - np.min(npy_x)) / (np.max(npy_x) - np.min(npy_x))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"D5F8709GI5Tc"},"source":["### save npy"]},{"cell_type":"code","metadata":{"id":"SvZuk1rPrUMe"},"source":["from sklearn.preprocessing import MinMaxScaler, MaxAbsScaler\n","import pickle\n","from sklearn.model_selection import train_test_split\n","from datetime import datetime\n","\n","from funcs_indicator import *\n","\n","pd.set_option('display.max_rows', 500)\n","pd.set_option('display.max_columns', 500)\n","pd.set_option('display.width', 1000)\n","\n","\n","start_stamp = 0\n","# start_stamp = datetime.timestamp(pd.to_datetime('2021-02-12'))\n","print(\"start_stamp :\", start_stamp)\n","# break\n","\n","np.random.shuffle(file_list)\n","candis = file_list\n","\n","long_index = 0\n","leverage = 5\n","prev_x = None\n","total_x = None\n","\n","seed = 1\n","random_state = 201\n","np.random.seed(seed)\n","\n","for i in range(len(candis)):\n","\n","  keys = [candis[i]]\n","  \n","  # if 'algo'.upper() not in candis[i]:\n","  #   continue\n","\n","\n","  # if '02-11' not in candis[i]:  # <-- 04-08 includes all timestamp range\n","  #   continue  \n","\n","  # if 'eth'.upper() not in candis[i]:\n","  #   continue\n","\n","  # if 'neo'.upper() not in candis[i]:\n","  #   continue\n","\n","  # plt.figure(figsize=(35, 10))\n","  # plt.suptitle('%s %s' % (interval, keys))\n","\n","\n","  #         get tp parameter        #\n","\n","  # plt.subplot(1,10,3)\n","  # for key in keys:  \n","  #   # plt.plot(profit_result_dict[key]['tp_list'], profit_result_dict[key]['ap_list'])\n","  #   argmax = np.argmax(profit_result_dict[key]['ap_list'][:, [long_index]])\n","  #   peak_tp = profit_result_dict[key]['tp_list'][argmax]\n","  #   # plt.axvline(peak_tp, linestyle='--')\n","  #   # plt.title('acc profit, max at %.4f' % (peak_tp))  \n","\n","  # plt.subplot(1,10,4)\n","  # plt.title('max acc profit by leverage')  \n","  # for key in keys:  \n","  #   # plt.plot(profit_result_dict[key]['tp_list'], profit_result_dict[key]['max_ap_list'], label=key)\n","  #   argmax = np.argmax(profit_result_dict[key]['max_ap_list'][:, [long_index]])\n","  #   max_peak_tp = profit_result_dict[key]['tp_list'][argmax]\n","  #   # plt.axvline(max_peak_tp, linestyle='--')\n","  #   # plt.title('max acc profit, max at %.4f' % (max_peak_tp))  \n","\n","\n","  for key in keys:  \n","\n","    # print(profit_result_dict[key]['leverage_ap_list'])\n","\n","    # for tp in [max_peak_tp]:\n","\n","      # if tp == peak_tp:\n","      #   plt.subplot(1,10,5)\n","      # else:\n","      #   plt.subplot(1,10,6)\n","\n","      #     leverage analysis     #\n","      # ohlcv = load_dict[key]['ohlcv']\n","\n","    if 'sxp'.upper() not in key:\n","      continue\n","    \n","    ohlcv = pd.read_excel(date_path + key, index_col=0)\n","    print('len(ohlcv) :', len(ohlcv))\n","\n","\n","    #       select timestamp range      #\n","    # time_index = ohlcv.index\n","    # total_stamp = list(map(lambda x: datetime.timestamp(x), time_index)) \n","\n","    # rm_index_amt = np.sum(np.array(total_stamp) < start_stamp)\n","\n","    # ohlcv = ohlcv.iloc[rm_index_amt:]\n","    # print(ohlcv.head())\n","\n","    # ohlcv = ohlcv.iloc[:-2603]  # exclude back_range\n","    # ohlcv = ohlcv.iloc[:-int(len(ohlcv) * 0.3)]  # exclude back_range\n","    # predictions = load_dict[key]['predictions']\n","    # err_ranges = load_dict[key]['err_ranges']\n","    print(\"ohlcv.index[0] :\", ohlcv.index[0])\n","    print(\"ohlcv.index[-1] :\", ohlcv.index[-1])\n","\n","    predictions = ohlcv['close'].shift(1).values\n","    err_ranges = np.zeros_like(predictions)\n","\n","    # leverage_list = profit_result_dict[key]['leverage_list']\n","    # temp_ap_list = list()\n","    # temp_pr_list = list()\n","\n","    try:\n","      print('-------------- %s --------------' % key)\n","      result = get_back_result(ohlcv, predictions, err_ranges, tp=0, leverage=leverage, show_plot=True, reverse_short=False, show_detail=False)\n","      # temp_ap_list.append(result[2])\n","      # temp_pr_list.append(result[3])\n","\n","      # if round(leverage) == 1:\n","      #   temp_pr_list = result[3]\n","      pr_list = result[3][long_index]\n","\n","    except Exception as e:\n","      print(e)\n","      break    \n","\n","\n","    \n","  # break\n","    #         clustering zone           #\n","\n","    #       set data features : ohlc, v, ep\n","    time_index = ohlcv.index[-len(predictions):]\n","\n","    sliced_ohlcv = ohlcv[-len(predictions):]\n","\n","    #       scale with price    #\n","    ohlc = ohlcv.iloc[-len(predictions):, :4]      \n","    long_ep = np.array(predictions)\n","    long_ep = long_ep.reshape(-1, 1)\n","    ha_ohlc = heikinashi(sliced_ohlcv).iloc[:, :4]\n","    sar = lucid_sar(sliced_ohlcv)\n","    ema1, ema2, ema3 = ema_ribbon(sliced_ohlcv)\n","    senkou1, senkou2 = ichimoku(sliced_ohlcv)\n","\n","    #     min max scale   #\n","    vol = sliced_ohlcv.iloc[-len(predictions):, [4]]\n","\n","    #     scale with baseline   #\n","    #     방법 1. 전체 기간 min_max scaling   #\n","    cbo, ema_cbo = cct_bbo(sliced_ohlcv, 21, 13) \n","\n","    _, _, bbw = bb_width(sliced_ohlcv, 20, 2) \n","    \n","    fish = fisher(sliced_ohlcv, 60)\n","    trix = trix_hist(sliced_ohlcv, 14, 1, 5) \n","    rsi_ = rsi(sliced_ohlcv)\n","    macd_hist = macd(sliced_ohlcv)\n","\n","\n","\n","\n","    # ohlcv['u_wick'] = ohlcv['high'] / np.maximum(ohlcv['close'] , ohlcv['open'])\n","    # ohlcv['d_wick'] = np.minimum(ohlcv['close'] , ohlcv['open']) / ohlcv['low']\n","    # ohlcv['body'] = ohlcv['close'] / ohlcv['open']\n","    # candle = ohlcv.iloc[-len(predictions):, -3:]\n","\n","\n","    print('len(ohlc) :', len(ohlc))\n","    print('long_ep.shape :', long_ep.shape)\n","    print('len(ha_ohlc) :', len(ha_ohlc))\n","    print('len(sar) :', len(sar))\n","    print('len(ema1) :', len(ema1))\n","    print('len(senkou1) :', len(senkou1))\n","    print('len(cbo) :', len(cbo))\n","    print('len(ema_cbo) :', len(ema_cbo))\n","    print('len(bbw) :', len(bbw))\n","    print('len(fish) :', len(fish))\n","    print('len(trix) :', len(trix))\n","    print('len(rsi_) :', len(rsi_))\n","    print('len(macd_hist) :', len(macd_hist))\n","\n","\n","    # break\n","\n","\n","    #       set params    #\n","    period = 45\n","    key_i = i\n","\n","    # plt.plot(cbo)\n","    # plt.plot(bbw)\n","    plt.plot(fish, color='b')\n","    # plt.plot(trix)\n","    # plt.plot(rsi_)\n","    # plt.plot(macd_hist)\n","    plt.show()\n","\n","    #      global scaling   #\n","    min_max = MinMaxScaler()\n","    cbo = min_max.fit_transform(cbo.values.reshape(-1, 1))\n","    ema_cbo = min_max.fit_transform(ema_cbo.values.reshape(-1, 1))\n","    bbw = min_max.fit_transform(bbw.values.reshape(-1, 1))\n","    fish = min_max.fit_transform(fish.values.reshape(-1, 1))\n","    trix = min_max.fit_transform(trix.values.reshape(-1, 1))\n","    rsi_ = min_max.fit_transform(rsi_.values.reshape(-1, 1))\n","    macd_hist = min_max.fit_transform(macd_hist.values.reshape(-1, 1))\n","\n","    # plt.plot(cbo)\n","    # plt.plot(bbw)\n","    plt.plot(fish, color='r')\n","    # plt.plot(trix)\n","    # plt.plot(rsi_)\n","    # plt.plot(macd_hist)\n","    plt.show()\n","    # break\n","\n","    plotting = True\n","\n","    for trial_number in range(1):\n","\n","      data_x, data_pr, data_updown = [], [], []\n","      data_index = []\n","\n","      for i in range(period, len(predictions)):\n","\n","        #   pr_list != 1 인 데이터만 사용한다\n","        # if 1:\n","        if pr_list[i] != 1:\n","\n","          min_max = MinMaxScaler()\n","          \n","          #   prediction 을 제외한 이전 데이터를 사용해야한다\n","          temp_ohlc = ohlc.iloc[i - period : i].values\n","          temp_long_ep = long_ep[i - period : i]          \n","          temp_ha_ohlc = ha_ohlc.iloc[i - period : i].values\n","          temp_sar = sar.iloc[i - period : i].values.reshape(-1, 1)\n","          temp_ema1 = ema1.iloc[i - period : i].values.reshape(-1, 1)\n","          temp_ema2 = ema2.iloc[i - period : i].values.reshape(-1, 1)\n","          temp_ema3 = ema3.iloc[i - period : i].values.reshape(-1, 1)\n","          temp_senkou1 = senkou1.iloc[i - period : i].values.reshape(-1, 1)\n","          temp_senkou2 = senkou2.iloc[i - period : i].values.reshape(-1, 1)      \n","\n","          temp_close = min_max_scale(temp_ohlc[:, [3]])    \n","\n","          price_data = np.hstack((temp_ohlc, temp_long_ep, temp_ha_ohlc, temp_sar, temp_ema1, temp_ema2, temp_ema3, temp_senkou1, temp_senkou2))\n","\n","          if np.isnan(np.sum(price_data)):\n","            continue\n","\n","          # print(\"price_data[:10] :\", price_data[:10])\n","          # print(\"temp_ohlc.shape :\", temp_ohlc.shape)\n","          # print(\"temp_long_ep.shape :\", temp_long_ep.shape)\n","          # print(\"temp_ha_ohlc.shape :\", temp_ha_ohlc.shape)\n","          # print(\"price_data.shape :\", price_data.shape)\n","\n","          if plotting:\n","            plt.plot(price_data)\n","            plt.show()\n","\n","          ind_temp_ohlc = min_max_scale(temp_ohlc)\n","          temp_price_data = min_max_scale(price_data)\n","          # temp_price_data = (price_data - np.min(price_data)) / (np.max(price_data) - np.min(price_data))\n","\n","          temp_ohlc = temp_price_data[:, :4]\n","          temp_long_ep = temp_price_data[:, [4]]\n","          temp_ha_ohlc = temp_price_data[:, 5:9]\n","          temp_sar, temp_ema1, temp_ema2, temp_ema3, temp_senkou1, temp_senkou2 = np.split(temp_price_data[:, 9:], 6, axis=1)\n","\n","          if plotting:\n","\n","            plt.plot(ind_temp_ohlc)\n","            plt.show()\n","            plt.plot(temp_price_data)\n","            plt.show()\n","\n","            plotting = False\n","          # break\n","\n","          #   vol -> min_max\n","          temp_vol = min_max.fit_transform(vol.iloc[i - period : i].values.reshape(-1, 1))\n","          \n","          # temp_candle = candle.iloc[i - period : i].values\n","\n","          temp_cbo = cbo[i - period : i]\n","          temp_ema_cbo = ema_cbo[i - period : i]\n","          temp_bbw = bbw[i - period : i]\n","          temp_fish = fish[i - period : i]\n","          temp_trix = trix[i - period : i]\n","          temp_rsi_ = rsi_[i - period : i]\n","          temp_macd_hist = macd_hist[i - period : i]\n","\n","          # print(temp_ohlc.shape)\n","          # print(temp_long_ep.shape)\n","          # print(temp_vol.shape)\n","          # print(temp_candle.shape)\n","          # break\n","\n","          trial_list = [temp_ha_ohlc, temp_sar, temp_ema1, temp_ema2, temp_ema3, temp_senkou1, temp_senkou2,\n","                        temp_cbo, temp_ema_cbo, temp_bbw, temp_fish, temp_trix, temp_rsi_, temp_macd_hist]\n","\n","          # trial_list = [temp_ohlc[:, [0], temp_ohlc[:, [1], temp_ohlc[:, [2], temp_ohlc[:, [3]], temp_vol, temp_ema_cbo]\n","\n","          #                   feature selection                   #  \n","          # temp_data = np.hstack((temp_ohlc, temp_long_ep, temp_vol, temp_candle))\n","          # temp_data = trial_list[trial_number]\n","          # temp_data = np.hstack((temp_ohlc[:, [3]], temp_ohlc[:, [1]], temp_bbw))\n","          temp_data = temp_ohlc[:, [3]]\n","          # temp_data = ind_temp_ohlc[:, [3]]\n","          # temp_data = temp_close\n","          # temp_data = np.hstack((temp_ohlc, temp_vol))\n","\n","          #     only close    #\n","          # temp_data = temp_ohlc[:, [-1]]\n","\n","          # temp_data = temp_ohlc\n","\n","          #     only volume    #\n","          # temp_data = temp_vol\n","\n","          # temp_data = np.hstack((temp_ohlc, temp_long_ep))\n","          # temp_data = temp_vol\n","\n","          #   scaler 설정\n","\n","          #   ohlc & ep -> max_abs\n","          # max_abs = MaxAbsScaler()\n","          # temp_data[:, :-1] = max_abs.fit_transform(temp_data[:, :-1])\n","\n","          \n","          # min_max = MinMaxScaler()\n","          # temp_data = min_max.fit_transform(temp_data)\n","\n","          #   candle -> max_abs    \n","          # max_abs = MaxAbsScaler()\n","          # temp_data[:, -3:] = max_abs.fit_transform(temp_data[:, -3:])\n","\n","          # min_max = MinMaxScaler()\n","          # temp_data[:, -3:] = min_max.fit_transform(temp_data[:, -3:])\n","\n","          if np.isnan(np.sum(temp_data)):\n","            continue\n","\n","          data_x.append(temp_data)\n","          data_pr.append(pr_list[i])\n","          data_index.append(time_index[i])\n","          data_updown.append(ohlc['close'].iloc[i] / ohlc['open'].iloc[i])\n","\n","\n","      print('np.array(data_x).shape :', np.array(data_x).shape)\n","      # print(data_x[0])\n","\n","\n","      #       Reshape data for image deep - learning     #\n","      _, row, col = np.array(data_x).shape\n","\n","      # input_x = np.array(data_x).reshape(-1, row, col, 1).astype(np.float32)\n","      input_x = np.array(data_x).reshape(-1, row, col).astype(np.float32)\n","\n","      #     1c to 3c    #\n","      # input_x = input_x * np.ones(3, dtype=np.float32)[None, None, None, :]\n","      # input_x = np.array(resize_npy(input_x))\n","\n","\n","      input_pr = np.array(data_pr).reshape(-1, 1).astype(np.float32)\n","      input_ud = np.array(data_updown).reshape(-1, 1).astype(np.float32)\n","      input_index = np.array(data_index).reshape(-1, 1)\n","      print('input_x.shape :', input_x.shape)\n","      print('input_x.dtype :', input_x.dtype)\n","      print('input_pr.shape :', input_pr.shape)\n","      print('input_ud.shape :', input_ud.shape)\n","      print('input_index.shape :', input_index.shape)\n","\n","\n","      # x_train_, x_test, pr_train_, pr_test, ud_train_, ud_test = train_test_split(input_x, input_pr, input_ud, test_size=0.4, shuffle=False, random_state=random_state)\n","      # x_train, x_val, pr_train, pr_val, ud_train, ud_val = train_test_split(x_train_, pr_train_, ud_train_, test_size=0.25, shuffle=False, random_state=random_state)\n","\n","      #     do stacking   #\n","      # if prev_x is None:\n","      prev_x = input_x\n","      prev_pr = input_pr\n","      prev_ud = input_ud\n","      prev_index = input_index\n","\n","      total_x = input_x\n","      total_pr = input_pr\n","      total_ud = input_ud\n","      total_index = input_index\n","\n","      # else:\n","      #   total_x = np.vstack((prev_x, input_x))\n","      #   total_pr = np.vstack((prev_pr, input_pr))\n","      #   total_ud = np.vstack((prev_ud, input_ud)) \n","      #   total_index = np.vstack((prev_index, input_index)) \n","\n","      #   prev_x = total_x\n","      #   prev_pr = total_pr\n","      #   prev_ud = total_ud\n","      #   prev_index = total_index\n","\n","      print('total_x.shape :', total_x.shape)\n","      print('total_pr.shape :', total_pr.shape)\n","      print('total_ud.shape :', total_ud.shape)\n","      print('prev_index.shape :', prev_index.shape)\n","\n","      \n","      # _, row, col, _ = input_x.shape\n","      _, row, col = input_x.shape\n","\n","      #       split new test      #\n","\n","      seed = 1\n","      random_state = 201\n","      np.random.seed(seed)\n","      from sklearn.model_selection import train_test_split\n","\n","\n","      #         get unique timestamp      #\n","      print(np.unique(total_index, return_counts=True))\n","      uniq_stamp = np.unique(total_index)\n","\n","      stamp_train_, stamp_test = train_test_split(uniq_stamp, test_size=0.2, shuffle=False, random_state=random_state)\n","      stamp_train, stamp_val = train_test_split(stamp_train_, test_size=0.25, shuffle=False, random_state=random_state)\n","\n","      print(\"stamp_train.shape :\", stamp_train.shape)\n","      print(\"stamp_val.shape :\", stamp_val.shape)\n","      print(\"stamp_test.shape :\", stamp_test.shape)\n","      # break\n","\n","\n","      #         split data by stamp     #\n","      x_train, x_val, x_test = [], [], []\n","      pr_train, pr_val, pr_test = [], [], []\n","      index_train, index_val, index_test = [], [], []\n","\n","\n","      from tqdm.notebook import tqdm\n","\n","      np.random.shuffle(total_index)\n","\n","      for i in tqdm(range(len(total_index))):\n","\n","        if total_index[i] in stamp_train:\n","          x_train.append(total_x[i])\n","          pr_train.append(total_pr[i])\n","          index_train.append(total_index[i])\n","\n","        elif total_index[i] in stamp_val:\n","          x_val.append(total_x[i])\n","          pr_val.append(total_pr[i])\n","          index_val.append(total_index[i])\n","        \n","        elif total_index[i] in stamp_test:\n","          x_test.append(total_x[i])\n","          pr_test.append(total_pr[i])\n","          index_test.append(total_index[i])\n","\n","\n","      x_train = np.array(x_train)\n","      x_val = np.array(x_val)\n","      x_test = np.array(x_test)\n","\n","      pr_train = np.array(pr_train)\n","      pr_val = np.array(pr_val)\n","      pr_test = np.array(pr_test)\n","\n","      index_train = np.array(index_train)\n","      index_val = np.array(index_val)\n","      index_test = np.array(index_test)\n","        \n","      print(\"x_train.shape :\", x_train.shape) # x_train.shape : (3807, 90, 12, 3)\n","      print(\"x_val.shape :\", x_val.shape) # x_train.shape : (3807, 90, 12, 3)\n","      print(\"x_test.shape :\", x_test.shape) # x_train.shape : (3807, 90, 12, 3)\n","\n","      symbol_name = key.split(' ')[1].split('.')[0]\n","\n","      x_save_path = current_path + 'npy/' + '%s_rnn_close_updown_x_train_robust_trial_re_%s_timesplit.npy' % (period, symbol_name)\n","      np.save(x_save_path, x_train)\n","      np.save(x_save_path.replace('x_train', 'x_val'), x_val)\n","      np.save(x_save_path.replace('x_train', 'x_test'), x_test)\n","      # np.save(x_save_path.replace('x_train', 'new_input_x'), new_input_x)\n","      print('x series saved !')\n","\n","      pr_save_path = current_path + 'npy/' + '%s_rnn_close_updown_pr_train_robust_trial_re_%s_timesplit.npy' % (period, symbol_name)\n","      np.save(pr_save_path, pr_train)\n","      np.save(pr_save_path.replace('pr_train', 'pr_val'), pr_val)\n","      np.save(pr_save_path.replace('pr_train', 'pr_test'), pr_test)\n","      # np.save(pr_save_path.replace('pr_train', 'new_input_pr'), new_input_pr)\n","      print('pr series saved !')\n","      \n","\n","  # break # --> use only one pair dataset\n","\n","  #         chunks 로 나누지 않아도, generator 에서 batch_size 만큼만 load 할 것   #\n","  try:\n","    if len(total_x) > 300000:\n","      break\n","  except:\n","    pass\n","\n","  \n","        "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1pOkWbn6JQFN"},"source":["### load npy"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wIqpv89dJNZz","executionInfo":{"status":"ok","timestamp":1621947764167,"user_tz":-540,"elapsed":316,"user":{"displayName":"7th June","photoUrl":"","userId":"08178289703395036410"}},"outputId":"2f6d1d10-9d94-4a4e-a51c-66f7816f5c0d"},"source":["from keras.utils import np_utils\n","\n","# from keras.preprocessing.image import ImageDataGenerator \n","from sklearn.utils import class_weight\n","\n","\n","symbol_name = \"SXPUSDT\"\n","period = 45\n","\n","x_save_path = current_path + 'npy/' + '%s_rnn_close_updown_x_train_%s_tssplit.npy' % (period, symbol_name)\n","\n","\n","x_train = np.load(x_save_path)\n","x_val = np.load(x_save_path.replace('x_train', 'x_val'))\n","x_test = np.load(x_save_path.replace('x_train', 'x_test'))\n","print('x series loaded !')\n","\n","pr_save_path = current_path + 'npy/' + '%s_rnn_close_updown_pr_train_%s_tssplit.npy' % (period, symbol_name)\n","\n","\n","pr_train = np.load(pr_save_path)\n","pr_val = np.load(pr_save_path.replace('pr_train', 'pr_val'))\n","pr_test = np.load(pr_save_path.replace('pr_train', 'pr_test'))\n","print('y series loaded !')\n","\n","# total_x = np.vstack((x_train, x_val, x_test))\n","# total_pr = np.vstack((pr_train, pr_val, pr_test))\n","\n","# print(\"total_x.shape :\", total_x.shape)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["x series loaded !\n","y series loaded !\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"j7huC8gvUTNf","executionInfo":{"status":"ok","timestamp":1621947770308,"user_tz":-540,"elapsed":471,"user":{"displayName":"7th June","photoUrl":"","userId":"08178289703395036410"}},"outputId":"e16c4e17-6972-40b1-e9e4-c4ab91679d07"},"source":["\n","def class_ratio(in_list):\n","\n","  return in_list / in_list[1]\n","  \n","y_train = np.where(pr_train > 1, 1, 0)\n","y_test = np.where(pr_test > 1, 1, 0)\n","y_val = np.where(pr_val > 1, 1, 0)\n","\n","print('x_train.shape :', x_train.shape)\n","print('x_test.shape :', x_test.shape)\n","print('x_val.shape :', x_val.shape)\n","print('y_train.shape :', y_train.shape)\n","print('y_test.shape :', y_test.shape)\n","print('y_val.shape :', y_val.shape)\n","\n","\n","print('np.unique(y_train, return_counts=True :', np.unique(y_train, return_counts=True), class_ratio(np.unique(y_train, return_counts=True)[1]))\n","print('np.unique(y_val, return_counts=True :', np.unique(y_val, return_counts=True), class_ratio(np.unique(y_val, return_counts=True)[1]))\n","print('np.unique(y_test, return_counts=True :', np.unique(y_test, return_counts=True), class_ratio(np.unique(y_test, return_counts=True)[1]))\n","\n","label = y_train.reshape(-1, )\n","class_weights = class_weight.compute_class_weight('balanced', \n","                                                    classes=np.unique(label),\n","                                                    y=label)\n","class_weights = dict(enumerate(class_weights))\n","print('class_weights :', class_weights)\n","\n","# sample_weight = np.ones(shape=(len(y_train),))\n","# sample_weight[(y_train == 1).reshape(-1,)] = 1.5\n","# print('sample_weight[:20] :', sample_weight[:20])\n","\n","\n","print('np.isnan(np.sum(x_train)) :', np.isnan(np.sum(x_train)))\n","print('np.isnan(np.sum(x_val)) :', np.isnan(np.sum(x_val)))\n","print('np.isnan(np.sum(x_test)) :', np.isnan(np.sum(x_test)))\n","\n","print('np.isnan(np.sum(y_train)) :', np.isnan(np.sum(y_train)))\n","print('np.isnan(np.sum(y_val)) :', np.isnan(np.sum(y_val)))\n","print('np.isnan(np.sum(y_test)) :', np.isnan(np.sum(y_test)))\n","\n","y_train_ohe = np_utils.to_categorical(y_train, num_classes)\n","y_val_ohe = np_utils.to_categorical(y_val, num_classes)\n","y_test_ohe = np_utils.to_categorical(y_test, num_classes)\n","print('y_train_ohe.shape :', y_train_ohe.shape)\n","print('y_val_ohe.shape :', y_val_ohe.shape)\n","print('y_test_ohe.shape :', y_test_ohe.shape)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["x_train.shape : (7806, 45, 1)\n","x_test.shape : (2603, 45, 1)\n","x_val.shape : (2602, 45, 1)\n","y_train.shape : (7806, 1)\n","y_test.shape : (2603, 1)\n","y_val.shape : (2602, 1)\n","np.unique(y_train, return_counts=True : (array([0, 1]), array([4170, 3636])) [1.14686469 1.        ]\n","np.unique(y_val, return_counts=True : (array([0, 1]), array([1392, 1210])) [1.15041322 1.        ]\n","np.unique(y_test, return_counts=True : (array([0, 1]), array([1390, 1213])) [1.14591921 1.        ]\n","class_weights : {0: 0.9359712230215828, 1: 1.0734323432343233}\n","np.isnan(np.sum(x_train)) : False\n","np.isnan(np.sum(x_val)) : False\n","np.isnan(np.sum(x_test)) : False\n","np.isnan(np.sum(y_train)) : False\n","np.isnan(np.sum(y_val)) : False\n","np.isnan(np.sum(y_test)) : False\n","y_train_ohe.shape : (7806, 2)\n","y_val_ohe.shape : (2602, 2)\n","y_test_ohe.shape : (2603, 2)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"-4Ta-VQ7JX6A"},"source":["### train"]},{"cell_type":"code","metadata":{"id":"USEDmvAzJYxM","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1621944049685,"user_tz":-540,"elapsed":386682,"user":{"displayName":"7th June","photoUrl":"","userId":"08178289703395036410"}},"outputId":"7a208d4a-092b-4c4e-b436-ccd55fdd1676"},"source":["\n","from keras.callbacks import ModelCheckpoint, EarlyStopping, TensorBoard\n","\n","ckpt_path = current_path + 'ckpt/'\n","board_path = current_path + 'graph/'\n","\n","batch_size = 512\n","\n","model = FER_Model(input_shape=x_train.shape[1:])\n","opt = Adam(lr=0.00001, decay=0.000005)\n","# opt = Adam(lr=0.001, decay=0.0005)\n","model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n","\n","model_name = 'classifier_%s_lstm_small_updown_%s_tvnon_shuffle.h5' % (period, symbol_name)\n","\n","\n","\n","checkpoint = ModelCheckpoint(ckpt_path + model_name, monitor='val_loss', verbose=1, save_best_only=True, mode='auto')\n","checkpoint2 = TensorBoard(log_dir=board_path,\n","                          histogram_freq=0,\n","                          write_graph=True,\n","                          write_images=True)\n","checkpoint3 = EarlyStopping(monitor='val_loss', patience=250)\n","callbacks_list = [checkpoint, checkpoint2, checkpoint3]\n","# callbacks_list = [checkpoint, checkpoint2]\n","\n","# keras.callbacks.Callback 로 부터 log 를 받아와 history log 를 작성할 수 있다.\n","\n","# we iterate 200 times over the entire training set\n","num_epochs = 1000                    \n","history = model.fit(x_train, y_train_ohe,\n","                    steps_per_epoch=int(len(x_train) / batch_size), \n","                    epochs=num_epochs,  \n","                    verbose=2,  \n","                    callbacks=callbacks_list,\n","                    class_weight=class_weights,\n","                    validation_data=(x_val, y_val_ohe),  \n","                    validation_steps=int(len(x_val) / batch_size),\n","                    shuffle=False)\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Model: \"model_3\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","input (InputLayer)           (None, 45, 1)             0         \n","_________________________________________________________________\n","lstm_3 (LSTM)                (None, 128)               66560     \n","_________________________________________________________________\n","dense_13 (Dense)             (None, 2)                 258       \n","=================================================================\n","Total params: 66,818\n","Trainable params: 66,818\n","Non-trainable params: 0\n","_________________________________________________________________\n","None\n","Train on 7806 samples, validate on 2602 samples\n","Epoch 1/1000\n"," - 2s - loss: 0.6930 - accuracy: 0.5359 - val_loss: 0.1385 - val_accuracy: 2.6556\n","\n","Epoch 00001: val_loss improved from inf to 0.13854, saving model to /content/drive/My Drive/Colab Notebooks/300/ckpt/classifier_45_lstm_small_updown_SXPUSDT_tvnon_shuffle.h5\n","Epoch 2/1000\n"," - 1s - loss: 0.6930 - accuracy: 0.5355 - val_loss: 0.1386 - val_accuracy: 2.6211\n","\n","Epoch 00002: val_loss did not improve from 0.13854\n","Epoch 3/1000\n"," - 1s - loss: 0.6929 - accuracy: 0.5301 - val_loss: 0.1386 - val_accuracy: 2.5884\n","\n","Epoch 00003: val_loss did not improve from 0.13854\n","Epoch 4/1000\n"," - 1s - loss: 0.6929 - accuracy: 0.5273 - val_loss: 0.1386 - val_accuracy: 2.5884\n","\n","Epoch 00004: val_loss did not improve from 0.13854\n","Epoch 5/1000\n"," - 1s - loss: 0.6928 - accuracy: 0.5267 - val_loss: 0.1386 - val_accuracy: 2.5884\n","\n","Epoch 00005: val_loss did not improve from 0.13854\n","Epoch 6/1000\n"," - 1s - loss: 0.6928 - accuracy: 0.5246 - val_loss: 0.1386 - val_accuracy: 2.5673\n","\n","Epoch 00006: val_loss did not improve from 0.13854\n","Epoch 7/1000\n"," - 1s - loss: 0.6928 - accuracy: 0.5205 - val_loss: 0.1386 - val_accuracy: 2.5615\n","\n","Epoch 00007: val_loss did not improve from 0.13854\n","Epoch 8/1000\n"," - 1s - loss: 0.6927 - accuracy: 0.5179 - val_loss: 0.1386 - val_accuracy: 2.5538\n","\n","Epoch 00008: val_loss did not improve from 0.13854\n","Epoch 9/1000\n"," - 1s - loss: 0.6927 - accuracy: 0.5173 - val_loss: 0.1386 - val_accuracy: 2.5404\n","\n","Epoch 00009: val_loss did not improve from 0.13854\n","Epoch 10/1000\n"," - 1s - loss: 0.6927 - accuracy: 0.5178 - val_loss: 0.1386 - val_accuracy: 2.5423\n","\n","Epoch 00010: val_loss did not improve from 0.13854\n","Epoch 11/1000\n"," - 1s - loss: 0.6926 - accuracy: 0.5182 - val_loss: 0.1386 - val_accuracy: 2.5269\n","\n","Epoch 00011: val_loss did not improve from 0.13854\n","Epoch 12/1000\n"," - 1s - loss: 0.6926 - accuracy: 0.5183 - val_loss: 0.1386 - val_accuracy: 2.5192\n","\n","Epoch 00012: val_loss did not improve from 0.13854\n","Epoch 13/1000\n"," - 1s - loss: 0.6925 - accuracy: 0.5185 - val_loss: 0.1386 - val_accuracy: 2.5115\n","\n","Epoch 00013: val_loss did not improve from 0.13854\n","Epoch 14/1000\n"," - 1s - loss: 0.6925 - accuracy: 0.5182 - val_loss: 0.1386 - val_accuracy: 2.5154\n","\n","Epoch 00014: val_loss did not improve from 0.13854\n","Epoch 15/1000\n"," - 1s - loss: 0.6925 - accuracy: 0.5187 - val_loss: 0.1386 - val_accuracy: 2.5269\n","\n","Epoch 00015: val_loss did not improve from 0.13854\n","Epoch 16/1000\n"," - 1s - loss: 0.6924 - accuracy: 0.5188 - val_loss: 0.1386 - val_accuracy: 2.5327\n","\n","Epoch 00016: val_loss did not improve from 0.13854\n","Epoch 17/1000\n"," - 1s - loss: 0.6924 - accuracy: 0.5183 - val_loss: 0.1386 - val_accuracy: 2.5327\n","\n","Epoch 00017: val_loss did not improve from 0.13854\n","Epoch 18/1000\n"," - 1s - loss: 0.6923 - accuracy: 0.5192 - val_loss: 0.1386 - val_accuracy: 2.5269\n","\n","Epoch 00018: val_loss did not improve from 0.13854\n","Epoch 19/1000\n"," - 1s - loss: 0.6923 - accuracy: 0.5190 - val_loss: 0.1386 - val_accuracy: 2.5269\n","\n","Epoch 00019: val_loss did not improve from 0.13854\n","Epoch 20/1000\n"," - 1s - loss: 0.6923 - accuracy: 0.5190 - val_loss: 0.1386 - val_accuracy: 2.5307\n","\n","Epoch 00020: val_loss did not improve from 0.13854\n","Epoch 21/1000\n"," - 1s - loss: 0.6922 - accuracy: 0.5190 - val_loss: 0.1386 - val_accuracy: 2.5423\n","\n","Epoch 00021: val_loss did not improve from 0.13854\n","Epoch 22/1000\n"," - 1s - loss: 0.6922 - accuracy: 0.5195 - val_loss: 0.1386 - val_accuracy: 2.5327\n","\n","Epoch 00022: val_loss did not improve from 0.13854\n","Epoch 23/1000\n"," - 1s - loss: 0.6921 - accuracy: 0.5191 - val_loss: 0.1386 - val_accuracy: 2.5288\n","\n","Epoch 00023: val_loss did not improve from 0.13854\n","Epoch 24/1000\n"," - 1s - loss: 0.6921 - accuracy: 0.5189 - val_loss: 0.1386 - val_accuracy: 2.5404\n","\n","Epoch 00024: val_loss did not improve from 0.13854\n","Epoch 25/1000\n"," - 1s - loss: 0.6921 - accuracy: 0.5192 - val_loss: 0.1386 - val_accuracy: 2.5423\n","\n","Epoch 00025: val_loss did not improve from 0.13854\n","Epoch 26/1000\n"," - 1s - loss: 0.6920 - accuracy: 0.5191 - val_loss: 0.1386 - val_accuracy: 2.5423\n","\n","Epoch 00026: val_loss did not improve from 0.13854\n","Epoch 27/1000\n"," - 1s - loss: 0.6920 - accuracy: 0.5190 - val_loss: 0.1386 - val_accuracy: 2.5442\n","\n","Epoch 00027: val_loss did not improve from 0.13854\n","Epoch 28/1000\n"," - 1s - loss: 0.6919 - accuracy: 0.5199 - val_loss: 0.1386 - val_accuracy: 2.5404\n","\n","Epoch 00028: val_loss did not improve from 0.13854\n","Epoch 29/1000\n"," - 1s - loss: 0.6919 - accuracy: 0.5203 - val_loss: 0.1386 - val_accuracy: 2.5500\n","\n","Epoch 00029: val_loss did not improve from 0.13854\n","Epoch 30/1000\n"," - 1s - loss: 0.6919 - accuracy: 0.5204 - val_loss: 0.1386 - val_accuracy: 2.5576\n","\n","Epoch 00030: val_loss did not improve from 0.13854\n","Epoch 31/1000\n"," - 1s - loss: 0.6918 - accuracy: 0.5210 - val_loss: 0.1386 - val_accuracy: 2.5442\n","\n","Epoch 00031: val_loss did not improve from 0.13854\n","Epoch 32/1000\n"," - 1s - loss: 0.6918 - accuracy: 0.5209 - val_loss: 0.1386 - val_accuracy: 2.5596\n","\n","Epoch 00032: val_loss did not improve from 0.13854\n","Epoch 33/1000\n"," - 1s - loss: 0.6918 - accuracy: 0.5204 - val_loss: 0.1386 - val_accuracy: 2.5519\n","\n","Epoch 00033: val_loss did not improve from 0.13854\n","Epoch 34/1000\n"," - 1s - loss: 0.6917 - accuracy: 0.5208 - val_loss: 0.1386 - val_accuracy: 2.5519\n","\n","Epoch 00034: val_loss did not improve from 0.13854\n","Epoch 35/1000\n"," - 1s - loss: 0.6917 - accuracy: 0.5211 - val_loss: 0.1386 - val_accuracy: 2.5423\n","\n","Epoch 00035: val_loss did not improve from 0.13854\n","Epoch 36/1000\n"," - 1s - loss: 0.6917 - accuracy: 0.5218 - val_loss: 0.1386 - val_accuracy: 2.5442\n","\n","Epoch 00036: val_loss did not improve from 0.13854\n","Epoch 37/1000\n"," - 1s - loss: 0.6916 - accuracy: 0.5224 - val_loss: 0.1386 - val_accuracy: 2.5519\n","\n","Epoch 00037: val_loss did not improve from 0.13854\n","Epoch 38/1000\n"," - 1s - loss: 0.6916 - accuracy: 0.5216 - val_loss: 0.1386 - val_accuracy: 2.5480\n","\n","Epoch 00038: val_loss did not improve from 0.13854\n","Epoch 39/1000\n"," - 1s - loss: 0.6916 - accuracy: 0.5209 - val_loss: 0.1386 - val_accuracy: 2.5384\n","\n","Epoch 00039: val_loss did not improve from 0.13854\n","Epoch 40/1000\n"," - 1s - loss: 0.6916 - accuracy: 0.5211 - val_loss: 0.1386 - val_accuracy: 2.5423\n","\n","Epoch 00040: val_loss did not improve from 0.13854\n","Epoch 41/1000\n"," - 1s - loss: 0.6916 - accuracy: 0.5215 - val_loss: 0.1386 - val_accuracy: 2.5423\n","\n","Epoch 00041: val_loss did not improve from 0.13854\n","Epoch 42/1000\n"," - 1s - loss: 0.6915 - accuracy: 0.5210 - val_loss: 0.1386 - val_accuracy: 2.5480\n","\n","Epoch 00042: val_loss did not improve from 0.13854\n","Epoch 43/1000\n"," - 1s - loss: 0.6915 - accuracy: 0.5213 - val_loss: 0.1386 - val_accuracy: 2.5557\n","\n","Epoch 00043: val_loss did not improve from 0.13854\n","Epoch 44/1000\n"," - 1s - loss: 0.6915 - accuracy: 0.5213 - val_loss: 0.1386 - val_accuracy: 2.5519\n","\n","Epoch 00044: val_loss did not improve from 0.13854\n","Epoch 45/1000\n"," - 1s - loss: 0.6915 - accuracy: 0.5218 - val_loss: 0.1386 - val_accuracy: 2.5500\n","\n","Epoch 00045: val_loss did not improve from 0.13854\n","Epoch 46/1000\n"," - 1s - loss: 0.6915 - accuracy: 0.5217 - val_loss: 0.1386 - val_accuracy: 2.5500\n","\n","Epoch 00046: val_loss did not improve from 0.13854\n","Epoch 47/1000\n"," - 1s - loss: 0.6915 - accuracy: 0.5220 - val_loss: 0.1386 - val_accuracy: 2.5519\n","\n","Epoch 00047: val_loss did not improve from 0.13854\n","Epoch 48/1000\n"," - 1s - loss: 0.6915 - accuracy: 0.5223 - val_loss: 0.1386 - val_accuracy: 2.5519\n","\n","Epoch 00048: val_loss did not improve from 0.13854\n","Epoch 49/1000\n"," - 1s - loss: 0.6914 - accuracy: 0.5229 - val_loss: 0.1386 - val_accuracy: 2.5500\n","\n","Epoch 00049: val_loss did not improve from 0.13854\n","Epoch 50/1000\n"," - 1s - loss: 0.6914 - accuracy: 0.5237 - val_loss: 0.1386 - val_accuracy: 2.5519\n","\n","Epoch 00050: val_loss did not improve from 0.13854\n","Epoch 51/1000\n"," - 1s - loss: 0.6914 - accuracy: 0.5236 - val_loss: 0.1386 - val_accuracy: 2.5538\n","\n","Epoch 00051: val_loss did not improve from 0.13854\n","Epoch 52/1000\n"," - 1s - loss: 0.6914 - accuracy: 0.5238 - val_loss: 0.1386 - val_accuracy: 2.5557\n","\n","Epoch 00052: val_loss did not improve from 0.13854\n","Epoch 53/1000\n"," - 1s - loss: 0.6914 - accuracy: 0.5239 - val_loss: 0.1386 - val_accuracy: 2.5615\n","\n","Epoch 00053: val_loss did not improve from 0.13854\n","Epoch 54/1000\n"," - 1s - loss: 0.6914 - accuracy: 0.5242 - val_loss: 0.1386 - val_accuracy: 2.5653\n","\n","Epoch 00054: val_loss did not improve from 0.13854\n","Epoch 55/1000\n"," - 1s - loss: 0.6914 - accuracy: 0.5243 - val_loss: 0.1386 - val_accuracy: 2.5596\n","\n","Epoch 00055: val_loss did not improve from 0.13854\n","Epoch 56/1000\n"," - 1s - loss: 0.6913 - accuracy: 0.5247 - val_loss: 0.1386 - val_accuracy: 2.5615\n","\n","Epoch 00056: val_loss did not improve from 0.13854\n","Epoch 57/1000\n"," - 1s - loss: 0.6913 - accuracy: 0.5248 - val_loss: 0.1386 - val_accuracy: 2.5596\n","\n","Epoch 00057: val_loss did not improve from 0.13854\n","Epoch 58/1000\n"," - 1s - loss: 0.6913 - accuracy: 0.5250 - val_loss: 0.1386 - val_accuracy: 2.5576\n","\n","Epoch 00058: val_loss did not improve from 0.13854\n","Epoch 59/1000\n"," - 1s - loss: 0.6913 - accuracy: 0.5250 - val_loss: 0.1386 - val_accuracy: 2.5576\n","\n","Epoch 00059: val_loss did not improve from 0.13854\n","Epoch 60/1000\n"," - 1s - loss: 0.6913 - accuracy: 0.5253 - val_loss: 0.1386 - val_accuracy: 2.5576\n","\n","Epoch 00060: val_loss did not improve from 0.13854\n","Epoch 61/1000\n"," - 1s - loss: 0.6913 - accuracy: 0.5255 - val_loss: 0.1386 - val_accuracy: 2.5653\n","\n","Epoch 00061: val_loss did not improve from 0.13854\n","Epoch 62/1000\n"," - 1s - loss: 0.6913 - accuracy: 0.5258 - val_loss: 0.1386 - val_accuracy: 2.5634\n","\n","Epoch 00062: val_loss did not improve from 0.13854\n","Epoch 63/1000\n"," - 1s - loss: 0.6912 - accuracy: 0.5257 - val_loss: 0.1386 - val_accuracy: 2.5673\n","\n","Epoch 00063: val_loss did not improve from 0.13854\n","Epoch 64/1000\n"," - 1s - loss: 0.6912 - accuracy: 0.5251 - val_loss: 0.1386 - val_accuracy: 2.5634\n","\n","Epoch 00064: val_loss did not improve from 0.13854\n","Epoch 65/1000\n"," - 1s - loss: 0.6912 - accuracy: 0.5253 - val_loss: 0.1386 - val_accuracy: 2.5634\n","\n","Epoch 00065: val_loss did not improve from 0.13854\n","Epoch 66/1000\n"," - 1s - loss: 0.6912 - accuracy: 0.5253 - val_loss: 0.1386 - val_accuracy: 2.5673\n","\n","Epoch 00066: val_loss did not improve from 0.13854\n","Epoch 67/1000\n"," - 1s - loss: 0.6912 - accuracy: 0.5254 - val_loss: 0.1386 - val_accuracy: 2.5711\n","\n","Epoch 00067: val_loss did not improve from 0.13854\n","Epoch 68/1000\n"," - 1s - loss: 0.6912 - accuracy: 0.5254 - val_loss: 0.1386 - val_accuracy: 2.5711\n","\n","Epoch 00068: val_loss did not improve from 0.13854\n","Epoch 69/1000\n"," - 1s - loss: 0.6911 - accuracy: 0.5252 - val_loss: 0.1386 - val_accuracy: 2.5692\n","\n","Epoch 00069: val_loss did not improve from 0.13854\n","Epoch 70/1000\n"," - 1s - loss: 0.6911 - accuracy: 0.5253 - val_loss: 0.1385 - val_accuracy: 2.5788\n","\n","Epoch 00070: val_loss did not improve from 0.13854\n","Epoch 71/1000\n"," - 1s - loss: 0.6911 - accuracy: 0.5254 - val_loss: 0.1385 - val_accuracy: 2.5692\n","\n","Epoch 00071: val_loss improved from 0.13854 to 0.13854, saving model to /content/drive/My Drive/Colab Notebooks/300/ckpt/classifier_45_lstm_small_updown_SXPUSDT_tvnon_shuffle.h5\n","Epoch 72/1000\n"," - 1s - loss: 0.6911 - accuracy: 0.5257 - val_loss: 0.1385 - val_accuracy: 2.5692\n","\n","Epoch 00072: val_loss improved from 0.13854 to 0.13854, saving model to /content/drive/My Drive/Colab Notebooks/300/ckpt/classifier_45_lstm_small_updown_SXPUSDT_tvnon_shuffle.h5\n","Epoch 73/1000\n"," - 1s - loss: 0.6911 - accuracy: 0.5257 - val_loss: 0.1385 - val_accuracy: 2.5692\n","\n","Epoch 00073: val_loss improved from 0.13854 to 0.13854, saving model to /content/drive/My Drive/Colab Notebooks/300/ckpt/classifier_45_lstm_small_updown_SXPUSDT_tvnon_shuffle.h5\n","Epoch 74/1000\n"," - 1s - loss: 0.6911 - accuracy: 0.5258 - val_loss: 0.1385 - val_accuracy: 2.5730\n","\n","Epoch 00074: val_loss improved from 0.13854 to 0.13853, saving model to /content/drive/My Drive/Colab Notebooks/300/ckpt/classifier_45_lstm_small_updown_SXPUSDT_tvnon_shuffle.h5\n","Epoch 75/1000\n"," - 1s - loss: 0.6910 - accuracy: 0.5260 - val_loss: 0.1385 - val_accuracy: 2.5749\n","\n","Epoch 00075: val_loss improved from 0.13853 to 0.13853, saving model to /content/drive/My Drive/Colab Notebooks/300/ckpt/classifier_45_lstm_small_updown_SXPUSDT_tvnon_shuffle.h5\n","Epoch 76/1000\n"," - 1s - loss: 0.6910 - accuracy: 0.5259 - val_loss: 0.1385 - val_accuracy: 2.5730\n","\n","Epoch 00076: val_loss improved from 0.13853 to 0.13852, saving model to /content/drive/My Drive/Colab Notebooks/300/ckpt/classifier_45_lstm_small_updown_SXPUSDT_tvnon_shuffle.h5\n","Epoch 77/1000\n"," - 1s - loss: 0.6910 - accuracy: 0.5272 - val_loss: 0.1385 - val_accuracy: 2.5769\n","\n","Epoch 00077: val_loss improved from 0.13852 to 0.13852, saving model to /content/drive/My Drive/Colab Notebooks/300/ckpt/classifier_45_lstm_small_updown_SXPUSDT_tvnon_shuffle.h5\n","Epoch 78/1000\n"," - 1s - loss: 0.6910 - accuracy: 0.5276 - val_loss: 0.1385 - val_accuracy: 2.5730\n","\n","Epoch 00078: val_loss improved from 0.13852 to 0.13851, saving model to /content/drive/My Drive/Colab Notebooks/300/ckpt/classifier_45_lstm_small_updown_SXPUSDT_tvnon_shuffle.h5\n","Epoch 79/1000\n"," - 1s - loss: 0.6909 - accuracy: 0.5278 - val_loss: 0.1385 - val_accuracy: 2.5673\n","\n","Epoch 00079: val_loss improved from 0.13851 to 0.13851, saving model to /content/drive/My Drive/Colab Notebooks/300/ckpt/classifier_45_lstm_small_updown_SXPUSDT_tvnon_shuffle.h5\n","Epoch 80/1000\n"," - 1s - loss: 0.6909 - accuracy: 0.5283 - val_loss: 0.1385 - val_accuracy: 2.5788\n","\n","Epoch 00080: val_loss improved from 0.13851 to 0.13850, saving model to /content/drive/My Drive/Colab Notebooks/300/ckpt/classifier_45_lstm_small_updown_SXPUSDT_tvnon_shuffle.h5\n","Epoch 81/1000\n"," - 1s - loss: 0.6909 - accuracy: 0.5282 - val_loss: 0.1385 - val_accuracy: 2.5788\n","\n","Epoch 00081: val_loss improved from 0.13850 to 0.13850, saving model to /content/drive/My Drive/Colab Notebooks/300/ckpt/classifier_45_lstm_small_updown_SXPUSDT_tvnon_shuffle.h5\n","Epoch 82/1000\n"," - 1s - loss: 0.6908 - accuracy: 0.5283 - val_loss: 0.1385 - val_accuracy: 2.5826\n","\n","Epoch 00082: val_loss improved from 0.13850 to 0.13849, saving model to /content/drive/My Drive/Colab Notebooks/300/ckpt/classifier_45_lstm_small_updown_SXPUSDT_tvnon_shuffle.h5\n","Epoch 83/1000\n"," - 1s - loss: 0.6908 - accuracy: 0.5291 - val_loss: 0.1385 - val_accuracy: 2.5903\n","\n","Epoch 00083: val_loss improved from 0.13849 to 0.13848, saving model to /content/drive/My Drive/Colab Notebooks/300/ckpt/classifier_45_lstm_small_updown_SXPUSDT_tvnon_shuffle.h5\n","Epoch 84/1000\n"," - 1s - loss: 0.6908 - accuracy: 0.5303 - val_loss: 0.1385 - val_accuracy: 2.5980\n","\n","Epoch 00084: val_loss improved from 0.13848 to 0.13847, saving model to /content/drive/My Drive/Colab Notebooks/300/ckpt/classifier_45_lstm_small_updown_SXPUSDT_tvnon_shuffle.h5\n","Epoch 85/1000\n"," - 1s - loss: 0.6907 - accuracy: 0.5314 - val_loss: 0.1385 - val_accuracy: 2.6076\n","\n","Epoch 00085: val_loss improved from 0.13847 to 0.13846, saving model to /content/drive/My Drive/Colab Notebooks/300/ckpt/classifier_45_lstm_small_updown_SXPUSDT_tvnon_shuffle.h5\n","Epoch 86/1000\n"," - 1s - loss: 0.6907 - accuracy: 0.5308 - val_loss: 0.1385 - val_accuracy: 2.6038\n","\n","Epoch 00086: val_loss improved from 0.13846 to 0.13845, saving model to /content/drive/My Drive/Colab Notebooks/300/ckpt/classifier_45_lstm_small_updown_SXPUSDT_tvnon_shuffle.h5\n","Epoch 87/1000\n"," - 1s - loss: 0.6906 - accuracy: 0.5307 - val_loss: 0.1384 - val_accuracy: 2.6057\n","\n","Epoch 00087: val_loss improved from 0.13845 to 0.13844, saving model to /content/drive/My Drive/Colab Notebooks/300/ckpt/classifier_45_lstm_small_updown_SXPUSDT_tvnon_shuffle.h5\n","Epoch 88/1000\n"," - 1s - loss: 0.6906 - accuracy: 0.5318 - val_loss: 0.1384 - val_accuracy: 2.6057\n","\n","Epoch 00088: val_loss improved from 0.13844 to 0.13842, saving model to /content/drive/My Drive/Colab Notebooks/300/ckpt/classifier_45_lstm_small_updown_SXPUSDT_tvnon_shuffle.h5\n","Epoch 89/1000\n"," - 1s - loss: 0.6905 - accuracy: 0.5324 - val_loss: 0.1384 - val_accuracy: 2.6057\n","\n","Epoch 00089: val_loss improved from 0.13842 to 0.13841, saving model to /content/drive/My Drive/Colab Notebooks/300/ckpt/classifier_45_lstm_small_updown_SXPUSDT_tvnon_shuffle.h5\n","Epoch 90/1000\n"," - 1s - loss: 0.6905 - accuracy: 0.5322 - val_loss: 0.1384 - val_accuracy: 2.6230\n","\n","Epoch 00090: val_loss improved from 0.13841 to 0.13839, saving model to /content/drive/My Drive/Colab Notebooks/300/ckpt/classifier_45_lstm_small_updown_SXPUSDT_tvnon_shuffle.h5\n","Epoch 91/1000\n"," - 1s - loss: 0.6904 - accuracy: 0.5332 - val_loss: 0.1384 - val_accuracy: 2.6191\n","\n","Epoch 00091: val_loss improved from 0.13839 to 0.13838, saving model to /content/drive/My Drive/Colab Notebooks/300/ckpt/classifier_45_lstm_small_updown_SXPUSDT_tvnon_shuffle.h5\n","Epoch 92/1000\n"," - 1s - loss: 0.6903 - accuracy: 0.5339 - val_loss: 0.1384 - val_accuracy: 2.6268\n","\n","Epoch 00092: val_loss improved from 0.13838 to 0.13836, saving model to /content/drive/My Drive/Colab Notebooks/300/ckpt/classifier_45_lstm_small_updown_SXPUSDT_tvnon_shuffle.h5\n","Epoch 93/1000\n"," - 1s - loss: 0.6903 - accuracy: 0.5346 - val_loss: 0.1384 - val_accuracy: 2.6153\n","\n","Epoch 00093: val_loss improved from 0.13836 to 0.13835, saving model to /content/drive/My Drive/Colab Notebooks/300/ckpt/classifier_45_lstm_small_updown_SXPUSDT_tvnon_shuffle.h5\n","Epoch 94/1000\n"," - 1s - loss: 0.6903 - accuracy: 0.5345 - val_loss: 0.1383 - val_accuracy: 2.6095\n","\n","Epoch 00094: val_loss improved from 0.13835 to 0.13834, saving model to /content/drive/My Drive/Colab Notebooks/300/ckpt/classifier_45_lstm_small_updown_SXPUSDT_tvnon_shuffle.h5\n","Epoch 95/1000\n"," - 1s - loss: 0.6902 - accuracy: 0.5342 - val_loss: 0.1383 - val_accuracy: 2.6076\n","\n","Epoch 00095: val_loss improved from 0.13834 to 0.13833, saving model to /content/drive/My Drive/Colab Notebooks/300/ckpt/classifier_45_lstm_small_updown_SXPUSDT_tvnon_shuffle.h5\n","Epoch 96/1000\n"," - 1s - loss: 0.6902 - accuracy: 0.5342 - val_loss: 0.1383 - val_accuracy: 2.6018\n","\n","Epoch 00096: val_loss improved from 0.13833 to 0.13832, saving model to /content/drive/My Drive/Colab Notebooks/300/ckpt/classifier_45_lstm_small_updown_SXPUSDT_tvnon_shuffle.h5\n","Epoch 97/1000\n"," - 1s - loss: 0.6901 - accuracy: 0.5341 - val_loss: 0.1383 - val_accuracy: 2.6038\n","\n","Epoch 00097: val_loss improved from 0.13832 to 0.13831, saving model to /content/drive/My Drive/Colab Notebooks/300/ckpt/classifier_45_lstm_small_updown_SXPUSDT_tvnon_shuffle.h5\n","Epoch 98/1000\n"," - 1s - loss: 0.6901 - accuracy: 0.5331 - val_loss: 0.1383 - val_accuracy: 2.6095\n","\n","Epoch 00098: val_loss improved from 0.13831 to 0.13830, saving model to /content/drive/My Drive/Colab Notebooks/300/ckpt/classifier_45_lstm_small_updown_SXPUSDT_tvnon_shuffle.h5\n","Epoch 99/1000\n"," - 1s - loss: 0.6900 - accuracy: 0.5328 - val_loss: 0.1383 - val_accuracy: 2.6115\n","\n","Epoch 00099: val_loss improved from 0.13830 to 0.13829, saving model to /content/drive/My Drive/Colab Notebooks/300/ckpt/classifier_45_lstm_small_updown_SXPUSDT_tvnon_shuffle.h5\n","Epoch 100/1000\n"," - 1s - loss: 0.6900 - accuracy: 0.5320 - val_loss: 0.1383 - val_accuracy: 2.6191\n","\n","Epoch 00100: val_loss improved from 0.13829 to 0.13828, saving model to /content/drive/My Drive/Colab Notebooks/300/ckpt/classifier_45_lstm_small_updown_SXPUSDT_tvnon_shuffle.h5\n","Epoch 101/1000\n"," - 1s - loss: 0.6899 - accuracy: 0.5320 - val_loss: 0.1383 - val_accuracy: 2.6249\n","\n","Epoch 00101: val_loss improved from 0.13828 to 0.13827, saving model to /content/drive/My Drive/Colab Notebooks/300/ckpt/classifier_45_lstm_small_updown_SXPUSDT_tvnon_shuffle.h5\n","Epoch 102/1000\n"," - 1s - loss: 0.6899 - accuracy: 0.5324 - val_loss: 0.1383 - val_accuracy: 2.6249\n","\n","Epoch 00102: val_loss improved from 0.13827 to 0.13827, saving model to /content/drive/My Drive/Colab Notebooks/300/ckpt/classifier_45_lstm_small_updown_SXPUSDT_tvnon_shuffle.h5\n","Epoch 103/1000\n"," - 1s - loss: 0.6898 - accuracy: 0.5323 - val_loss: 0.1382 - val_accuracy: 2.6268\n","\n","Epoch 00103: val_loss improved from 0.13827 to 0.13825, saving model to /content/drive/My Drive/Colab Notebooks/300/ckpt/classifier_45_lstm_small_updown_SXPUSDT_tvnon_shuffle.h5\n","Epoch 104/1000\n"," - 1s - loss: 0.6898 - accuracy: 0.5327 - val_loss: 0.1383 - val_accuracy: 2.6230\n","\n","Epoch 00104: val_loss did not improve from 0.13825\n","Epoch 105/1000\n"," - 1s - loss: 0.6898 - accuracy: 0.5332 - val_loss: 0.1382 - val_accuracy: 2.6307\n","\n","Epoch 00105: val_loss improved from 0.13825 to 0.13825, saving model to /content/drive/My Drive/Colab Notebooks/300/ckpt/classifier_45_lstm_small_updown_SXPUSDT_tvnon_shuffle.h5\n","Epoch 106/1000\n"," - 1s - loss: 0.6897 - accuracy: 0.5327 - val_loss: 0.1382 - val_accuracy: 2.6268\n","\n","Epoch 00106: val_loss improved from 0.13825 to 0.13825, saving model to /content/drive/My Drive/Colab Notebooks/300/ckpt/classifier_45_lstm_small_updown_SXPUSDT_tvnon_shuffle.h5\n","Epoch 107/1000\n"," - 1s - loss: 0.6897 - accuracy: 0.5339 - val_loss: 0.1382 - val_accuracy: 2.6268\n","\n","Epoch 00107: val_loss improved from 0.13825 to 0.13824, saving model to /content/drive/My Drive/Colab Notebooks/300/ckpt/classifier_45_lstm_small_updown_SXPUSDT_tvnon_shuffle.h5\n","Epoch 108/1000\n"," - 1s - loss: 0.6896 - accuracy: 0.5355 - val_loss: 0.1382 - val_accuracy: 2.6326\n","\n","Epoch 00108: val_loss improved from 0.13824 to 0.13824, saving model to /content/drive/My Drive/Colab Notebooks/300/ckpt/classifier_45_lstm_small_updown_SXPUSDT_tvnon_shuffle.h5\n","Epoch 109/1000\n"," - 1s - loss: 0.6896 - accuracy: 0.5365 - val_loss: 0.1382 - val_accuracy: 2.6345\n","\n","Epoch 00109: val_loss improved from 0.13824 to 0.13823, saving model to /content/drive/My Drive/Colab Notebooks/300/ckpt/classifier_45_lstm_small_updown_SXPUSDT_tvnon_shuffle.h5\n","Epoch 110/1000\n"," - 1s - loss: 0.6896 - accuracy: 0.5365 - val_loss: 0.1382 - val_accuracy: 2.6326\n","\n","Epoch 00110: val_loss improved from 0.13823 to 0.13822, saving model to /content/drive/My Drive/Colab Notebooks/300/ckpt/classifier_45_lstm_small_updown_SXPUSDT_tvnon_shuffle.h5\n","Epoch 111/1000\n"," - 1s - loss: 0.6895 - accuracy: 0.5366 - val_loss: 0.1382 - val_accuracy: 2.6134\n","\n","Epoch 00111: val_loss did not improve from 0.13822\n","Epoch 112/1000\n"," - 1s - loss: 0.6895 - accuracy: 0.5369 - val_loss: 0.1382 - val_accuracy: 2.6268\n","\n","Epoch 00112: val_loss did not improve from 0.13822\n","Epoch 113/1000\n"," - 1s - loss: 0.6894 - accuracy: 0.5368 - val_loss: 0.1382 - val_accuracy: 2.6230\n","\n","Epoch 00113: val_loss did not improve from 0.13822\n","Epoch 114/1000\n"," - 1s - loss: 0.6894 - accuracy: 0.5366 - val_loss: 0.1382 - val_accuracy: 2.6134\n","\n","Epoch 00114: val_loss did not improve from 0.13822\n","Epoch 115/1000\n"," - 1s - loss: 0.6894 - accuracy: 0.5370 - val_loss: 0.1382 - val_accuracy: 2.6153\n","\n","Epoch 00115: val_loss did not improve from 0.13822\n","Epoch 116/1000\n"," - 1s - loss: 0.6893 - accuracy: 0.5373 - val_loss: 0.1382 - val_accuracy: 2.6249\n","\n","Epoch 00116: val_loss did not improve from 0.13822\n","Epoch 117/1000\n"," - 1s - loss: 0.6893 - accuracy: 0.5372 - val_loss: 0.1382 - val_accuracy: 2.6172\n","\n","Epoch 00117: val_loss did not improve from 0.13822\n","Epoch 118/1000\n"," - 1s - loss: 0.6893 - accuracy: 0.5372 - val_loss: 0.1382 - val_accuracy: 2.6172\n","\n","Epoch 00118: val_loss did not improve from 0.13822\n","Epoch 119/1000\n"," - 1s - loss: 0.6892 - accuracy: 0.5366 - val_loss: 0.1382 - val_accuracy: 2.6191\n","\n","Epoch 00119: val_loss did not improve from 0.13822\n","Epoch 120/1000\n"," - 1s - loss: 0.6892 - accuracy: 0.5361 - val_loss: 0.1382 - val_accuracy: 2.6134\n","\n","Epoch 00120: val_loss did not improve from 0.13822\n","Epoch 121/1000\n"," - 1s - loss: 0.6891 - accuracy: 0.5361 - val_loss: 0.1382 - val_accuracy: 2.6287\n","\n","Epoch 00121: val_loss did not improve from 0.13822\n","Epoch 122/1000\n"," - 1s - loss: 0.6891 - accuracy: 0.5361 - val_loss: 0.1382 - val_accuracy: 2.6230\n","\n","Epoch 00122: val_loss did not improve from 0.13822\n","Epoch 123/1000\n"," - 1s - loss: 0.6890 - accuracy: 0.5361 - val_loss: 0.1383 - val_accuracy: 2.6364\n","\n","Epoch 00123: val_loss did not improve from 0.13822\n","Epoch 124/1000\n"," - 1s - loss: 0.6890 - accuracy: 0.5354 - val_loss: 0.1383 - val_accuracy: 2.6422\n","\n","Epoch 00124: val_loss did not improve from 0.13822\n","Epoch 125/1000\n"," - 1s - loss: 0.6889 - accuracy: 0.5349 - val_loss: 0.1383 - val_accuracy: 2.6345\n","\n","Epoch 00125: val_loss did not improve from 0.13822\n","Epoch 126/1000\n"," - 1s - loss: 0.6889 - accuracy: 0.5353 - val_loss: 0.1383 - val_accuracy: 2.6268\n","\n","Epoch 00126: val_loss did not improve from 0.13822\n","Epoch 127/1000\n"," - 1s - loss: 0.6888 - accuracy: 0.5366 - val_loss: 0.1383 - val_accuracy: 2.6230\n","\n","Epoch 00127: val_loss did not improve from 0.13822\n","Epoch 128/1000\n"," - 1s - loss: 0.6888 - accuracy: 0.5376 - val_loss: 0.1383 - val_accuracy: 2.6191\n","\n","Epoch 00128: val_loss did not improve from 0.13822\n","Epoch 129/1000\n"," - 1s - loss: 0.6887 - accuracy: 0.5383 - val_loss: 0.1383 - val_accuracy: 2.6057\n","\n","Epoch 00129: val_loss did not improve from 0.13822\n","Epoch 130/1000\n"," - 1s - loss: 0.6887 - accuracy: 0.5388 - val_loss: 0.1383 - val_accuracy: 2.6018\n","\n","Epoch 00130: val_loss did not improve from 0.13822\n","Epoch 131/1000\n"," - 1s - loss: 0.6886 - accuracy: 0.5405 - val_loss: 0.1383 - val_accuracy: 2.6057\n","\n","Epoch 00131: val_loss did not improve from 0.13822\n","Epoch 132/1000\n"," - 1s - loss: 0.6886 - accuracy: 0.5419 - val_loss: 0.1383 - val_accuracy: 2.6095\n","\n","Epoch 00132: val_loss did not improve from 0.13822\n","Epoch 133/1000\n"," - 1s - loss: 0.6885 - accuracy: 0.5424 - val_loss: 0.1383 - val_accuracy: 2.6057\n","\n","Epoch 00133: val_loss did not improve from 0.13822\n","Epoch 134/1000\n"," - 1s - loss: 0.6885 - accuracy: 0.5425 - val_loss: 0.1383 - val_accuracy: 2.6115\n","\n","Epoch 00134: val_loss did not improve from 0.13822\n","Epoch 135/1000\n"," - 1s - loss: 0.6884 - accuracy: 0.5424 - val_loss: 0.1383 - val_accuracy: 2.6153\n","\n","Epoch 00135: val_loss did not improve from 0.13822\n","Epoch 136/1000\n"," - 1s - loss: 0.6884 - accuracy: 0.5435 - val_loss: 0.1384 - val_accuracy: 2.5999\n","\n","Epoch 00136: val_loss did not improve from 0.13822\n","Epoch 137/1000\n"," - 1s - loss: 0.6884 - accuracy: 0.5444 - val_loss: 0.1383 - val_accuracy: 2.6134\n","\n","Epoch 00137: val_loss did not improve from 0.13822\n","Epoch 138/1000\n"," - 1s - loss: 0.6883 - accuracy: 0.5448 - val_loss: 0.1383 - val_accuracy: 2.5980\n","\n","Epoch 00138: val_loss did not improve from 0.13822\n","Epoch 139/1000\n"," - 1s - loss: 0.6883 - accuracy: 0.5448 - val_loss: 0.1384 - val_accuracy: 2.5903\n","\n","Epoch 00139: val_loss did not improve from 0.13822\n","Epoch 140/1000\n"," - 1s - loss: 0.6882 - accuracy: 0.5447 - val_loss: 0.1383 - val_accuracy: 2.5884\n","\n","Epoch 00140: val_loss did not improve from 0.13822\n","Epoch 141/1000\n"," - 1s - loss: 0.6882 - accuracy: 0.5451 - val_loss: 0.1383 - val_accuracy: 2.5730\n","\n","Epoch 00141: val_loss did not improve from 0.13822\n","Epoch 142/1000\n"," - 1s - loss: 0.6882 - accuracy: 0.5445 - val_loss: 0.1384 - val_accuracy: 2.5826\n","\n","Epoch 00142: val_loss did not improve from 0.13822\n","Epoch 143/1000\n"," - 1s - loss: 0.6881 - accuracy: 0.5442 - val_loss: 0.1384 - val_accuracy: 2.5788\n","\n","Epoch 00143: val_loss did not improve from 0.13822\n","Epoch 144/1000\n"," - 1s - loss: 0.6881 - accuracy: 0.5443 - val_loss: 0.1383 - val_accuracy: 2.5903\n","\n","Epoch 00144: val_loss did not improve from 0.13822\n","Epoch 145/1000\n"," - 1s - loss: 0.6880 - accuracy: 0.5448 - val_loss: 0.1383 - val_accuracy: 2.5922\n","\n","Epoch 00145: val_loss did not improve from 0.13822\n","Epoch 146/1000\n"," - 1s - loss: 0.6879 - accuracy: 0.5457 - val_loss: 0.1383 - val_accuracy: 2.5980\n","\n","Epoch 00146: val_loss did not improve from 0.13822\n","Epoch 147/1000\n"," - 1s - loss: 0.6879 - accuracy: 0.5459 - val_loss: 0.1383 - val_accuracy: 2.5903\n","\n","Epoch 00147: val_loss did not improve from 0.13822\n","Epoch 148/1000\n"," - 1s - loss: 0.6878 - accuracy: 0.5461 - val_loss: 0.1384 - val_accuracy: 2.5942\n","\n","Epoch 00148: val_loss did not improve from 0.13822\n","Epoch 149/1000\n"," - 1s - loss: 0.6877 - accuracy: 0.5455 - val_loss: 0.1384 - val_accuracy: 2.5826\n","\n","Epoch 00149: val_loss did not improve from 0.13822\n","Epoch 150/1000\n"," - 1s - loss: 0.6877 - accuracy: 0.5453 - val_loss: 0.1384 - val_accuracy: 2.5865\n","\n","Epoch 00150: val_loss did not improve from 0.13822\n","Epoch 151/1000\n"," - 1s - loss: 0.6876 - accuracy: 0.5441 - val_loss: 0.1385 - val_accuracy: 2.5865\n","\n","Epoch 00151: val_loss did not improve from 0.13822\n","Epoch 152/1000\n"," - 1s - loss: 0.6875 - accuracy: 0.5443 - val_loss: 0.1385 - val_accuracy: 2.5692\n","\n","Epoch 00152: val_loss did not improve from 0.13822\n","Epoch 153/1000\n"," - 1s - loss: 0.6874 - accuracy: 0.5444 - val_loss: 0.1385 - val_accuracy: 2.5769\n","\n","Epoch 00153: val_loss did not improve from 0.13822\n","Epoch 154/1000\n"," - 1s - loss: 0.6873 - accuracy: 0.5442 - val_loss: 0.1385 - val_accuracy: 2.5865\n","\n","Epoch 00154: val_loss did not improve from 0.13822\n","Epoch 155/1000\n"," - 1s - loss: 0.6872 - accuracy: 0.5448 - val_loss: 0.1385 - val_accuracy: 2.5749\n","\n","Epoch 00155: val_loss did not improve from 0.13822\n","Epoch 156/1000\n"," - 1s - loss: 0.6872 - accuracy: 0.5448 - val_loss: 0.1385 - val_accuracy: 2.5884\n","\n","Epoch 00156: val_loss did not improve from 0.13822\n","Epoch 157/1000\n"," - 1s - loss: 0.6871 - accuracy: 0.5445 - val_loss: 0.1385 - val_accuracy: 2.5846\n","\n","Epoch 00157: val_loss did not improve from 0.13822\n","Epoch 158/1000\n"," - 1s - loss: 0.6870 - accuracy: 0.5447 - val_loss: 0.1386 - val_accuracy: 2.5846\n","\n","Epoch 00158: val_loss did not improve from 0.13822\n","Epoch 159/1000\n"," - 1s - loss: 0.6870 - accuracy: 0.5452 - val_loss: 0.1386 - val_accuracy: 2.5961\n","\n","Epoch 00159: val_loss did not improve from 0.13822\n","Epoch 160/1000\n"," - 1s - loss: 0.6869 - accuracy: 0.5439 - val_loss: 0.1386 - val_accuracy: 2.5980\n","\n","Epoch 00160: val_loss did not improve from 0.13822\n","Epoch 161/1000\n"," - 1s - loss: 0.6869 - accuracy: 0.5429 - val_loss: 0.1386 - val_accuracy: 2.6057\n","\n","Epoch 00161: val_loss did not improve from 0.13822\n","Epoch 162/1000\n"," - 1s - loss: 0.6868 - accuracy: 0.5427 - val_loss: 0.1387 - val_accuracy: 2.6018\n","\n","Epoch 00162: val_loss did not improve from 0.13822\n","Epoch 163/1000\n"," - 1s - loss: 0.6868 - accuracy: 0.5431 - val_loss: 0.1387 - val_accuracy: 2.5980\n","\n","Epoch 00163: val_loss did not improve from 0.13822\n","Epoch 164/1000\n"," - 1s - loss: 0.6867 - accuracy: 0.5433 - val_loss: 0.1387 - val_accuracy: 2.5846\n","\n","Epoch 00164: val_loss did not improve from 0.13822\n","Epoch 165/1000\n"," - 1s - loss: 0.6867 - accuracy: 0.5431 - val_loss: 0.1387 - val_accuracy: 2.6076\n","\n","Epoch 00165: val_loss did not improve from 0.13822\n","Epoch 166/1000\n"," - 1s - loss: 0.6866 - accuracy: 0.5427 - val_loss: 0.1387 - val_accuracy: 2.6076\n","\n","Epoch 00166: val_loss did not improve from 0.13822\n","Epoch 167/1000\n"," - 1s - loss: 0.6866 - accuracy: 0.5425 - val_loss: 0.1387 - val_accuracy: 2.5961\n","\n","Epoch 00167: val_loss did not improve from 0.13822\n","Epoch 168/1000\n"," - 1s - loss: 0.6865 - accuracy: 0.5423 - val_loss: 0.1387 - val_accuracy: 2.6038\n","\n","Epoch 00168: val_loss did not improve from 0.13822\n","Epoch 169/1000\n"," - 1s - loss: 0.6865 - accuracy: 0.5432 - val_loss: 0.1387 - val_accuracy: 2.6057\n","\n","Epoch 00169: val_loss did not improve from 0.13822\n","Epoch 170/1000\n"," - 1s - loss: 0.6865 - accuracy: 0.5429 - val_loss: 0.1387 - val_accuracy: 2.6153\n","\n","Epoch 00170: val_loss did not improve from 0.13822\n","Epoch 171/1000\n"," - 1s - loss: 0.6864 - accuracy: 0.5425 - val_loss: 0.1388 - val_accuracy: 2.6076\n","\n","Epoch 00171: val_loss did not improve from 0.13822\n","Epoch 172/1000\n"," - 1s - loss: 0.6864 - accuracy: 0.5421 - val_loss: 0.1388 - val_accuracy: 2.6115\n","\n","Epoch 00172: val_loss did not improve from 0.13822\n","Epoch 173/1000\n"," - 1s - loss: 0.6864 - accuracy: 0.5421 - val_loss: 0.1387 - val_accuracy: 2.6057\n","\n","Epoch 00173: val_loss did not improve from 0.13822\n","Epoch 174/1000\n"," - 1s - loss: 0.6863 - accuracy: 0.5426 - val_loss: 0.1387 - val_accuracy: 2.6018\n","\n","Epoch 00174: val_loss did not improve from 0.13822\n","Epoch 175/1000\n"," - 1s - loss: 0.6863 - accuracy: 0.5426 - val_loss: 0.1388 - val_accuracy: 2.6038\n","\n","Epoch 00175: val_loss did not improve from 0.13822\n","Epoch 176/1000\n"," - 1s - loss: 0.6862 - accuracy: 0.5424 - val_loss: 0.1388 - val_accuracy: 2.5980\n","\n","Epoch 00176: val_loss did not improve from 0.13822\n","Epoch 177/1000\n"," - 1s - loss: 0.6862 - accuracy: 0.5424 - val_loss: 0.1388 - val_accuracy: 2.5922\n","\n","Epoch 00177: val_loss did not improve from 0.13822\n","Epoch 178/1000\n"," - 1s - loss: 0.6862 - accuracy: 0.5432 - val_loss: 0.1387 - val_accuracy: 2.6153\n","\n","Epoch 00178: val_loss did not improve from 0.13822\n","Epoch 179/1000\n"," - 1s - loss: 0.6861 - accuracy: 0.5434 - val_loss: 0.1388 - val_accuracy: 2.6038\n","\n","Epoch 00179: val_loss did not improve from 0.13822\n","Epoch 180/1000\n"," - 1s - loss: 0.6861 - accuracy: 0.5431 - val_loss: 0.1388 - val_accuracy: 2.6038\n","\n","Epoch 00180: val_loss did not improve from 0.13822\n","Epoch 181/1000\n"," - 1s - loss: 0.6860 - accuracy: 0.5435 - val_loss: 0.1388 - val_accuracy: 2.6038\n","\n","Epoch 00181: val_loss did not improve from 0.13822\n","Epoch 182/1000\n"," - 1s - loss: 0.6860 - accuracy: 0.5438 - val_loss: 0.1388 - val_accuracy: 2.5999\n","\n","Epoch 00182: val_loss did not improve from 0.13822\n","Epoch 183/1000\n"," - 1s - loss: 0.6860 - accuracy: 0.5444 - val_loss: 0.1388 - val_accuracy: 2.5999\n","\n","Epoch 00183: val_loss did not improve from 0.13822\n","Epoch 184/1000\n"," - 1s - loss: 0.6859 - accuracy: 0.5452 - val_loss: 0.1388 - val_accuracy: 2.5980\n","\n","Epoch 00184: val_loss did not improve from 0.13822\n","Epoch 185/1000\n"," - 1s - loss: 0.6859 - accuracy: 0.5453 - val_loss: 0.1389 - val_accuracy: 2.6076\n","\n","Epoch 00185: val_loss did not improve from 0.13822\n","Epoch 186/1000\n"," - 1s - loss: 0.6859 - accuracy: 0.5458 - val_loss: 0.1389 - val_accuracy: 2.6153\n","\n","Epoch 00186: val_loss did not improve from 0.13822\n","Epoch 187/1000\n"," - 1s - loss: 0.6858 - accuracy: 0.5467 - val_loss: 0.1390 - val_accuracy: 2.6018\n","\n","Epoch 00187: val_loss did not improve from 0.13822\n","Epoch 188/1000\n"," - 1s - loss: 0.6858 - accuracy: 0.5470 - val_loss: 0.1389 - val_accuracy: 2.6095\n","\n","Epoch 00188: val_loss did not improve from 0.13822\n","Epoch 189/1000\n"," - 1s - loss: 0.6857 - accuracy: 0.5473 - val_loss: 0.1389 - val_accuracy: 2.6153\n","\n","Epoch 00189: val_loss did not improve from 0.13822\n","Epoch 190/1000\n"," - 1s - loss: 0.6857 - accuracy: 0.5479 - val_loss: 0.1389 - val_accuracy: 2.6134\n","\n","Epoch 00190: val_loss did not improve from 0.13822\n","Epoch 191/1000\n"," - 1s - loss: 0.6857 - accuracy: 0.5478 - val_loss: 0.1389 - val_accuracy: 2.6211\n","\n","Epoch 00191: val_loss did not improve from 0.13822\n","Epoch 192/1000\n"," - 1s - loss: 0.6856 - accuracy: 0.5479 - val_loss: 0.1389 - val_accuracy: 2.6211\n","\n","Epoch 00192: val_loss did not improve from 0.13822\n","Epoch 193/1000\n"," - 1s - loss: 0.6856 - accuracy: 0.5474 - val_loss: 0.1390 - val_accuracy: 2.6230\n","\n","Epoch 00193: val_loss did not improve from 0.13822\n","Epoch 194/1000\n"," - 1s - loss: 0.6856 - accuracy: 0.5476 - val_loss: 0.1390 - val_accuracy: 2.6307\n","\n","Epoch 00194: val_loss did not improve from 0.13822\n","Epoch 195/1000\n"," - 1s - loss: 0.6855 - accuracy: 0.5473 - val_loss: 0.1390 - val_accuracy: 2.6287\n","\n","Epoch 00195: val_loss did not improve from 0.13822\n","Epoch 196/1000\n"," - 1s - loss: 0.6855 - accuracy: 0.5483 - val_loss: 0.1390 - val_accuracy: 2.6230\n","\n","Epoch 00196: val_loss did not improve from 0.13822\n","Epoch 197/1000\n"," - 1s - loss: 0.6854 - accuracy: 0.5485 - val_loss: 0.1390 - val_accuracy: 2.6230\n","\n","Epoch 00197: val_loss did not improve from 0.13822\n","Epoch 198/1000\n"," - 1s - loss: 0.6854 - accuracy: 0.5492 - val_loss: 0.1390 - val_accuracy: 2.6191\n","\n","Epoch 00198: val_loss did not improve from 0.13822\n","Epoch 199/1000\n"," - 1s - loss: 0.6853 - accuracy: 0.5499 - val_loss: 0.1390 - val_accuracy: 2.6172\n","\n","Epoch 00199: val_loss did not improve from 0.13822\n","Epoch 200/1000\n"," - 1s - loss: 0.6853 - accuracy: 0.5498 - val_loss: 0.1391 - val_accuracy: 2.5999\n","\n","Epoch 00200: val_loss did not improve from 0.13822\n","Epoch 201/1000\n"," - 1s - loss: 0.6853 - accuracy: 0.5499 - val_loss: 0.1391 - val_accuracy: 2.6134\n","\n","Epoch 00201: val_loss did not improve from 0.13822\n","Epoch 202/1000\n"," - 1s - loss: 0.6852 - accuracy: 0.5502 - val_loss: 0.1391 - val_accuracy: 2.6211\n","\n","Epoch 00202: val_loss did not improve from 0.13822\n","Epoch 203/1000\n"," - 1s - loss: 0.6852 - accuracy: 0.5509 - val_loss: 0.1391 - val_accuracy: 2.6115\n","\n","Epoch 00203: val_loss did not improve from 0.13822\n","Epoch 204/1000\n"," - 1s - loss: 0.6851 - accuracy: 0.5508 - val_loss: 0.1391 - val_accuracy: 2.6038\n","\n","Epoch 00204: val_loss did not improve from 0.13822\n","Epoch 205/1000\n"," - 1s - loss: 0.6851 - accuracy: 0.5508 - val_loss: 0.1391 - val_accuracy: 2.5961\n","\n","Epoch 00205: val_loss did not improve from 0.13822\n","Epoch 206/1000\n"," - 1s - loss: 0.6850 - accuracy: 0.5510 - val_loss: 0.1391 - val_accuracy: 2.6076\n","\n","Epoch 00206: val_loss did not improve from 0.13822\n","Epoch 207/1000\n"," - 1s - loss: 0.6850 - accuracy: 0.5506 - val_loss: 0.1392 - val_accuracy: 2.6095\n","\n","Epoch 00207: val_loss did not improve from 0.13822\n","Epoch 208/1000\n"," - 1s - loss: 0.6849 - accuracy: 0.5511 - val_loss: 0.1392 - val_accuracy: 2.6076\n","\n","Epoch 00208: val_loss did not improve from 0.13822\n","Epoch 209/1000\n"," - 1s - loss: 0.6849 - accuracy: 0.5509 - val_loss: 0.1392 - val_accuracy: 2.6134\n","\n","Epoch 00209: val_loss did not improve from 0.13822\n","Epoch 210/1000\n"," - 1s - loss: 0.6848 - accuracy: 0.5511 - val_loss: 0.1392 - val_accuracy: 2.6172\n","\n","Epoch 00210: val_loss did not improve from 0.13822\n","Epoch 211/1000\n"," - 1s - loss: 0.6848 - accuracy: 0.5516 - val_loss: 0.1392 - val_accuracy: 2.6211\n","\n","Epoch 00211: val_loss did not improve from 0.13822\n","Epoch 212/1000\n"," - 1s - loss: 0.6847 - accuracy: 0.5518 - val_loss: 0.1392 - val_accuracy: 2.6211\n","\n","Epoch 00212: val_loss did not improve from 0.13822\n","Epoch 213/1000\n"," - 1s - loss: 0.6846 - accuracy: 0.5525 - val_loss: 0.1392 - val_accuracy: 2.6172\n","\n","Epoch 00213: val_loss did not improve from 0.13822\n","Epoch 214/1000\n"," - 1s - loss: 0.6846 - accuracy: 0.5528 - val_loss: 0.1393 - val_accuracy: 2.5961\n","\n","Epoch 00214: val_loss did not improve from 0.13822\n","Epoch 215/1000\n"," - 1s - loss: 0.6845 - accuracy: 0.5524 - val_loss: 0.1393 - val_accuracy: 2.6038\n","\n","Epoch 00215: val_loss did not improve from 0.13822\n","Epoch 216/1000\n"," - 1s - loss: 0.6845 - accuracy: 0.5524 - val_loss: 0.1393 - val_accuracy: 2.6153\n","\n","Epoch 00216: val_loss did not improve from 0.13822\n","Epoch 217/1000\n"," - 1s - loss: 0.6844 - accuracy: 0.5527 - val_loss: 0.1393 - val_accuracy: 2.6172\n","\n","Epoch 00217: val_loss did not improve from 0.13822\n","Epoch 218/1000\n"," - 1s - loss: 0.6843 - accuracy: 0.5536 - val_loss: 0.1393 - val_accuracy: 2.6134\n","\n","Epoch 00218: val_loss did not improve from 0.13822\n","Epoch 219/1000\n"," - 1s - loss: 0.6843 - accuracy: 0.5535 - val_loss: 0.1393 - val_accuracy: 2.6134\n","\n","Epoch 00219: val_loss did not improve from 0.13822\n","Epoch 220/1000\n"," - 1s - loss: 0.6842 - accuracy: 0.5531 - val_loss: 0.1393 - val_accuracy: 2.6134\n","\n","Epoch 00220: val_loss did not improve from 0.13822\n","Epoch 221/1000\n"," - 1s - loss: 0.6842 - accuracy: 0.5536 - val_loss: 0.1393 - val_accuracy: 2.6134\n","\n","Epoch 00221: val_loss did not improve from 0.13822\n","Epoch 222/1000\n"," - 1s - loss: 0.6841 - accuracy: 0.5541 - val_loss: 0.1393 - val_accuracy: 2.6172\n","\n","Epoch 00222: val_loss did not improve from 0.13822\n","Epoch 223/1000\n"," - 1s - loss: 0.6840 - accuracy: 0.5541 - val_loss: 0.1393 - val_accuracy: 2.6172\n","\n","Epoch 00223: val_loss did not improve from 0.13822\n","Epoch 224/1000\n"," - 1s - loss: 0.6840 - accuracy: 0.5548 - val_loss: 0.1393 - val_accuracy: 2.6115\n","\n","Epoch 00224: val_loss did not improve from 0.13822\n","Epoch 225/1000\n"," - 1s - loss: 0.6839 - accuracy: 0.5545 - val_loss: 0.1393 - val_accuracy: 2.6153\n","\n","Epoch 00225: val_loss did not improve from 0.13822\n","Epoch 226/1000\n"," - 1s - loss: 0.6838 - accuracy: 0.5539 - val_loss: 0.1394 - val_accuracy: 2.6249\n","\n","Epoch 00226: val_loss did not improve from 0.13822\n","Epoch 227/1000\n"," - 1s - loss: 0.6838 - accuracy: 0.5537 - val_loss: 0.1393 - val_accuracy: 2.6268\n","\n","Epoch 00227: val_loss did not improve from 0.13822\n","Epoch 228/1000\n"," - 1s - loss: 0.6837 - accuracy: 0.5538 - val_loss: 0.1394 - val_accuracy: 2.6345\n","\n","Epoch 00228: val_loss did not improve from 0.13822\n","Epoch 229/1000\n"," - 1s - loss: 0.6837 - accuracy: 0.5548 - val_loss: 0.1394 - val_accuracy: 2.6326\n","\n","Epoch 00229: val_loss did not improve from 0.13822\n","Epoch 230/1000\n"," - 1s - loss: 0.6836 - accuracy: 0.5555 - val_loss: 0.1394 - val_accuracy: 2.6191\n","\n","Epoch 00230: val_loss did not improve from 0.13822\n","Epoch 231/1000\n"," - 1s - loss: 0.6835 - accuracy: 0.5548 - val_loss: 0.1394 - val_accuracy: 2.6095\n","\n","Epoch 00231: val_loss did not improve from 0.13822\n","Epoch 232/1000\n"," - 1s - loss: 0.6834 - accuracy: 0.5553 - val_loss: 0.1394 - val_accuracy: 2.6038\n","\n","Epoch 00232: val_loss did not improve from 0.13822\n","Epoch 233/1000\n"," - 1s - loss: 0.6834 - accuracy: 0.5559 - val_loss: 0.1394 - val_accuracy: 2.6076\n","\n","Epoch 00233: val_loss did not improve from 0.13822\n","Epoch 234/1000\n"," - 1s - loss: 0.6833 - accuracy: 0.5564 - val_loss: 0.1394 - val_accuracy: 2.5922\n","\n","Epoch 00234: val_loss did not improve from 0.13822\n","Epoch 235/1000\n"," - 1s - loss: 0.6832 - accuracy: 0.5565 - val_loss: 0.1394 - val_accuracy: 2.5903\n","\n","Epoch 00235: val_loss did not improve from 0.13822\n","Epoch 236/1000\n"," - 1s - loss: 0.6831 - accuracy: 0.5574 - val_loss: 0.1393 - val_accuracy: 2.5884\n","\n","Epoch 00236: val_loss did not improve from 0.13822\n","Epoch 237/1000\n"," - 1s - loss: 0.6831 - accuracy: 0.5584 - val_loss: 0.1394 - val_accuracy: 2.5922\n","\n","Epoch 00237: val_loss did not improve from 0.13822\n","Epoch 238/1000\n"," - 1s - loss: 0.6830 - accuracy: 0.5585 - val_loss: 0.1394 - val_accuracy: 2.5846\n","\n","Epoch 00238: val_loss did not improve from 0.13822\n","Epoch 239/1000\n"," - 1s - loss: 0.6829 - accuracy: 0.5587 - val_loss: 0.1394 - val_accuracy: 2.5807\n","\n","Epoch 00239: val_loss did not improve from 0.13822\n","Epoch 240/1000\n"," - 1s - loss: 0.6828 - accuracy: 0.5594 - val_loss: 0.1394 - val_accuracy: 2.5807\n","\n","Epoch 00240: val_loss did not improve from 0.13822\n","Epoch 241/1000\n"," - 1s - loss: 0.6827 - accuracy: 0.5605 - val_loss: 0.1394 - val_accuracy: 2.5846\n","\n","Epoch 00241: val_loss did not improve from 0.13822\n","Epoch 242/1000\n"," - 1s - loss: 0.6826 - accuracy: 0.5622 - val_loss: 0.1394 - val_accuracy: 2.6018\n","\n","Epoch 00242: val_loss did not improve from 0.13822\n","Epoch 243/1000\n"," - 1s - loss: 0.6825 - accuracy: 0.5627 - val_loss: 0.1394 - val_accuracy: 2.5999\n","\n","Epoch 00243: val_loss did not improve from 0.13822\n","Epoch 244/1000\n"," - 1s - loss: 0.6824 - accuracy: 0.5618 - val_loss: 0.1394 - val_accuracy: 2.5846\n","\n","Epoch 00244: val_loss did not improve from 0.13822\n","Epoch 245/1000\n"," - 1s - loss: 0.6824 - accuracy: 0.5617 - val_loss: 0.1394 - val_accuracy: 2.5788\n","\n","Epoch 00245: val_loss did not improve from 0.13822\n","Epoch 246/1000\n"," - 1s - loss: 0.6823 - accuracy: 0.5622 - val_loss: 0.1394 - val_accuracy: 2.5769\n","\n","Epoch 00246: val_loss did not improve from 0.13822\n","Epoch 247/1000\n"," - 1s - loss: 0.6822 - accuracy: 0.5623 - val_loss: 0.1394 - val_accuracy: 2.5749\n","\n","Epoch 00247: val_loss did not improve from 0.13822\n","Epoch 248/1000\n"," - 1s - loss: 0.6821 - accuracy: 0.5624 - val_loss: 0.1394 - val_accuracy: 2.5749\n","\n","Epoch 00248: val_loss did not improve from 0.13822\n","Epoch 249/1000\n"," - 1s - loss: 0.6820 - accuracy: 0.5629 - val_loss: 0.1394 - val_accuracy: 2.5807\n","\n","Epoch 00249: val_loss did not improve from 0.13822\n","Epoch 250/1000\n"," - 1s - loss: 0.6820 - accuracy: 0.5632 - val_loss: 0.1395 - val_accuracy: 2.5942\n","\n","Epoch 00250: val_loss did not improve from 0.13822\n","Epoch 251/1000\n"," - 1s - loss: 0.6819 - accuracy: 0.5630 - val_loss: 0.1394 - val_accuracy: 2.5903\n","\n","Epoch 00251: val_loss did not improve from 0.13822\n","Epoch 252/1000\n"," - 1s - loss: 0.6818 - accuracy: 0.5630 - val_loss: 0.1395 - val_accuracy: 2.6018\n","\n","Epoch 00252: val_loss did not improve from 0.13822\n","Epoch 253/1000\n"," - 1s - loss: 0.6818 - accuracy: 0.5627 - val_loss: 0.1393 - val_accuracy: 2.5903\n","\n","Epoch 00253: val_loss did not improve from 0.13822\n","Epoch 254/1000\n"," - 1s - loss: 0.6817 - accuracy: 0.5628 - val_loss: 0.1394 - val_accuracy: 2.5961\n","\n","Epoch 00254: val_loss did not improve from 0.13822\n","Epoch 255/1000\n"," - 1s - loss: 0.6816 - accuracy: 0.5626 - val_loss: 0.1395 - val_accuracy: 2.5903\n","\n","Epoch 00255: val_loss did not improve from 0.13822\n","Epoch 256/1000\n"," - 1s - loss: 0.6816 - accuracy: 0.5629 - val_loss: 0.1395 - val_accuracy: 2.5942\n","\n","Epoch 00256: val_loss did not improve from 0.13822\n","Epoch 257/1000\n"," - 1s - loss: 0.6815 - accuracy: 0.5631 - val_loss: 0.1395 - val_accuracy: 2.5961\n","\n","Epoch 00257: val_loss did not improve from 0.13822\n","Epoch 258/1000\n"," - 1s - loss: 0.6815 - accuracy: 0.5633 - val_loss: 0.1395 - val_accuracy: 2.5922\n","\n","Epoch 00258: val_loss did not improve from 0.13822\n","Epoch 259/1000\n"," - 1s - loss: 0.6814 - accuracy: 0.5636 - val_loss: 0.1395 - val_accuracy: 2.5922\n","\n","Epoch 00259: val_loss did not improve from 0.13822\n","Epoch 260/1000\n"," - 1s - loss: 0.6813 - accuracy: 0.5635 - val_loss: 0.1395 - val_accuracy: 2.5903\n","\n","Epoch 00260: val_loss did not improve from 0.13822\n","Epoch 261/1000\n"," - 1s - loss: 0.6813 - accuracy: 0.5632 - val_loss: 0.1396 - val_accuracy: 2.5807\n","\n","Epoch 00261: val_loss did not improve from 0.13822\n","Epoch 262/1000\n"," - 1s - loss: 0.6812 - accuracy: 0.5637 - val_loss: 0.1396 - val_accuracy: 2.5884\n","\n","Epoch 00262: val_loss did not improve from 0.13822\n","Epoch 263/1000\n"," - 1s - loss: 0.6812 - accuracy: 0.5637 - val_loss: 0.1395 - val_accuracy: 2.5903\n","\n","Epoch 00263: val_loss did not improve from 0.13822\n","Epoch 264/1000\n"," - 1s - loss: 0.6811 - accuracy: 0.5638 - val_loss: 0.1396 - val_accuracy: 2.5903\n","\n","Epoch 00264: val_loss did not improve from 0.13822\n","Epoch 265/1000\n"," - 1s - loss: 0.6810 - accuracy: 0.5634 - val_loss: 0.1396 - val_accuracy: 2.5865\n","\n","Epoch 00265: val_loss did not improve from 0.13822\n","Epoch 266/1000\n"," - 1s - loss: 0.6810 - accuracy: 0.5636 - val_loss: 0.1396 - val_accuracy: 2.5903\n","\n","Epoch 00266: val_loss did not improve from 0.13822\n","Epoch 267/1000\n"," - 1s - loss: 0.6809 - accuracy: 0.5642 - val_loss: 0.1396 - val_accuracy: 2.5884\n","\n","Epoch 00267: val_loss did not improve from 0.13822\n","Epoch 268/1000\n"," - 1s - loss: 0.6809 - accuracy: 0.5644 - val_loss: 0.1397 - val_accuracy: 2.5865\n","\n","Epoch 00268: val_loss did not improve from 0.13822\n","Epoch 269/1000\n"," - 1s - loss: 0.6808 - accuracy: 0.5649 - val_loss: 0.1396 - val_accuracy: 2.5865\n","\n","Epoch 00269: val_loss did not improve from 0.13822\n","Epoch 270/1000\n"," - 1s - loss: 0.6808 - accuracy: 0.5651 - val_loss: 0.1396 - val_accuracy: 2.5846\n","\n","Epoch 00270: val_loss did not improve from 0.13822\n","Epoch 271/1000\n"," - 1s - loss: 0.6807 - accuracy: 0.5652 - val_loss: 0.1396 - val_accuracy: 2.5922\n","\n","Epoch 00271: val_loss did not improve from 0.13822\n","Epoch 272/1000\n"," - 1s - loss: 0.6806 - accuracy: 0.5656 - val_loss: 0.1396 - val_accuracy: 2.5788\n","\n","Epoch 00272: val_loss did not improve from 0.13822\n","Epoch 273/1000\n"," - 1s - loss: 0.6806 - accuracy: 0.5654 - val_loss: 0.1396 - val_accuracy: 2.5788\n","\n","Epoch 00273: val_loss did not improve from 0.13822\n","Epoch 274/1000\n"," - 1s - loss: 0.6805 - accuracy: 0.5659 - val_loss: 0.1396 - val_accuracy: 2.5826\n","\n","Epoch 00274: val_loss did not improve from 0.13822\n","Epoch 275/1000\n"," - 1s - loss: 0.6805 - accuracy: 0.5661 - val_loss: 0.1396 - val_accuracy: 2.5903\n","\n","Epoch 00275: val_loss did not improve from 0.13822\n","Epoch 276/1000\n"," - 1s - loss: 0.6804 - accuracy: 0.5662 - val_loss: 0.1396 - val_accuracy: 2.5903\n","\n","Epoch 00276: val_loss did not improve from 0.13822\n","Epoch 277/1000\n"," - 1s - loss: 0.6803 - accuracy: 0.5662 - val_loss: 0.1397 - val_accuracy: 2.5884\n","\n","Epoch 00277: val_loss did not improve from 0.13822\n","Epoch 278/1000\n"," - 1s - loss: 0.6803 - accuracy: 0.5655 - val_loss: 0.1396 - val_accuracy: 2.5980\n","\n","Epoch 00278: val_loss did not improve from 0.13822\n","Epoch 279/1000\n"," - 1s - loss: 0.6802 - accuracy: 0.5655 - val_loss: 0.1397 - val_accuracy: 2.5903\n","\n","Epoch 00279: val_loss did not improve from 0.13822\n","Epoch 280/1000\n"," - 1s - loss: 0.6802 - accuracy: 0.5654 - val_loss: 0.1397 - val_accuracy: 2.5865\n","\n","Epoch 00280: val_loss did not improve from 0.13822\n","Epoch 281/1000\n"," - 1s - loss: 0.6801 - accuracy: 0.5654 - val_loss: 0.1397 - val_accuracy: 2.5884\n","\n","Epoch 00281: val_loss did not improve from 0.13822\n","Epoch 282/1000\n"," - 1s - loss: 0.6800 - accuracy: 0.5654 - val_loss: 0.1397 - val_accuracy: 2.5788\n","\n","Epoch 00282: val_loss did not improve from 0.13822\n","Epoch 283/1000\n"," - 1s - loss: 0.6800 - accuracy: 0.5655 - val_loss: 0.1397 - val_accuracy: 2.5788\n","\n","Epoch 00283: val_loss did not improve from 0.13822\n","Epoch 284/1000\n"," - 1s - loss: 0.6799 - accuracy: 0.5659 - val_loss: 0.1397 - val_accuracy: 2.5769\n","\n","Epoch 00284: val_loss did not improve from 0.13822\n","Epoch 285/1000\n"," - 1s - loss: 0.6799 - accuracy: 0.5662 - val_loss: 0.1397 - val_accuracy: 2.5807\n","\n","Epoch 00285: val_loss did not improve from 0.13822\n","Epoch 286/1000\n"," - 1s - loss: 0.6798 - accuracy: 0.5659 - val_loss: 0.1397 - val_accuracy: 2.5692\n","\n","Epoch 00286: val_loss did not improve from 0.13822\n","Epoch 287/1000\n"," - 1s - loss: 0.6797 - accuracy: 0.5657 - val_loss: 0.1397 - val_accuracy: 2.5769\n","\n","Epoch 00287: val_loss did not improve from 0.13822\n","Epoch 288/1000\n"," - 1s - loss: 0.6797 - accuracy: 0.5663 - val_loss: 0.1397 - val_accuracy: 2.5807\n","\n","Epoch 00288: val_loss did not improve from 0.13822\n","Epoch 289/1000\n"," - 1s - loss: 0.6796 - accuracy: 0.5668 - val_loss: 0.1398 - val_accuracy: 2.5865\n","\n","Epoch 00289: val_loss did not improve from 0.13822\n","Epoch 290/1000\n"," - 1s - loss: 0.6796 - accuracy: 0.5668 - val_loss: 0.1398 - val_accuracy: 2.5807\n","\n","Epoch 00290: val_loss did not improve from 0.13822\n","Epoch 291/1000\n"," - 1s - loss: 0.6795 - accuracy: 0.5671 - val_loss: 0.1398 - val_accuracy: 2.5826\n","\n","Epoch 00291: val_loss did not improve from 0.13822\n","Epoch 292/1000\n"," - 1s - loss: 0.6794 - accuracy: 0.5671 - val_loss: 0.1398 - val_accuracy: 2.5846\n","\n","Epoch 00292: val_loss did not improve from 0.13822\n","Epoch 293/1000\n"," - 1s - loss: 0.6793 - accuracy: 0.5675 - val_loss: 0.1398 - val_accuracy: 2.5826\n","\n","Epoch 00293: val_loss did not improve from 0.13822\n","Epoch 294/1000\n"," - 1s - loss: 0.6793 - accuracy: 0.5675 - val_loss: 0.1398 - val_accuracy: 2.5961\n","\n","Epoch 00294: val_loss did not improve from 0.13822\n","Epoch 295/1000\n"," - 1s - loss: 0.6792 - accuracy: 0.5670 - val_loss: 0.1398 - val_accuracy: 2.5788\n","\n","Epoch 00295: val_loss did not improve from 0.13822\n","Epoch 296/1000\n"," - 1s - loss: 0.6791 - accuracy: 0.5666 - val_loss: 0.1398 - val_accuracy: 2.5807\n","\n","Epoch 00296: val_loss did not improve from 0.13822\n","Epoch 297/1000\n"," - 1s - loss: 0.6791 - accuracy: 0.5665 - val_loss: 0.1398 - val_accuracy: 2.5846\n","\n","Epoch 00297: val_loss did not improve from 0.13822\n","Epoch 298/1000\n"," - 1s - loss: 0.6790 - accuracy: 0.5672 - val_loss: 0.1399 - val_accuracy: 2.5749\n","\n","Epoch 00298: val_loss did not improve from 0.13822\n","Epoch 299/1000\n"," - 1s - loss: 0.6789 - accuracy: 0.5672 - val_loss: 0.1398 - val_accuracy: 2.5903\n","\n","Epoch 00299: val_loss did not improve from 0.13822\n","Epoch 300/1000\n"," - 1s - loss: 0.6789 - accuracy: 0.5671 - val_loss: 0.1398 - val_accuracy: 2.5980\n","\n","Epoch 00300: val_loss did not improve from 0.13822\n","Epoch 301/1000\n"," - 1s - loss: 0.6788 - accuracy: 0.5673 - val_loss: 0.1399 - val_accuracy: 2.5942\n","\n","Epoch 00301: val_loss did not improve from 0.13822\n","Epoch 302/1000\n"," - 1s - loss: 0.6787 - accuracy: 0.5678 - val_loss: 0.1399 - val_accuracy: 2.5980\n","\n","Epoch 00302: val_loss did not improve from 0.13822\n","Epoch 303/1000\n"," - 1s - loss: 0.6787 - accuracy: 0.5676 - val_loss: 0.1399 - val_accuracy: 2.5961\n","\n","Epoch 00303: val_loss did not improve from 0.13822\n","Epoch 304/1000\n"," - 1s - loss: 0.6786 - accuracy: 0.5675 - val_loss: 0.1398 - val_accuracy: 2.6038\n","\n","Epoch 00304: val_loss did not improve from 0.13822\n","Epoch 305/1000\n"," - 1s - loss: 0.6785 - accuracy: 0.5675 - val_loss: 0.1399 - val_accuracy: 2.5980\n","\n","Epoch 00305: val_loss did not improve from 0.13822\n","Epoch 306/1000\n"," - 1s - loss: 0.6785 - accuracy: 0.5673 - val_loss: 0.1399 - val_accuracy: 2.5999\n","\n","Epoch 00306: val_loss did not improve from 0.13822\n","Epoch 307/1000\n"," - 1s - loss: 0.6784 - accuracy: 0.5677 - val_loss: 0.1399 - val_accuracy: 2.5999\n","\n","Epoch 00307: val_loss did not improve from 0.13822\n","Epoch 308/1000\n"," - 1s - loss: 0.6783 - accuracy: 0.5678 - val_loss: 0.1398 - val_accuracy: 2.6076\n","\n","Epoch 00308: val_loss did not improve from 0.13822\n","Epoch 309/1000\n"," - 1s - loss: 0.6783 - accuracy: 0.5678 - val_loss: 0.1399 - val_accuracy: 2.5961\n","\n","Epoch 00309: val_loss did not improve from 0.13822\n","Epoch 310/1000\n"," - 1s - loss: 0.6782 - accuracy: 0.5677 - val_loss: 0.1399 - val_accuracy: 2.6038\n","\n","Epoch 00310: val_loss did not improve from 0.13822\n","Epoch 311/1000\n"," - 1s - loss: 0.6782 - accuracy: 0.5674 - val_loss: 0.1399 - val_accuracy: 2.5903\n","\n","Epoch 00311: val_loss did not improve from 0.13822\n","Epoch 312/1000\n"," - 1s - loss: 0.6781 - accuracy: 0.5673 - val_loss: 0.1399 - val_accuracy: 2.5922\n","\n","Epoch 00312: val_loss did not improve from 0.13822\n","Epoch 313/1000\n"," - 1s - loss: 0.6780 - accuracy: 0.5676 - val_loss: 0.1400 - val_accuracy: 2.5865\n","\n","Epoch 00313: val_loss did not improve from 0.13822\n","Epoch 314/1000\n"," - 1s - loss: 0.6780 - accuracy: 0.5681 - val_loss: 0.1400 - val_accuracy: 2.5999\n","\n","Epoch 00314: val_loss did not improve from 0.13822\n","Epoch 315/1000\n"," - 1s - loss: 0.6779 - accuracy: 0.5679 - val_loss: 0.1400 - val_accuracy: 2.5922\n","\n","Epoch 00315: val_loss did not improve from 0.13822\n","Epoch 316/1000\n"," - 1s - loss: 0.6778 - accuracy: 0.5680 - val_loss: 0.1400 - val_accuracy: 2.6038\n","\n","Epoch 00316: val_loss did not improve from 0.13822\n","Epoch 317/1000\n"," - 1s - loss: 0.6778 - accuracy: 0.5678 - val_loss: 0.1399 - val_accuracy: 2.6134\n","\n","Epoch 00317: val_loss did not improve from 0.13822\n","Epoch 318/1000\n"," - 1s - loss: 0.6777 - accuracy: 0.5677 - val_loss: 0.1400 - val_accuracy: 2.6038\n","\n","Epoch 00318: val_loss did not improve from 0.13822\n","Epoch 319/1000\n"," - 1s - loss: 0.6776 - accuracy: 0.5677 - val_loss: 0.1400 - val_accuracy: 2.5980\n","\n","Epoch 00319: val_loss did not improve from 0.13822\n","Epoch 320/1000\n"," - 1s - loss: 0.6776 - accuracy: 0.5674 - val_loss: 0.1400 - val_accuracy: 2.5961\n","\n","Epoch 00320: val_loss did not improve from 0.13822\n","Epoch 321/1000\n"," - 1s - loss: 0.6775 - accuracy: 0.5669 - val_loss: 0.1400 - val_accuracy: 2.5961\n","\n","Epoch 00321: val_loss did not improve from 0.13822\n","Epoch 322/1000\n"," - 1s - loss: 0.6775 - accuracy: 0.5672 - val_loss: 0.1401 - val_accuracy: 2.5999\n","\n","Epoch 00322: val_loss did not improve from 0.13822\n","Epoch 323/1000\n"," - 1s - loss: 0.6774 - accuracy: 0.5668 - val_loss: 0.1400 - val_accuracy: 2.5942\n","\n","Epoch 00323: val_loss did not improve from 0.13822\n","Epoch 324/1000\n"," - 1s - loss: 0.6773 - accuracy: 0.5664 - val_loss: 0.1400 - val_accuracy: 2.5980\n","\n","Epoch 00324: val_loss did not improve from 0.13822\n","Epoch 325/1000\n"," - 1s - loss: 0.6772 - accuracy: 0.5664 - val_loss: 0.1401 - val_accuracy: 2.5903\n","\n","Epoch 00325: val_loss did not improve from 0.13822\n","Epoch 326/1000\n"," - 1s - loss: 0.6772 - accuracy: 0.5664 - val_loss: 0.1401 - val_accuracy: 2.5942\n","\n","Epoch 00326: val_loss did not improve from 0.13822\n","Epoch 327/1000\n"," - 1s - loss: 0.6771 - accuracy: 0.5667 - val_loss: 0.1402 - val_accuracy: 2.5961\n","\n","Epoch 00327: val_loss did not improve from 0.13822\n","Epoch 328/1000\n"," - 1s - loss: 0.6771 - accuracy: 0.5668 - val_loss: 0.1401 - val_accuracy: 2.5884\n","\n","Epoch 00328: val_loss did not improve from 0.13822\n","Epoch 329/1000\n"," - 1s - loss: 0.6770 - accuracy: 0.5662 - val_loss: 0.1401 - val_accuracy: 2.5942\n","\n","Epoch 00329: val_loss did not improve from 0.13822\n","Epoch 330/1000\n"," - 1s - loss: 0.6769 - accuracy: 0.5664 - val_loss: 0.1401 - val_accuracy: 2.5865\n","\n","Epoch 00330: val_loss did not improve from 0.13822\n","Epoch 331/1000\n"," - 1s - loss: 0.6769 - accuracy: 0.5667 - val_loss: 0.1401 - val_accuracy: 2.5865\n","\n","Epoch 00331: val_loss did not improve from 0.13822\n","Epoch 332/1000\n"," - 1s - loss: 0.6768 - accuracy: 0.5671 - val_loss: 0.1402 - val_accuracy: 2.5922\n","\n","Epoch 00332: val_loss did not improve from 0.13822\n","Epoch 333/1000\n"," - 1s - loss: 0.6767 - accuracy: 0.5667 - val_loss: 0.1401 - val_accuracy: 2.6018\n","\n","Epoch 00333: val_loss did not improve from 0.13822\n","Epoch 334/1000\n"," - 1s - loss: 0.6767 - accuracy: 0.5672 - val_loss: 0.1402 - val_accuracy: 2.5903\n","\n","Epoch 00334: val_loss did not improve from 0.13822\n","Epoch 335/1000\n"," - 1s - loss: 0.6766 - accuracy: 0.5679 - val_loss: 0.1402 - val_accuracy: 2.6018\n","\n","Epoch 00335: val_loss did not improve from 0.13822\n","Epoch 336/1000\n"," - 1s - loss: 0.6765 - accuracy: 0.5676 - val_loss: 0.1402 - val_accuracy: 2.5980\n","\n","Epoch 00336: val_loss did not improve from 0.13822\n","Epoch 337/1000\n"," - 1s - loss: 0.6765 - accuracy: 0.5672 - val_loss: 0.1402 - val_accuracy: 2.5961\n","\n","Epoch 00337: val_loss did not improve from 0.13822\n","Epoch 338/1000\n"," - 1s - loss: 0.6764 - accuracy: 0.5674 - val_loss: 0.1402 - val_accuracy: 2.6076\n","\n","Epoch 00338: val_loss did not improve from 0.13822\n","Epoch 339/1000\n"," - 1s - loss: 0.6763 - accuracy: 0.5677 - val_loss: 0.1402 - val_accuracy: 2.6095\n","\n","Epoch 00339: val_loss did not improve from 0.13822\n","Epoch 340/1000\n"," - 1s - loss: 0.6763 - accuracy: 0.5679 - val_loss: 0.1402 - val_accuracy: 2.6018\n","\n","Epoch 00340: val_loss did not improve from 0.13822\n","Epoch 341/1000\n"," - 1s - loss: 0.6762 - accuracy: 0.5678 - val_loss: 0.1403 - val_accuracy: 2.6018\n","\n","Epoch 00341: val_loss did not improve from 0.13822\n","Epoch 342/1000\n"," - 1s - loss: 0.6762 - accuracy: 0.5682 - val_loss: 0.1403 - val_accuracy: 2.6095\n","\n","Epoch 00342: val_loss did not improve from 0.13822\n","Epoch 343/1000\n"," - 1s - loss: 0.6761 - accuracy: 0.5684 - val_loss: 0.1403 - val_accuracy: 2.5961\n","\n","Epoch 00343: val_loss did not improve from 0.13822\n","Epoch 344/1000\n"," - 1s - loss: 0.6760 - accuracy: 0.5680 - val_loss: 0.1403 - val_accuracy: 2.6057\n","\n","Epoch 00344: val_loss did not improve from 0.13822\n","Epoch 345/1000\n"," - 1s - loss: 0.6760 - accuracy: 0.5683 - val_loss: 0.1403 - val_accuracy: 2.6038\n","\n","Epoch 00345: val_loss did not improve from 0.13822\n","Epoch 346/1000\n"," - 1s - loss: 0.6759 - accuracy: 0.5682 - val_loss: 0.1403 - val_accuracy: 2.6095\n","\n","Epoch 00346: val_loss did not improve from 0.13822\n","Epoch 347/1000\n"," - 1s - loss: 0.6758 - accuracy: 0.5684 - val_loss: 0.1404 - val_accuracy: 2.5884\n","\n","Epoch 00347: val_loss did not improve from 0.13822\n","Epoch 348/1000\n"," - 1s - loss: 0.6758 - accuracy: 0.5686 - val_loss: 0.1404 - val_accuracy: 2.5922\n","\n","Epoch 00348: val_loss did not improve from 0.13822\n","Epoch 349/1000\n"," - 1s - loss: 0.6757 - accuracy: 0.5687 - val_loss: 0.1404 - val_accuracy: 2.5999\n","\n","Epoch 00349: val_loss did not improve from 0.13822\n","Epoch 350/1000\n"," - 1s - loss: 0.6756 - accuracy: 0.5688 - val_loss: 0.1403 - val_accuracy: 2.6038\n","\n","Epoch 00350: val_loss did not improve from 0.13822\n","Epoch 351/1000\n"," - 1s - loss: 0.6756 - accuracy: 0.5682 - val_loss: 0.1404 - val_accuracy: 2.5942\n","\n","Epoch 00351: val_loss did not improve from 0.13822\n","Epoch 352/1000\n"," - 1s - loss: 0.6755 - accuracy: 0.5686 - val_loss: 0.1404 - val_accuracy: 2.5961\n","\n","Epoch 00352: val_loss did not improve from 0.13822\n","Epoch 353/1000\n"," - 1s - loss: 0.6754 - accuracy: 0.5687 - val_loss: 0.1404 - val_accuracy: 2.5884\n","\n","Epoch 00353: val_loss did not improve from 0.13822\n","Epoch 354/1000\n"," - 1s - loss: 0.6753 - accuracy: 0.5682 - val_loss: 0.1404 - val_accuracy: 2.5903\n","\n","Epoch 00354: val_loss did not improve from 0.13822\n","Epoch 355/1000\n"," - 1s - loss: 0.6753 - accuracy: 0.5682 - val_loss: 0.1404 - val_accuracy: 2.5865\n","\n","Epoch 00355: val_loss did not improve from 0.13822\n","Epoch 356/1000\n"," - 1s - loss: 0.6752 - accuracy: 0.5682 - val_loss: 0.1403 - val_accuracy: 2.5961\n","\n","Epoch 00356: val_loss did not improve from 0.13822\n","Epoch 357/1000\n"," - 1s - loss: 0.6752 - accuracy: 0.5686 - val_loss: 0.1404 - val_accuracy: 2.5826\n","\n","Epoch 00357: val_loss did not improve from 0.13822\n","Epoch 358/1000\n"," - 1s - loss: 0.6751 - accuracy: 0.5688 - val_loss: 0.1404 - val_accuracy: 2.5749\n","\n","Epoch 00358: val_loss did not improve from 0.13822\n","Epoch 359/1000\n"," - 1s - loss: 0.6750 - accuracy: 0.5689 - val_loss: 0.1404 - val_accuracy: 2.5673\n","\n","Epoch 00359: val_loss did not improve from 0.13822\n","Epoch 360/1000\n"," - 1s - loss: 0.6749 - accuracy: 0.5695 - val_loss: 0.1404 - val_accuracy: 2.5711\n","\n","Epoch 00360: val_loss did not improve from 0.13822\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"KIk0rYDvKfse"},"source":["### load model"]},{"cell_type":"code","metadata":{"id":"3jwBpTS0Kgt0"},"source":["ckpt_path = current_path + 'ckpt/'\n","board_path = current_path + 'graph/'\n","\n","model_name = 'classifier_%s_lstm_small_updown_%s_tvnon_shuffle.h5' % (period, symbol_name)\n","\n","\n","model = keras.models.load_model(ckpt_path + model_name)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cPz7DZxeJc4P"},"source":["### test"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":838},"id":"baSlNaUxJd2r","executionInfo":{"status":"ok","timestamp":1621947797041,"user_tz":-540,"elapsed":8298,"user":{"displayName":"7th June","photoUrl":"","userId":"08178289703395036410"}},"outputId":"ca6fed95-c9ea-4767-a4d8-4dd55b1af479"},"source":["# x_test = new_input_x\n","# y_test = np.where(new_input_pr > 1, 1, 0)\n","# pr_test = new_input_pr\n","\n","# x_test = org_x_test\n","# y_test = org_y_test\n","# pr_test = org_pr_test\n","\n","# x_test = x_train\n","# y_test = y_train\n","# pr_test = pr_train\n","\n","# x_test = x_val\n","# y_test = y_val\n","# pr_test = pr_val\n","\n","# x_test = concat_x\n","# y_test = np.where(concat_pr > 1, 1, 0)\n","# pr_test = concat_pr\n","\n","test_result = model.predict(x_test)\n","# test_result = model.predict(test_set)\n","\n","print('test_result.shape :', test_result.shape)\n","# print('pr_val.shape :', pr_val.shape)\n","\n","y_score = test_result[:, [1]]\n","print('y_test[:5] :', y_test.reshape(-1,)[:5])\n","# print('np.unique(y_test) :', np.unique(y_test, return_counts=True))\n","print('y_score[:5] :', y_score[:5])\n","# print('np.unique(y_score) :', np.unique(y_score, return_counts=True))\n","\n","print('y_test.shape :', y_test.shape)\n","print('y_score.shape :', y_score.shape)\n","\n","print('len(y_test) :', len(y_test))\n","\n","#     precision recall curve   #\n","precision, recall, threshold = precision_recall_curve(y_test, y_score)\n","precision, recall = precision[:-1], recall[:-1]\n","\n","plt.plot(threshold, precision, label='precision')\n","plt.plot(threshold, recall, label='recall')\n","plt.legend()\n","plt.title('precision recall')\n","plt.show()\n","# print(y_pred)\n","\n","\n","# threshold = [0.65]\n","# print('threshold :', threshold)\n","# break\n","\n","acc_pr_bythr = []\n","new_thresh = []\n","\n","for thresh in threshold:\n","  \n","  # if thresh < 0.5:\n","  #     continue\n","\n","  y_pred = np.where(y_score[:, -1] > thresh, 1, 0)\n","  # print('y_pred.shape :', y_pred.shape)\n","  # print('y_pred :', y_pred)\n","\n","  #     compare precision     #\n","\n","  # print('precision :', precision_score(y_test, y_pred))\n","  # print('recall :', recall_score(y_test, y_pred))\n","  # print()\n","\n","  # print('np.isnan(np.sum(x_test)) :', np.isnan(np.sum(x_test)))\n","  # print('np.isnan(np.sum(y_test)) :', np.isnan(np.sum(y_test)))\n","\n","  # # plot_confusion_matrix(best_model, x_test, y_test, normalize=None)\n","  # # plt.show()  \n","  # print()\n","\n","  #     check win-ratio improvement     #\n","  cmat = confusion_matrix(y_test, y_pred)\n","  # print(cmat)\n","  # print(np.sum(cmat, axis=1))\n","\n","  test_size = len(y_test)\n","  test_pr_list = pr_test\n","  # print('origin ac_pr :', np.cumprod(test_pr_list)[-1])\n","\n","  org_wr = np.sum(cmat, axis=1)[-1] / sum(np.sum(cmat, axis=1))\n","  ml_wr = cmat[1][1] / np.sum(cmat, axis=0)[-1]\n","  # print('win ratio improvement %.2f --> %.2f' % (org_wr, ml_wr))\n","\n","  # print('pr_test.shape :', pr_test.shape)\n","\n","  # print(y_pred)\n","  # print(test_pr_list)\n","\n","  pred_pr_list = np.where(y_pred == 1, test_pr_list.reshape(-1, ), 1.0)\n","  # pred_pr_list = np.where(np.isnan(pred_pr_list), 1.0, pred_pr_list)\n","  # pred_pr_list = np.where(pred_pr_list == 0.0, 1.0, pred_pr_list)\n","  # print('pred_pr_list.shape :', pred_pr_list.shape)\n","\n","  # if np.cumprod(test_pr_list)[-1] < np.cumprod(pred_pr_list)[-1]:\n","  #   print('accum_pr increased ! : %.3f --> %.3f' % (np.cumprod(test_pr_list)[-1], np.cumprod(pred_pr_list)[-1]))\n","  #   print('thresh :', thresh)\n","    \n","  # if len(threshold) == 1:\n","#   plt.figure(figsize=(10, 5))\n","#   plt.subplot(121)\n","#   plt.plot(np.cumprod(test_pr_list))\n","#   plt.title('%.3f' % (np.cumprod(test_pr_list)[-1]))\n","# # plt.show()\n","\n","#   plt.subplot(122)\n","#   plt.plot(np.cumprod(pred_pr_list))\n","#   plt.title('%.3f' % (np.cumprod(pred_pr_list)[-1]))\n","#   # plt.axvline(len(org_pr_test), linestyle='--', color='r')\n","#   plt.show()\n","\n","\n","  acc_pr_bythr.append(np.cumprod(pred_pr_list)[-1])\n","  new_thresh.append(thresh)\n","\n","\n","print('acc_pr_bythr :', acc_pr_bythr)\n","\n","plt.figure(figsize=(10, 5))\n","plt.subplot(121)\n","plt.plot(threshold, precision, label='precision')\n","plt.plot(threshold, recall, label='recall')\n","plt.legend()\n","plt.title('precision recall')\n","# plt.show()\n","plt.subplot(122)\n","plt.plot(new_thresh, acc_pr_bythr)\n","plt.axhline(np.cumprod(test_pr_list)[-1], linestyle='--', color='r')\n","plt.title(symbol_name)\n","plt.show()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["test_result.shape : (2603, 2)\n","y_test[:5] : [1 0 0 0 1]\n","y_score[:5] : [[0.47480527]\n"," [0.49306253]\n"," [0.48749533]\n"," [0.5002598 ]\n"," [0.47202632]]\n","y_test.shape : (2603, 1)\n","y_score.shape : (2603, 1)\n","len(y_test) : 2603\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:89: RuntimeWarning: invalid value encountered in long_scalars\n"],"name":"stderr"},{"output_type":"stream","text":["acc_pr_bythr : [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.5077617, 2.6540802, 2.836036, 3.1639838, 3.175346, 3.2788646, 3.271488, 3.556569, 3.9133615, 3.9826937, 4.0292773, 4.233906, 4.260415, 4.1108885, 4.789744, 4.776908, 5.849656, 5.372002, 5.108837, 5.0193567, 5.0212426, 4.4345636, 4.228673, 4.2157297, 4.155304, 4.1254787, 4.0831738, 3.5961864, 3.2684042, 3.2233574, 3.1664317, 3.2024932, 3.0383048, 2.6094372, 2.7168546, 2.7814631, 2.6895664, 2.317054, 2.3626804, 2.0810838, 1.9547856, 2.217972, 2.6945713, 2.5786395, 2.50288, 2.541115, 2.2999287, 2.2853143, 2.1694522, 2.2380953, 2.2840466, 2.3737679, 2.4615064, 2.3958752, 3.0002031, 3.2579968, 3.3057725, 3.2436564, 3.3287263, 3.1835995, 3.021191, 2.7419074, 2.7775645, 1.7660972, 1.6627501, 1.7289568, 1.7485158, 1.6879221, 1.689491, 1.7822728, 2.2278697, 2.2476614, 1.5365893, 1.7563657, 1.8289036, 1.7351748, 1.7193587, 1.6275876, 1.8257267, 1.7903671, 2.022321, 2.0582561, 2.2075808, 2.2491512, 2.2551796, 2.719221, 2.9496694, 2.9500506, 3.0353973, 2.9508998, 3.1476545, 3.4118526, 3.5869803, 3.4443853, 3.8194432, 3.5494285, 3.310949, 3.5915308, 3.6480508, 3.9896028, 4.1603765, 4.270896, 5.788678, 5.8060975, 5.626223, 5.2300234, 4.8314753, 4.6770606, 4.567785, 4.8152027, 4.881991, 4.872586, 6.021011, 5.574633, 4.7620955, 4.422068, 4.276643, 4.311369, 4.09313, 4.4676404, 4.587744, 4.4092703, 3.9802063, 3.7962337, 3.8319237, 3.3908434, 3.4113574, 3.4066358, 3.336148, 3.2885425, 3.6033583, 3.7694232, 3.9369855, 3.9973722, 3.9848619, 4.080246, 4.5224595, 4.1361437, 4.04495, 4.794993, 3.9736278, 3.4096286, 3.4552279, 2.7135863, 2.527818, 2.5840197, 2.4660242, 2.4380639, 2.273211, 2.2100735, 1.9554001, 1.7725865, 1.6002073, 1.5706092, 1.4553778, 1.385468, 1.3280835, 1.3082794, 1.2938763, 1.1647096, 1.3006867, 1.3542017, 1.3059483, 1.255681, 1.1894759, 1.1975335, 1.1040653, 1.0777375, 1.1747712, 1.0386562, 0.98860246, 0.9922955, 0.8898496, 1.0]\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 720x360 with 2 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"code","metadata":{"id":"mVDwWWvE45Wr"},"source":["plt.subplot(122)\n","plt.plot(new_thresh, acc_pr_bythr)\n","plt.axhline(np.cumprod(test_pr_list)[-1], linestyle='--', color='r')\n","plt.axvline(new_thresh[np.argmax(acc_pr_bythr)], linestyle='--', color='r')\n","plt.title(symbol_name + '\\n' + str(new_thresh[np.argmax(acc_pr_bythr)]))\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":265},"id":"zejEkNZI4Kyv","executionInfo":{"status":"ok","timestamp":1620614941396,"user_tz":-540,"elapsed":1104,"user":{"displayName":"7th June","photoUrl":"","userId":"08178289703395036410"}},"outputId":"80ef829a-8d2e-468d-afb7-8a3423d979ef"},"source":["thresh = 0.5866\n","y_pred = np.where(y_score[:600, -1] > thresh, 1, 0)\n","# print('y_pred.shape :', y_pred.shape)\n","# print('y_pred :', y_pred)\n","\n","#     compare precision     #\n","\n","# print('precision :', precision_score(y_test, y_pred))\n","# print('recall :', recall_score(y_test, y_pred))\n","# print()\n","\n","# print('np.isnan(np.sum(x_test)) :', np.isnan(np.sum(x_test)))\n","# print('np.isnan(np.sum(y_test)) :', np.isnan(np.sum(y_test)))\n","\n","# # plot_confusion_matrix(best_model, x_test, y_test, normalize=None)\n","# # plt.show()  \n","# print()\n","\n","#     check win-ratio improvement     #\n","cmat = confusion_matrix(y_test[:len(y_pred)], y_pred)\n","# print(cmat)\n","# print(np.sum(cmat, axis=1))\n","\n","test_size = len(y_test[:len(y_pred)])\n","test_pr_list = pr_test[:len(y_pred)]\n","# print('origin ac_pr :', np.cumprod(test_pr_list)[-1])\n","\n","org_wr = np.sum(cmat, axis=1)[-1] / sum(np.sum(cmat, axis=1))\n","ml_wr = cmat[1][1] / np.sum(cmat, axis=0)[-1]\n","# print('win ratio improvement %.2f --> %.2f' % (org_wr, ml_wr))\n","\n","# print('pr_test.shape :', pr_test.shape)\n","\n","# print(y_pred)\n","# print(test_pr_list)\n","pred_pr_list = np.where(y_pred == 1, test_pr_list.reshape(-1, ), 1.0)\n","pred_pr_list = np.where(np.isnan(pred_pr_list), 1.0, pred_pr_list)\n","pred_pr_list = np.where(pred_pr_list == 0.0, 1.0, pred_pr_list)\n","\n","plt.plot(np.cumprod(pred_pr_list))\n","plt.show()"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAZU0lEQVR4nO3de5Bc5Xnn8e8z3T0zmtEVaQRCElK4yoC5ZRaj4BgBmwRYKt4krBcqFRMvW9p4nYpd62QLJ7tOeddVu65K7LAmhmiNY0hRjtd3lsVOsA22Y2LwIAshEMaSLZBkyRJImntfTvezf/RpaTSaS89Mz5w+b/8+VVPqPud09/vqdP/mnee8p4+5OyIikn5tSTdAREQaQ4EuIhIIBbqISCAU6CIigVCgi4gEIpvUC69atco3btyY1MuLiKTS888//4a790y0LrFA37hxI319fUm9vIhIKpnZa5OtU8lFRCQQCnQRkUAo0EVEAqFAFxEJhAJdRCQQCnQRkUAo0EVEApHYPHQRkRC4O599Zh/Hh4t1P6Z341m84+IJzw2aEwW6iMgc/PSNYT7yf18GwKy+x/zBDRco0EVEms1wIQLgobt7ufktZyfaFtXQRUTmYKRYBmBReybhlijQRUTmZKRYHaF3tSdf8FCgi4jMQW2E3qURuohIup0sueQU6CIiqTaqEbqISBhqI/TuDtXQRURSbbQYYQYd2eTjNPkWiIik2EixTFcug9V7VtE8UqCLiMzBcLHMoiaYsgg6U1REZEoHT4zyH/6u72StfLyjAwVWdLcvcKsmpkAXEZnCq4cH2XVwgLdfuIrlXbkzN1gD11+4auEbNgEFuojIFErlCgD33rqJy9cuS7g1U1MNXURkCuWKA5BpS/6g53QU6CIiUyjFgZ7LKNBFRFKtXKmWXDJtzR+Xzd9CEZEEReXqCD2rkouISLpFccklq5KLiEi6nQx0lVxERNItiqctquQiIpJyJ6ctquQiIpJupfigaC6EkouZdZrZc2b2gpm9ZGYfmWCbDjP7vJntMbNnzWzjfDRWRGShnZq2GMYIvQDc5O5XAlcBt5jZdeO2uQc47u4XAp8APtbYZoqIJOPUQdEAAt2rhuK7ufjHx232TuDh+PYXgZutGb4cWERkjqKy02bQFkKgA5hZxsx2AEeAJ9392XGbrAX2A7h7BPQDKyd4nq1m1mdmfUePHp1by0VEFkBUcbKZ5q+fQ52B7u5ld78KWAdca2aXz+bF3H2bu/e6e29PT89snkJEZEFF5Uoqyi0ww1ku7n4CeAq4Zdyqg8B6ADPLAsuANxvRQBGRJEUVT8UBUahvlkuPmS2Pby8Cfg14ZdxmjwF3x7fvAL7t7uPr7CIiqVOuOLmUlFzqucDFGuBhM8tQ/QXwf9z9cTP7b0Cfuz8GPAT8nZntAY4Bd85bi0VEFlBUqaRmhD5toLv7TuDqCZZ/eMztPPBvGts0EZHkRWUnl5JAT8ffESIiCYkqnorT/kGBLiIypajiqTjtHxToIiJTisrpqaEr0EVEphDUtEURkVaWpmmL6WiliEhCSiq5iIiEoTpCV6CLiKReVFYNXUQk9YpRhef2HUvFBaJBgS4iMqln9r4BpOO70EGBLiIyqeFCGYAP3bop4ZbUR4EuIjKJQlQN9K72TMItqY8CXURkEoWoeoHojqwCXUQk1Qql6gi9I5uOqExHK0VEEpCvjdBz6YjKdLRSRCQBhVI10Nt16r+ISLoVojLZNiOrQBcRSbdCVElN/RwU6CIikypEZTpz6ZjhAgp0EZFJFUrpGqFPe5FoEZE0u//bP2H/sdHTlm1c1c17t1ww7WMLUYWOFI3QFegiEqz+kRJ/8Y+vsqQzS3d7Ne6GCxGDhYj3XL9x2nJKISprhC4i0gwG8iUA/uvtl/Ku3vUA/M139vI/vv4KFfdpH6+DoiIiTWIwHwGwtPPU2LX23eblyvSBni+VU3PaP2iELiIBG4xH6Is7cieXtVk10PtHS3xj12GiKYL95yfybFjZNb+NbCAFuogEa6hQHaEvmWCE/oW+A9z3rZ9M+xy/csHK+WncPFCgi0iwaiWXsYFeu1hFbd13/mTLlGWVniUd89jCxlKgi0iwBuMR+uKxI/S45JKPv+t83Yqu1FwzdDo6KCoiwarV0Jd2nqqh176WJV8qk2mzYMIcFOgiErDBfEQuY6dNPawdFC2UKuQy4YQ5KNBFJGA/PTrEucsXYXYquGsj8nypnJqvxa3XtL0xs/Vm9pSZvWxmL5nZ+yfYZouZ9ZvZjvjnw/PTXBGR6e0/NsK27+7luZ8d45rzVpy2rhboo6Uy7Sk6aage9RwUjYAPuvt2M1sCPG9mT7r7y+O2+5673974JoqIzMyD39nLo8++DsCNm1aftq5WcsmXyuQCG6FPG+jufgg4FN8eNLPdwFpgfKCLiDSFEyMlzu/p5ok/+tUzvq/lVMmlElygz6g3ZrYRuBp4doLVm83sBTP7upldNsnjt5pZn5n1HT16dMaNFRGpR/9oieWLchN++VbbmGmLoZVc6u6NmS0GvgR8wN0Hxq3eDmxw9yuBTwJfneg53H2bu/e6e29PT89s2ywiMqWBfImli3ITrjs5Qi+GV3KpqzdmlqMa5o+6+5fHr3f3AXcfim8/AeTMbFVDWyoiUqf+0RLLJg306r+jpTLtrTZt0arzfR4Cdrv7xyfZ5px4O8zs2vh532xkQ0VE6jUwWjrtZKKxTh0UDa+GXs8sl+uB3wNeNLMd8bI/Bc4DcPcHgTuA95pZBIwCd7rX8WXDIiIN5u4M5COWLpo43k6WXKLwSi71zHL5J2DKv0vc/X7g/kY1SkSkXpWK85nv/4wTI9XT/EuVCuWKT15yiUfo7gR3UFRfziUiqbbn6BAf/X+7MTtVTunMtfGWNUsn3L5tzHe3tNwIXUSkmQ2MVkfmj/y7a/nVi6afPTf2y7jasy12UFREpJnVviK3u6O+8WmbhTtCD6s3ItJyhmoXsagz0E8boSvQRUSax9AEF7GYSmbsCD2wg6Jh9UZEWs5wLdDrLbmMST2N0EVEmkjt2qDd7TMvuaTpeqH1UKCLSKoNFSK62zOnTUecytiSyx/ccMF8NSsRmrYoIgvO3Tk8kKdcmfsJ5b8YyNddP4fT56GHdD1RUKCLSAI+/8P93PvlFxv2fJvOWVL3tmNH6KFRoIvIgvvZG8O0Z9r46G9d3pDnu/zcZXVvG9qofCwFuogsuGPDRVYubuddvesX/LXrrbWnkQ6KisiCOzZcZEVXeyKvHXLJRYEuIgvu2EiRs7qTCfS2gFMv4K6JSLM6PlxkRUKBHvIIXTV0Eanb13Yc5Bu7Ds/5eX5+Is+WSxIK9IBr6Ap0Eanbw8/s45XDg6xbsWhOz3N+Tzc3XJzMheJDPiiqQBeRuuVLFX7lglV8+u7epJsyayGXXFRDF5G6FaIyHbl0x0bIJZd07xkRWVD5UoXObCbpZsxJm0boIiJQiCoaoTexdO8ZEVlQhahMR8ovChFwnivQRaR+hVKFzly6Sy6mkouItLpKxSmWK6kfoYdM0xZFpC7FcgWAjpQfFAW481+s5zcuOyfpZjScAl1E6pIvlQHoTPlBUYD/+TtXJN2EeZH+PSMiC6IQhTNCD5UCXUTqUihVAz2EEXqotGdEpC75qFpy0Qi9eSnQRaQutRG6Zrk0Lx0UFWlRw4WI4yPFurc/cHwEIPXz0EM2baCb2XrgEeBswIFt7n7fuG0MuA+4DRgBft/dtze+uSLSKDf/5Xc4PJCf8eMWd2oc2Kzq2TMR8EF3325mS4DnzexJd395zDa3AhfFP28DHoj/FZEmlC+VOTyQ57a3nsOWS1bX/bjFHVmuWLtsHlsmczFtoLv7IeBQfHvQzHYDa4Gxgf5O4BF3d+AHZrbczNbEjxWRJjOQLwGw+YJVvKt3fcKtkUaZ0dENM9sIXA08O27VWmD/mPsH4mUi0oQGRquBvlTlk6DUHehmthj4EvABdx+YzYuZ2VYz6zOzvqNHj87mKUSkAfrjQF+2KJdwS6SR6gp0M8tRDfNH3f3LE2xyEBj7d9u6eNlp3H2bu/e6e29PTzLXExQRGBiNAFiqQA/KtIEez2B5CNjt7h+fZLPHgHdb1XVAv+rnIs1LI/Qw1VNAux74PeBFM9sRL/tT4DwAd38QeILqlMU9VKctvqfxTRWRer325jDHhiefY777ULVqurRTgR6Sema5/BMw5TfCx7Nb3teoRonI7B0bLnLjXzxNxaferj3bphF6YHSIWyQwA6MlKg5b33E+my9YOel2a5Z10q7T+IOiQBcJTCm+EMUV65Zx4wxOGpL0069nkcDUriyUy+jj3Wq0x0UCUypXi+e5TLgXQ5aJKdBFAlPSCL1laY+LBKYUKdBblfa4SGBUQ29d2uMiganV0NsV6C1He1wkMCdr6FkdFG01CnSRwOigaOvSHhcJjEourUt7XCQwGqG3Lu1xkcCcCnTV0FuNAl0kMMXaPHR98VbL0R4XCYxq6K1Le1wkMLWSS7ZNJZdWo0AXCUypXMEMMgr0lqNAFwlMsVwhl2mjejlgaSUKdJHAlCJX/bxFaa+LBCaqVDRlsUUp0EUCki+VeWH/CTJt+mi3Iu11kYD82Vd28cKBfpYt0uWCW5ECXSQQlYrz9V2HuP7Clfzvd/cm3RxJgAJdJBD/5Wu7GCmW+VdvPZfzexYn3RxJgAJdJBDHhooA/PY1axNuiSRFgS4SiKjiXLpmKZ25TNJNkYQo0EUCEVUqZDVdsaUp0EUCEZVd39/S4hToIoGIKhWymn/e0rT3RQIRlV0llxanQBcJRFRxsvoOl5amvS8SiGrJRSP0VjZtoJvZZ8zsiJntmmT9FjPrN7Md8c+HG99MEZmODopKPV/48FngfuCRKbb5nrvf3pAWicisVEsuCvRWNu0I3d2/CxxbgLaIyBxEZc1yaXWN2vubzewFM/u6mV022UZmttXM+sys7+jRow16aREBjdClMYG+Hdjg7lcCnwS+OtmG7r7N3Xvdvbenp6cBLy0iNaqhy5wD3d0H3H0ovv0EkDOzVXNumYjMiKYtypz3vpmdY/HVaM3s2vg535zr84rIzGjaokw7y8XMPgdsAVaZ2QHgz4EcgLs/CNwBvNfMImAUuNPdfd5aLCITqpZcNEJvZdMGurvfNc36+6lOaxSRBOnbFkW/zkUCoYOiokAXCYC766CoKNBFQlCuVA9baYTe2hToIgGIaoGuGnpLU6CLBCDSCF1QoIsEISpXADRtscVp74sEoDZCz6nk0tIU6CIBiMrVQM9ohN7StPdFAhBV4pKLRugtTYEuEoDaCF0HRVubAl0kAF/b8XMAnVjU4rT3RQLw4sF+AK45b3nCLZEkKdBFAlAsV7hy/XLWrehKuimSIAW6SACKUZmOrD7OrU7vAJEAFKKKAl0U6CIhKEYV2nVAtOXpHSASgGJUoSOnj3Or0ztAJAAFjdAFBbpIEIpRhXbV0Fue3gEiASiWK3RkM0k3QxKmQBcJQKFU1ghdFOgiIaiO0PVxbnV6B4ikXKXilMquEboo0EXSrhhfrUiBLnoHiKRcIaoGug6KigJdJOWKkUboUqV3gEjCClGZV38xOKfHA3ToxKKWp3eASML+5As7+fVPfJeBfGlWj6+N0HXqv2STbkArOzKY54mdh4gv2F6XZYty/PY1azHTpcbSpFJxPviFFzhwfOSMdT/cdxyAwXzE0s7chI/ffWiA33ngGUZL5TPWefz+UQ1dFOgJeviZffz1U3tn/Li3rlvGxWcvmYcWyXw5MljgKz86yIWrF7N6ScfJ5f2jp0blQ/lo0se/+otBRopl3r15A8sXnRn6HbkMb79oVWMbLamjQE9Q/2iJFV05nv7jG+va/vt73+A/PrqdocLkH3xpTocH8gDce8sm/uWlZ59cfmy4yDX//UkAhgqTl1wG4rD/w5suZPWSznlsqaTZtIFuZp8BbgeOuPvlE6w34D7gNmAE+H13397ohoZouFBmcWeWZV0T/5k93lnd7QDki2f+2Z0GB0+M8sTOQzgzqDEFINPWxuKOajnknGWnh/FZ3e3cvXkDD//zawxOMUIfiEfyk5VkRKC+EfpngfuBRyZZfytwUfzzNuCB+F+ZxmA+oru9/j+SFuWqoTBRHTUNPv29n/K339+XdDMSsaSzup9XL+04Y93vXlcN9Kn+8hoYLdGebaMzpzq5TG7aNHH375rZxik2eSfwiLs78AMzW25ma9z9UIPaeJod+0/wyDP7ptxmy6bV/OaV587HyzfUcCFicccMAr093YE+MBqxZlkn3/xPNyTdlAX17x/u459/+iaZNmNV95mBXnsPTFVDH8iXWDZB7VxkrEbU0NcC+8fcPxAvOyPQzWwrsBXgvPPOm9WLHR8u8sPXjk26/o3BIq8cHkxHoBcjVnS11719ZzyLIV+qzFeT5lXtF1j3DH6JheCPf+NiHv3B67xlzVLa2s6cnbQ4Hr1PNULvHy2xtLO1/t9k5hb0HeLu24BtAL29vbMqpN64aTXf23TTpOv/6HM/YueBE7Nr4AIbKkSsX9FV9/ad7dV5xmkdoQ8Xo5YLc4Bf3nAWv7zhrEnX18pux4aLDE8S6seHNUKX6TXi03UQWD/m/rp4WSK6OzIMp+Sg4VB+hiWXuH6a1oOiw4WI7g7VgMfLtBlLOrN86um9fOrpyaex3rxp9QK2StKoEYH+GPCHZvb3VA+G9s9X/bweXe1ZRlIyra8acPXvgtoBsXxKR+gjxTKrFp9ZQxb45F1XT3v6/w0XK9BlavVMW/wcsAVYZWYHgD8HcgDu/iDwBNUpi3uoTlt8z3w1th7d7RlGSmWKUYW/+uarp5240WyGi+WT09nqkcu0kW2z1JZchmb4C6yVbLlkNVsuUWDL3NQzy+WuadY78L6GtWiOujqyuMP214/zqaf3srQz27TfQnf20g6u3rBiRo9ZlMukNtBHimWVXETmUXDDpdoI8PVj1e/MeOSet3HV+uVJNqmhOtszqS25DBVmNu9eRGYmuE9XdzxX+/U3q4G+srv+aYFp0JlrYzAfTTobollFFacYVVRyEZlHwX26utpPH6GHdhBucUeOx3ce4vGdiR13npMlmkstMm+C+3TVarSvHRuhuz1z8uzKUHz0X1/G868dT7oZs5Jta+O3rl6bdDNEghVcoNdG6C8d7Ofc5YsSbk3jTXeSioi0ruAC/bJzl/Jve9czWCjxjot6km6OiMiCCS7QO3MZPnbHFUk3Q0RkwTXnBG0REZkxBbqISCAU6CIigVCgi4gEQoEuIhIIBbqISCAU6CIigVCgi4gEwqpfZ57AC5sdBV6b5cNXAW80sDlJUl+ak/rSfELpB8ytLxvcfcLT4BML9Lkwsz537026HY2gvjQn9aX5hNIPmL++qOQiIhIIBbqISCDSGujbkm5AA6kvzUl9aT6h9APmqS+prKGLiMiZ0jpCFxGRcRToIiKBSF2gm9ktZvZjM9tjZvcm3Z7pmNlnzOyIme0as+wsM3vSzH4S/7siXm5m9r/ivu00s2uSa/npzGy9mT1lZi+b2Utm9v54eRr70mlmz5nZC3FfPhIv/yUzezZu8+fNrD1e3hHf3xOv35hk+ydiZhkz+5GZPR7fT2VfzGyfmb1oZjvMrC9elrr3GICZLTezL5rZK2a228w2z3dfUhXoZpYB/hq4FbgUuMvMLk22VdP6LHDLuGX3At9y94uAb8X3odqvi+KfrcADC9TGekTAB939UuA64H3x/30a+1IAbnL3K4GrgFvM7DrgY8An3P1C4DhwT7z9PcDxePkn4u2azfuB3WPup7kvN7r7VWPmaafxPQZwH/ANd98EXEl1/8xvX9w9NT/AZuAfxtz/EPChpNtVR7s3ArvG3P8xsCa+vQb4cXz7b4C7Jtqu2X6ArwG/lva+AF3AduBtVM/cy45/rwH/AGyOb2fj7Szpto/pw7o4HG4CHgcsxX3ZB6watyx17zFgGfCz8f+3892XVI3QgbXA/jH3D8TL0uZsdz8U3z4MnB3fTkX/4j/TrwaeJaV9iUsUO4AjwJPAXuCEu0fxJmPbe7Iv8fp+YOXCtnhKfwX8Z6AS319JevviwD+a2fNmtjVelsb32C8BR4G/jUthnzazbua5L2kL9OB49ddxauaOmtli4EvAB9x9YOy6NPXF3cvufhXV0e21wKaEmzQrZnY7cMTdn0+6LQ3ydne/hmoJ4n1m9o6xK1P0HssC1wAPuPvVwDCnyivA/PQlbYF+EFg/5v66eFna/MLM1gDE/x6Jlzd1/8wsRzXMH3X3L8eLU9mXGnc/ATxFtSyx3Myy8aqx7T3Zl3j9MuDNBW7qZK4HftPM9gF/T7Xsch/p7AvufjD+9wjwFaq/bNP4HjsAHHD3Z+P7X6Qa8PPal7QF+g+Bi+Ij+O3AncBjCbdpNh4D7o5v3021Hl1b/u74iPd1QP+YP88SZWYGPATsdvePj1mVxr70mNny+PYiqscCdlMN9jvizcb3pdbHO4Bvx6OrxLn7h9x9nbtvpPp5+La7/y4p7IuZdZvZktpt4NeBXaTwPebuh4H9ZnZJvOhm4GXmuy9JHzyYxcGG24BXqdY8/yzp9tTR3s8Bh4AS1d/a91CtWX4L+AnwTeCseFujOotnL/Ai0Jt0+8f04+1U/zzcCeyIf25LaV+uAH4U92UX8OF4+fnAc8Ae4AtAR7y8M76/J15/ftJ9mKRfW4DH09qXuM0vxD8v1T7faXyPxe27CuiL32dfBVbMd1906r+ISCDSVnIREZFJKNBFRAKhQBcRCYQCXUQkEAp0EZFAKNBFRAKhQBcRCcT/Bw0W4SYKhNwfAAAAAElFTkSuQmCC\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"code","metadata":{"id":"QYwWJxCSLF62"},"source":["min_max = MinMaxScaler()\n","cbo1 = min_max.fit_transform(cbo.values.reshape(-1, 1))\n","bbw1 = min_max.fit_transform(bbw.values.reshape(-1, 1))\n","\n","min_max = MinMaxScaler()\n","bbw2 = min_max.fit_transform(bbw.values.reshape(-1, 1))\n","\n","print(bbw1[-10:])\n","print(bbw2[-10:])\n","plt.plot(bbw1)\n","plt.plot(bbw2)\n","\n","plt.show()\n","\n","print(len(bbw1))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6XibtKgphXyQ"},"source":["### Check shuffled index"]},{"cell_type":"code","metadata":{"id":"KH8eEW8ChZtV","colab":{"base_uri":"https://localhost:8080/","height":244},"executionInfo":{"status":"error","timestamp":1621939962590,"user_tz":-540,"elapsed":415,"user":{"displayName":"7th June","photoUrl":"","userId":"08178289703395036410"}},"outputId":"4e77ff24-c30b-4b2d-d516-cdb6b60466b1"},"source":["# print(index_val)\n","from datetime import datetime\n","\n","# print(index_test)\n","# print(index_train)\n","# print(index_val)\n","total_stamp = list(map(lambda x: datetime.timestamp(x[0]), input_index)) \n","timestamp_train = list(map(lambda x: datetime.timestamp(x[0]), index_train)) \n","timestamp_val = list(map(lambda x: datetime.timestamp(x[0]), index_val)) \n","timestamp_test = list(map(lambda x: datetime.timestamp(x[0]), index_test)) \n","# print(total_stamp)\n","# print(timestamp_train)\n","plt.figure(figsize=(40, 4))\n","plt.scatter(range(len(timestamp_train)), timestamp_train, label='train')\n","plt.scatter(range(len(timestamp_val)), timestamp_val, color='orange', label='val')\n","plt.scatter(range(len(timestamp_test)), timestamp_test, color='red', label='test')\n","plt.ylim(min(total_stamp), max(total_stamp))\n","plt.legend(fontsize=20)\n","\n","# print(new_input_index)"],"execution_count":null,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-10-72a922dd9685>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# print(index_train)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# print(index_val)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mtotal_stamp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimestamp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mtimestamp_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimestamp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mtimestamp_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimestamp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'input_index' is not defined"]}]},{"cell_type":"markdown","metadata":{"id":"b1UEFg1GVSLS"},"source":["### Load Data"]},{"cell_type":"code","metadata":{"id":"oa0CYY1zKH0l","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1620033617416,"user_tz":-540,"elapsed":6824,"user":{"displayName":"7th June","photoUrl":"","userId":"08178289703395036410"}},"outputId":"54203fc6-893d-4a9f-d7f2-cc35eb640a39"},"source":["period = 45\n","\n","x_save_path = current_path + 'npy/' + '%s_rnn_close_updown_x_train_neo_timesplit.npy' % period\n","x_train = np.load(x_save_path)\n","x_val = np.load(x_save_path.replace('x_train', 'x_val'))\n","x_test = np.load(x_save_path.replace('x_train', 'x_test'))\n","print('x series loaded !')\n","\n","pr_save_path = current_path + 'npy/' + '%s_rnn_close_updown_pr_train_neo_timesplit.npy' % period\n","pr_train = np.load(pr_save_path)\n","pr_val = np.load(pr_save_path.replace('pr_train', 'pr_val'))\n","pr_test = np.load(pr_save_path.replace('pr_train', 'pr_test'))\n","print('y series loaded !')\n","\n","_, row, col = x_train.shape\n","\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["x series loaded !\n","y series loaded !\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"zscZynIgMbAq","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1620051746733,"user_tz":-540,"elapsed":5243,"user":{"displayName":"7th June","photoUrl":"","userId":"08178289703395036410"}},"outputId":"c236475b-70f6-4ba9-e175-72abc31c6800"},"source":["print(keras.__version__)\n","print(tf.__version__)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["2.3.1\n","1.15.2\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"-jo3k5MdhFyg"},"source":["#### **clustering output**"]},{"cell_type":"code","metadata":{"id":"njxxm-TJ-RP-"},"source":["# x_train_for_k = test_result.flatten().reshape(-1, 1)\n","x_train_for_k = test_result\n","print(x_train_for_k[:10])\n","# x_train_for_k = test_result[:, [1]]\n","pr_train = pr_test\n","\n","print('x_train_for_k.shape :', x_train_for_k.shape)\n","print('pr_train.shape :', pr_train.shape)\n","\n","K = range(2, 10)\n","s_dist = []\n","sil = []\n","for k in K:\n","  # if cen_data.shape[0] < k:\n","  #   break\n","\n","  km = KMeans(n_clusters=k)\n","  km = km.fit(x_train_for_k)\n","\n","  labels = km.labels_\n","  # print('len(labels) :', len(labels))\n","  # print('labels[:10] :', labels[:10])\n","  sil.append(silhouette_score(x_train_for_k, labels, metric='euclidean'))\n","\n","  # inertia = km.inertia_\n","  # s_dist.append(inertia)\n","\n","best_k = K[np.argmax(np.array(sil))]\n","\n","plt.figure(figsize=(10, 5))\n","plt.plot(K, sil)\n","plt.axvline(best_k, linestyle='--')\n","# plt.plot(K, s_dist)\n","plt.show()\n","\n","\n","\n","\n","\n","#   with best_k, label 별 pr_list 확인\n","km = KMeans(n_clusters=best_k)\n","km = km.fit(x_train_for_k)\n","\n","labels = km.labels_\n","\n","print(km.score(x_train_for_k))\n","print(len(labels), len(pr_train))\n","\n","\n","\n","\n","\n","#   label 별로 profit 을 저장, 승률을 확인한다\n","label_types = np.unique(labels, return_counts=False)\n","\n","label_pr_dict = {}\n","#   init dict   #\n","for label in label_types:\n","  label_pr_dict[label] = []\n","print(label_pr_dict)\n","# break\n","\n","for i, (label, pr) in enumerate(zip(labels, pr_train)):\n","  label_pr_dict[label].append(pr[0])\n","\n","  \n","# for label in label_types:\n","print(label_pr_dict)\n","\n","\n","\n","\n","\n","def win_ratio(list_x):\n","\n","  win_cnt = np.sum(np.array(list_x) > 1)\n","  return win_cnt / len(list_x)\n","\n","\n","def acc_pr(list_x):\n","\n","  return np.cumprod(np.array(list_x))[-1]\n","\n","\n","for key in label_pr_dict:\n","  \n","  print(key, ':', 'win_ratio : %.2f' % (win_ratio(label_pr_dict[key])), 'acc_pr : %.2f' % (acc_pr(label_pr_dict[key])))\n","\n","\n","\n","\n","\n","#     predict test && test 의 라벨에 따른 win_ratio 확인\n","# test_labels = km.predict(x_test)\n","# # print(test_labels)\n","\n","# label_pr_dict = {}\n","# #   init dict   #\n","# for label in label_types:\n","#   label_pr_dict[label] = []\n","# print(label_pr_dict)\n","# # break\n","\n","# for i, (label, pr) in enumerate(zip(test_labels, pr_test)):\n","#   label_pr_dict[label].append(pr[0])\n","\n","# for key in label_pr_dict:\n","\n","#   print(key, ':', 'win_ratio : %.2f' % (win_ratio(label_pr_dict[key])), 'acc_pr : %.2f' % (acc_pr(label_pr_dict[key])))\n","\n"],"execution_count":null,"outputs":[]}]}