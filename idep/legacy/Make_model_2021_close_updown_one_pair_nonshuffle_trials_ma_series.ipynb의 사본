{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python [conda env:tensorflow2_p36]","language":"python","name":"conda-env-tensorflow2_p36-py"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.5"},"colab":{"name":"Make_model_2021_close_updown_one_pair_nonshuffle_trials_ma_series.ipynb의 사본","provenance":[{"file_id":"1z4z_KLPzc6RWsxo3X_dHbpByxUAgjoMJ","timestamp":1583754134002}],"collapsed_sections":[],"toc_visible":true,"machine_shape":"hm"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AK9FjWwLOyay","executionInfo":{"status":"ok","timestamp":1619014872572,"user_tz":-540,"elapsed":26217,"user":{"displayName":"7th June","photoUrl":"","userId":"08178289703395036410"}},"outputId":"ce44b0a5-ba07-48db-8615-be1fba338eac"},"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","import os, sys\n","\n","current_path = '/content/drive/My Drive/Colab Notebooks/Project_Stock/'\n","\n","os.chdir(current_path)"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n","Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"8uqYv5StTazo"},"source":["### **Requirements**"]},{"cell_type":"code","metadata":{"id":"9qGt60DKTZmf","executionInfo":{"status":"ok","timestamp":1619014865102,"user_tz":-540,"elapsed":18785,"user":{"displayName":"7th June","photoUrl":"","userId":"08178289703395036410"}}},"source":["# !pip install statsmodels==0.12.2\n","\n","# import statsmodels\n","# statsmodels.__version__"],"execution_count":2,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"y7bVjhlwPI_-"},"source":["### **ARIMA**"]},{"cell_type":"code","metadata":{"id":"NvdpArctN_6l","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619014872564,"user_tz":-540,"elapsed":26230,"user":{"displayName":"7th June","photoUrl":"","userId":"08178289703395036410"}},"outputId":"fa0e6994-45fc-4227-cdbb-6be238461bac"},"source":["from statsmodels.tsa.arima_model import ARIMA\n","# from statsmodels.tsa.arima.model import ARIMA\n","\n","from datetime import datetime\n","\n","\n","def arima_test(target, use_rows=None):\n","\n","  size = int(len(target) * 0.66)\n","  train, test = target[:size].values, target[size:]\n","  test_shift = test.shift(1).values\n","  test = test.values\n","  # break\n","\n","  history = list(train)\n","  predictions = list()\n","  err_ranges = list()\n","  for t in range(len(test)):\n","    \n","      if use_rows is not None:\n","        history = history[-use_rows:]\n","        \n","      model = ARIMA(history, order=(0, 2, 4))\n","      model_fit = model.fit()\n","      output = model_fit.forecast()\n","      # print(output)\n","      # break\n","\n","      predictions.append(output[0])\n","      err_ranges.append(output[1])\n","      obs = test[t]\n","      # print('obs :', obs)\n","      history.append(obs)\n","      # break\n","      print('\\r %.2f%%' % (t / len(test) * 100), end='')\n","\n","  print(len(test), len(predictions))\n","\n","  return predictions, err_ranges\n","\n","\n","# print(high)\n","\n","\n","def get_back_result(ohlcv, predictions, err_ranges, tp=0.04, sl=None, leverage=1, show_detail=False, show_plot=False, return_pr=False, cumsum=False, \n","                    close_ver=False, reverse_short=False):\n","\n","  \n","  high, low, test = np.split(ohlcv.values[-len(predictions):, [1, 2, 3]], 3, axis=1)\n","\n","  if close_ver:\n","    predictions = ohlcv['close'].shift(1).values[-len(test):]\n","\n","  fee = 0.0006\n","  long_profits = []\n","  short_profits = []\n","  liquidations = []\n","  win_cnt = 0\n","  for i in range(len(test)):\n","\n","    long_ep = predictions[i]\n","    if sl is not None:\n","      long_sl = long_ep * (1 / (sl + 1))\n","\n","    # assert long_ep < long_exit, 'long_exit < long_ep !, %s, %s' % (long_exit, long_ep)\n","    \n","    short_ep = (predictions[i] + err_ranges[i]) * (1 + tp)\n","    # short_ep = (predictions[i] + err_ranges[i]) * (1 / (1 - tp))\n","    if sl is not None:\n","      short_sl = short_ep * (1 / (1 - sl))\n","\n","    # print((low[i]))\n","\n","    #    long 우선   # <-- long & short 둘다 체결된 상황에서는 long 체결을 우선으로 한다.\n","    if low[i] < long_ep:\n","      \n","      liquidation = low[i] / long_ep - fee\n","      l_liquidation = 1 + (liquidation - 1) * leverage\n","      liquidations.append(l_liquidation)\n","\n","      if max(l_liquidation, 0) == 0:\n","        l_profit = 0\n","        # print('low[i], long_ep, l_liquidation :', low[i], long_ep, l_liquidation)\n","      else:\n","\n","        if sl is not None:\n","          if low[i] < long_sl:\n","            profit = long_sl / long_ep - fee\n","          else:\n","            profit = test[i] / long_ep - fee\n","\n","        else:\n","          profit = test[i] / long_ep - fee\n","\n","        l_profit = 1 + (profit - 1) * leverage\n","        l_profit = max(l_profit, 0)\n","        \n","        if profit >= 1:\n","          win_cnt += 1\n","\n","      long_profits.append(l_profit)\n","      short_profits.append(1.0)\n","\n","      if show_detail:\n","        print(test[i], predictions[i], long_ep)\n","\n","    # if high[i] > short_ep > low[i]: # 지정 대기가 아니라, 해당 price 가 지나면, long 한다.\n","\n","    #   if not reverse_short:\n","    #     liquidation = short_ep / high[i]  - fee\n","    #   else:\n","    #     liquidation = low[i] / short_ep  - fee\n","    #   l_liquidation = 1 + (liquidation - 1) * leverage\n","\n","    #   if max(l_liquidation, 0) == 0:\n","    #     l_profit = 0\n","    #   else:\n","\n","    #     if sl is not None:\n","    #       if high[i] > short_sl:\n","\n","    #         if not reverse_short:\n","    #           profit = short_ep / short_sl - fee\n","    #         else:\n","    #           profit = short_sl / short_ep - fee\n","\n","    #       else:\n","    #         if not reverse_short:\n","    #           profit = short_ep / test[i] - fee\n","    #         else:\n","    #           profit = test[i] / short_ep - fee\n","\n","    #     else:\n","\n","    #       if not reverse_short:\n","    #         profit = short_ep / test[i] - fee\n","    #       else:\n","    #         profit = test[i] / short_ep - fee\n","\n","    #     l_profit = 1 + (profit - 1) * leverage\n","    #     l_profit = max(l_profit, 0)\n","\n","    #     if profit >= 1:\n","    #       win_cnt += 1\n","\n","    #   short_profits.append(l_profit)\n","    #   long_profits.append(1.0)\n","\n","    #   if show_detail:\n","    #     print(test[i], predictions[i], short_ep)\n","    \n","    else:\n","      long_profits.append(1.0)\n","      short_profits.append(1.0)\n","      liquidations.append(1.0)\n","\n","\n","  long_win_ratio = sum(np.array(long_profits) > 1.0) / sum(np.array(long_profits) != 1.0)\n","  short_win_ratio = sum(np.array(short_profits) > 1.0) / sum(np.array(short_profits) != 1.0)\n","  long_frequency = sum(np.array(long_profits) != 1.0) / len(test)\n","  short_frequency = sum(np.array(short_profits) != 1.0) / len(test)\n","  if not cumsum:\n","    long_accum_profit = np.array(long_profits).cumprod()\n","    short_accum_profit = np.array(short_profits).cumprod()\n","  else:\n","    long_accum_profit = (np.array(long_profits) - 1.0).cumsum()\n","    short_accum_profit = (np.array(short_profits) - 1.0).cumsum()\n","\n","  # print(win_ratio)\n","\n","  if show_plot:\n","\n","    plt.figure(figsize=(10, 5))\n","    plt.suptitle('tp=%.4f, lvrg=%d' % (tp, leverage))\n","\n","    plt.subplot(151)\n","    plt.plot(liquidations)\n","    plt.title('liquidations')\n","\n","    plt.subplot(152)\n","    plt.plot(long_profits)\n","    plt.title('Win Ratio : %.2f %%\\nrequency : %.2f %%' % (long_win_ratio * 100, long_frequency * 100), color='black')\n","    # plt.show()\n","\n","    # print()\n","    plt.subplot(153)\n","    plt.plot(long_accum_profit)\n","    plt.title('Accum_profit : %.2f' % long_accum_profit[-1], color='black')\n","\n","    plt.subplot(154)\n","    plt.plot(short_profits)\n","    plt.title('Win Ratio : %.2f %%\\nrequency : %.2f %%' % (short_win_ratio * 100, short_frequency * 100), color='black')\n","    # plt.show()\n","\n","    # print()\n","    plt.subplot(155)\n","    plt.plot(short_accum_profit)\n","    plt.title('Accum_profit : %.2f' % short_accum_profit[-1], color='black')\n","    plt.show()\n","\n","  return [long_win_ratio, short_win_ratio], [long_frequency, short_frequency], [long_accum_profit[-1], short_accum_profit[-1]], [long_profits, short_profits]\n","\n","\n","# get_back_result(tp=0.04, leverage=1, show_plot=True)\n"],"execution_count":3,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n","  import pandas.util.testing as tm\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"aDkU3tMiM2lO","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619014875442,"user_tz":-540,"elapsed":2853,"user":{"displayName":"7th June","photoUrl":"","userId":"08178289703395036410"}},"outputId":"aa056a4b-abc2-4380-e2ba-a792f50e06be"},"source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","interval = '30m'\n","date_path = './candlestick_concated/%s/2021-02-11/' % interval\n","file_list = os.listdir(date_path)\n","\n","print((file_list))"],"execution_count":5,"outputs":[{"output_type":"stream","text":["['2021-02-11 BTCUSDT.xlsx', '2021-02-11 ETHUSDT.xlsx', '2021-02-11 BCHUSDT.xlsx', '2021-02-11 XRPUSDT.xlsx', '2021-02-11 EOSUSDT.xlsx', '2021-02-11 LTCUSDT.xlsx', '2021-02-11 ETCUSDT.xlsx', '2021-02-11 LINKUSDT.xlsx', '2021-02-11 XLMUSDT.xlsx', '2021-02-11 ADAUSDT.xlsx', '2021-02-11 XMRUSDT.xlsx', '2021-02-11 SXPUSDT.xlsx', '2021-02-11 KAVAUSDT.xlsx', '2021-02-11 BANDUSDT.xlsx', '2021-02-11 DASHUSDT.xlsx', '2021-02-11 ZECUSDT.xlsx', '2021-02-11 XTZUSDT.xlsx', '2021-02-11 BNBUSDT.xlsx', '2021-02-11 ATOMUSDT.xlsx', '2021-02-11 ONTUSDT.xlsx', '2021-02-11 IOTAUSDT.xlsx', '2021-02-11 BATUSDT.xlsx', '2021-02-11 NEOUSDT.xlsx', '2021-02-11 QTUMUSDT.xlsx', '2021-02-11 WAVESUSDT.xlsx', '2021-02-11 MKRUSDT.xlsx', '2021-02-11 SNXUSDT.xlsx', '2021-02-11 DOTUSDT.xlsx', '2021-02-11 THETAUSDT.xlsx', '2021-02-11 ALGOUSDT.xlsx', '2021-02-11 KNCUSDT.xlsx', '2021-02-11 ZRXUSDT.xlsx', '2021-02-11 COMPUSDT.xlsx', '2021-02-11 OMGUSDT.xlsx']\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"0en4ihETQ32K"},"source":["### **Data Stacking**"]},{"cell_type":"code","metadata":{"id":"OgZyYJPg3RJa","executionInfo":{"status":"ok","timestamp":1619014891076,"user_tz":-540,"elapsed":763,"user":{"displayName":"7th June","photoUrl":"","userId":"08178289703395036410"}}},"source":["def resize_npy(x):\n","\n","  temp_x = []\n","\n","  for d_i, data in enumerate(x):\n","    # resized_data = cv2.resize(data, (row * 2, col * 2)) --> input image 홰손된다\n","    # resized_data = data.repeat(2, axis=0).repeat(2, axis=1)\n","    data = data.repeat(2, axis=0).repeat(2, axis=1)\n","    # resized_data = data.repeat(1, axis=0).repeat(1, axis=1)\n","    # cmapped = plt.cm.Set1(resized_data)[:, :, :3]  # Drop Alpha Channel\n","    \n","    if d_i == 0:\n","      plt.imshow(data)\n","      plt.show()\n","      # plt.imshow(resized_data)\n","      # plt.show()\n","    # print('resized_data.shape :', resized_data.shape)\n","    # break\n","    temp_x.append(data)\n","\n","  return temp_x"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"SvZuk1rPrUMe","colab":{"base_uri":"https://localhost:8080/","height":953},"executionInfo":{"status":"ok","timestamp":1619014924147,"user_tz":-540,"elapsed":22874,"user":{"displayName":"7th June","photoUrl":"","userId":"08178289703395036410"}},"outputId":"0a409a65-6176-460b-aa0c-43db5f323d92"},"source":["from sklearn.preprocessing import MinMaxScaler, MaxAbsScaler\n","import pickle\n","from sklearn.model_selection import train_test_split\n","\n","with open('./arima_result/arima_opt_profit_ls_only_long_result_%s.pickle' % interval, 'rb') as f:\n","  load_dict = pickle.load(f)\n","\n","candis = list(load_dict.keys())\n","long_index = 0\n","leverage = 5\n","prev_x = None\n","\n","seed = 1\n","random_state = 20\n","np.random.seed(seed)\n","\n","for i in range(len(candis)):\n","\n","  keys = [candis[i]]\n","  \n","  # if 'algo'.upper() not in candis[i]:\n","  #   continue\n","  if '2021-03-02 DOTUSDT.xlsx' in candis[i]:\n","    # print('')\n","    continue\n","\n","  if '04-08' not in candis[i]:  # <-- 04-08 includes all timestamp range\n","    continue\n","\n","  if 'eth'.upper() not in candis[i]:\n","    continue\n","\n","  # plt.figure(figsize=(35, 10))\n","  # plt.suptitle('%s %s' % (interval, keys))\n","\n","\n","  #         get tp parameter        #\n","\n","  # plt.subplot(1,10,3)\n","  # for key in keys:  \n","  #   # plt.plot(profit_result_dict[key]['tp_list'], profit_result_dict[key]['ap_list'])\n","  #   argmax = np.argmax(profit_result_dict[key]['ap_list'][:, [long_index]])\n","  #   peak_tp = profit_result_dict[key]['tp_list'][argmax]\n","  #   # plt.axvline(peak_tp, linestyle='--')\n","  #   # plt.title('acc profit, max at %.4f' % (peak_tp))  \n","\n","  # plt.subplot(1,10,4)\n","  # plt.title('max acc profit by leverage')  \n","  # for key in keys:  \n","  #   # plt.plot(profit_result_dict[key]['tp_list'], profit_result_dict[key]['max_ap_list'], label=key)\n","  #   argmax = np.argmax(profit_result_dict[key]['max_ap_list'][:, [long_index]])\n","  #   max_peak_tp = profit_result_dict[key]['tp_list'][argmax]\n","  #   # plt.axvline(max_peak_tp, linestyle='--')\n","  #   # plt.title('max acc profit, max at %.4f' % (max_peak_tp))  \n","\n","\n","  for key in keys:  \n","    # print(profit_result_dict[key]['leverage_ap_list'])\n","\n","    # for tp in [max_peak_tp]:\n","\n","      # if tp == peak_tp:\n","      #   plt.subplot(1,10,5)\n","      # else:\n","      #   plt.subplot(1,10,6)\n","\n","      #     leverage analysis     #\n","      ohlcv = load_dict[key]['ohlcv']\n","      # ohlcv = ohlcv.iloc[:-int(len(ohlcv) * 0.3)]  # exclude back_range\n","      # predictions = load_dict[key]['predictions']\n","      # err_ranges = load_dict[key]['err_ranges']\n","      print(\"ohlcv.index[0] :\", ohlcv.index[0])\n","      print(\"ohlcv.index[-1] :\", ohlcv.index[-1])\n","\n","      predictions = ohlcv['close'].shift(1).values\n","      err_ranges = np.zeros_like(predictions)\n","\n","      # leverage_list = profit_result_dict[key]['leverage_list']\n","      # temp_ap_list = list()\n","      # temp_pr_list = list()\n","\n","      try:\n","        print('-------------- %s --------------' % key)\n","        result = get_back_result(ohlcv, predictions, err_ranges, tp=0, leverage=leverage, show_plot=True, reverse_short=False, show_detail=False)\n","        # temp_ap_list.append(result[2])\n","        # temp_pr_list.append(result[3])\n","\n","        # if round(leverage) == 1:\n","        #   temp_pr_list = result[3]\n","        pr_list = result[3][long_index]\n","\n","      except Exception as e:\n","        print(e)\n","        break    \n","  # break\n","\n","\n","      pd.set_option('display.max_rows', 500)\n","      pd.set_option('display.max_columns', 500)\n","      pd.set_option('display.width', 1000)\n","\n","      #         clustering zone           #\n","\n","      #       set data features : ohlc, v, ep\n","      ohlc = ohlcv.iloc[-len(predictions):, :4]\n","      vol = ohlcv.iloc[-len(predictions):, [4]]\n","      long_ep = np.array(predictions)\n","      long_ep = long_ep.reshape(-1, 1)\n","\n","      #       ma series 7, 30, 60, 120    #\n","      ma7 = ohlcv['close'].rolling(7).mean()\n","      ma30 = ohlcv['close'].rolling(30).mean()\n","      ma60 = ohlcv['close'].rolling(60).mean()\n","      ma120 = ohlcv['close'].rolling(120).mean()\n","\n","\n","      ohlcv['u_wick'] = ohlcv['high'] / np.maximum(ohlcv['close'] , ohlcv['open'])\n","      ohlcv['d_wick'] = np.minimum(ohlcv['close'] , ohlcv['open']) / ohlcv['low']\n","      ohlcv['body'] = ohlcv['close'] / ohlcv['open']\n","\n","      candle = ohlcv.iloc[-len(predictions):, -3:]\n","\n","\n","      print('len(ohlc) :', len(ohlc))\n","      print('long_ep.shape :', long_ep.shape)\n","      print('len(pr_list) :', len(pr_list))\n","\n","\n","      #       set params    #\n","      period = 45\n","      data_x, data_pr, data_updown = [], [], []\n","      key_i = i\n","\n","      for i in range(period, len(predictions)):\n","\n","        #   pr_list != 1 인 데이터만 사용한다\n","        # if 1:\n","        if pr_list[i] != 1:\n","          \n","          #   prediction 을 제외한 이전 데이터를 사용해야한다\n","          temp_ohlc = ohlc.iloc[i - period : i].values\n","          temp_long_ep = long_ep[i - period : i]\n","          temp_vol = vol.iloc[i - period : i].values\n","          temp_candle = candle.iloc[i - period : i].values\n","\n","          temp_ma7 = ma7.iloc[i - period : i].values.reshape(-1, 1)\n","          temp_ma30 = ma30.iloc[i - period : i].values.reshape(-1, 1)\n","          temp_ma60 = ma60.iloc[i - period : i].values.reshape(-1, 1)\n","          temp_ma120 = ma120.iloc[i - period : i].values.reshape(-1, 1)\n","\n","          # print(temp_ohlc.shape)\n","          # print(temp_long_ep.shape)\n","          # print(temp_vol.shape)\n","          # print(temp_candle.shape)\n","          # break\n","\n","          #   stacking  \n","          # temp_data = np.hstack((temp_ohlc, temp_long_ep, temp_vol, temp_candle))\n","          # temp_data = np.hstack((temp_ohlc, temp_long_ep, temp_vol))\n","          temp_data = np.hstack((temp_ohlc, temp_long_ep))\n","\n","          # temp_data = np.hstack((temp_ohlc, temp_long_ep, temp_ma7, temp_vol))\n","          # temp_data = np.hstack((temp_ohlc, temp_long_ep, temp_ma30, temp_vol))\n","          # temp_data = np.hstack((temp_ohlc, temp_long_ep, temp_ma60, temp_vol))\n","          # temp_data = np.hstack((temp_ohlc, temp_long_ep, temp_ma120, temp_vol))\n","\n","\n","          # temp_data = np.hstack((temp_ohlc, temp_vol))\n","\n","          # temp_data = np.hstack((temp_ohlc, temp_long_ep))\n","          # temp_data = temp_vol\n","\n","          #   scaler 설정\n","\n","          #   ohlc & ep -> max_abs\n","          # max_abs = MaxAbsScaler()\n","          # temp_data[:, :5] = max_abs.fit_transform(temp_data[:, :5])\n","\n","\n","          min_max = MinMaxScaler()\n","          # temp_data[:, :5] = min_max.fit_transform(temp_data[:, :5])\n","          # # temp_data[:, :-1] = min_max.fit_transform(temp_data[:, :-1])\n","          temp_data[:, :] = min_max.fit_transform(temp_data[:, :])\n","\n","\n","          #   vol -> min_max\n","          # min_max = MinMaxScaler()\n","          # temp_data[:, [-1]] = min_max.fit_transform(temp_data[:, [-1]])\n","\n","\n","          #   candle -> max_abs    \n","          # max_abs = MaxAbsScaler()\n","          # temp_data[:, -3:] = max_abs.fit_transform(temp_data[:, -3:])\n","\n","          # min_max = MinMaxScaler()\n","          # temp_data[:, -3:] = min_max.fit_transform(temp_data[:, -3:])\n","\n","          if np.isnan(np.sum(temp_data)):\n","            continue\n","\n","          data_x.append(temp_data)\n","          data_pr.append(pr_list[i])\n","          data_updown.append(ohlc['close'].iloc[i] / ohlc['open'].iloc[i])\n","\n","\n","      print('np.array(data_x).shape :', np.array(data_x).shape)\n","      # print(data_x[0])\n","\n","\n","      #       Reshape data for image deep - learning     #\n","      _, row, col = np.array(data_x).shape\n","\n","      input_x = np.array(data_x).reshape(-1, row, col, 1).astype(np.float32)\n","\n","      #     1c to 3c    #\n","      input_x = input_x * np.ones(3, dtype=np.float32)[None, None, None, :]\n","      input_x = np.array(resize_npy(input_x))\n","\n","\n","      input_pr = np.array(data_pr).reshape(-1, 1).astype(np.float32)\n","      input_ud = np.array(data_updown).reshape(-1, 1).astype(np.float32)\n","      print('input_x.shape :', input_x.shape)\n","      print('input_x.dtype :', input_x.dtype)\n","      print('input_pr.shape :', input_pr.shape)\n","      print('input_ud.shape :', input_ud.shape)\n","\n","      \n","      # x_train_, x_test, pr_train_, pr_test, ud_train_, ud_test = train_test_split(input_x, input_pr, input_ud, test_size=0.4, shuffle=False, random_state=random_state)\n","      # x_train, x_val, pr_train, pr_val, ud_train, ud_val = train_test_split(x_train_, pr_train_, ud_train_, test_size=0.25, shuffle=False, random_state=random_state)\n","\n","      #     do stacking   #\n","      #     do stacking   #\n","      if prev_x is None:\n","        prev_x = input_x\n","        prev_pr = input_pr\n","        prev_ud = input_ud\n","      else:\n","        total_x = np.vstack((prev_x, input_x))\n","        total_pr = np.vstack((prev_pr, input_pr))\n","        total_ud = np.vstack((prev_ud, input_ud))\n","\n","        prev_x = total_x\n","        prev_pr = total_pr\n","        prev_ud = total_ud\n","\n","        print('total_x.shape :', total_x.shape)\n","        print('total_pr.shape :', total_pr.shape)\n","        print('total_ud.shape :', total_ud.shape)\n","      # if prev_train_x is None:\n","      #   prev_train_x = x_train\n","      #   prev_val_x = x_val\n","      #   prev_test_x = x_test\n","        \n","      #   prev_train_pr = pr_train\n","      #   prev_val_pr = pr_val\n","      #   prev_test_pr = pr_test\n","\n","      #   prev_train_ud = ud_train\n","      #   prev_val_ud = ud_val\n","      #   prev_test_ud = ud_test\n","\n","      # else:\n","\n","      #   total_train_x = np.vstack((prev_train_x, x_train))\n","      #   total_val_x = np.vstack((prev_val_x, x_val))\n","      #   total_test_x = np.vstack((prev_test_x, x_test))\n","      #   total_train_pr = np.vstack((prev_train_pr, pr_train))\n","      #   total_val_pr = np.vstack((prev_val_pr, pr_val))\n","      #   total_test_pr = np.vstack((prev_test_pr, pr_test))\n","      #   total_train_ud = np.vstack((prev_train_ud, ud_train))\n","      #   total_val_ud = np.vstack((prev_val_ud, ud_val))\n","      #   total_test_ud = np.vstack((prev_test_ud, ud_test))\n","        \n","      #   prev_train_x = total_train_x\n","      #   prev_val_x = total_val_x\n","      #   prev_test_x = total_test_x\n","        \n","      #   prev_train_pr = total_train_pr\n","      #   prev_val_pr = total_val_pr\n","      #   prev_test_pr = total_test_pr\n","\n","      #   prev_train_ud = total_train_ud\n","      #   prev_val_ud = total_val_ud\n","      #   prev_test_ud = total_test_ud\n","        \n","      #   print(\"total_train_x.shape :\", total_train_x.shape)\n","      #   print(\"total_val_x.shape :\", total_val_x.shape)\n","      #   print(\"total_test_x.shape :\", total_test_x.shape)\n","      #   print(\"total_train_pr.shape :\", total_train_pr.shape)\n","      #   print(\"total_val_pr.shape :\", total_val_pr.shape)\n","      #   print(\"total_test_pr.shape :\", total_test_pr.shape)\n","      #   print(\"total_train_ud.shape :\", total_train_ud.shape)\n","      #   print(\"total_val_ud.shape :\", total_val_ud.shape)\n","      #   print(\"total_test_ud.shape :\", total_test_ud.shape)\n","\n","  break # --> use only one pair dataset\n","\n","  #         chunks 로 나누지 않아도, generator 에서 batch_size 만큼만 load 할 것   #\n","  # try:\n","  #   if len(total_x) > 300000:\n","  #     break\n","  # except:\n","  #   pass\n","\n","  \n","        "],"execution_count":7,"outputs":[{"output_type":"stream","text":["ohlcv.index[0] : 2020-04-17 00:29:59.999000\n","ohlcv.index[-1] : 2021-04-08 20:29:59.999000\n","-------------- 2021-04-08 ETHUSDT.xlsx --------------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:158: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:159: RuntimeWarning: invalid value encountered in long_scalars\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:160: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:163: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n","/usr/local/lib/python3.7/dist-packages/numpy/core/_asarray.py:136: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n","  return array(a, dtype, copy=False, order=order, subok=True)\n"],"name":"stderr"},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAmUAAAFTCAYAAAB4RHsKAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nOzdeZgU1dX48e+BYRj2fRBmBkcEkUVQBPcgihEcIyZqiGiiqJGXn5ho3KLGuGvQLK9JcIkxia8aIWaFRJkYF2KiIqJRAqiAAsKAyo6CbMP5/XGrh5qe3rt6nfN5Hh6mq27fut1V1XXq3lv3iqpijDHGGGNyq0WuC2CMMcYYYywoM8YYY4zJCxaUGWOMMcbkAQvKjDHGGGPygAVlxhhjjDF5wIIyY4wxxpg8YEGZMcb4iMijInJnrsthjGl+LCgzphkSkZUickqG8v6OiHwkIttE5Nci0jpG2jEi8q6I7BCRF0XkQN+61t77t3n5XRXUe/OdFxjuFpHPfP9a5rpcxpjMsqDMGBMYERkLXA+MAQ4E+gK3RUnbHfgT8H2gK7AA+J0vya1Afy+fk4DrRGRcuu/NFBEpCTjLe1W1ve9ffcD5G2PyjAVlxjQzIvI40Af4q1cDc52IVIuIishkEVkrIutE5JoUsr8Q+JWqLlbVzcAdwKQoac8CFqvq71V1Jy6QGiYih/ryukNVN6vqO8AvfXml896Eicg7IvIl3+sSEVkvIsN939klIvIh8IKItBSRH4vIBhFZISKXe2mCDtiMMUXIgjJjmhlV/QbwIXCGVwNzr2/1SbgaplOB74aaOEXkPBHZEuNfH+/9g4G3ffm9DfQUkW4RitIorapuB94HBotIF6BXhLwGB/DeZMwAJvpejwU2qOqbvmUnAgO9dZcCpwGHA8OBL/szE5EHYnyHC8O2fZmIbBKRN0Tk7BTKbowpMBaUmawRkS+IyHu5LgeAiPSxfjoR3aaq21X1v8Bv8AISVX1SVTvH+Peh9/72wFZffqG/O0TYVnjaUPoO3jpomlcon3Tem4wngfEi0tZ7fR4uUPO71fvOPgcmAD9V1TVeTeE0f0JVvSzGdzjUl/RnuOC4HNdE+6iIHJ9C+VNm56sx2WdBmUmZiNwgInPCli2LsuxcVf2Xqg5IcVuTRKTe+2HeJiJv+5uVEnh/o47tqvphpvvpiEhXr6nr375lg0RkgYhs9v49JyKDYuRRLSLPeGk/EpHpoaYwETlERGZ529gkIn8Xkajfr1fbtU5EVgJlvuUHA3/0Xq72vWUV0DvJj/0Z0NH3OvT3pwmkDaX/1FsHTfMK5ZPOexOmqsuBd4AzvMBsPC5Q8/N/Z73DXq8mBar6pqpuVNW9qvoM8Ftck23K7HzNLRHp5J2jW0Tkt/4AU0QeFpG09q8pDhaUmXS8BBwX+nERkV5AK+CIsGX9vLTpelVV2wOdgQeAmSLSOYB8M+Ue3AXdby1wDq5zendgNjAzRh4PAJ/gmuMOxzWVXeat6+y9fwDQE5gPzIqUiRfITcM1qV3ubT/kZ7i+XwBVvuV9vPIiIudL4ycBw/+Fmi8XA8N8eQwDPlbVjRGK1SitiLQDDsb1FdsMrIuQ1+IA3pusUBPmmcASL1DzU9/f64BK32v/94mIPBTjO4xVPgUkxfKH2PmaW/8D/Ad3rlYDXwEQkWOB3qr6p9wVzeQNVbV/9i+lf0ApsAM40ns9Adfk9c+wZcu9v0cDa3zvXwlcAyzENS/9DiiLsq1JwL99r9viLlQjvdcHAy8AG4ENuJqFzt66x4F9wOe4WpTrcD+KCpR4aXrjApxNwHLg0jS/m+OAV4GL/OUOS1MCTAV2xMjnHaDG9/qHwC+ipO3qfaZuEdb1xF0kwdWS1QOTcQHiw77v47fedzsYFwyemuTnHgd8BAzCXYxfAKZFSdvD2+9ne2W6B5jnWz/NO5a6AIfiAp5x6b7XW6/A6CjlehS40/e6F+44fwm4wre80THkLft/uOCvwvv8/whPk+D3eA6uGbYFrn/fp9HKa+dreucrMBd3U/Ky9z0/C3T3rf897pje6h0Dg8OOlfuBp733vgYcHGU7DwJjfcfndUBLYB7QN519a/+K55/VlJmUqepu3I/QKG/RKOBfwL/DlsW6656Au5AfBAwlgSfkvLv6i4A9uCY2cLUIP8D9WA/E1VDc6pUzVsf2kJnAGu/95wB3i8jJUbZ/XoRO2eHlm46rkdIoabYAO4GfA3fH+Lj3AeeKSFsRqcB1Iq+NknYU8JFGrpVaD3QTkUrgi8AHuL5Kv8N97pB/4i5yzwM/UtVnY5StCVWtBe4FXsR956uAW0LrRWSxiJzvpV2PC6ruAjYDRwPn+rK7Bdd5f5VXrh96+af1XhGpwl1A/5vgZ1qHC7CPo/GwG5H8EndRX4irFXkG2IsLgpNxBVAHbMEF4peq6twk82jEzteYzvPKWI4LXv1PHs9hf/++N3EBpN+5uGFfuuDOnbuibGMRcIqItAG+gAvevw3MUdUP4pTPNBe5jgrtX2H/w/2Q/tn7+23cj9e4sGUXen+Ppumd99d9r+8FHoqynUm4i9sW3I/758CEGOX6MvCfsG2d4ntdjXfnjbsg1AMdfOt/ADya4nfyHeBBX7mj1ZS1wzVFnh4jr4HAG95nV9yduURIV4m7iE+MkdcY3F35P3FNoT8BLsE9cfmql/+wXB9TWThmvw78IEvbOg1YlevP7CuPna9Ntz0XuMn3+jKgNkrazl45OnmvHwUe8a2vAd6N8t4yXK30QlxNWSUuyOsEPIQLhu9M5TPYv+L5ZzVlJl0vASeISFegh6ouA17B9V3pCgwh9p33R76/d7D/yblI5qlqZ9wd6Wzc3SYAItJTRGaKSJ2IbAOewPXZSkRvYJOq+juCr8I1QSVFRHrj7n6/Fy+tumEcHgIeE5HyCHm1wNWK/QkXwHXHffZ7wtL1wNXOPKCq4U8G+rf3vKoeo6on4i4sI3AXlcfYXzPwi3jlLnSq+oSq3pCJvEWkjYjUiBvPrAJXY/fnTGwrRXa+Rhbxc4kbd26aiLzvlXOll6Z7vPeGU9WdqjpZVYeq6vXA/wI3AufjmqlPBI6WDA9ybPKbBWUmXa/i7vQuxfXJQFW34TqIXwqsVdUVQW5QVT/D9d35hogc4S2+GxdoHKaqHXG1If6O0RGbET1rga4i4h8yoQ+u5ilZR+H6IC0RkY+AnwJHeU9ORnqcvwWuv02kC0pXrxzTVXWXumbJ3+DuxgEQNybXs8BsVY3WbNKIiAiuefXbuItLS/Z/1qHR3mcSIrimrM245st3gJtzWqLG7HxNznm4BzxOwX1v1d7ytB668AIvUdekfhiwQFUVNzOFnYPNmAVlJi3qxmZaAFyF658S8m9vWRBPcUXa7ibgEfZf8DrgOgVv9Woorg17y8e4KX8i5bUaV1vwAxEpE5GhuGa9J1Io2hzcD/fh3r+bcRfnw1W1XkS+KCJHeHfgHXFNiJtp+pQmqroBWAH8P6/mpTNupPqFAN77/w687N15J+qbwJuq+hauo3UbXGB4Gq6vmUmRqu5Q1ZGq2kFVy1X1Ii/oyQt2viatA7ALd560JXb/z4SISBmu+fJKb9EKYLSIlALHY+dgs2ZBmQnCP3GdYP/tW/Yvb1lGfuQ99wE13o/ybbjhHrbinoQKf7z8B8BN3hhBkaYPmogLptbimptuUdXnIm3UGx4i4vAFXo3WR6F/Xnn2eH+D65Myw1v+Pu4ptHHqpgpCRG4MGzfqLFyfn/W4TsR7cH3WwD1SPxK4KMrwFJHK3h3Xifz7Xnn34h5IeAHXlPqtaO81RcPO18Q9hmsarQOW4PpkputG4LeqGnrA5he4Guv1uIcX8qm522SZuBpTY4wxxhiTS1ZTZowxxhiTBywoM8YYY4zJAxaUGWOMMcbkAQvKjDHGGGPyQFEGZSKyUkRO8Z5keySgPBeLyOgo60aLyJpI6xLM+yER+X7KhTPGBEpEBojIWyLyqYh8285RY/JDsZ+bRRmUhajq3ar6zYDyGqxpzj0HICKTRMT/KDqqOkVV70g3b5MfROQMEVnkDU/xiogM8q17KGz4il0i8mms/Lz3XSAiKiJNjmcRKRWRd2LdGIhIlYjME5FNIvLjsHVzRGRE2LK5IrJZRFon9qmLznXAi954Yz/zn6Pp3oR5ebQWkV+LyDZvYOGr4qT/jpdum/e+1r511SLyoojsEJF3ReSUdMrWXIjI4SLyhve9vSEih8dI21VE/iwi20VklYicF7b+PG/5dhH5i7jZESLlUyJuJoMtIlLrjTUYWndjvOPAS2fnZmbPzQne7/YOEZmbQPqo+z7ecRNJUQdlJvtEpCTXZcglEemPm7B4Cm5Msr8Cs0Pfi/cD0j70Dzdm2e/j5NkFN7ZRtLGWrsWNcRTLDcD/4SaS/nIoCBORrwErVHWBb3vVuClxFBgfJ9+Ck+AxeiDRv+8g3Iqbd/JA3Nyj10mU6XVEZCxwPW7u0gNxg6re5ksyAzdAcTfc9F5/EDf1VlR2nkopMAs34GwX3Lkxy1seyf3AbqAnblqkB0VksJfXYNxYY9/w1u8AHoiSz1m486o7boy2yV4eB+HOtZ/FKXc1dm5m+tzchBtTb1q8hAns+6jHTVS5nnwzE//wJrPF/fA94Vv+DdxAgBtxP14r8Sa9xc0BeKcv7WiaTsYbStvGS78ZN6DgtWFpr8cNDPqpt/4r3vKBwE7cZLqfAVuibPtS3EChm3BzxvX2rVPcBX8ZbrLf+9k/3lw/3MCQW4ENwO+y+H1/FzfS/C7cpMHH4Ebd3oKb5Hi0L/1BXjk/Bf6Bm/LniUjfe4TvvoXv+90IPAV09dZVe9/PhcCH3nfwPV8+LXHBTWjfvIGb3Ph+4Mdh25wNfCeF7+Jy4Gnf6xa4yZjHREjbzivHiXHyfAg3SfJc4Jth6w7CzQZwWvj3FpZuDjDA+3smMAHoiLuYdw5LezNuCp6fAH8LW1eFG+hzvff9Tw87bt9h/3E/3HfM9vOlazjeQ/sbd/f7CbAONzl1DbAUdw7cmMD3fivwB+B33vbfxDe5OpGP0fG4H/ct3nc70Ev7Au4c3Yk7Tw8JldnbZ58D+7x1n+E7P5M4TtYCp/pe3wHMjJL2SeBu3+sxwEfe34d4n8c/Ofe/gCl2nsb8/k/FDQgrvmUf4gZyjnSe7gYO8S17HJjm/X038KRv3cFe+g4R8vou8D/e31Nw89WCu3k7PoFy27mZ4XPTV65vAnPjpIm67+MdN9H+NZuaMq8J6UFcYNYbd1dZmWJ2t+C+/IOBsbgfF7/3cXcznXB3tE+ISC9VfQd3Ir6qrqakc4RynowbzXoCbg7FVbiLqN+XcCO5D/XSjfWW34GbB7GL99l+nuLnS8VE4HRc7VBP3Cjdd+Lmb7wG+KPv7v1J3A9td6/M4d9fLN/C/TCciNuPm3E/1n4nAANwF6+bRWSgt/wqr5w1uIDkYtydzf8BE8VNAB4a9f4Ur5xNiMjfRCTWtEYS9rfgJnoOdzbuBzTqKOoichRu4vCHoiT5Oe4C9nmM8gAsAr4obqqmI3E/eHcA96nqlrC0F+Bq+34LjBWRnl5ZWgJ/wx2T1bj5Omd6676K+/G9APfdjsddGBJxAFDm5Xcz8EvcXIhH4s6j73s1CfGciat17Irbd38RkVa+9f5jtC+uhulKoAfwDPBXESlV1ZNxgc3l3nm6NJSBuknkT8PNERmq8VwbXhCvSWNhpEJ6NZ+9cEFQyNtAtDvowRHS9hSRbt66D7Tx5Nyx8rLz1BkMLFTvSulZSOTv7RBgr/84oPF33Gj/qOr7eBfjCHktAk72mh5PAhaLyFeADar6cpSy+tm5mcFzMwWx9n284yaiZhOUAefg7ixeUtVduGlm9qWY1wTgLlXdpG4etkZVzqr6e1Vdq6r7VPV3uFqtoxLM+3zg16r6plfOG4BjvWrrkGmqukVVPwRexM2xCG4KngNxdwc7VbVR37UM+5mqrlY3t97XgWdU9RnvO/gHbr69GnFTAI0Evq9uSqKXcHeJiZqCu6te430/twLnhFV736aqn6vq27iTYJi3/JvATar6njpvq+pGVZ2Pq10c46U7F3eH9HGkAqjql1Q1WtX2c8CJXt+GUlzAVIqbNy/chcBjYReGBt4P7QO4H6Amx6r3Y95SVROZluUHuB/Rf3p5luKC+r+KyJMi8pKIXC4iJ+COoadU9Q3cDUaoH8RRuAvstaq6PewY+yZwr6q+7n23y1V1VQLlAnfc3qWqe3AXku7AT1X1U1VdjLuzHxYrA88bqvoHL5+f4C4mx/jW+4/Rr+FqNP/hpf8Rrgb8uATLHJOqPqmq0SaWbu/9v9W3bCvu7jpa+vC0eOnD18XLy85TJ5nvrT0QPn+pP20yeT2Dm+vydS/NTNxN/nUicpd3Hj4QqRnVzs2snJvJirXv4x03ETWnoKw3sDr0wouqE71biJkX7u6kgbhO2W95nTm34GpJuieRd0N+qvqZV84KX5qPfH/vYP+P/HW4Wpn54p4WvTjBbQbB/30cCHw19Pm97+AEXO1Ab2Cz9/2HJPoDEcr7z75838FVZ/f0pYn2/VThfsgi+T/cRQrv/8eTKFMDVX0XF2xNx1X3d8f9cDXqfOpd9Ebj5taL5jLc3XyT+fZEpB1wL/DtBMu1SVW/pqrDgJ/iati+hWtiWoSrcZiCmxfzWXWToYO7qw3VkFQBq9TNlxku1ncbz0ZVrff+DtX4+S+0n7N/H8biP7/34b7z3pHW0/Q82+et959nmfKZ939H37KOuKadaOnD0+KlD18XLy87T51kvrd4aRPOywuKrlfVoao6GXf+PYQLgEfgahZLcbWD4S7Ezs18E2vfJ3tuAq7ttrlYh+vTBYCItMU1YYZsp3FtxgFx8qpif2fDhgmgReRAXBXvGFwzZb2IvMX+Jq14k42uxf2ghfJr55WzLs77UDfp9aXe+04AnhORl1R1ebz3BsD/uVYDj6vqpeGJvO+ni4i08/3g9/G9v9F+8GqL/J2WVwMXa4Sq/rDaxEhW45qcF0VY9wSwSESG4Y6Tv8TJKypV/QOuDwVec+EluDtjv28AL6vqBzGyGoOrdavxXncFjhD3lNgjuGaKf4kIuB/yTiLyEXCMqq6Mke9kYJ6qLhKRw4D/VdXdIrIE1zS+z8sHoDXQ2fteVgN9RKQkwo9/6LuNZAdNz620npCKoir0h9fEVYk7n0L8x+ha4DBfevHeH/c8I/45HPvNqptFZB2uhuEf3uJhRO+8vNhb/5Qv7cequlHcRNt9RaSD7m/CHEaUJj3sPA1ZDFwtIuKrqR5K0yZWcP2nSkSkv6ou85b591do/wAgIn1x581SYvDOveNw/amuxdUmqYi8Tljtk4i0wbXQtLRzM6ZsT+Yda9/vI/ZxE1Fzqin7A/AlETnBqxq+ncaf/y1ctX1XETkA154dzVPADSLSRUQqcTUOIe1wB8Z6ABG5iMb9iT4GKiNVT3tmABeJe1y7Na4j4WtxLrJ42/qqVx5wfTiU1Jto0/EEcIaIjBWRliJS5jXnVXrV5guA28QN5XACcIbvvUuBMhE53etzcBPuIA95CLjLu2ggIj1E5MwEy/UIcIeI9BdnqLh+OajqGlzg9DjwR68aPSUicqT3uXsADwOzvRo0vwtwHVRjmYS78Bzu/VuA66P4PdwFq8q37pu4Y+twGt91hpetHJiKa04C15Rykoi0xzVv7gEG+fIdiOvDcQEwH3dDMk1E2nn79Xgvn0eAa7zPLiLSL7SPcOfWed53Mg5XG5AJR4rIWV4T2ZW4TsNNahk9TwGni8gY7zi72kv/SgLb+RjoJiKd0ijrY8BN3m/IobibqUdjpL1ERAZ5Qf5NobTq+qu8Bdzi7Y+v4IKLPyZQhuZ8ns7F1dx9W9zwJJd7y18IT+gFpX8CbveO++NxfaRCtXS/xX2PXxB3E3078Cdt3M+vES/QmA5826sJWgGErk0nAuE3a1/2ymvnZmxpn5uhcwFXadXC+y5bRUkedd8ncNxE1GyCMq/9eyruDnIdLmjx3xE8juvXsBLXWf53MbK7DVe9usJL2/Alq+oS4MfAq7gD5DDc0zIhL+Ai5Y9EZANhVPU5XH+3P3rlPBjXdyIRI4HXROQz3FNJV8SpickIdf3szsT1p1qPCxKuZf/xdh5wNO7pnVvwNeGp6lZcs90juDuj7TTeTz/FfbZnxY3vNc/LKxE/wZ3wz+La+n+F66sQ8n+4/RXzpBE3rteNMZL8FPfU0Hu446xRTYSIHIu7U2wyFIY/b3X9Bj8K/cN1IN2mqltVdW/Yuk3APu91fXi+Pj8CbveaxcH1NTsZt492Ao+o6odheU/H9XUU3IW5H+5JtTW4/h+o6u+Bu3Dn16e4GozQeD1XeO/b4uWTci1kHLO88mzG1USe5fVJaUJV38M1f/0c9/TfGcAZqro73ka8AHsG8IG45rne4WlE5HxxtVjR3IJrUlqF6+f3Q1Wt9d7bR9wYdn287dXimqpfxH3vq7z3h5yLa/rajHuM/xxVjTdESrM+T739/GVcQLMF11z45dD+Fzdm2BzfWy7zyvAJbt//P++aErq2TMFdoD/B9Rm6LM5nvAhY5PUNA3fxXovbD91wN3N+FwK/sXMztoDOzW/gmmUfxN2ofo5r/Qq9/zMR+YK3vXj7PupxE01oKIVmSURW4oYYeC7XZWnORORW3GPZX4+XNsPlGIWrPThQm/OJUYDy5RgqZvnyHdt5Wljy5bgpFM2mpsyYWLzq6StwNUX2Q29MHrLz1BQ7C8pMsydufKQtuKfO7stxcUwUXnPUZxH+xWpKNkXCztP8ZedmcJp186UxxhhjTL6wmjJjjDHGmDxgQZkxxhhjTB7I28Fju3fvrtXV1bkuRrP2xhtvbFDVHvFTxmf7M7dsXxYX25/Fw/Zl8QhiX+ZtUFZdXc2CBQtyXYxmTUSSmVYlJtufuWX7srjY/iweti+LRxD70povjTHGGGPygAVlxhhjjDF5wIIyY4wxxpg8YEGZMcYYY0wesKDMGGOMMSYPWFBmjDHGGJMHLCgzxhhjjMkDgQRlIvJrEflERBZFWS8i8jMRWS4iC0VkeBDbNcYYY4wpFkHVlD0KjIux/jSgv/dvMvBgQNs1xhhjjCkKgYzor6oviUh1jCRnAo+pqgLzRKSziPRS1XWJbuOe2nd5cO77AHxxUE+6tStFBOq27KS8Q2uWrN3GknXbAOhX3p4v9O/Ob15e2SiPw6s689bqLfQvb8+yTz5rso1hVZ0ZVtmJpR9/yrwPNkUtS+e2rTi4R3u279qLKrRu1YIV67fz6a69HNyjHcce3I0n5n3Y5H2H9GxPz45l/GvZBnp3KmPvPqVL21Le+/hTAPp2b8fgik789e21DKnoyNDKznRtW8qe+n0sWLWZN1ZtpqJzG/4y9XhG3vUch1d1Zsyh5XRs04oVG7bTq1MZP5jzbsP2zju6D797fTX1+5TSli340rBerNywnY+37aJ+n/LRtp0AHN+vG7+6cCRlrVomujtMhvx3zVYqurSha7vSXBclYW9+uJn+5e3pUNYq10UxxpiCJi5OCiAjF5T9TVWHRFj3N2Caqv7be/088F1VXRCWbjKuJo0+ffocuWrV/hkLqq9/OpBymuhWTju90WsReUNVRwSR94gRI9Sm/4iv+vqnObBbW/557UmB5pupfblzTz2Hfr+WY/p2ZebkY4PI3iTAzs3iYfuyeASxL/Oqo7+qPqyqI1R1RI8egczPapqh5Z98xqvvb8x1MVK2auOOXBchYXv3uZu6hWu25rgkxhhT+LI1IXkdUOV7XektM3lk1956WpcUfhPmKT/5J9C05s8ET7z/A6pwN8aYZi1bNWWzgQu8pzCPAbYm05/MZMfuvftyXQQTsIsvvpjy8nKAwZHWp/tktEj8NMYYYxIT1JAYM4BXgQEiskZELhGRKSIyxUvyDPABsBz4JXBZENs1wWphV9iiM2nSJGpra2MlCeTJaMWqyowxJl1BPX05Mc56BaYGsS2TOSUtLSgrNqNGjWLlypWxkqT1ZHQokLfmS2OMSV9edfQ3uWUX1mapAljte73GW5YUO3aMMSZ9FpSZBvvsymqiEJHJIrJARBasX7++YXnLFq6mbFhVp1wVzRhjioYFZcY0bwk9GR1tuJpWLVsgAsf07Zb5khpjTJGzoMw0KG1ph0MzlPaT0YI1XxpjTBCyNU6ZKQAlFpQVnYkTJzJ37lyA1iKyBrgFaAWgqg/hnoyuwT0ZvQO4KNltiIg9fWmMMQGwoMyYHPho6062fr6HAQd0yOh2ZsyYAYCIvBlp+o8gnoy2mjJjjAmGBWXG5MAxP3geKI5ZB0SwejJjjAmAtVcZk0P31r6LFng105565cG577NzT32ui2KMMQXNgjJjcuiBue+zpz52UKaqTH3yTV5ZviFLpUrN9l17c10EY4wpaBaUGZOHFtVt5ft/WYSqsmvvPp5euI5Jj76e62IZY4zJoGYTlPXqVJbrIhiTsPN+OY/H561i2+f7a59swnhjjCluzSYoO2NY74a/xw0+gLGDezZaf8GxB2a7SMYAkSfzLuxeZsYYY1LRbIKyffv2X+Yqu7ThB2cN5X9O7NuwrG/3drkolmmG9tYnUeNVQHPEWyBpjDHpaTZBmYRd3Lq2K+WG0wY2vA76gnLNqYek/N73764JsCTNl6oy66265IKgLFi7ZWej14vqtvHpzj18unNPjkpkjDEmHzSboCybFtx0Cv17pjYoaI8OrWmRw9qRiy++mPLycoYMGRJxvYicLyILReS/IvKKiAzLchETNvvttVwx8y1+8dIHuS5KTP9Y8jGH3fosh9367P6FVu1kjDHNTrMJyuINBZXKUFHlHVpHXN69feuUx57q3akMCa/Wy6JJkyZRW1sbK8kK4ERVPQy4A3g4KwVLwebtuwH4ZNvOqGleWb6B2kUfZatIANSHHRuxpijK4aFgciTejZH321IlIsu9G6Th/vUi0lFE1ojI9CwU18Rg+9Ikq9kEZfsyUPNw1EFdo64r1PFAR40aRdeusT6XvqKqm72X84DKrBQsBYnsgvMeeY0pT7wRM82f/7OG2/+6JOHtfr67nllv1UVc98c31gJt1IkAACAASURBVHDL7MVx8yjQw8cEIN6N0Zw5cwDKgP7AZODBsCR3AC9lqnwmcbYvTbKaTVA2srpLw9/+2of53xvDy9efnFKesQKvZnJRvQSYk+tCxJNuzeN3fvc2v355RcLpb/vrYq6Y+Ravr9zUZN3Vv3+bl5auTzivQqooK9QbkXwT78Zo1qxZABvVmQd0FpFeACJyJNATeDZqBiZrbF+aZBVMUHZhgkNWPHj+8CbLag47gMoubSOmL+9QRkXnNikFUYpyQr/ukdeleoEqkPYqETkJF5R9N0aaySKyQEQWrF+feCASlFwFCeu2uubSzxIc4V4ihF6FPvWSyZy6ujqA3b5Fa4AKEWkB/Bi4JhflMsmzfWnCFUxQlqiWLYThfTo3WZ6JWEcVTugfJShLsa6sEEIyERkKPAKcqaobo6VT1YdVdYSqjujRo0f2ClhgYvcpE6uBMom6DHhGVdfES5jrGyYTl+3LZqrogrIWEaIvQRjcuyNlraJ/3FRqJmI2X8bJ7t6zhya9vXwgIn2APwHfUNWluS5PLJmOZVZv2hHMfI8RCrp9t5vc+72PPk0/f1NUKioqAEp9iyqBOuBY4HIRWQn8CLhARKZFysNumPKD7UsTrmCCskQvsNFqxESE75xySMPfQQgP8uZ/bwy1V34BiF/e4Qd2iZMiNyZOnMixxx7Le++9BzBURC4RkSkiMsVLcjPQDXhARN4SkQXZKtvfF3/E0wvXJf2+TLUIf+HeFzn/kdfSzufp/0b/THPf+yTt/E1xGT9+PEA3cY4BtqrqOlU9X1X7qGo1rtnrMVW9PpdlNbHZvjThSnJdgKAdd3B37n9xeeOFaVyUX7txDO+v/4zzftn04nvcwd3ZuH1/d4DyDmWUd3BzbIbXvJWWtEho7sJcdSn7gtcMO2PGDF9ZZKGq/sqfTlW/CXwzq4Xz/M/j7inJ04eenlB6/z5Y9vGnfPF/X+L5q0/k4B7tAyvTW6u3pJ3Hms2fB1ASUywmTpzI3Llz2bBhA5WVldx2223s2eMGFp4yZQo1NTUAu4DlwA7gotyV1sRi+9Ikq2CCskRbF9uUtgx0uz07ltGzY5TJzJMIoP5+5ShO+tHcQMoUtDMP782Pvpq3Y8Cm7M6n3wFc8/Wst9YC8MzCdXxrTP9cFisphdDH0ATLf2MUiVfT/6GqjoiWRlUfBR4NslwmebYvTbIKJigLOaq6K/MjDDUQS+jCls3+0uFB5IFdw5/+zH3v7fatS/jNRSM5rKITrVoWTEt2SlJ98CLNjaZPJDdlN8YYk3UFcyW++ISDqOrahunnHxE3bbxLWKTah6CfcAtdSI/p25X5N45JuFkykzUjxx3crcmykdVdKWsVbO1ivhHZv3/9+2Fbhuaa3Bd2MC3/5DMenPt+SnlZTZkxxjQfBROUHdS9Hf+67uSGPlvJyOS0RacMLI+4PHRdPqBjGeUdE586KZNlLZAh0DIiFCb5v98FvhrXF98NpkP9tp17+NeyDY2WnfPQK9xT+y4799QnnV/4PvvjG3GfkDfGGFOgCiYoi+THvn5QZw9Pb7afSE9DHlUdfSRmcLUY0SYeDwVlkYboiOWMob2SSp+oxy85invPGcb5R/dpWNZcYrRVG3fETfPXhWsD2dbXIzyNuWNX8sFYNHc8nfh0T9lmzazGGJOeQIIyERknIu95k6o2eWxXRPqIyIsi8h9v0tWadLbXp0n/LPjxhGGsnBb5qbxEgo8jIwRlpwyKXAvWkG+MgKtViftqk33w4MLjqpNKn6jhfbpQ0bkNd33lsIzkn8+ee+fjiM2Xf/MNr/GnNyPPVZmshWu2NlkWClaCqKncsmMPtYuSHxbEGGNM/ks7KBORlsD9wGnAIGCiiAwKS3YT8JSqHgGcCzyQzjZnTT2eZ779haTfl82R0WuGHMAVY/pz/WmHJvW+SIHePWfHDqRmTT0+gXyTKkbG3ffc0qwOjNoQGPlC9KACsbjbDgWEKdZNhh+3U554k/97ZSXV1z/Npu27I7/JGGNMwQmipuwoYLmqfqCqu4GZwJlhaRTo6P3dCUirrahLu1IG9e4YP6GnSUAS5do495rRjV7HC+JiXWJLWrbgO188hA5lreIVL67Skti7aVhV02ml8tnOPfXc99wyznrg5extNEJNWZY3TYsAtz3z9dUArN1iY5wZY0yxCCIoqwBW+16v8Zb53Qp8XUTWAM8A3wpgu1F7sISCqWT7mVV3bxc1/5euPYl5N4xptD50gR89oAftAh4fLVFnDOudk+0GYe8+RVV5euE6du3d3+9q5576Rq+D0NDRP9BcG7vk0dfDtum2GhrENsiHON5Zty2wvAJjXcqMMSYt2eroPxF4VFUrgRrgcRFpsu2gJ1Zt19oFSqleCv01ZX26teWATo2f/DzWG2Li0YuOYvHt41LcSnyxmr06lCU21FzEPPKgSfOV9zcy9ck3ufFPixqWHfr9Wkbe+Vyg2wkFRj+Y8y51Gapdej7KE5z7vOPon0uTf8Iz1SZPY4wxhSeIoKwOqPK9Dk2o6ncJ8BSAqr4KlAHdwzMKemLV8ObHoJ8O69WpTaD5+Y0d3DNjef/mopEZyzsZCsxf4Yal+OOba3jx3U+ovv5pALbt3MvUJ98MbFvrtu5s+HvByk28nuQAxEH475o8rN0KkFWUGWNMeoIIyl4H+ovIQSJSiuvIPzsszYfAGAARGYgLytKvCktQeLNRorUPQQdxv7pwBF3blSaU9hffGMGZhwfXNOn/Cob3yZ/J0Hf55gP9zSsrG61LZfLxRLYjInz1oVebpHlr9RbWbI4/fEa4hWu2NJnrFJoeZ//73NKk84bYwc5z73ycUp7GGGPyT9pBmaruBS4H/g68g3vKcrGI3C4i471kVwOXisjbwAxgkka6igUs3aAq6BKOGdiT124cw3t3JtbUmcj2U2rcypMqjd179/HqBxsbXr+0NHNxuv+7jPadffn+lznhnheTznv89Jcb5tbMtvueW5aT7UaSzaebjTGmGAUy96WqPoPrwO9fdrPv7yVA/HEbMiSfeuWkMsdkIv3DX73hZM74+cts+GxXxPUtIzz6lw/fy9urt8Rcv/XzPZS0ENq1Tu9Q9d8DZOIJzA/WfxZ8psB/67awauP2hNJu/GwXr6/cxLghmRmA2BhjTGYV9Ij+0cR7KrPQJFLuXp3aNDzYEO7tW05tFAy2buX+/uKgAwIpXyYNu+1Zjr77+UbLdu6p5/o/LkxqjK7w+SiDVh8h/931+3huSXrNi8+98wmn/+zfCaU98s7nmPLEm2zO0dhlNqK/McakJ5CasrwXVjOSaE1JJltYE2qaTLJGJ1ryTm0aj5VW1qolr904JuH+bUFL9nN9tmtvwwMAAH/5T13DOF3Tzh6aUB7v+gaqTfWJxn37lBmvf8g5R1bSuqRxALzs46Y1ZbfMWsxH23Y2WZ5pe/dZcGSMMYWoKGvKQtKNqXJds5bs9vv2aJ9w2p4dy1JqSs0Hoa8l9P3s88Y7i8X/9OUvXno/Ztq6LZ/zwruuhuvHz77XsHzW23V878+LmP7C8oSaFLMZkH2+O9hx3VKR6/PFGGMKXWFelZNU6GM9JVqzdN+5h3NAx7L4CQuc/+vYvH03fW98hl+/vDLh90ean9Lv+GkvcPGjC9i1t56fv7C8Yfm2z/cC8PMXlnPiD+c2LlMOD7G/vr2WgTfX5q4AxhhjAlHUQVm6N+6FduPfsawVJx2a/vhu+e6f3lOaijbUgN3xtyXsqd8X621Jm/Hah41eRxscFsjqPJ7hXohRLmOMMYWjKIKys4aHz+rUWKgW46QB5QCcOiixgVmtOSYzXlm+MX6iGOYs+giApxasabT8J/9IbRywaN5f37iJMtaQHSs3Jj++WVCWf5KZJz+TZaeLMcakp6CDsqj9iLzlfb25LA/3Juwe1LsjK6edzhEJDp6a66fJEtl6eLNZIQSSdz69JLC8Fq3d3xQZqbN9Oh6ftyrQ/DLlv3WNm2MvfWxBjkpijDEmHUXx9GW0PmMjqrsy95rRHNitbZZLZGIJMm687g8LG/6OMBRbs/TW6i3s3FNPWavIQ6RkShbGgzbGmKJW0DVliaju3q7JNEvFrBCui6EO80FbuXE7u/cG26+sUN3xN1cbWVtby4ABAwCGiMj14elEpI+IvCgi/xGRhSJSk+WiGmOM8RR9UJaOQghwClH9vswETks//oyb/vLfjORdaH772ofU19czdepU5syZA7AYmCgig8KS3oSbGu0I3Ly1D6S6TTtfjDEmPRaUxZDra0wqzUG57geXiPoMDm4a3vm/OZs/fz79+vWjb9++4A7nmcCZYckU6Oj93QnIzSSexhhjiqNPWcbkya1/sTW/ZjIoM/vV1dVRVVXlX7QGODos2a3AsyLyLaAdcEp2SmeMMSZcUdaUNedLfmWX/H+oYXsejD5vGkwEHlXVSqAGeFxEmvwuiMhkEVkgIgvWr48+NIgxxpjUFWVQFpJu/VJl1/wPcMJdNvpgHrlgRK6LYfJARUUFq1ev9i+qBOrCkl0CPAWgqq8CZUD38LxU9WFVHaGqI3r0iDxAcZ5ULBtjTMEq6qAsHU9+82i+emRlTsuQ0DhlYaFnScsWnJLg4LiRXHzxxZSXlzNkyJDI23N+JiLLvaf1hqe8MZNRI0eOZNmyZaxYsQLcPcq5wOywZB8CYwBEZCAuKLOqMGOMyQELyqI4rl/3vOnLlc1STJo0idramPMongb09/5NBh7MRrlM8kpKSpg+fTpjx44FGIx7ynKxiNwuIuO9ZFcDl4rI28AMYJKmOOBYITxkYowx+cw6+udIvl6+Ro0axcqVK2MlORN4zLtwzxORziLSS1XXZaWAJik1NTXU1NQgIotU9S4AVb05tF5VlwDH56yAxhhjGlhNWQE7ZWA53zq5X8w0XxtRFXN9CioAf0elNd4y08xZnzJjjElPQdeURbsGFM3FIcbnmDn5GI7p2y1uFu1a524Xi8hkXBMnffr0yVk5jDHGmEJQFDVl0bp+5UmXsIxIJCDLkDrAX/0W6Yk+ILEn9owxxhjjFEVQVrQCCCqPOqhL+pk0Nhu4wHsK8xhgq/UnM5C//SSNMaZQFHTzpYlveJ/kgrKJEycyd+5cNmzYADBURC4BWgGo6kPAM7hBRpcDO4CLAi2wMcYY00xZUJbPclD1MGPGjIa/RWShqv7Kv9576nJqtstl8l+KI2kYY4zxWPNljvUvb8+Tl4ZPR9hYMfeNM8YYY4xTlEFZNgaxPPSADoHlddzBTWa1MabgWD2ZMcakpyiDspDwKYiC1CIL1VeBBJdWy2aMMcYUhKIOyopFJoNLY4JiXcqMMSY9FpSlyK4/xhhjjAmSBWVFqnv7UsBq2Uw22a1KEC6++GLKy8sZMmRIxPXeU65VIrJcRBaKyHAAETlcRF4VkcXe8q9lsdgmAtuXJlmBBGUiMk5E3vMOrOujpJkgIku8g+zJILabS9l4/N+ag4xpfiZNmkRtbW3U9XPmzAEoA/rjpjF70Fu1A7hAVQcD44D7RKRzZktrYrF9aZKVdlAmIi2B+4HTgEHARBEZFJamP3ADcLx3kF2Z7naBqDfmhRDMdGrTCoCRB3WNm9aGxIivX3n7XBeh2SuE864QjBo1iq5do/8uzJo1C2CjOvOAziLSS1WXquoyAFVdC3wC2PxmOWT70iQriMFjjwKWq+oHACIyEzgTWOJLcylwv6puBlDVTwLYboNoMUs+BzM9O5bx3FWj6NO1XUbyz+cLZJe2rdi8Y09g+T37nVEc0rMDr6/cxFcfejWwfI3JR3V1dQC7fYvWABVAw3RnInIUUAq8HykPEZmMq5mhT58+mSqqicP2pQkXRPNlBbDa9zp0UPkdAhwiIi+LyDwRGRcpIxGZLCILRGTB+vXrAyhafutX3oHSksx268vHwPTurxzW8PcVY/on/f4/XXZco9eH9HRjxo2s7srKaafzs4lHpFfAInD60F65LoLJERHpBTwOXKSq+yKlUdWHVXWEqo7o0cMqYPKV7cvmJ1sd/UtwbeajgYnALyO1j9vB1Vg+13alo6Tl/sMulb558ebzHD+sd9J5pqNNq5ZZ3V4iqru1zfo2i/RwzTsVFRXgak5CKoE6ABHpCDwNfM9rDjN5zPalCRdEUFYHVPleNxxUPmuA2aq6R1VXAEtxQVrBuvrUAVnbVh5WdgVmX4QreWtf7WGvTmVZLE1kYwf3jLn+khMOylJJEldWkn+BognG+PHjAbqJcwywVVXXiUgp8GfgMVX9Q04LaRJi+9KECyIoex3oLyIHeQfSucDssDR/wdWSISLdcc2ZHwSw7Zz54qDYF+qghYa4SFQ+11r4g8x9EWrKzhq+v/W7lVerVtoyu6O3+JuV7z9veMy08WZeCHJKrkSVZPn7Alj/6a6sb7MYTZw4kWOPPZb33nuPyspKfvWrX/HQQw/x0EMPAVBTUwOwC1gO/BK4zHvrBGAUMElE3vL+HZ6Dj2A8ti9NstLu6K+qe0XkcuDvQEvg16q6WERuBxao6mxv3akisgSoB65V1Y3pbjt6mTKVc+7MuvwE/rtmC1OeeDOp9+V7LVukmrKSFr7mzSyGlyunnU719U8DcPKAcmoXf0TXdqURAxyRxI6zldNOZ9x9LwVd1Ih+PWkEFz+6AMhNX8JH/vUBx/ezeVzTNWPGjJjrxe3cD1V1hH+5qj4BPJG5kplk2b40yQrkdlpVn1HVQ1T1YFW9y1t2sxeQ4T3ue5WqDlLVw1R1ZhDbLXYVXdoAbviMis5tGDekODpv+wOGeH3KQquzGZwBnHm465c2sjpy/7W/XzmKHh1aZ7NIcZ18aE/euOkUxg7uycSjsv8UVhHeCxljTFbZiP557LpxA3jo68M5rshqH0Ydsv8hjkjNl37d27vAJzzZ9acdGvN9D3/jSCYdV91o2TeOOTDxQnrCZ0QYVuWeTzmkZwcu8PKLN2uCZLDa6tnvjGr0ulv71vziGyMaxsHLpmKsoTbGmGyyoCyPtS5pmXLtWDZmHEhVK19zYJvSpi3oIvDB3TXce/ZQHjg/cn+uKSceHHMbpw4+gHOOrGy0bMKIqiipmzq8jwu+Jh7duMbpiUuOovbKLyScD+T3vghS8/iUxhiTOUEMHmvyWCZraYIwrLJTk2U9O5bRooUwYWQVO/fUp5x3Oh+9V6c2rJx2epPlHcpacegBjWuhst20CvDDc4ayafvuvKqdapHfh5oxxuQ9C8pMTnVp1/Sp0iP6NJ3iLYjYI+g+YIkGfZkIjEcPKKdHh9a899GngeedqhZ5fgNgjDH5rqCbL6PVUORR5YGJo2WEC7m/j1aQ1/kDOpUx/8Yxjfq05bP/3npq1HX5GP/kYZGMMaagFEVNWbQLVCYuXP3K28cdTDQf5Htg+uhFI+nevjX1kcbEiCDdflmTR/UFoLxjGS0LJHpoHWMA2Hz8CPkYKBpjTCEp6JqyXHjuqhO5dmzsJ//ySb5eJ0cPKGdIRdP+ZND44h7vycZEDOzVkRtrBja8vvuswzjv6MwPGfH9Lw1K6X0HdCzjvq8dHnNe1HzqKzioV0fvr/wpkzHGFCILygpQeZ6Nj5UNQdb89erUptGk6ACzLz8+5nvuOfsw7vrKkKS2Mzysb9zfvnVCQu977JKj+PIRFTHThIc/h/Rsn0zRAhUa1iSP4kRjjClIRdF82ZzMv3EMbUqLb17DoZWdWLhmK9B47suQTD9lOLSy6cMFfl8b2bRm7dyj+vDPpeu58Nhq7n/x/ajvzUSsEgqAcvHkZzTpPClrjDHGgrK88dxVJ9I2gWCrvGNiE3Tn01AJyTq8an+AlE7tSyrfwf+M6ku71omdFt3bt+b3U46Lm+6UQT1Zsm5boDWciZYxG971ngD917INOS6JMcYUtvz5ZQ9QIQ7W2a88M81PhdikFHR/qWRyu8HX9ywoV47pz4XHHki39okFZQd0ih14d2hd0jAAbxB97tL1paG9+NvCdbkuhjHGFLyiDMpC8uGCZTLnuatOpDTCZOG5dPbwSv745hpgfz+4Fi0k4YAs0oC1seRD82VFZzdHa1mr/NoXxhhTaOxX1OS1WGF1v/L29OnWNmtlScSPJwxr1PyaEXl2rxHq6G+DxxpjTHosKCtShdKE29qrXemeYE2SiSyXtcJtWrm+kB3Lsj8JujHGFBMLyopcKhfr2tpaBgwYADBERK5vkqdIHxF5UUT+IyILRaQm1fIdekBH7j17KD+ZMCzVLArK7yYfw4xLj8l1MQJ1pjd8xxWn9M9xSYwxprBZUGYaqa+vZ+rUqcyZMwdgMTBRRMJHQb0JeEpVjwDOBR5IZ5sTRlbRuW3TOTAhvwZJTVakysqj+3Zr6IOVKv830rd7e0ZWd+Husw6Lmj7TSryZyCMNZWKMMSZxBd3Rv0Ba6ArK/Pnz6devH3379gXXV30mcCawxJdMgdAw7p2AtVktZJ7LdBx53MHdG/4uLWmR0LAc2WDnozHGpKcobm2jNdEVcCVL2u45eyjV3drSviy5uLuuro6qqir/ojVA+PDytwJfF5E1wDPAtyLlJSKTRWSBiCxYv359UuXIppMPLc91EaJ6+tsn8O/vntTw+vmrT+S+cw/PYYmasqecjTEmGEURlJmmTjusF3OvPYmWLTJywZwIPKqqlUAN8LiINDmWVPVhVR2hqiN69OiR0oaycbnP3EMR6ec7uHcnKrvsf8L04B7tKWuV+IwO8foHAojIBBFZIiKLReTJVMtqFWXGGJMeC8pMIxUVFaxevdq/qBKoC0t2CfAUgKq+CpQB3SkwHbxaxOvGBTvBfL7UGyXSP1BE+gM3AMer6mDgymS305xrpI0xJkgWlJlGRo4cybJly1ixYgW4+OJcYHZYsg+BMQAiMhAXlOVv+2QUoRqnbu0iP2SQLfecnZlO+jH6B/pdCtyvqpsBVPWTVLdXKMOwGGNMvrKgzDRSUlLC9OnTGTt2LMBg3FOWi0XkdhEZ7yW7GrhURN4GZgCTNENX5EKshUn2i8jU+F4J9g88BDhERF4WkXkiMi4jhTHGGBNXQT99GY3dsKenpqaGmpoaRGSRqt4FoKo3h9ar6hLg+JwVMEn5Htjl+HAtAfoDo3FN1S+JyGGqusWfSEQmA5MB+vTpEzEjO+2MMSY9RV1Tlu8XYxNfIY5TlmyJM3UTkWD/wDXAbFXdo6orgKW4IC2sjNEf2ijAXWSMMXmpKGvKjMm2X104gg/Wb891MRqJ0j/wvLBkf8E9TfsbEemOa878IKUNWlWZMcakxYIyYwIwZmBPxgzMdSkai9A/8I5Q/0BggarOBv4OnCoiS4B64FpV3ZjMdgqxNtMYY/KRBWXGZEi0Zsmu7Rs/7akZrGJKoH+gAld5/9KSyc9hjDHNQVH3KTMmF+LVHLVvXcLKaac3vC70B1NCn7bQP4cxxuRaIEGZiIwTkfdEZHm0UcO9dGeLiIrIiCC2G+0aYHfsxal/eXv6dG0bP2GYfA8W8rx4cVnrpTHGBCPt5ksRaQncD3wR9yTX6yIy2xs2wZ+uA3AF8Fq622xahijL82ZsdROEf1x1Yq6LkJREg63SlsVRYV3owaUxxuRaEFeDo4DlqvqBqu4m8qjhAHcA9wA7A9imMXkr0VuBDq3dPdHg3h0zV5gssJsfY4wJRhBBWQXgHwypyajhIjIcqFLVpwPYnjFFoWObzIzknyv53kxsjDH5LuPtJiLSAvgJbmqeeGkni8gCEVmwfn3BTaVoCsyxfbsBUFbaMifbL5a+WMXyOYwxJteCGBKjDvBPsBc+angHYAgw13sq7QBgtoiMV9UF/oxU9WHgYYARI0bYfbfJqHvPGcq3x/TP2NyTidYcqcJJA3rQIUPlyBZ7wMYYY9ITRFD2OtBfRA7CBWONRg1X1a1A99BrEZkLXBMekAXJmlFMIspataRfefvA80205sif7jcXHRV4ObLFKsqMMSYYaTdfqupe4HLcyODvAE+FRg0XkfHp5p8Oa1YxkP/HQbHUMNnNkDHGpCeQEf1V9RngmbBlN0dJOzqIbRpT6EJPLRZ8MJPnQa8xxhSK4hggyZg8cv1ph1LdrW3coS7yvQYvWYUeWxpjTK7Z3JfGBOzIA7sy99qTEk5f6MGMjVNmjDHBsJoyY3Kk6EKZgm+HzQ8XX3wx5eXlDBkyJOJ6N4c8Vd60dgu9cSABEJELRWSZ9+/CLBXZRGH70iSroIOyaNeAaWcP5ajqrlR3a5fdAhmTAi3wYKbYmmFzbdKkSdTW1kZdP2fOHIAyoD8wGXgQQES6ArcAR+NmWrlFRLpkurwmOtuXJlkFHZSFhF8UjjywC09NOZbSkqL4eKZISZFFM4UdWuaPUaNG0bVr16jrZ82aBbBRnXlAZxHpBYwF/qGqm1R1M/APYFw2ymwis31pkmV9yozJkd6dy1ixYTutEpiQ/OcTj6BHh9ZZKFXyQqFlgVf4FYy6ujqA3b5Foant4k55F899zy1l1ltr0y2i8fl800es2LCdk340F4Drxg7gtMN6AZndl2u3fM75j7yWcrlNfNXd2gY+xqQFZcbkyP3nDeelZRuo6to2btozhvXOQolSU2w1fs2BiEzGNZfRp0+fhuW9O7fhsIpOuSpWUdpWup1lrVo2fK9d2pUGmn+0fdmqZQvblxnWq1NZ4HlaUGZMjnRuW8r4PA62klXofeMKRUVFBYD/yh6a2q4OGB22fG6kPKJNaTdhRBUTRlRFeotJ0cqVXVjwcBk/m3hEk3WZ3Jc9OrSOuE2T36zTlTEmLVZPll3jx48H6CbOMcBWVV2Hm1XlVBHp4nUKP9VbZvKU7UsTzmrKTNGyipvssq87GBMnTmTu3Lls2LCByspKbrvtNvbs2QPAlClTqKmpAdgFLAd2ABcBqOomEbkDNx8xwO2quin7n8CE2L40ybKgzBQ96/KUWfb9BmvGjBkx13t9+D5U1RHh61T118CvhEJ4BAAAF/hJREFUM1MykyzblyZZ1nxpCsIpA3vmuggmDquZNMaY9FhNmcl7b998Km1KW+a6GCYKm2bJGGOCYUGZyXud2rbKdRFMAqyizBhj0mPNl6aJ2tpaBgwYADBERK6PlEZEJojIEhFZLCJPZreEJq9YRZkxxgSioIMytXvzwNXX1zN16tTQnGyLgYkiMsifRkT6AzcAx6vqYODK7JfU5Bsbp8wYY9JT0EHZfnarHpT58+fTr18/+vbtC65FaiZwZliyS4H7vTnZUNVPsltKk0/s6UtjjAlGkQRlJih1dXVUVTUa0TvSnGuHAIeIyMsiMk9EbKJcY4wxJk3W0d+kogToj5sGpBJ4SUQOU9Ut/kTR5mQzxcUqyowxJhhWU2YaqaioYPXq1f5FobnY/NYAs1V1j6quAJbigrRGVPVhVR2hqiN69OiRsTKb/GBdyowxJj0WlJlGRo4cybJly1ixYgW4SpBzgdlhyf6CN1muiHTHNWd+kMVimjzijUpuD94YY0yaLCgzjZSUlDB9+nTGjh0LMBh4SlUXi8jtIjLeS/Z3YKOILAFeBK5V1Y05KnJUFiRkhzVfGmNMMKxPmWmipqaGmpoaRGSRqt4FoKo3h9arG/vgKu9f3rMR57PDmi+NMSY9VlNmjEmLDYlhjDHBsKDMGBMIqygzxpj0WFBmjEmLNQ8bY0wwLCgzxgTC+pQZY0x6CjooG1bZGYDRA2wMLGNyxfqUGWNMMAr66cshFZ14785xtC5pmeuiGNPs1e/bl+siGGNMQQukpkxExonIeyKyXESuj7D+KhFZIiILReR5ETkwiO0CFpAZE0NtbS0DBgwAGBLp3AwRkbNFREVkRKrb+tGzS1N9qzHGGAIIykSkJXA/cBowCJgoIoPCkv0HGKGqQ4E/APemu11jTGz19fVMnTqVOXPmACwm8rmJiHQArgBeS3ebz7/zcbpZGGNMsxVETdlRwHJV/UBVdwMzgTP9CVT1RVXd4b2ch5tP0RiTQfPnz6dfv3707dsX3IgVTc5Nzx3APcDOVLbj71O2cuOO6AmNMcbEFERQVgH4Z7Be4y2L5hJgTqQVIjJZRBaIyIL169cHUDRjmq+6ujqqqqr8i5qcmyIyHKhS1aeD2GZL6/RvjDEpy+rTlyLydWAE8MNI61X1YVUdoaojevSwJypNemyIhthEpAXwE+DqBNJGvWHyj1PWsoVFZcYYk6oggrI6wH87Xukta0RETgG+B4xX1V0BbNeYhDTXIRsqKipYvdpfid3k3OwADAHmishK4BhgdqTO/oneMH26a28QRTfGmGYpiKDsdaC/iBwkIqXAucBsfwIROQL4BS4g+ySAbRpj4hg5ciTLli1jxYoVAELYuamqW1W1u6pWq2o1rr/neFVdkMx2/EHvvn1WPWmMMalKOyhT1b3A5cDfgXeAp1R1sYjcLiLjvWQ/BNoDvxeRt0RkdpTsjDEBKSkpYfr06YwdOxZgMJHPzUC1sOZLY4xJWSCDx6rqM8AzYctu9v19ShDbMcYkp6amhpqaGkRkkareBY3PTT9VHZ3KNiwMM8aYYBT0iP6muDx/9Ym5LoJJ02/nfchlo/vluhjGGFOQLCgzeePgHu1zXQSTprotn+e6CMYYU7AKekJyY0zuSXN9vNUYYwJmQZkxJi0WkhljTDAsKDPGpMWeuDTGmGBYUGaMMcYYkwcsKDPGGGOMyQMWlJmiZWPLG2OMKSQWlJmiZz2ejDHGFAILyowxxhhj8oAFZcYYY4wxecCCMmOMMcaYPGBBmTHGGGNMHrCgzDRRW1vLgAEDAIaIyPXR0onI2SKiIjIie6UzxhhjipMFZaaR+vp6pk6dypw5cwAWAxNFZFB4OhHpAFwBvJblIpo8NOXEg3NdhKIRuinq168f06ZNa7J+1apVAIeIyEIRmSsilaF1InKviCwWkXdE5GdiE5PmXLz9CZSKyPO2Pw1YUGbCzJ8/n379+tG3b19wQ33NBM6MkPQO4B5gZxaLZ/LUlaf0z3URioL/pmjJkiXMmDGDJUuWNEpzzTXXAGxU1aHA7cAPAETkOOB4YCgwBBgJnJjN8pvGEtmfQCXwmO1PAxaUmTB1dXVUVVX5F60BKvwLRGQ4UKWqT2ezbCZ/2f17MPw3RaWlpZx77rnMmjWrURrvor7Ne/ki+2+aFCgDSoHWQCvg46wU3ESUyP4E2gAveH/b/mzmLCgzSRGRFsBPgKsTSDtZRBaIyIL169dnvnAmZ1pYVBaI8JuiyspK6urqGqUZNmwYQBfv5VeADiLSTVVfxV3U13n//q6q70Tajp2b2ZHI/gR2AGd5fye9P21fFhcLykwjFRUVrF692r+oEvD/inTAVaXPFZGVwDHA7Eid/VX1YVUdoaojevTokcFSm1xraUFZ1vzoRz8Cd+H+D645qw6oF5F+wEDcOVsBnCwiX4iUh52beWUNcGKq+9P2ZXGxoMw0MnLkSJYtW8aKFSvAzVB0LjA7tF5Vt6pqd1WtVtVqYB4wXlUX5KTAMaja7JfZYjFZMMJvitasWUNFRaPeA/Tu3RvgfVU9AvgegKpuwdWyzFPVz1T1M2AOcGyWim4iSGR/AntU9SzbnwYsKDNhSkpKmD59OmPHjgUYDDylqotF5HYRGZ/j4qXGIoaMs4fCguG/Kdq9ezczZ85k/PjGp92GDRv8L28Afu39/SGuxqVERFrhal0iNl+a7EhkfwIlXrcQsP3Z7FlQZpqoqalh6dKlAItU9S4AVb1ZVWeHp1XV0flYS2Zy5+3VW3JdhILlvykaOHAgEyZMYPDgwdx8883Mnu1Ov7lz54IbQ3Ap0PP/t3f/sXKVZQLHv8+908sCYilQG7i9iOVCIyWKesuajejughTHpGXduovJxirsstl0fySGP7ohy26amK1iojE1m+2qCWhCVbIuNdi6UCW6P7CW5VepobdQSHtFKVBBEWh7++4fc1pnpvf2Tmemc+ac+/0kk3vmPWfmPO887Z3nnh/vC3w6e/ndwFPA48CjwKMppe/0vBM6ppV8Ursk5EnzKYBK3gFIp5ynMXtqxZf+m2fWfTjvMAqrWq1SrVYb2tauXXtseeXKlVD7g6nhOs6U0iTwlz0IUSdhpnwCB5pzCeZztvJImUrLU2qSpCKxKJMkSeoDFmWSJEl9wKJMkiSpD1iUSeoK57+UpM50pSiLiOsi4smI2B0Ra6ZYf1pEfCNb/+OIuKgb+5XUP85702l5hyBJhdZxURYRg8CXgA8BlwEfi4jLmja7idptv6PA54HPdLpfSf3lmRdezTsESSq0bhwpuxLYnVJ6OqV0ENjIb2e5P2oFcEe2fDdwdThegVQqX/6vPXmHIEmF1o2ibBion8F6X9Y25TYppcPAy8C5Xdi3NC3nvuytT33w0rxDkKRC66sL/SPi5ojYHhHb9+/fn3c4KgsPyvbExfPflHcIklRo3SjKJoCRuucLs7Ypt4mICjAXeLH5jVJKG1JKYymlsfnz53chNEm9Mjhg8StJnehGUfYT4JKIeFtEDAE3AM0TV28CVmXLK4HvJ88tSaVSsSiTpI50PCF5SulwRPw18D1gEPhqSumJiFgLbE8pbQK+AnwtInYDL1Er3CSVSGXQokySOtFxUQaQUvou8N2mttvqll8HPtqNfUnqT56+lKTO9NWF/pKK6+6H9uUdgiQVmkWZpK549Y3DeYcgSYVmUSapKxbOOyPvECSp0CzKpBLbsmULixcvBrh8mnlpPxUROyPisYjYGhFvbXdf1y5Z0EmokjTrWZRJJTU5Ocnq1avZvHkzwBNMPS/tw8BYSukd1KZA+2y7+7t8eG7bsUqSLMqk0tq2bRujo6MsWrQIIDHFvLQppR+klH6TPX2Q2uDPbTlzqCs3c0vSrGVRptKa7aMTT0xMMDJSP9nGlPPS1rsJ2DzVilamQHNIDEnqjEWZSs9SYWYR8WfAGHD7VOtbnQLt+isu4MJzvOBfktrh+QappIaHh9m7d29901Tz0hIR1wC3Ah9IKb3RyT4HIkiz/hilJLXHI2VSSS1dupTx8XH27NkDtQOGx81LGxHvAv4VWJ5Ser7jnQYcOdLxu0jSrGRRJpVUpVJh/fr1LFu2DGAJ8M2j89JGxPJss9uBNwHfiohHImLTdO/Xik2P/IyJX77G64cmOwtekmYhT19KJVatVqlWq0TEjpTSp+G4eWmv6eb+Dh+pnbrc/swB3nfJed18a0kqPY+USeq6Q57DlKSTZlGm4/RyFHiV02sHPX0pSSfLokwNej0KvMppzqC/WiTpZPmbUw16PQq8yun8ub+TdwiSVDgWZWrQzVHgNXvd+/hzeYcgSYVjUaa2zTQKfCtT86icvv1/x41RK0magUWZGrQxCvzy6UaBb3VqnlMlObB8bn7+yut5hyBJhWNRpga5jAJ/ioWTX/bMVY5NJkltsyhTgzxGgVd5WJRJUvssynScarXKrl27ABpGgU8pbcqWr0kpLUgpXZE9lp/o/TR7XP32BXmHIEmFZVEmqWtG5p0BwOIFZ+UciSQVj0WZpK4Zqgxw5tCgpzElqQ0WZZK6anAgjk1MLklqnUWZpK6aMzjAoUknJJekk2VRJqmrBgeCSY+UtW3Lli0sXryY0dFR1q1bd9z6Z599FuDSiHgsIh6IiGPTnEXEhRHxnxHx04jYGREX9SxwTWmmfAJDEbHVfAosyiR1We1ImUVZOyYnJ1m9ejWbN29m586d3HXXXezcubNhm1tuuQXgxZTSO4C1wD/Xrb4TuD2l9HbgSqDvxxEss1bySW2A7jvNp6DDoiwizomI+yJiPPs5b4ptroiI/42IJ7K/BP60k31K6m+VwWDyiKcv27Ft2zZGR0dZtGgRQ0ND3HDDDdxzzz0N22Rf6q9kT38ArACIiMuASkrpPoCU0q9TSr/pXfRq1ko+gdOB72fL5nOW6/RI2Rpga0rpEmBr9rzZb4CPp5SWANcBX4iIszvcr6Q+NTgQHPL0ZVsmJiYYGRk59nzhwoVMTDTOcvbOd74T4OgfwH8EnBUR5wKXAr+MiH+PiIcj4vaIGOxN5JpKK/mk9h35kWzZfM5ynRZlK4A7suU7gOubN0gp7UopjWfLP6N2+LX3EyFqFrIwyMOcgQEOe6H/KfO5z30Oal/cDwMfoDY37SRQAa4CbgGWAouAT0z1HhFxc0Rsj4jt+/fv70XYmt4+4APt5tNclkunRdmClNJz2fLPgRMO5x0RVwJDwFMd7ldqmVNf9pYX+rdveHiYvXv3Hnu+b98+hoeHG7a54IILAJ5KKb0LuBUgpfRLal/uj6SUnk4pHQb+A3j3VPtJKW1IKY2llMbmz/dv5FOllXwCh1JKH2k3n+ayXGYsyiLi/ojYMcVjRf12KaXECQ5NRMT5wNeAT6aUpvwz2opfKr45g+GF/m1aunQp4+Pj7Nmzh4MHD7Jx40aWL2+cxeyFF16of/r3wFez5Z8AZ0fE0W/mPwSOu6pcvdNKPoFKRBz9Ljafs1xlpg1SStdMty4ifhER56eUnsuKrinvDImINwP3AremlB48wb42ABsAxsbG/K0uFVBlcIDDXujflkqlwvr161m2bBmTk5PceOONLFmyhNtuu42xsTGWL1/OAw88AHB5ROwCfgisBkgpTUbELcDWiAjgIeDf8uqLWssncBbwZEQkzOesN2NRNoNNwCpgXfbzuNtKImII+Da1W37v7nB/kvrcQ88eyDuEQqtWq1Sr1Ya2tWvXHlteuXIlwI6U0ljza7M79d5xikPUSZgpn8CBqXIJ5nM26vSasnXAByNiHLgme05EjEXEl7Nt/gR4P/CJiHgke1zR4X4l9TmvK5Okk9PRkbKU0ovA1VO0bwf+PFv+OvD1TvYjqXheee0Q884cyjsMSSoMR/SXdEocdFgMSTopFmWSTonH9r2cdwiSVCgWZZJOib+4c3veIUhSoViUSeqqm973NgB+7+Jzc45EkorFokxSV/3V718MwGEHkJWkk2JRptJK1gS5mDNQ+7Wy7ZmXco5EkorFokylVxsMW70yp+LnLUntsCiT1FVnDP12+MMjDiArSS2zKJPUde+/tDaH8ouvHsw5EkkqDosySV33saUjAOz/1Rs5RyJJxWFRJqnr5gzWfrUcPuKo/pLUKosySV03OFC72N9LyiSpdRZlkrru6A2vk1ZlktQyizJJXffU/lcB+ML9u3KORJKKw6JMx9myZQuLFy8GuDwi1jSvj4jTIuIbEbE7In4cERf1Oka1Jq9cvvWcMwD40fgLPPTsgW68pSSVnkWZGkxOTrJ69Wo2b94M8ATwsYi4rGmzm4ADKaVR4PPAZ3ocplqQZy6vuWzBseU//pf/4TuP/qwbbytJpVaZeRPNJtu2bWN0dJRFixYBJGAjsALYWbfZCuCfsuW7gfURESk5sVE/yTuXz6z7MBetuReAv7nrYbb+9BccnDzCWafN4T0XzaMy0Djyf/PEC0FMu65Z86wNzZvXrw5m2u/0r21eO/NrO5/doDIY/MHit3T8PpL6n0VZk6suOY8fjb+Qdxi5mZiYYGRkpL5pH/C7TZsNA3sBUkqHI+Jl4Fyg4YOLiJuBmwEuvPDCUxXytC44+3QArn777PxC64dcPrPuw+x+/lf83cZHuP+nz/PrNw4D8I3te0+uM7PY3NPn8Og/Xpt3GJJ6wKKsyVdWLeX1w5N5h1EKKaUNwAaAsbGxnh9Fu+Ds03n4Hz7I2WfM6fWuS6eTXI6+5Szu/durjj1/+bVDvPLaIY7UHYxrPi5X/7T5oF3zzo8/ppemXT/Ta9OJXnsS23bTgHO3SrOGRVmTocoAQ5XZe6nd8PAwe/c2HMVYCEw0bTYBjAD7IqICzAVe7E2EJ2femUN5h5Cbfs3l3NPnMPd0C2VJajZ7qw9NaenSpYyPj7Nnzx6oXSJzA7CpabNNwKpseSXwfa8n6z/mUpKKxaJMDSqVCuvXr2fZsmUAS4BvppSeiIi1EbE82+wrwLkRsRv4FHDcUAvKn7mUpGLx9KWOU61WqVarRMSOlNKnAVJKtx1dn1J6HfhobgGqZeZSkorDI2WSJEl9wKJMkiSpD1iUSZIk9QGLMkmSpD5gUSZJktQHLMokSZL6gEWZJElSH4h+Hbw7IvYDzzY1n0fTRMkFVoS+vDWlNL8bbzRFPovQ/1YVoS+nMpdQjM+gVUXoi/83W1OEvvh/s3X93peOc9m3RdlUImJ7Smks7zi6oUx9aUeZ+l+mvrSrTJ9BmfrSjjL1v0x9aVeZPoMy9WU6nr6UJEnqAxZlkiRJfaBoRdmGvAPoojL1pR1l6n+Z+tKuMn0GZepLO8rU/zL1pV1l+gzK1JcpFeqaMkmSpLIq2pEySZKkUipEURYR10XEkxGxOyLW5B3PURHx1Yh4PiJ21LWdExH3RcR49nNe1h4R8cWsD49FxLvrXrMq2348IlbVtb8nIh7PXvPFiIje9vDUMJ/lyae5LE8uwXyWKZ/msqC5TCn19QMYBJ4CFgFDwKPAZXnHlcX2fuDdwI66ts8Ca7LlNcBnsuUqsBkI4L3Aj7P2c4Cns5/zsuV52bpt2baRvfZDeffZfJpPc1m+XJrPcuXTXBY3l0U4UnYlsDul9HRK6SCwEViRc0wApJR+CLzU1LwCuCNbvgO4vq79zlTzIHB2RJwPLAPuSym9lFI6ANwHXJete3NK6cFU+5d2Z917FZn5LE8+zWV5cgnms0z5NJcFzWURirJhYG/d831ZW79akFJ6Llv+ObAgW56uHydq3zdFe9GZz8b2IjOXje1FZz4b24vMXDa2F0YRirLCyip1b28tCfNZHuayXMxnecz2XBahKJsARuqeL8za+tUvskOoZD+fz9qn68eJ2hdO0V505rOxvcjMZWN70ZnPxvYiM5eN7YVRhKLsJ8AlEfG2iBgCbgA25RzTiWwCjt4Jsgq4p67949ndJO8FXs4O134PuDYi5mV3nFwLfC9b90pEvDe7e+Tjde9VZOazPPk0l+XJJZjPMuXTXBY1l3nfadDKg9odGLuo3U1ya97x1MV1F/AccIjaueubgHOBrcA4cD9wTrZtAF/K+vA4MFb3PjcCu7PHJ+vax4Ad2WvWkw32W/SH+SxPPs1leXJpPsuVT3NZzFw6or8kSVIfKMLpS0mSpNKzKJMkSeoDFmWSJEl9wKJMkiSpD1iUSZIk9QGLMkmSpD5gUSZJktQHLMokSZL6wP8DcqIandqDb5IAAAAASUVORK5CYII=\n","text/plain":["<Figure size 720x360 with 5 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}},{"output_type":"stream","text":["len(ohlc) : 17129\n","long_ep.shape : (17129, 1)\n","len(pr_list) : 17129\n","np.array(data_x).shape : (16646, 45, 5)\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}},{"output_type":"stream","text":["input_x.shape : (16646, 90, 10, 3)\n","input_x.dtype : float32\n","input_pr.shape : (16646, 1)\n","input_ud.shape : (16646, 1)\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:220: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"XT_30lBZcGJb","executionInfo":{"status":"ok","timestamp":1619008736861,"user_tz":-540,"elapsed":1432,"user":{"displayName":"7th June","photoUrl":"","userId":"08178289703395036410"}}},"source":["\n","#         save to npy     #\n","x_save_path = current_path + 'npy/' + '%s_close_updown_theta_x_ex_ma30.npy' % period\n","y_save_path = current_path + 'npy/' + '%s_close_updown_theta_y_ex_ma30.npy' % period\n","\n","\n","\n","np.save(x_save_path, input_x)\n","np.save(y_save_path, input_pr)"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"76BQc5XlUSS6","executionInfo":{"status":"ok","timestamp":1619014928345,"user_tz":-540,"elapsed":708,"user":{"displayName":"7th June","photoUrl":"","userId":"08178289703395036410"}}},"source":["period = 45\n","# x_save_path = current_path + 'npy/' + '%s_close_updown_x.npy' % period\n","# y_save_path = current_path + 'npy/' + '%s_close_updown_y.npy' % period\n","\n","# total_x = np.load(x_save_path)\n","# total_pr = np.load(y_save_path)\n","\n","_, row, col, _ = input_x.shape\n","\n","\n","\n","seed = 1\n","random_state = 20\n","np.random.seed(seed)\n","from sklearn.model_selection import train_test_split\n","#         train / test split      #\n","# x_train, x_test_, pr_train, pr_test_, ud_train, ud_test_ = train_test_split(re_input_x, total_pr, total_ud, test_size=0.4, shuffle=True, random_state=random_state)\n","x_train_, x_test, pr_train_, pr_test = train_test_split(input_x, input_pr, test_size=0.2, shuffle=False, random_state=random_state)\n","x_train, x_val, pr_train, pr_val = train_test_split(x_train_, pr_train_, test_size=0.25, shuffle=False, random_state=random_state)\n","# break\n","#         pr label   #\n","y_train = np.where(pr_train > 1, 1, 0)\n","y_test = np.where(pr_test > 1, 1, 0)\n","y_val = np.where(pr_val > 1, 1, 0)\n","\n","del input_x"],"execution_count":8,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GmmgsEUMqUjN"},"source":["### **Model**"]},{"cell_type":"code","metadata":{"id":"mcDUjgQzqUSr","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619014937185,"user_tz":-540,"elapsed":4761,"user":{"displayName":"7th June","photoUrl":"","userId":"08178289703395036410"}},"outputId":"97a045ae-4a5a-4902-e441-69e5b7b4fcc9"},"source":["import os\n","# os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   \n","# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","\n","%tensorflow_version 1.x\n","\n","import keras\n","import tensorflow as tf\n","\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import scipy.misc \n","from math import sqrt \n","import itertools\n","from IPython.display import display\n","\n","%matplotlib inline\n","\n","from keras.utils import plot_model\n","import keras.backend as K\n","from keras.models import Model, Sequential\n","import keras.layers as layers\n","from keras.optimizers import Adam, SGD\n","from keras.regularizers import l1, l2\n","\n","from sklearn.cluster import KMeans\n","from sklearn.metrics import silhouette_score\n","from sklearn.metrics import precision_score, recall_score, precision_recall_curve\n","from sklearn.metrics import confusion_matrix\n","from sklearn.metrics import plot_confusion_matrix\n","\n","\n","gdrive_path = current_path\n","\n","num_classes = 2\n","\n","def FER_Model(input_shape=(row, col, 3)):\n","    # first input model\n","    visible = layers.Input(shape=input_shape, name='input')\n","    \n","    net = layers.Conv2D(32, kernel_size=3, padding='same', kernel_initializer='he_normal')(visible)\n","    # net = layers.Conv2D(256, kernel_size=3, padding='same', kernel_initializer='he_normal')(visible)\n","    # net = layers.BatchNormalization()(net)\n","    net = layers.LeakyReLU()(net)\n","\n","    net = layers.Conv2D(64, kernel_size=3, padding='same', kernel_initializer='he_normal')(net)\n","    # net = layers.Conv2D(128, kernel_size=3, padding='same', kernel_initializer='he_normal')(net)\n","    # net = layers.BatchNormalization()(net)\n","    # net = layers.Activation('relu')(net)\n","    net = layers.LeakyReLU()(net)\n","    # net = layers.MaxPool2D(pool_size=2)(net)\n","    # net = layers.AveragePooling2D(padding='same')(net)\n","\n","    shortcut_1 = net\n","\n","    # net = layers.Conv2D(64, kernel_size=3, padding='same', kernel_initializer='he_normal')(net)\n","    net = layers.Conv2D(128, kernel_size=3, padding='same', kernel_initializer='he_normal')(net)\n","    net = layers.LeakyReLU()(net)\n","\n","    net = layers.Conv2D(256, kernel_size=3, padding='same', kernel_initializer='he_normal')(net)\n","    # net = layers.BatchNormalization()(net)\n","    # net = layers.Activation('relu')(net)\n","    net = layers.LeakyReLU()(net)\n","    # net = layers.MaxPool2D(pool_size=2)(net)\n","\n","    shortcut_2 = net\n","\n","#     net = layers.Conv2D(256, kernel_size=3, padding='same')(net)\n","#     # net = layers.Activation('relu')(net)\n","#     net = layers.LeakyReLU()(net)\n","#     net = layers.MaxPool2D(pool_size=2)(net)\n","\n","#     shortcut_3 = net\n","\n","#     net = layers.Conv2D(128, kernel_size=1, padding='same')(net)\n","#     # net = layers.Activation('relu')(net)\n","#     net = layers.LeakyReLU()(net)\n","#     net = layers.MaxPool2D(pool_size=2)(net)\n","\n","    net = layers.Flatten()(net)\n","    net = layers.Dense(128)(net)\n","    net = layers.LeakyReLU()(net)\n","\n","    net = layers.Dense(64)(net)\n","    net = layers.LeakyReLU()(net)\n","\n","    net = layers.Dense(num_classes, activation='softmax')(net)\n","\n","    # create model \n","    model = Model(inputs=visible, outputs=net)\n","    # summary layers\n","    # print(model.summary())\n","    \n","    return model"],"execution_count":9,"outputs":[{"output_type":"stream","text":["TensorFlow 1.x selected.\n"],"name":"stdout"},{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"zscZynIgMbAq","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619014937186,"user_tz":-540,"elapsed":2074,"user":{"displayName":"7th June","photoUrl":"","userId":"08178289703395036410"}},"outputId":"d512d59d-e4af-441b-d447-103e92283a64"},"source":["print(keras.__version__)\n","print(tf.__version__)"],"execution_count":10,"outputs":[{"output_type":"stream","text":["2.3.1\n","1.15.2\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"fWUEyjzF21cJ"},"source":["### **Data Split**"]},{"cell_type":"code","metadata":{"scrolled":false,"id":"2iYLNSeSEp7p","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1619014945799,"user_tz":-540,"elapsed":1656,"user":{"displayName":"7th June","photoUrl":"","userId":"08178289703395036410"}},"outputId":"9654124c-ad7b-4e90-c347-2253cf5328e1"},"source":["from sklearn.model_selection import train_test_split\n","from keras.utils import np_utils\n","from keras.preprocessing.image import ImageDataGenerator \n","from sklearn.utils import class_weight\n","# import cv2\n","\n","\n","_, row, col, _ = x_train.shape\n","\n","#         pr label   #\n","# y_train = np.where(pr_train > 1, 1, 0)\n","# y_test = np.where(pr_test > 1, 1, 0)\n","# y_val = np.where(pr_val > 1, 1, 0)\n","\n","\n","seed = 1\n","random_state = 20\n","np.random.seed(seed)\n","# tf.random.set_seed(seed)\n","\n","#         up label      #\n","# y_train = np.where(ud_train > 1, 1, 0)\n","# y_test = np.where(ud_test > 1, 1, 0)\n","# y_val = np.where(ud_val > 1, 1, 0)\n","\n","print('pr_train[:5] :', pr_train[:5])\n","# print('ud_train[:5] :', ud_train[:5])\n","print('y_train[:5] :', y_train[:5])\n","print('y_train.dtype :', y_train.dtype)\n","\n","print('x_train.shape :', x_train.shape)\n","print('x_test.shape :', x_test.shape)\n","print('x_val.shape :', x_val.shape)\n","print('y_train.shape :', y_train.shape)\n","print('y_test.shape :', y_test.shape)\n","print('y_val.shape :', y_val.shape)\n","\n","def class_ratio(in_list):\n","\n","  return in_list / in_list[1]\n","\n","print('np.unique(y_train, return_counts=True :', np.unique(y_train, return_counts=True), class_ratio(np.unique(y_train, return_counts=True)[1]))\n","print('np.unique(y_val, return_counts=True :', np.unique(y_val, return_counts=True), class_ratio(np.unique(y_val, return_counts=True)[1]))\n","print('np.unique(y_test, return_counts=True :', np.unique(y_test, return_counts=True), class_ratio(np.unique(y_test, return_counts=True)[1]))\n","\n","label = y_train.reshape(-1, )\n","class_weights = class_weight.compute_class_weight('balanced', \n","                                                    classes=np.unique(label),\n","                                                    y=label)\n","class_weights = dict(enumerate(class_weights))\n","print('class_weights :', class_weights)\n","\n","# sample_weight = np.ones(shape=(len(y_train),))\n","# sample_weight[(y_train == 1).reshape(-1,)] = 1.5\n","# print('sample_weight[:20] :', sample_weight[:20])\n","\n","\n","print('np.isnan(np.sum(x_train)) :', np.isnan(np.sum(x_train)))\n","print('np.isnan(np.sum(x_val)) :', np.isnan(np.sum(x_val)))\n","print('np.isnan(np.sum(x_test)) :', np.isnan(np.sum(x_test)))\n","\n","print('np.isnan(np.sum(y_train)) :', np.isnan(np.sum(y_train)))\n","print('np.isnan(np.sum(y_val)) :', np.isnan(np.sum(y_val)))\n","print('np.isnan(np.sum(y_test)) :', np.isnan(np.sum(y_test)))\n","\n","y_train_ohe = np_utils.to_categorical(y_train, num_classes)\n","y_val_ohe = np_utils.to_categorical(y_val, num_classes)\n","y_test_ohe = np_utils.to_categorical(y_test, num_classes)\n","print('y_train_ohe.shape :', y_train_ohe.shape)\n","print('y_val_ohe.shape :', y_val_ohe.shape)\n","print('y_test_ohe.shape :', y_test_ohe.shape)\n","\n","datagen = ImageDataGenerator( \n","    rotation_range = 45,\n","    # zoom_range = 0.5,\n","    # shear_range = 0.5,\n","    # horizontal_flip = True,\n","    # vertical_flip = True,\n","    # width_shift_range=0.5,\n","    # height_shift_range=0.5,\n","    # fill_mode = 'nearest'\n","    )\n","\n","valgen = ImageDataGenerator( \n","    )\n","\n","datagen.fit(x_train)\n","valgen.fit(x_val)\n","\n","batch_size = 16\n","\n","for x_batch, _ in datagen.flow(x_train, y_train_ohe, batch_size=9):\n","\n","    plt.suptitle(\"train x_batch\")\n","\n","    for i in range(0, 9): \n","        plt.subplot(330 + 1 + i) \n","        # resized = cv2.resize(x_batch[i].reshape(row, col), (row * 2, col * 10))\n","        # cmapped = plt.cm.Set1(resized)\n","        # plt.imshow(cmapped)\n","        # plt.imshow(x_batch[i].reshape(row, col))\n","        plt.imshow(x_batch[i])\n","        plt.axis('off') \n","    plt.show() \n","    break\n","\n","for x_batch, _ in valgen.flow(x_val, y_val_ohe, batch_size=9):\n","\n","    plt.suptitle(\"val x_batch\")\n","\n","    for i in range(0, 9): \n","        plt.subplot(330 + 1 + i) \n","        # resized = cv2.resize(x_batch[i].reshape(row, col), (row * 2, col * 10))\n","        # cmapped = plt.cm.Set1(resized)\n","        # plt.imshow(cmapped)\n","        # plt.imshow(x_batch[i].reshape(row, col))\n","        plt.imshow(x_batch[i])\n","        plt.axis('off') \n","    plt.show() \n","    break\n","    \n","train_flow = datagen.flow(x_train, y_train_ohe, batch_size=batch_size) \n","val_flow = valgen.flow(x_val, y_val_ohe, batch_size=batch_size) \n","# break\n","\n"],"execution_count":11,"outputs":[{"output_type":"stream","text":["pr_train[:5] : [[0.98730326]\n"," [0.9846347 ]\n"," [0.99877083]\n"," [1.0043759 ]\n"," [1.0117302 ]]\n","y_train[:5] : [[0]\n"," [0]\n"," [0]\n"," [1]\n"," [1]]\n","y_train.dtype : int64\n","x_train.shape : (9987, 90, 10, 3)\n","x_test.shape : (3330, 90, 10, 3)\n","x_val.shape : (3329, 90, 10, 3)\n","y_train.shape : (9987, 1)\n","y_test.shape : (3330, 1)\n","y_val.shape : (3329, 1)\n","np.unique(y_train, return_counts=True : (array([0, 1]), array([5793, 4194])) [1.38125894 1.        ]\n","np.unique(y_val, return_counts=True : (array([0, 1]), array([1798, 1531])) [1.17439582 1.        ]\n","np.unique(y_test, return_counts=True : (array([0, 1]), array([1763, 1567])) [1.12507977 1.        ]\n","class_weights : {0: 0.8619886069394096, 1: 1.1906294706723892}\n","np.isnan(np.sum(x_train)) : False\n","np.isnan(np.sum(x_val)) : False\n","np.isnan(np.sum(x_test)) : False\n","np.isnan(np.sum(y_train)) : False\n","np.isnan(np.sum(y_val)) : False\n","np.isnan(np.sum(y_test)) : False\n","y_train_ohe.shape : (9987, 2)\n","y_val_ohe.shape : (3329, 2)\n","y_test_ohe.shape : (3330, 2)\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 9 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 9 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"code","metadata":{"id":"MFt-suNmXPEb"},"source":["# print(x_train[0])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"s-W6LL5c2VN2"},"source":["### **Training**"]},{"cell_type":"code","metadata":{"id":"EkVg1hVI2TNP","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619027619489,"user_tz":-540,"elapsed":11207510,"user":{"displayName":"7th June","photoUrl":"","userId":"08178289703395036410"}},"outputId":"68d65f28-8e88-4190-a23c-1ed65b93da68"},"source":["(_, row, col, _) = x_train.shape\n","  \n","from keras.callbacks import ModelCheckpoint, EarlyStopping, TensorBoard\n","\n","ckpt_path = current_path + 'ckpt/'\n","board_path = current_path + 'graph/'\n","model_name = 'classifier_%s_close_updown_pr_theta_non_shuffle_non_volume.h5' % period\n","\n","model = keras.models.load_model(ckpt_path + model_name)\n","\n","# model = FER_Model(input_shape=(row, col, 3))\n","# opt = Adam(lr=0.00001, decay=0.000005)\n","# model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n","\n","checkpoint = ModelCheckpoint(ckpt_path + model_name, monitor='val_loss', verbose=1, save_best_only=True, mode='auto')\n","checkpoint2 = TensorBoard(log_dir=board_path,\n","                          histogram_freq=0,\n","                          write_graph=True,\n","                          write_images=True)\n","checkpoint3 = EarlyStopping(monitor='val_loss', patience=40)\n","# callbacks_list = [checkpoint, checkpoint2, checkpoint3]\n","callbacks_list = [checkpoint, checkpoint2]\n","\n","# keras.callbacks.Callback 로 부터 log 를 받아와 history log 를 작성할 수 있다.\n","\n","# we iterate 200 times over the entire training set\n","num_epochs = 1000\n","history = model.fit_generator(train_flow, \n","                    steps_per_epoch=len(x_train) / batch_size, \n","                    epochs=num_epochs,  \n","                    verbose=2,  \n","                    callbacks=callbacks_list,\n","                    class_weight=class_weights,\n","                    validation_data=val_flow,  \n","                    validation_steps=len(x_val) / batch_size,\n","                    shuffle=False)"],"execution_count":18,"outputs":[{"output_type":"stream","text":["Epoch 1/1000\n"," - 12s - loss: 0.6270 - accuracy: 0.6371 - val_loss: 0.4555 - val_accuracy: 0.5191\n","\n","Epoch 00001: val_loss improved from inf to 0.45555, saving model to /content/drive/My Drive/Colab Notebooks/Project_Stock/ckpt/classifier_45_close_updown_pr_theta_non_shuffle_non_volume.h5\n","Epoch 2/1000\n"," - 11s - loss: 0.6162 - accuracy: 0.6414 - val_loss: 0.8015 - val_accuracy: 0.5209\n","\n","Epoch 00002: val_loss did not improve from 0.45555\n","Epoch 3/1000\n"," - 11s - loss: 0.6188 - accuracy: 0.6383 - val_loss: 0.3809 - val_accuracy: 0.5233\n","\n","Epoch 00003: val_loss improved from 0.45555 to 0.38087, saving model to /content/drive/My Drive/Colab Notebooks/Project_Stock/ckpt/classifier_45_close_updown_pr_theta_non_shuffle_non_volume.h5\n","Epoch 4/1000\n"," - 11s - loss: 0.6155 - accuracy: 0.6445 - val_loss: 0.5749 - val_accuracy: 0.5173\n","\n","Epoch 00004: val_loss did not improve from 0.38087\n","Epoch 5/1000\n"," - 11s - loss: 0.6084 - accuracy: 0.6542 - val_loss: 0.4663 - val_accuracy: 0.5167\n","\n","Epoch 00005: val_loss did not improve from 0.38087\n","Epoch 6/1000\n"," - 11s - loss: 0.6094 - accuracy: 0.6499 - val_loss: 0.6566 - val_accuracy: 0.5248\n","\n","Epoch 00006: val_loss did not improve from 0.38087\n","Epoch 7/1000\n"," - 11s - loss: 0.6037 - accuracy: 0.6565 - val_loss: 0.5487 - val_accuracy: 0.5257\n","\n","Epoch 00007: val_loss did not improve from 0.38087\n","Epoch 8/1000\n"," - 11s - loss: 0.6047 - accuracy: 0.6544 - val_loss: 0.5958 - val_accuracy: 0.5164\n","\n","Epoch 00008: val_loss did not improve from 0.38087\n","Epoch 9/1000\n"," - 11s - loss: 0.6047 - accuracy: 0.6556 - val_loss: 0.1584 - val_accuracy: 0.5185\n","\n","Epoch 00009: val_loss improved from 0.38087 to 0.15838, saving model to /content/drive/My Drive/Colab Notebooks/Project_Stock/ckpt/classifier_45_close_updown_pr_theta_non_shuffle_non_volume.h5\n","Epoch 10/1000\n"," - 11s - loss: 0.6005 - accuracy: 0.6551 - val_loss: 0.5216 - val_accuracy: 0.5149\n","\n","Epoch 00010: val_loss did not improve from 0.15838\n","Epoch 11/1000\n"," - 11s - loss: 0.6040 - accuracy: 0.6547 - val_loss: 0.0061 - val_accuracy: 0.5134\n","\n","Epoch 00011: val_loss improved from 0.15838 to 0.00613, saving model to /content/drive/My Drive/Colab Notebooks/Project_Stock/ckpt/classifier_45_close_updown_pr_theta_non_shuffle_non_volume.h5\n","Epoch 12/1000\n"," - 11s - loss: 0.5975 - accuracy: 0.6625 - val_loss: 1.1361 - val_accuracy: 0.5098\n","\n","Epoch 00012: val_loss did not improve from 0.00613\n","Epoch 13/1000\n"," - 11s - loss: 0.5942 - accuracy: 0.6561 - val_loss: 0.4419 - val_accuracy: 0.5209\n","\n","Epoch 00013: val_loss did not improve from 0.00613\n","Epoch 14/1000\n"," - 11s - loss: 0.5919 - accuracy: 0.6652 - val_loss: 1.4077 - val_accuracy: 0.5227\n","\n","Epoch 00014: val_loss did not improve from 0.00613\n","Epoch 15/1000\n"," - 11s - loss: 0.5882 - accuracy: 0.6679 - val_loss: 1.2288 - val_accuracy: 0.5209\n","\n","Epoch 00015: val_loss did not improve from 0.00613\n","Epoch 16/1000\n"," - 11s - loss: 0.5846 - accuracy: 0.6697 - val_loss: 0.9131 - val_accuracy: 0.5170\n","\n","Epoch 00016: val_loss did not improve from 0.00613\n","Epoch 17/1000\n"," - 11s - loss: 0.5825 - accuracy: 0.6717 - val_loss: 0.7800 - val_accuracy: 0.5149\n","\n","Epoch 00017: val_loss did not improve from 0.00613\n","Epoch 18/1000\n"," - 11s - loss: 0.5783 - accuracy: 0.6797 - val_loss: 1.0997 - val_accuracy: 0.5188\n","\n","Epoch 00018: val_loss did not improve from 0.00613\n","Epoch 19/1000\n"," - 11s - loss: 0.5727 - accuracy: 0.6845 - val_loss: 0.1408 - val_accuracy: 0.5215\n","\n","Epoch 00019: val_loss did not improve from 0.00613\n","Epoch 20/1000\n"," - 11s - loss: 0.5774 - accuracy: 0.6789 - val_loss: 1.7373 - val_accuracy: 0.5242\n","\n","Epoch 00020: val_loss did not improve from 0.00613\n","Epoch 21/1000\n"," - 11s - loss: 0.5717 - accuracy: 0.6884 - val_loss: 0.4114 - val_accuracy: 0.5278\n","\n","Epoch 00021: val_loss did not improve from 0.00613\n","Epoch 22/1000\n"," - 11s - loss: 0.5678 - accuracy: 0.6878 - val_loss: 0.1429 - val_accuracy: 0.5296\n","\n","Epoch 00022: val_loss did not improve from 0.00613\n","Epoch 23/1000\n"," - 11s - loss: 0.5656 - accuracy: 0.6913 - val_loss: 0.3673 - val_accuracy: 0.5149\n","\n","Epoch 00023: val_loss did not improve from 0.00613\n","Epoch 24/1000\n"," - 11s - loss: 0.5666 - accuracy: 0.6877 - val_loss: 0.6936 - val_accuracy: 0.5125\n","\n","Epoch 00024: val_loss did not improve from 0.00613\n","Epoch 25/1000\n"," - 11s - loss: 0.5678 - accuracy: 0.6864 - val_loss: 0.9717 - val_accuracy: 0.5170\n","\n","Epoch 00025: val_loss did not improve from 0.00613\n","Epoch 26/1000\n"," - 11s - loss: 0.5649 - accuracy: 0.6890 - val_loss: 1.5350 - val_accuracy: 0.5185\n","\n","Epoch 00026: val_loss did not improve from 0.00613\n","Epoch 27/1000\n"," - 11s - loss: 0.5556 - accuracy: 0.6992 - val_loss: 0.1639 - val_accuracy: 0.5299\n","\n","Epoch 00027: val_loss did not improve from 0.00613\n","Epoch 28/1000\n"," - 11s - loss: 0.5560 - accuracy: 0.6957 - val_loss: 0.6477 - val_accuracy: 0.5182\n","\n","Epoch 00028: val_loss did not improve from 0.00613\n","Epoch 29/1000\n"," - 11s - loss: 0.5491 - accuracy: 0.7018 - val_loss: 1.4683 - val_accuracy: 0.5164\n","\n","Epoch 00029: val_loss did not improve from 0.00613\n","Epoch 30/1000\n"," - 11s - loss: 0.5511 - accuracy: 0.7036 - val_loss: 0.6780 - val_accuracy: 0.5158\n","\n","Epoch 00030: val_loss did not improve from 0.00613\n","Epoch 31/1000\n"," - 11s - loss: 0.5518 - accuracy: 0.7059 - val_loss: 0.1710 - val_accuracy: 0.5218\n","\n","Epoch 00031: val_loss did not improve from 0.00613\n","Epoch 32/1000\n"," - 11s - loss: 0.5443 - accuracy: 0.7116 - val_loss: 1.0480 - val_accuracy: 0.5152\n","\n","Epoch 00032: val_loss did not improve from 0.00613\n","Epoch 33/1000\n"," - 11s - loss: 0.5389 - accuracy: 0.7163 - val_loss: 0.4923 - val_accuracy: 0.5278\n","\n","Epoch 00033: val_loss did not improve from 0.00613\n","Epoch 34/1000\n"," - 11s - loss: 0.5423 - accuracy: 0.7093 - val_loss: 0.9781 - val_accuracy: 0.5152\n","\n","Epoch 00034: val_loss did not improve from 0.00613\n","Epoch 35/1000\n"," - 11s - loss: 0.5376 - accuracy: 0.7131 - val_loss: 0.0804 - val_accuracy: 0.5143\n","\n","Epoch 00035: val_loss did not improve from 0.00613\n","Epoch 36/1000\n"," - 11s - loss: 0.5329 - accuracy: 0.7126 - val_loss: 1.2198 - val_accuracy: 0.5134\n","\n","Epoch 00036: val_loss did not improve from 0.00613\n","Epoch 37/1000\n"," - 11s - loss: 0.5319 - accuracy: 0.7158 - val_loss: 2.0055 - val_accuracy: 0.5203\n","\n","Epoch 00037: val_loss did not improve from 0.00613\n","Epoch 38/1000\n"," - 11s - loss: 0.5328 - accuracy: 0.7174 - val_loss: 0.2675 - val_accuracy: 0.5176\n","\n","Epoch 00038: val_loss did not improve from 0.00613\n","Epoch 39/1000\n"," - 11s - loss: 0.5304 - accuracy: 0.7207 - val_loss: 0.4920 - val_accuracy: 0.5158\n","\n","Epoch 00039: val_loss did not improve from 0.00613\n","Epoch 40/1000\n"," - 11s - loss: 0.5277 - accuracy: 0.7201 - val_loss: 0.5520 - val_accuracy: 0.5158\n","\n","Epoch 00040: val_loss did not improve from 0.00613\n","Epoch 41/1000\n"," - 11s - loss: 0.5221 - accuracy: 0.7247 - val_loss: 0.7787 - val_accuracy: 0.5164\n","\n","Epoch 00041: val_loss did not improve from 0.00613\n","Epoch 42/1000\n"," - 11s - loss: 0.5246 - accuracy: 0.7200 - val_loss: 1.3511 - val_accuracy: 0.5119\n","\n","Epoch 00042: val_loss did not improve from 0.00613\n","Epoch 43/1000\n"," - 11s - loss: 0.5167 - accuracy: 0.7298 - val_loss: 0.1690 - val_accuracy: 0.5158\n","\n","Epoch 00043: val_loss did not improve from 0.00613\n","Epoch 44/1000\n"," - 11s - loss: 0.5215 - accuracy: 0.7204 - val_loss: 0.1333 - val_accuracy: 0.5155\n","\n","Epoch 00044: val_loss did not improve from 0.00613\n","Epoch 45/1000\n"," - 11s - loss: 0.5087 - accuracy: 0.7326 - val_loss: 0.0063 - val_accuracy: 0.5311\n","\n","Epoch 00045: val_loss did not improve from 0.00613\n","Epoch 46/1000\n"," - 11s - loss: 0.5113 - accuracy: 0.7285 - val_loss: 2.7496 - val_accuracy: 0.5119\n","\n","Epoch 00046: val_loss did not improve from 0.00613\n","Epoch 47/1000\n"," - 11s - loss: 0.5107 - accuracy: 0.7333 - val_loss: 0.9810 - val_accuracy: 0.5155\n","\n","Epoch 00047: val_loss did not improve from 0.00613\n","Epoch 48/1000\n"," - 11s - loss: 0.5084 - accuracy: 0.7357 - val_loss: 0.5705 - val_accuracy: 0.5101\n","\n","Epoch 00048: val_loss did not improve from 0.00613\n","Epoch 49/1000\n"," - 11s - loss: 0.5010 - accuracy: 0.7452 - val_loss: 0.2176 - val_accuracy: 0.5122\n","\n","Epoch 00049: val_loss did not improve from 0.00613\n","Epoch 50/1000\n"," - 11s - loss: 0.5010 - accuracy: 0.7414 - val_loss: 0.2781 - val_accuracy: 0.5110\n","\n","Epoch 00050: val_loss did not improve from 0.00613\n","Epoch 51/1000\n"," - 11s - loss: 0.4984 - accuracy: 0.7434 - val_loss: 0.3303 - val_accuracy: 0.5149\n","\n","Epoch 00051: val_loss did not improve from 0.00613\n","Epoch 52/1000\n"," - 11s - loss: 0.4997 - accuracy: 0.7443 - val_loss: 0.2633 - val_accuracy: 0.5026\n","\n","Epoch 00052: val_loss did not improve from 0.00613\n","Epoch 53/1000\n"," - 11s - loss: 0.4980 - accuracy: 0.7471 - val_loss: 0.8032 - val_accuracy: 0.5050\n","\n","Epoch 00053: val_loss did not improve from 0.00613\n","Epoch 54/1000\n"," - 11s - loss: 0.4924 - accuracy: 0.7479 - val_loss: 1.0989 - val_accuracy: 0.5065\n","\n","Epoch 00054: val_loss did not improve from 0.00613\n","Epoch 55/1000\n"," - 11s - loss: 0.4913 - accuracy: 0.7481 - val_loss: 2.2911 - val_accuracy: 0.5131\n","\n","Epoch 00055: val_loss did not improve from 0.00613\n","Epoch 56/1000\n"," - 11s - loss: 0.4893 - accuracy: 0.7445 - val_loss: 0.4444 - val_accuracy: 0.5146\n","\n","Epoch 00056: val_loss did not improve from 0.00613\n","Epoch 57/1000\n"," - 11s - loss: 0.4845 - accuracy: 0.7536 - val_loss: 0.0799 - val_accuracy: 0.5215\n","\n","Epoch 00057: val_loss did not improve from 0.00613\n","Epoch 58/1000\n"," - 11s - loss: 0.4856 - accuracy: 0.7527 - val_loss: 0.3485 - val_accuracy: 0.5059\n","\n","Epoch 00058: val_loss did not improve from 0.00613\n","Epoch 59/1000\n"," - 11s - loss: 0.4861 - accuracy: 0.7463 - val_loss: 0.8664 - val_accuracy: 0.5221\n","\n","Epoch 00059: val_loss did not improve from 0.00613\n","Epoch 60/1000\n"," - 11s - loss: 0.4781 - accuracy: 0.7612 - val_loss: 0.1323 - val_accuracy: 0.5008\n","\n","Epoch 00060: val_loss did not improve from 0.00613\n","Epoch 61/1000\n"," - 11s - loss: 0.4769 - accuracy: 0.7558 - val_loss: 0.0337 - val_accuracy: 0.5107\n","\n","Epoch 00061: val_loss did not improve from 0.00613\n","Epoch 62/1000\n"," - 11s - loss: 0.4763 - accuracy: 0.7596 - val_loss: 0.1048 - val_accuracy: 0.5176\n","\n","Epoch 00062: val_loss did not improve from 0.00613\n","Epoch 63/1000\n"," - 11s - loss: 0.4684 - accuracy: 0.7666 - val_loss: 0.0328 - val_accuracy: 0.5101\n","\n","Epoch 00063: val_loss did not improve from 0.00613\n","Epoch 64/1000\n"," - 11s - loss: 0.4694 - accuracy: 0.7618 - val_loss: 0.1868 - val_accuracy: 0.5134\n","\n","Epoch 00064: val_loss did not improve from 0.00613\n","Epoch 65/1000\n"," - 11s - loss: 0.4693 - accuracy: 0.7637 - val_loss: 0.6007 - val_accuracy: 0.5188\n","\n","Epoch 00065: val_loss did not improve from 0.00613\n","Epoch 66/1000\n"," - 11s - loss: 0.4665 - accuracy: 0.7643 - val_loss: 0.1648 - val_accuracy: 0.5104\n","\n","Epoch 00066: val_loss did not improve from 0.00613\n","Epoch 67/1000\n"," - 11s - loss: 0.4661 - accuracy: 0.7654 - val_loss: 0.5882 - val_accuracy: 0.5092\n","\n","Epoch 00067: val_loss did not improve from 0.00613\n","Epoch 68/1000\n"," - 11s - loss: 0.4634 - accuracy: 0.7664 - val_loss: 0.7709 - val_accuracy: 0.5182\n","\n","Epoch 00068: val_loss did not improve from 0.00613\n","Epoch 69/1000\n"," - 11s - loss: 0.4559 - accuracy: 0.7735 - val_loss: 2.3693 - val_accuracy: 0.5068\n","\n","Epoch 00069: val_loss did not improve from 0.00613\n","Epoch 70/1000\n"," - 11s - loss: 0.4530 - accuracy: 0.7737 - val_loss: 0.7746 - val_accuracy: 0.5176\n","\n","Epoch 00070: val_loss did not improve from 0.00613\n","Epoch 71/1000\n"," - 11s - loss: 0.4527 - accuracy: 0.7746 - val_loss: 0.4364 - val_accuracy: 0.5176\n","\n","Epoch 00071: val_loss did not improve from 0.00613\n","Epoch 72/1000\n"," - 11s - loss: 0.4516 - accuracy: 0.7743 - val_loss: 2.4617 - val_accuracy: 0.5197\n","\n","Epoch 00072: val_loss did not improve from 0.00613\n","Epoch 73/1000\n"," - 11s - loss: 0.4535 - accuracy: 0.7727 - val_loss: 0.0804 - val_accuracy: 0.5050\n","\n","Epoch 00073: val_loss did not improve from 0.00613\n","Epoch 74/1000\n"," - 11s - loss: 0.4490 - accuracy: 0.7730 - val_loss: 0.9204 - val_accuracy: 0.5164\n","\n","Epoch 00074: val_loss did not improve from 0.00613\n","Epoch 75/1000\n"," - 11s - loss: 0.4476 - accuracy: 0.7808 - val_loss: 0.0194 - val_accuracy: 0.5161\n","\n","Epoch 00075: val_loss did not improve from 0.00613\n","Epoch 76/1000\n"," - 11s - loss: 0.4465 - accuracy: 0.7731 - val_loss: 0.2650 - val_accuracy: 0.5110\n","\n","Epoch 00076: val_loss did not improve from 0.00613\n","Epoch 77/1000\n"," - 11s - loss: 0.4398 - accuracy: 0.7847 - val_loss: 0.9087 - val_accuracy: 0.5095\n","\n","Epoch 00077: val_loss did not improve from 0.00613\n","Epoch 78/1000\n"," - 11s - loss: 0.4428 - accuracy: 0.7824 - val_loss: 1.9719 - val_accuracy: 0.5140\n","\n","Epoch 00078: val_loss did not improve from 0.00613\n","Epoch 79/1000\n"," - 11s - loss: 0.4378 - accuracy: 0.7832 - val_loss: 1.5719 - val_accuracy: 0.5161\n","\n","Epoch 00079: val_loss did not improve from 0.00613\n","Epoch 80/1000\n"," - 11s - loss: 0.4362 - accuracy: 0.7830 - val_loss: 0.7065 - val_accuracy: 0.5041\n","\n","Epoch 00080: val_loss did not improve from 0.00613\n","Epoch 81/1000\n"," - 11s - loss: 0.4372 - accuracy: 0.7822 - val_loss: 2.1635 - val_accuracy: 0.5041\n","\n","Epoch 00081: val_loss did not improve from 0.00613\n","Epoch 82/1000\n"," - 11s - loss: 0.4322 - accuracy: 0.7871 - val_loss: 0.2379 - val_accuracy: 0.5044\n","\n","Epoch 00082: val_loss did not improve from 0.00613\n","Epoch 83/1000\n"," - 11s - loss: 0.4312 - accuracy: 0.7888 - val_loss: 0.7569 - val_accuracy: 0.5101\n","\n","Epoch 00083: val_loss did not improve from 0.00613\n","Epoch 84/1000\n"," - 11s - loss: 0.4301 - accuracy: 0.7913 - val_loss: 0.0901 - val_accuracy: 0.5131\n","\n","Epoch 00084: val_loss did not improve from 0.00613\n","Epoch 85/1000\n"," - 11s - loss: 0.4290 - accuracy: 0.7872 - val_loss: 0.0019 - val_accuracy: 0.5125\n","\n","Epoch 00085: val_loss improved from 0.00613 to 0.00191, saving model to /content/drive/My Drive/Colab Notebooks/Project_Stock/ckpt/classifier_45_close_updown_pr_theta_non_shuffle_non_volume.h5\n","Epoch 86/1000\n"," - 11s - loss: 0.4183 - accuracy: 0.7970 - val_loss: 0.7067 - val_accuracy: 0.5074\n","\n","Epoch 00086: val_loss did not improve from 0.00191\n","Epoch 87/1000\n"," - 11s - loss: 0.4177 - accuracy: 0.7984 - val_loss: 2.3976 - val_accuracy: 0.5062\n","\n","Epoch 00087: val_loss did not improve from 0.00191\n","Epoch 88/1000\n"," - 11s - loss: 0.4187 - accuracy: 0.7971 - val_loss: 0.0102 - val_accuracy: 0.5197\n","\n","Epoch 00088: val_loss did not improve from 0.00191\n","Epoch 89/1000\n"," - 11s - loss: 0.4146 - accuracy: 0.7991 - val_loss: 1.7210 - val_accuracy: 0.5098\n","\n","Epoch 00089: val_loss did not improve from 0.00191\n","Epoch 90/1000\n"," - 11s - loss: 0.4122 - accuracy: 0.8016 - val_loss: 6.9626 - val_accuracy: 0.5098\n","\n","Epoch 00090: val_loss did not improve from 0.00191\n","Epoch 91/1000\n"," - 11s - loss: 0.4108 - accuracy: 0.7994 - val_loss: 3.8074 - val_accuracy: 0.5104\n","\n","Epoch 00091: val_loss did not improve from 0.00191\n","Epoch 92/1000\n"," - 11s - loss: 0.4077 - accuracy: 0.7958 - val_loss: 1.0390 - val_accuracy: 0.4968\n","\n","Epoch 00092: val_loss did not improve from 0.00191\n","Epoch 93/1000\n"," - 11s - loss: 0.4066 - accuracy: 0.8000 - val_loss: 1.9746 - val_accuracy: 0.5017\n","\n","Epoch 00093: val_loss did not improve from 0.00191\n","Epoch 94/1000\n"," - 11s - loss: 0.4089 - accuracy: 0.7989 - val_loss: 0.8028 - val_accuracy: 0.5077\n","\n","Epoch 00094: val_loss did not improve from 0.00191\n","Epoch 95/1000\n"," - 11s - loss: 0.4003 - accuracy: 0.8085 - val_loss: 0.5432 - val_accuracy: 0.5116\n","\n","Epoch 00095: val_loss did not improve from 0.00191\n","Epoch 96/1000\n"," - 11s - loss: 0.4037 - accuracy: 0.8002 - val_loss: 0.0078 - val_accuracy: 0.5134\n","\n","Epoch 00096: val_loss did not improve from 0.00191\n","Epoch 97/1000\n"," - 11s - loss: 0.4066 - accuracy: 0.8015 - val_loss: 1.5589 - val_accuracy: 0.5104\n","\n","Epoch 00097: val_loss did not improve from 0.00191\n","Epoch 98/1000\n"," - 11s - loss: 0.4020 - accuracy: 0.8051 - val_loss: 0.0705 - val_accuracy: 0.5074\n","\n","Epoch 00098: val_loss did not improve from 0.00191\n","Epoch 99/1000\n"," - 11s - loss: 0.3958 - accuracy: 0.8101 - val_loss: 1.6242 - val_accuracy: 0.5056\n","\n","Epoch 00099: val_loss did not improve from 0.00191\n","Epoch 100/1000\n"," - 11s - loss: 0.3895 - accuracy: 0.8144 - val_loss: 0.0041 - val_accuracy: 0.5068\n","\n","Epoch 00100: val_loss did not improve from 0.00191\n","Epoch 101/1000\n"," - 11s - loss: 0.3888 - accuracy: 0.8166 - val_loss: 1.3302 - val_accuracy: 0.5083\n","\n","Epoch 00101: val_loss did not improve from 0.00191\n","Epoch 102/1000\n"," - 11s - loss: 0.3908 - accuracy: 0.8100 - val_loss: 0.0052 - val_accuracy: 0.5089\n","\n","Epoch 00102: val_loss did not improve from 0.00191\n","Epoch 103/1000\n"," - 11s - loss: 0.3885 - accuracy: 0.8134 - val_loss: 6.8425 - val_accuracy: 0.5071\n","\n","Epoch 00103: val_loss did not improve from 0.00191\n","Epoch 104/1000\n"," - 11s - loss: 0.3868 - accuracy: 0.8178 - val_loss: 0.2139 - val_accuracy: 0.5173\n","\n","Epoch 00104: val_loss did not improve from 0.00191\n","Epoch 105/1000\n"," - 11s - loss: 0.3841 - accuracy: 0.8190 - val_loss: 0.0901 - val_accuracy: 0.5131\n","\n","Epoch 00105: val_loss did not improve from 0.00191\n","Epoch 106/1000\n"," - 11s - loss: 0.3887 - accuracy: 0.8137 - val_loss: 4.1548 - val_accuracy: 0.5092\n","\n","Epoch 00106: val_loss did not improve from 0.00191\n","Epoch 107/1000\n"," - 11s - loss: 0.3819 - accuracy: 0.8180 - val_loss: 3.1265 - val_accuracy: 0.5164\n","\n","Epoch 00107: val_loss did not improve from 0.00191\n","Epoch 108/1000\n"," - 11s - loss: 0.3820 - accuracy: 0.8179 - val_loss: 3.0682 - val_accuracy: 0.5104\n","\n","Epoch 00108: val_loss did not improve from 0.00191\n","Epoch 109/1000\n"," - 11s - loss: 0.3809 - accuracy: 0.8142 - val_loss: 0.4465 - val_accuracy: 0.5173\n","\n","Epoch 00109: val_loss did not improve from 0.00191\n","Epoch 110/1000\n"," - 11s - loss: 0.3743 - accuracy: 0.8268 - val_loss: 1.3468 - val_accuracy: 0.5107\n","\n","Epoch 00110: val_loss did not improve from 0.00191\n","Epoch 111/1000\n"," - 11s - loss: 0.3699 - accuracy: 0.8276 - val_loss: 0.0131 - val_accuracy: 0.5038\n","\n","Epoch 00111: val_loss did not improve from 0.00191\n","Epoch 112/1000\n"," - 11s - loss: 0.3739 - accuracy: 0.8255 - val_loss: 0.0719 - val_accuracy: 0.5152\n","\n","Epoch 00112: val_loss did not improve from 0.00191\n","Epoch 113/1000\n"," - 11s - loss: 0.3711 - accuracy: 0.8219 - val_loss: 3.9500 - val_accuracy: 0.5173\n","\n","Epoch 00113: val_loss did not improve from 0.00191\n","Epoch 114/1000\n"," - 11s - loss: 0.3701 - accuracy: 0.8256 - val_loss: 2.7852 - val_accuracy: 0.5038\n","\n","Epoch 00114: val_loss did not improve from 0.00191\n","Epoch 115/1000\n"," - 11s - loss: 0.3755 - accuracy: 0.8200 - val_loss: 1.3733 - val_accuracy: 0.5050\n","\n","Epoch 00115: val_loss did not improve from 0.00191\n","Epoch 116/1000\n"," - 11s - loss: 0.3644 - accuracy: 0.8306 - val_loss: 0.2158 - val_accuracy: 0.4995\n","\n","Epoch 00116: val_loss did not improve from 0.00191\n","Epoch 117/1000\n"," - 11s - loss: 0.3689 - accuracy: 0.8241 - val_loss: 0.8899 - val_accuracy: 0.5053\n","\n","Epoch 00117: val_loss did not improve from 0.00191\n","Epoch 118/1000\n"," - 11s - loss: 0.3649 - accuracy: 0.8267 - val_loss: 1.4834 - val_accuracy: 0.5113\n","\n","Epoch 00118: val_loss did not improve from 0.00191\n","Epoch 119/1000\n"," - 11s - loss: 0.3591 - accuracy: 0.8328 - val_loss: 1.8095 - val_accuracy: 0.5173\n","\n","Epoch 00119: val_loss did not improve from 0.00191\n","Epoch 120/1000\n"," - 11s - loss: 0.3628 - accuracy: 0.8289 - val_loss: 2.8629 - val_accuracy: 0.4986\n","\n","Epoch 00120: val_loss did not improve from 0.00191\n","Epoch 121/1000\n"," - 11s - loss: 0.3621 - accuracy: 0.8277 - val_loss: 0.8329 - val_accuracy: 0.5068\n","\n","Epoch 00121: val_loss did not improve from 0.00191\n","Epoch 122/1000\n"," - 11s - loss: 0.3590 - accuracy: 0.8354 - val_loss: 5.9147 - val_accuracy: 0.5053\n","\n","Epoch 00122: val_loss did not improve from 0.00191\n","Epoch 123/1000\n"," - 11s - loss: 0.3563 - accuracy: 0.8349 - val_loss: 1.1952 - val_accuracy: 0.5206\n","\n","Epoch 00123: val_loss did not improve from 0.00191\n","Epoch 124/1000\n"," - 11s - loss: 0.3559 - accuracy: 0.8343 - val_loss: 0.2337 - val_accuracy: 0.5089\n","\n","Epoch 00124: val_loss did not improve from 0.00191\n","Epoch 125/1000\n"," - 11s - loss: 0.3544 - accuracy: 0.8357 - val_loss: 0.3926 - val_accuracy: 0.5188\n","\n","Epoch 00125: val_loss did not improve from 0.00191\n","Epoch 126/1000\n"," - 11s - loss: 0.3548 - accuracy: 0.8313 - val_loss: 0.0089 - val_accuracy: 0.5110\n","\n","Epoch 00126: val_loss did not improve from 0.00191\n","Epoch 127/1000\n"," - 11s - loss: 0.3554 - accuracy: 0.8285 - val_loss: 7.2108e-04 - val_accuracy: 0.5041\n","\n","Epoch 00127: val_loss improved from 0.00191 to 0.00072, saving model to /content/drive/My Drive/Colab Notebooks/Project_Stock/ckpt/classifier_45_close_updown_pr_theta_non_shuffle_non_volume.h5\n","Epoch 128/1000\n"," - 11s - loss: 0.3491 - accuracy: 0.8370 - val_loss: 0.7557 - val_accuracy: 0.5134\n","\n","Epoch 00128: val_loss did not improve from 0.00072\n","Epoch 129/1000\n"," - 11s - loss: 0.3480 - accuracy: 0.8365 - val_loss: 0.8127 - val_accuracy: 0.5179\n","\n","Epoch 00129: val_loss did not improve from 0.00072\n","Epoch 130/1000\n"," - 11s - loss: 0.3385 - accuracy: 0.8435 - val_loss: 0.0190 - val_accuracy: 0.5116\n","\n","Epoch 00130: val_loss did not improve from 0.00072\n","Epoch 131/1000\n"," - 11s - loss: 0.3396 - accuracy: 0.8399 - val_loss: 1.5786 - val_accuracy: 0.5107\n","\n","Epoch 00131: val_loss did not improve from 0.00072\n","Epoch 132/1000\n"," - 11s - loss: 0.3431 - accuracy: 0.8378 - val_loss: 1.9601 - val_accuracy: 0.5128\n","\n","Epoch 00132: val_loss did not improve from 0.00072\n","Epoch 133/1000\n"," - 11s - loss: 0.3360 - accuracy: 0.8452 - val_loss: 1.6809 - val_accuracy: 0.5119\n","\n","Epoch 00133: val_loss did not improve from 0.00072\n","Epoch 134/1000\n"," - 11s - loss: 0.3444 - accuracy: 0.8396 - val_loss: 1.1987 - val_accuracy: 0.5164\n","\n","Epoch 00134: val_loss did not improve from 0.00072\n","Epoch 135/1000\n"," - 11s - loss: 0.3399 - accuracy: 0.8408 - val_loss: 0.8371 - val_accuracy: 0.5119\n","\n","Epoch 00135: val_loss did not improve from 0.00072\n","Epoch 136/1000\n"," - 11s - loss: 0.3434 - accuracy: 0.8400 - val_loss: 0.9570 - val_accuracy: 0.5140\n","\n","Epoch 00136: val_loss did not improve from 0.00072\n","Epoch 137/1000\n"," - 11s - loss: 0.3328 - accuracy: 0.8475 - val_loss: 2.6272 - val_accuracy: 0.5188\n","\n","Epoch 00137: val_loss did not improve from 0.00072\n","Epoch 138/1000\n"," - 11s - loss: 0.3300 - accuracy: 0.8512 - val_loss: 0.3609 - val_accuracy: 0.5053\n","\n","Epoch 00138: val_loss did not improve from 0.00072\n","Epoch 139/1000\n"," - 11s - loss: 0.3273 - accuracy: 0.8516 - val_loss: 1.9819 - val_accuracy: 0.5119\n","\n","Epoch 00139: val_loss did not improve from 0.00072\n","Epoch 140/1000\n"," - 11s - loss: 0.3337 - accuracy: 0.8462 - val_loss: 0.0428 - val_accuracy: 0.5050\n","\n","Epoch 00140: val_loss did not improve from 0.00072\n","Epoch 141/1000\n"," - 11s - loss: 0.3212 - accuracy: 0.8563 - val_loss: 0.0021 - val_accuracy: 0.5101\n","\n","Epoch 00141: val_loss did not improve from 0.00072\n","Epoch 142/1000\n"," - 11s - loss: 0.3252 - accuracy: 0.8530 - val_loss: 4.9024 - val_accuracy: 0.5071\n","\n","Epoch 00142: val_loss did not improve from 0.00072\n","Epoch 143/1000\n"," - 11s - loss: 0.3265 - accuracy: 0.8485 - val_loss: 2.8010 - val_accuracy: 0.5053\n","\n","Epoch 00143: val_loss did not improve from 0.00072\n","Epoch 144/1000\n"," - 11s - loss: 0.3272 - accuracy: 0.8470 - val_loss: 0.7632 - val_accuracy: 0.5056\n","\n","Epoch 00144: val_loss did not improve from 0.00072\n","Epoch 145/1000\n"," - 11s - loss: 0.3165 - accuracy: 0.8563 - val_loss: 0.0437 - val_accuracy: 0.5164\n","\n","Epoch 00145: val_loss did not improve from 0.00072\n","Epoch 146/1000\n"," - 11s - loss: 0.3189 - accuracy: 0.8508 - val_loss: 0.0695 - val_accuracy: 0.5140\n","\n","Epoch 00146: val_loss did not improve from 0.00072\n","Epoch 147/1000\n"," - 11s - loss: 0.3149 - accuracy: 0.8594 - val_loss: 1.0975 - val_accuracy: 0.5128\n","\n","Epoch 00147: val_loss did not improve from 0.00072\n","Epoch 148/1000\n"," - 11s - loss: 0.3196 - accuracy: 0.8488 - val_loss: 0.3892 - val_accuracy: 0.5155\n","\n","Epoch 00148: val_loss did not improve from 0.00072\n","Epoch 149/1000\n"," - 11s - loss: 0.3256 - accuracy: 0.8469 - val_loss: 0.3555 - val_accuracy: 0.5134\n","\n","Epoch 00149: val_loss did not improve from 0.00072\n","Epoch 150/1000\n"," - 11s - loss: 0.3101 - accuracy: 0.8610 - val_loss: 3.1882 - val_accuracy: 0.5125\n","\n","Epoch 00150: val_loss did not improve from 0.00072\n","Epoch 151/1000\n"," - 11s - loss: 0.3109 - accuracy: 0.8547 - val_loss: 1.3569 - val_accuracy: 0.5116\n","\n","Epoch 00151: val_loss did not improve from 0.00072\n","Epoch 152/1000\n"," - 11s - loss: 0.3134 - accuracy: 0.8604 - val_loss: 0.1804 - val_accuracy: 0.5068\n","\n","Epoch 00152: val_loss did not improve from 0.00072\n","Epoch 153/1000\n"," - 11s - loss: 0.3141 - accuracy: 0.8568 - val_loss: 1.7282 - val_accuracy: 0.5146\n","\n","Epoch 00153: val_loss did not improve from 0.00072\n","Epoch 154/1000\n"," - 11s - loss: 0.3110 - accuracy: 0.8603 - val_loss: 1.7550 - val_accuracy: 0.5113\n","\n","Epoch 00154: val_loss did not improve from 0.00072\n","Epoch 155/1000\n"," - 11s - loss: 0.3074 - accuracy: 0.8597 - val_loss: 7.5410 - val_accuracy: 0.5053\n","\n","Epoch 00155: val_loss did not improve from 0.00072\n","Epoch 156/1000\n"," - 11s - loss: 0.3096 - accuracy: 0.8608 - val_loss: 0.1385 - val_accuracy: 0.5095\n","\n","Epoch 00156: val_loss did not improve from 0.00072\n","Epoch 157/1000\n"," - 11s - loss: 0.3034 - accuracy: 0.8621 - val_loss: 0.1071 - val_accuracy: 0.5104\n","\n","Epoch 00157: val_loss did not improve from 0.00072\n","Epoch 158/1000\n"," - 11s - loss: 0.3043 - accuracy: 0.8629 - val_loss: 0.4540 - val_accuracy: 0.5251\n","\n","Epoch 00158: val_loss did not improve from 0.00072\n","Epoch 159/1000\n"," - 11s - loss: 0.3038 - accuracy: 0.8634 - val_loss: 0.1556 - val_accuracy: 0.5155\n","\n","Epoch 00159: val_loss did not improve from 0.00072\n","Epoch 160/1000\n"," - 11s - loss: 0.3004 - accuracy: 0.8646 - val_loss: 0.5247 - val_accuracy: 0.5137\n","\n","Epoch 00160: val_loss did not improve from 0.00072\n","Epoch 161/1000\n"," - 11s - loss: 0.2955 - accuracy: 0.8678 - val_loss: 0.0039 - val_accuracy: 0.5179\n","\n","Epoch 00161: val_loss did not improve from 0.00072\n","Epoch 162/1000\n"," - 11s - loss: 0.2961 - accuracy: 0.8688 - val_loss: 0.4791 - val_accuracy: 0.5143\n","\n","Epoch 00162: val_loss did not improve from 0.00072\n","Epoch 163/1000\n"," - 11s - loss: 0.2951 - accuracy: 0.8684 - val_loss: 0.2756 - val_accuracy: 0.5191\n","\n","Epoch 00163: val_loss did not improve from 0.00072\n","Epoch 164/1000\n"," - 11s - loss: 0.2973 - accuracy: 0.8620 - val_loss: 0.0720 - val_accuracy: 0.5182\n","\n","Epoch 00164: val_loss did not improve from 0.00072\n","Epoch 165/1000\n"," - 11s - loss: 0.2959 - accuracy: 0.8675 - val_loss: 0.2038 - val_accuracy: 0.5158\n","\n","Epoch 00165: val_loss did not improve from 0.00072\n","Epoch 166/1000\n"," - 11s - loss: 0.2945 - accuracy: 0.8647 - val_loss: 0.1438 - val_accuracy: 0.5089\n","\n","Epoch 00166: val_loss did not improve from 0.00072\n","Epoch 167/1000\n"," - 11s - loss: 0.2916 - accuracy: 0.8684 - val_loss: 0.1005 - val_accuracy: 0.5137\n","\n","Epoch 00167: val_loss did not improve from 0.00072\n","Epoch 168/1000\n"," - 11s - loss: 0.2900 - accuracy: 0.8697 - val_loss: 2.2861 - val_accuracy: 0.5149\n","\n","Epoch 00168: val_loss did not improve from 0.00072\n","Epoch 169/1000\n"," - 11s - loss: 0.2888 - accuracy: 0.8716 - val_loss: 4.6439 - val_accuracy: 0.5224\n","\n","Epoch 00169: val_loss did not improve from 0.00072\n","Epoch 170/1000\n"," - 11s - loss: 0.2923 - accuracy: 0.8677 - val_loss: 0.0104 - val_accuracy: 0.5122\n","\n","Epoch 00170: val_loss did not improve from 0.00072\n","Epoch 171/1000\n"," - 11s - loss: 0.2861 - accuracy: 0.8718 - val_loss: 0.2828 - val_accuracy: 0.5080\n","\n","Epoch 00171: val_loss did not improve from 0.00072\n","Epoch 172/1000\n"," - 11s - loss: 0.2877 - accuracy: 0.8676 - val_loss: 1.3544 - val_accuracy: 0.5083\n","\n","Epoch 00172: val_loss did not improve from 0.00072\n","Epoch 173/1000\n"," - 11s - loss: 0.2827 - accuracy: 0.8760 - val_loss: 0.5158 - val_accuracy: 0.5182\n","\n","Epoch 00173: val_loss did not improve from 0.00072\n","Epoch 174/1000\n"," - 11s - loss: 0.2902 - accuracy: 0.8674 - val_loss: 7.5196 - val_accuracy: 0.5161\n","\n","Epoch 00174: val_loss did not improve from 0.00072\n","Epoch 175/1000\n"," - 11s - loss: 0.2802 - accuracy: 0.8723 - val_loss: 0.0521 - val_accuracy: 0.5155\n","\n","Epoch 00175: val_loss did not improve from 0.00072\n","Epoch 176/1000\n"," - 11s - loss: 0.2789 - accuracy: 0.8748 - val_loss: 0.9566 - val_accuracy: 0.5065\n","\n","Epoch 00176: val_loss did not improve from 0.00072\n","Epoch 177/1000\n"," - 11s - loss: 0.2812 - accuracy: 0.8757 - val_loss: 6.9006 - val_accuracy: 0.5119\n","\n","Epoch 00177: val_loss did not improve from 0.00072\n","Epoch 178/1000\n"," - 11s - loss: 0.2791 - accuracy: 0.8784 - val_loss: 0.2564 - val_accuracy: 0.5137\n","\n","Epoch 00178: val_loss did not improve from 0.00072\n","Epoch 179/1000\n"," - 11s - loss: 0.2728 - accuracy: 0.8795 - val_loss: 0.1913 - val_accuracy: 0.5041\n","\n","Epoch 00179: val_loss did not improve from 0.00072\n","Epoch 180/1000\n"," - 11s - loss: 0.2781 - accuracy: 0.8735 - val_loss: 6.0908 - val_accuracy: 0.5224\n","\n","Epoch 00180: val_loss did not improve from 0.00072\n","Epoch 181/1000\n"," - 11s - loss: 0.2783 - accuracy: 0.8766 - val_loss: 2.7128e-04 - val_accuracy: 0.5140\n","\n","Epoch 00181: val_loss improved from 0.00072 to 0.00027, saving model to /content/drive/My Drive/Colab Notebooks/Project_Stock/ckpt/classifier_45_close_updown_pr_theta_non_shuffle_non_volume.h5\n","Epoch 182/1000\n"," - 11s - loss: 0.2676 - accuracy: 0.8787 - val_loss: 0.2901 - val_accuracy: 0.5209\n","\n","Epoch 00182: val_loss did not improve from 0.00027\n","Epoch 183/1000\n"," - 11s - loss: 0.2715 - accuracy: 0.8819 - val_loss: 0.0674 - val_accuracy: 0.5206\n","\n","Epoch 00183: val_loss did not improve from 0.00027\n","Epoch 184/1000\n"," - 11s - loss: 0.2733 - accuracy: 0.8800 - val_loss: 0.5074 - val_accuracy: 0.5164\n","\n","Epoch 00184: val_loss did not improve from 0.00027\n","Epoch 185/1000\n"," - 11s - loss: 0.2701 - accuracy: 0.8820 - val_loss: 5.5032 - val_accuracy: 0.5209\n","\n","Epoch 00185: val_loss did not improve from 0.00027\n","Epoch 186/1000\n"," - 11s - loss: 0.2658 - accuracy: 0.8841 - val_loss: 6.6521 - val_accuracy: 0.5140\n","\n","Epoch 00186: val_loss did not improve from 0.00027\n","Epoch 187/1000\n"," - 11s - loss: 0.2688 - accuracy: 0.8820 - val_loss: 0.1220 - val_accuracy: 0.5146\n","\n","Epoch 00187: val_loss did not improve from 0.00027\n","Epoch 188/1000\n"," - 11s - loss: 0.2657 - accuracy: 0.8835 - val_loss: 0.7550 - val_accuracy: 0.5161\n","\n","Epoch 00188: val_loss did not improve from 0.00027\n","Epoch 189/1000\n"," - 11s - loss: 0.2645 - accuracy: 0.8806 - val_loss: 0.0265 - val_accuracy: 0.5104\n","\n","Epoch 00189: val_loss did not improve from 0.00027\n","Epoch 190/1000\n"," - 11s - loss: 0.2613 - accuracy: 0.8843 - val_loss: 0.2223 - val_accuracy: 0.5155\n","\n","Epoch 00190: val_loss did not improve from 0.00027\n","Epoch 191/1000\n"," - 11s - loss: 0.2596 - accuracy: 0.8894 - val_loss: 2.6350 - val_accuracy: 0.5134\n","\n","Epoch 00191: val_loss did not improve from 0.00027\n","Epoch 192/1000\n"," - 11s - loss: 0.2622 - accuracy: 0.8849 - val_loss: 0.9488 - val_accuracy: 0.5158\n","\n","Epoch 00192: val_loss did not improve from 0.00027\n","Epoch 193/1000\n"," - 11s - loss: 0.2597 - accuracy: 0.8855 - val_loss: 0.0566 - val_accuracy: 0.5155\n","\n","Epoch 00193: val_loss did not improve from 0.00027\n","Epoch 194/1000\n"," - 11s - loss: 0.2599 - accuracy: 0.8831 - val_loss: 3.0793 - val_accuracy: 0.5140\n","\n","Epoch 00194: val_loss did not improve from 0.00027\n","Epoch 195/1000\n"," - 11s - loss: 0.2604 - accuracy: 0.8875 - val_loss: 2.4825 - val_accuracy: 0.5137\n","\n","Epoch 00195: val_loss did not improve from 0.00027\n","Epoch 196/1000\n"," - 11s - loss: 0.2559 - accuracy: 0.8864 - val_loss: 3.4066 - val_accuracy: 0.5161\n","\n","Epoch 00196: val_loss did not improve from 0.00027\n","Epoch 197/1000\n"," - 11s - loss: 0.2615 - accuracy: 0.8840 - val_loss: 6.8540 - val_accuracy: 0.5131\n","\n","Epoch 00197: val_loss did not improve from 0.00027\n","Epoch 198/1000\n"," - 11s - loss: 0.2540 - accuracy: 0.8856 - val_loss: 6.3202 - val_accuracy: 0.5197\n","\n","Epoch 00198: val_loss did not improve from 0.00027\n","Epoch 199/1000\n"," - 11s - loss: 0.2489 - accuracy: 0.8924 - val_loss: 0.0049 - val_accuracy: 0.5119\n","\n","Epoch 00199: val_loss did not improve from 0.00027\n","Epoch 200/1000\n"," - 11s - loss: 0.2487 - accuracy: 0.8894 - val_loss: 1.9593 - val_accuracy: 0.5089\n","\n","Epoch 00200: val_loss did not improve from 0.00027\n","Epoch 201/1000\n"," - 11s - loss: 0.2539 - accuracy: 0.8898 - val_loss: 1.6224 - val_accuracy: 0.5083\n","\n","Epoch 00201: val_loss did not improve from 0.00027\n","Epoch 202/1000\n"," - 11s - loss: 0.2524 - accuracy: 0.8885 - val_loss: 0.0000e+00 - val_accuracy: 0.5149\n","\n","Epoch 00202: val_loss improved from 0.00027 to 0.00000, saving model to /content/drive/My Drive/Colab Notebooks/Project_Stock/ckpt/classifier_45_close_updown_pr_theta_non_shuffle_non_volume.h5\n","Epoch 203/1000\n"," - 11s - loss: 0.2435 - accuracy: 0.8990 - val_loss: 7.1526e-07 - val_accuracy: 0.5122\n","\n","Epoch 00203: val_loss did not improve from 0.00000\n","Epoch 204/1000\n"," - 11s - loss: 0.2454 - accuracy: 0.8930 - val_loss: 0.3778 - val_accuracy: 0.5017\n","\n","Epoch 00204: val_loss did not improve from 0.00000\n","Epoch 205/1000\n"," - 11s - loss: 0.2495 - accuracy: 0.8900 - val_loss: 5.8324e-04 - val_accuracy: 0.5035\n","\n","Epoch 00205: val_loss did not improve from 0.00000\n","Epoch 206/1000\n"," - 11s - loss: 0.2480 - accuracy: 0.8938 - val_loss: 11.7895 - val_accuracy: 0.5050\n","\n","Epoch 00206: val_loss did not improve from 0.00000\n","Epoch 207/1000\n"," - 11s - loss: 0.2475 - accuracy: 0.8893 - val_loss: 1.0512 - val_accuracy: 0.5080\n","\n","Epoch 00207: val_loss did not improve from 0.00000\n","Epoch 208/1000\n"," - 11s - loss: 0.2460 - accuracy: 0.8936 - val_loss: 5.4678e-04 - val_accuracy: 0.5104\n","\n","Epoch 00208: val_loss did not improve from 0.00000\n","Epoch 209/1000\n"," - 11s - loss: 0.2390 - accuracy: 0.9014 - val_loss: 1.7093e-04 - val_accuracy: 0.5119\n","\n","Epoch 00209: val_loss did not improve from 0.00000\n","Epoch 210/1000\n"," - 11s - loss: 0.2451 - accuracy: 0.8929 - val_loss: 1.6898 - val_accuracy: 0.5092\n","\n","Epoch 00210: val_loss did not improve from 0.00000\n","Epoch 211/1000\n"," - 11s - loss: 0.2443 - accuracy: 0.8920 - val_loss: 3.8342 - val_accuracy: 0.5077\n","\n","Epoch 00211: val_loss did not improve from 0.00000\n","Epoch 212/1000\n"," - 11s - loss: 0.2462 - accuracy: 0.8890 - val_loss: 0.8525 - val_accuracy: 0.5071\n","\n","Epoch 00212: val_loss did not improve from 0.00000\n","Epoch 213/1000\n"," - 11s - loss: 0.2409 - accuracy: 0.8931 - val_loss: 0.0036 - val_accuracy: 0.5092\n","\n","Epoch 00213: val_loss did not improve from 0.00000\n","Epoch 214/1000\n"," - 11s - loss: 0.2316 - accuracy: 0.9013 - val_loss: 0.0429 - val_accuracy: 0.5023\n","\n","Epoch 00214: val_loss did not improve from 0.00000\n","Epoch 215/1000\n"," - 11s - loss: 0.2362 - accuracy: 0.9014 - val_loss: 0.9292 - val_accuracy: 0.5074\n","\n","Epoch 00215: val_loss did not improve from 0.00000\n","Epoch 216/1000\n"," - 11s - loss: 0.2399 - accuracy: 0.8979 - val_loss: 0.2174 - val_accuracy: 0.5083\n","\n","Epoch 00216: val_loss did not improve from 0.00000\n","Epoch 217/1000\n"," - 11s - loss: 0.2406 - accuracy: 0.8980 - val_loss: 0.1278 - val_accuracy: 0.5155\n","\n","Epoch 00217: val_loss did not improve from 0.00000\n","Epoch 218/1000\n"," - 11s - loss: 0.2376 - accuracy: 0.8966 - val_loss: 2.6055 - val_accuracy: 0.5044\n","\n","Epoch 00218: val_loss did not improve from 0.00000\n","Epoch 219/1000\n"," - 11s - loss: 0.2376 - accuracy: 0.8990 - val_loss: 0.0241 - val_accuracy: 0.5086\n","\n","Epoch 00219: val_loss did not improve from 0.00000\n","Epoch 220/1000\n"," - 11s - loss: 0.2307 - accuracy: 0.8988 - val_loss: 0.1200 - val_accuracy: 0.5152\n","\n","Epoch 00220: val_loss did not improve from 0.00000\n","Epoch 221/1000\n"," - 11s - loss: 0.2336 - accuracy: 0.8988 - val_loss: 0.8748 - val_accuracy: 0.5062\n","\n","Epoch 00221: val_loss did not improve from 0.00000\n","Epoch 222/1000\n"," - 11s - loss: 0.2262 - accuracy: 0.9050 - val_loss: 0.0995 - val_accuracy: 0.5062\n","\n","Epoch 00222: val_loss did not improve from 0.00000\n","Epoch 223/1000\n"," - 11s - loss: 0.2292 - accuracy: 0.9029 - val_loss: 0.0539 - val_accuracy: 0.5086\n","\n","Epoch 00223: val_loss did not improve from 0.00000\n","Epoch 224/1000\n"," - 11s - loss: 0.2262 - accuracy: 0.9046 - val_loss: 0.0184 - val_accuracy: 0.5128\n","\n","Epoch 00224: val_loss did not improve from 0.00000\n","Epoch 225/1000\n"," - 11s - loss: 0.2218 - accuracy: 0.9037 - val_loss: 5.7204e-04 - val_accuracy: 0.5086\n","\n","Epoch 00225: val_loss did not improve from 0.00000\n","Epoch 226/1000\n"," - 11s - loss: 0.2267 - accuracy: 0.9015 - val_loss: 2.7718 - val_accuracy: 0.5110\n","\n","Epoch 00226: val_loss did not improve from 0.00000\n","Epoch 227/1000\n"," - 11s - loss: 0.2261 - accuracy: 0.9004 - val_loss: 0.3323 - val_accuracy: 0.5086\n","\n","Epoch 00227: val_loss did not improve from 0.00000\n","Epoch 228/1000\n"," - 11s - loss: 0.2294 - accuracy: 0.8964 - val_loss: 3.7631 - val_accuracy: 0.5029\n","\n","Epoch 00228: val_loss did not improve from 0.00000\n","Epoch 229/1000\n"," - 11s - loss: 0.2246 - accuracy: 0.9044 - val_loss: 0.0996 - val_accuracy: 0.5002\n","\n","Epoch 00229: val_loss did not improve from 0.00000\n","Epoch 230/1000\n"," - 11s - loss: 0.2195 - accuracy: 0.9065 - val_loss: 2.4836 - val_accuracy: 0.5071\n","\n","Epoch 00230: val_loss did not improve from 0.00000\n","Epoch 231/1000\n"," - 11s - loss: 0.2262 - accuracy: 0.9007 - val_loss: 0.3358 - val_accuracy: 0.5086\n","\n","Epoch 00231: val_loss did not improve from 0.00000\n","Epoch 232/1000\n"," - 11s - loss: 0.2239 - accuracy: 0.9053 - val_loss: 0.1612 - val_accuracy: 0.5062\n","\n","Epoch 00232: val_loss did not improve from 0.00000\n","Epoch 233/1000\n"," - 11s - loss: 0.2193 - accuracy: 0.9057 - val_loss: 1.3070 - val_accuracy: 0.5074\n","\n","Epoch 00233: val_loss did not improve from 0.00000\n","Epoch 234/1000\n"," - 11s - loss: 0.2126 - accuracy: 0.9086 - val_loss: 0.3901 - val_accuracy: 0.5011\n","\n","Epoch 00234: val_loss did not improve from 0.00000\n","Epoch 235/1000\n"," - 11s - loss: 0.2165 - accuracy: 0.9091 - val_loss: 1.7061 - val_accuracy: 0.5062\n","\n","Epoch 00235: val_loss did not improve from 0.00000\n","Epoch 236/1000\n"," - 11s - loss: 0.2171 - accuracy: 0.9096 - val_loss: 0.1800 - val_accuracy: 0.5137\n","\n","Epoch 00236: val_loss did not improve from 0.00000\n","Epoch 237/1000\n"," - 11s - loss: 0.2161 - accuracy: 0.9072 - val_loss: 3.9922 - val_accuracy: 0.5077\n","\n","Epoch 00237: val_loss did not improve from 0.00000\n","Epoch 238/1000\n"," - 11s - loss: 0.2130 - accuracy: 0.9096 - val_loss: 0.3425 - val_accuracy: 0.5161\n","\n","Epoch 00238: val_loss did not improve from 0.00000\n","Epoch 239/1000\n"," - 11s - loss: 0.2159 - accuracy: 0.9093 - val_loss: 0.0229 - val_accuracy: 0.4998\n","\n","Epoch 00239: val_loss did not improve from 0.00000\n","Epoch 240/1000\n"," - 11s - loss: 0.2159 - accuracy: 0.9100 - val_loss: 2.5453 - val_accuracy: 0.5089\n","\n","Epoch 00240: val_loss did not improve from 0.00000\n","Epoch 241/1000\n"," - 11s - loss: 0.2158 - accuracy: 0.9069 - val_loss: 6.5684 - val_accuracy: 0.5083\n","\n","Epoch 00241: val_loss did not improve from 0.00000\n","Epoch 242/1000\n"," - 11s - loss: 0.2095 - accuracy: 0.9109 - val_loss: 4.0882 - val_accuracy: 0.5080\n","\n","Epoch 00242: val_loss did not improve from 0.00000\n","Epoch 243/1000\n"," - 11s - loss: 0.2126 - accuracy: 0.9087 - val_loss: 6.6020 - val_accuracy: 0.5071\n","\n","Epoch 00243: val_loss did not improve from 0.00000\n","Epoch 244/1000\n"," - 11s - loss: 0.2176 - accuracy: 0.9039 - val_loss: 0.0105 - val_accuracy: 0.5158\n","\n","Epoch 00244: val_loss did not improve from 0.00000\n","Epoch 245/1000\n"," - 11s - loss: 0.2116 - accuracy: 0.9113 - val_loss: 6.6476 - val_accuracy: 0.5116\n","\n","Epoch 00245: val_loss did not improve from 0.00000\n","Epoch 246/1000\n"," - 11s - loss: 0.2052 - accuracy: 0.9125 - val_loss: 0.1767 - val_accuracy: 0.5164\n","\n","Epoch 00246: val_loss did not improve from 0.00000\n","Epoch 247/1000\n"," - 11s - loss: 0.2116 - accuracy: 0.9107 - val_loss: 11.1729 - val_accuracy: 0.5137\n","\n","Epoch 00247: val_loss did not improve from 0.00000\n","Epoch 248/1000\n"," - 11s - loss: 0.2099 - accuracy: 0.9090 - val_loss: 1.5349 - val_accuracy: 0.5137\n","\n","Epoch 00248: val_loss did not improve from 0.00000\n","Epoch 249/1000\n"," - 11s - loss: 0.2123 - accuracy: 0.9092 - val_loss: 1.0550 - val_accuracy: 0.5122\n","\n","Epoch 00249: val_loss did not improve from 0.00000\n","Epoch 250/1000\n"," - 11s - loss: 0.2117 - accuracy: 0.9124 - val_loss: 4.1664 - val_accuracy: 0.5092\n","\n","Epoch 00250: val_loss did not improve from 0.00000\n","Epoch 251/1000\n"," - 11s - loss: 0.2036 - accuracy: 0.9139 - val_loss: 0.2323 - val_accuracy: 0.5062\n","\n","Epoch 00251: val_loss did not improve from 0.00000\n","Epoch 252/1000\n"," - 11s - loss: 0.2045 - accuracy: 0.9125 - val_loss: 7.1405 - val_accuracy: 0.5158\n","\n","Epoch 00252: val_loss did not improve from 0.00000\n","Epoch 253/1000\n"," - 11s - loss: 0.2052 - accuracy: 0.9142 - val_loss: 5.5472 - val_accuracy: 0.5176\n","\n","Epoch 00253: val_loss did not improve from 0.00000\n","Epoch 254/1000\n"," - 11s - loss: 0.1991 - accuracy: 0.9161 - val_loss: 1.7399 - val_accuracy: 0.5044\n","\n","Epoch 00254: val_loss did not improve from 0.00000\n","Epoch 255/1000\n"," - 11s - loss: 0.2013 - accuracy: 0.9162 - val_loss: 0.0155 - val_accuracy: 0.5080\n","\n","Epoch 00255: val_loss did not improve from 0.00000\n","Epoch 256/1000\n"," - 11s - loss: 0.2012 - accuracy: 0.9153 - val_loss: 9.2383 - val_accuracy: 0.5104\n","\n","Epoch 00256: val_loss did not improve from 0.00000\n","Epoch 257/1000\n"," - 11s - loss: 0.1964 - accuracy: 0.9191 - val_loss: 0.6305 - val_accuracy: 0.5074\n","\n","Epoch 00257: val_loss did not improve from 0.00000\n","Epoch 258/1000\n"," - 11s - loss: 0.2011 - accuracy: 0.9126 - val_loss: 0.1865 - val_accuracy: 0.5086\n","\n","Epoch 00258: val_loss did not improve from 0.00000\n","Epoch 259/1000\n"," - 11s - loss: 0.2035 - accuracy: 0.9145 - val_loss: 11.4242 - val_accuracy: 0.5056\n","\n","Epoch 00259: val_loss did not improve from 0.00000\n","Epoch 260/1000\n"," - 11s - loss: 0.1979 - accuracy: 0.9157 - val_loss: 0.0701 - val_accuracy: 0.5137\n","\n","Epoch 00260: val_loss did not improve from 0.00000\n","Epoch 261/1000\n"," - 11s - loss: 0.2020 - accuracy: 0.9159 - val_loss: 5.1576 - val_accuracy: 0.5068\n","\n","Epoch 00261: val_loss did not improve from 0.00000\n","Epoch 262/1000\n"," - 11s - loss: 0.2000 - accuracy: 0.9178 - val_loss: 0.0212 - val_accuracy: 0.5125\n","\n","Epoch 00262: val_loss did not improve from 0.00000\n","Epoch 263/1000\n"," - 11s - loss: 0.1931 - accuracy: 0.9163 - val_loss: 0.0769 - val_accuracy: 0.5062\n","\n","Epoch 00263: val_loss did not improve from 0.00000\n","Epoch 264/1000\n"," - 11s - loss: 0.1918 - accuracy: 0.9215 - val_loss: 0.5407 - val_accuracy: 0.5113\n","\n","Epoch 00264: val_loss did not improve from 0.00000\n","Epoch 265/1000\n"," - 11s - loss: 0.2013 - accuracy: 0.9164 - val_loss: 3.9437 - val_accuracy: 0.5074\n","\n","Epoch 00265: val_loss did not improve from 0.00000\n","Epoch 266/1000\n"," - 11s - loss: 0.1941 - accuracy: 0.9192 - val_loss: 5.5670 - val_accuracy: 0.5149\n","\n","Epoch 00266: val_loss did not improve from 0.00000\n","Epoch 267/1000\n"," - 11s - loss: 0.1925 - accuracy: 0.9197 - val_loss: 7.7721e-05 - val_accuracy: 0.5119\n","\n","Epoch 00267: val_loss did not improve from 0.00000\n","Epoch 268/1000\n"," - 11s - loss: 0.1985 - accuracy: 0.9159 - val_loss: 5.1787 - val_accuracy: 0.5080\n","\n","Epoch 00268: val_loss did not improve from 0.00000\n","Epoch 269/1000\n"," - 11s - loss: 0.1957 - accuracy: 0.9152 - val_loss: 1.2642 - val_accuracy: 0.5119\n","\n","Epoch 00269: val_loss did not improve from 0.00000\n","Epoch 270/1000\n"," - 11s - loss: 0.1909 - accuracy: 0.9221 - val_loss: 0.0170 - val_accuracy: 0.5089\n","\n","Epoch 00270: val_loss did not improve from 0.00000\n","Epoch 271/1000\n"," - 11s - loss: 0.1914 - accuracy: 0.9205 - val_loss: 2.1141 - val_accuracy: 0.5113\n","\n","Epoch 00271: val_loss did not improve from 0.00000\n","Epoch 272/1000\n"," - 11s - loss: 0.1902 - accuracy: 0.9214 - val_loss: 2.9686 - val_accuracy: 0.5095\n","\n","Epoch 00272: val_loss did not improve from 0.00000\n","Epoch 273/1000\n"," - 11s - loss: 0.1873 - accuracy: 0.9239 - val_loss: 5.8336 - val_accuracy: 0.5089\n","\n","Epoch 00273: val_loss did not improve from 0.00000\n","Epoch 274/1000\n"," - 11s - loss: 0.1819 - accuracy: 0.9257 - val_loss: 0.3816 - val_accuracy: 0.5011\n","\n","Epoch 00274: val_loss did not improve from 0.00000\n","Epoch 275/1000\n"," - 11s - loss: 0.1940 - accuracy: 0.9170 - val_loss: 0.0015 - val_accuracy: 0.5074\n","\n","Epoch 00275: val_loss did not improve from 0.00000\n","Epoch 276/1000\n"," - 11s - loss: 0.1882 - accuracy: 0.9228 - val_loss: 3.1709e-05 - val_accuracy: 0.5122\n","\n","Epoch 00276: val_loss did not improve from 0.00000\n","Epoch 277/1000\n"," - 11s - loss: 0.1942 - accuracy: 0.9205 - val_loss: 8.1720e-04 - val_accuracy: 0.5131\n","\n","Epoch 00277: val_loss did not improve from 0.00000\n","Epoch 278/1000\n"," - 11s - loss: 0.1800 - accuracy: 0.9245 - val_loss: 4.3510e-05 - val_accuracy: 0.5077\n","\n","Epoch 00278: val_loss did not improve from 0.00000\n","Epoch 279/1000\n"," - 11s - loss: 0.1779 - accuracy: 0.9280 - val_loss: 6.6800 - val_accuracy: 0.5011\n","\n","Epoch 00279: val_loss did not improve from 0.00000\n","Epoch 280/1000\n"," - 11s - loss: 0.1886 - accuracy: 0.9235 - val_loss: 2.9792 - val_accuracy: 0.5104\n","\n","Epoch 00280: val_loss did not improve from 0.00000\n","Epoch 281/1000\n"," - 11s - loss: 0.1899 - accuracy: 0.9187 - val_loss: 4.5629 - val_accuracy: 0.5083\n","\n","Epoch 00281: val_loss did not improve from 0.00000\n","Epoch 282/1000\n"," - 11s - loss: 0.1789 - accuracy: 0.9252 - val_loss: 0.0831 - val_accuracy: 0.5092\n","\n","Epoch 00282: val_loss did not improve from 0.00000\n","Epoch 283/1000\n"," - 11s - loss: 0.1853 - accuracy: 0.9223 - val_loss: 1.9112 - val_accuracy: 0.5083\n","\n","Epoch 00283: val_loss did not improve from 0.00000\n","Epoch 284/1000\n"," - 11s - loss: 0.1753 - accuracy: 0.9297 - val_loss: 0.0991 - val_accuracy: 0.5104\n","\n","Epoch 00284: val_loss did not improve from 0.00000\n","Epoch 285/1000\n"," - 11s - loss: 0.1834 - accuracy: 0.9218 - val_loss: 0.0183 - val_accuracy: 0.5128\n","\n","Epoch 00285: val_loss did not improve from 0.00000\n","Epoch 286/1000\n"," - 11s - loss: 0.1859 - accuracy: 0.9253 - val_loss: 2.0479 - val_accuracy: 0.5125\n","\n","Epoch 00286: val_loss did not improve from 0.00000\n","Epoch 287/1000\n"," - 11s - loss: 0.1842 - accuracy: 0.9229 - val_loss: 0.0968 - val_accuracy: 0.5020\n","\n","Epoch 00287: val_loss did not improve from 0.00000\n","Epoch 288/1000\n"," - 11s - loss: 0.1827 - accuracy: 0.9228 - val_loss: 0.4259 - val_accuracy: 0.5053\n","\n","Epoch 00288: val_loss did not improve from 0.00000\n","Epoch 289/1000\n"," - 11s - loss: 0.1805 - accuracy: 0.9243 - val_loss: 0.0433 - val_accuracy: 0.5077\n","\n","Epoch 00289: val_loss did not improve from 0.00000\n","Epoch 290/1000\n"," - 11s - loss: 0.1775 - accuracy: 0.9259 - val_loss: 1.9481 - val_accuracy: 0.5155\n","\n","Epoch 00290: val_loss did not improve from 0.00000\n","Epoch 291/1000\n"," - 11s - loss: 0.1763 - accuracy: 0.9253 - val_loss: 1.9979 - val_accuracy: 0.5056\n","\n","Epoch 00291: val_loss did not improve from 0.00000\n","Epoch 292/1000\n"," - 11s - loss: 0.1730 - accuracy: 0.9284 - val_loss: 0.3106 - val_accuracy: 0.5110\n","\n","Epoch 00292: val_loss did not improve from 0.00000\n","Epoch 293/1000\n"," - 11s - loss: 0.1771 - accuracy: 0.9254 - val_loss: 4.5082 - val_accuracy: 0.5113\n","\n","Epoch 00293: val_loss did not improve from 0.00000\n","Epoch 294/1000\n"," - 11s - loss: 0.1789 - accuracy: 0.9274 - val_loss: 0.2024 - val_accuracy: 0.5059\n","\n","Epoch 00294: val_loss did not improve from 0.00000\n","Epoch 295/1000\n"," - 11s - loss: 0.1715 - accuracy: 0.9319 - val_loss: 0.0838 - val_accuracy: 0.5065\n","\n","Epoch 00295: val_loss did not improve from 0.00000\n","Epoch 296/1000\n"," - 11s - loss: 0.1726 - accuracy: 0.9266 - val_loss: 13.0630 - val_accuracy: 0.5089\n","\n","Epoch 00296: val_loss did not improve from 0.00000\n","Epoch 297/1000\n"," - 11s - loss: 0.1748 - accuracy: 0.9295 - val_loss: 0.0090 - val_accuracy: 0.5068\n","\n","Epoch 00297: val_loss did not improve from 0.00000\n","Epoch 298/1000\n"," - 11s - loss: 0.1725 - accuracy: 0.9289 - val_loss: 3.5931 - val_accuracy: 0.5086\n","\n","Epoch 00298: val_loss did not improve from 0.00000\n","Epoch 299/1000\n"," - 11s - loss: 0.1730 - accuracy: 0.9299 - val_loss: 3.4186 - val_accuracy: 0.5143\n","\n","Epoch 00299: val_loss did not improve from 0.00000\n","Epoch 300/1000\n"," - 11s - loss: 0.1775 - accuracy: 0.9256 - val_loss: 9.7900 - val_accuracy: 0.5047\n","\n","Epoch 00300: val_loss did not improve from 0.00000\n","Epoch 301/1000\n"," - 11s - loss: 0.1766 - accuracy: 0.9228 - val_loss: 0.0209 - val_accuracy: 0.5071\n","\n","Epoch 00301: val_loss did not improve from 0.00000\n","Epoch 302/1000\n"," - 11s - loss: 0.1674 - accuracy: 0.9365 - val_loss: 0.0163 - val_accuracy: 0.5023\n","\n","Epoch 00302: val_loss did not improve from 0.00000\n","Epoch 303/1000\n"," - 11s - loss: 0.1620 - accuracy: 0.9363 - val_loss: 0.8259 - val_accuracy: 0.5134\n","\n","Epoch 00303: val_loss did not improve from 0.00000\n","Epoch 304/1000\n"," - 11s - loss: 0.1673 - accuracy: 0.9302 - val_loss: 4.8907 - val_accuracy: 0.5083\n","\n","Epoch 00304: val_loss did not improve from 0.00000\n","Epoch 305/1000\n"," - 11s - loss: 0.1750 - accuracy: 0.9282 - val_loss: 3.1316 - val_accuracy: 0.5065\n","\n","Epoch 00305: val_loss did not improve from 0.00000\n","Epoch 306/1000\n"," - 11s - loss: 0.1737 - accuracy: 0.9283 - val_loss: 1.3474 - val_accuracy: 0.5110\n","\n","Epoch 00306: val_loss did not improve from 0.00000\n","Epoch 307/1000\n"," - 11s - loss: 0.1634 - accuracy: 0.9335 - val_loss: 0.0000e+00 - val_accuracy: 0.5038\n","\n","Epoch 00307: val_loss did not improve from 0.00000\n","Epoch 308/1000\n"," - 11s - loss: 0.1667 - accuracy: 0.9309 - val_loss: 8.3375 - val_accuracy: 0.5050\n","\n","Epoch 00308: val_loss did not improve from 0.00000\n","Epoch 309/1000\n"," - 11s - loss: 0.1650 - accuracy: 0.9331 - val_loss: 0.0066 - val_accuracy: 0.5020\n","\n","Epoch 00309: val_loss did not improve from 0.00000\n","Epoch 310/1000\n"," - 11s - loss: 0.1699 - accuracy: 0.9293 - val_loss: 0.0039 - val_accuracy: 0.5107\n","\n","Epoch 00310: val_loss did not improve from 0.00000\n","Epoch 311/1000\n"," - 11s - loss: 0.1649 - accuracy: 0.9335 - val_loss: 4.0660 - val_accuracy: 0.5029\n","\n","Epoch 00311: val_loss did not improve from 0.00000\n","Epoch 312/1000\n"," - 11s - loss: 0.1651 - accuracy: 0.9350 - val_loss: 6.6388 - val_accuracy: 0.4977\n","\n","Epoch 00312: val_loss did not improve from 0.00000\n","Epoch 313/1000\n"," - 11s - loss: 0.1634 - accuracy: 0.9345 - val_loss: 2.1502 - val_accuracy: 0.5026\n","\n","Epoch 00313: val_loss did not improve from 0.00000\n","Epoch 314/1000\n"," - 11s - loss: 0.1619 - accuracy: 0.9339 - val_loss: 3.0106 - val_accuracy: 0.5086\n","\n","Epoch 00314: val_loss did not improve from 0.00000\n","Epoch 315/1000\n"," - 11s - loss: 0.1582 - accuracy: 0.9364 - val_loss: 7.7486e-06 - val_accuracy: 0.5059\n","\n","Epoch 00315: val_loss did not improve from 0.00000\n","Epoch 316/1000\n"," - 11s - loss: 0.1603 - accuracy: 0.9349 - val_loss: 5.1358 - val_accuracy: 0.5086\n","\n","Epoch 00316: val_loss did not improve from 0.00000\n","Epoch 317/1000\n"," - 11s - loss: 0.1647 - accuracy: 0.9342 - val_loss: 9.1429e-05 - val_accuracy: 0.5137\n","\n","Epoch 00317: val_loss did not improve from 0.00000\n","Epoch 318/1000\n"," - 11s - loss: 0.1659 - accuracy: 0.9314 - val_loss: 0.0154 - val_accuracy: 0.5083\n","\n","Epoch 00318: val_loss did not improve from 0.00000\n","Epoch 319/1000\n"," - 11s - loss: 0.1569 - accuracy: 0.9380 - val_loss: 5.7338e-05 - val_accuracy: 0.5077\n","\n","Epoch 00319: val_loss did not improve from 0.00000\n","Epoch 320/1000\n"," - 11s - loss: 0.1654 - accuracy: 0.9323 - val_loss: 3.8460 - val_accuracy: 0.5101\n","\n","Epoch 00320: val_loss did not improve from 0.00000\n","Epoch 321/1000\n"," - 11s - loss: 0.1541 - accuracy: 0.9378 - val_loss: 13.3842 - val_accuracy: 0.5032\n","\n","Epoch 00321: val_loss did not improve from 0.00000\n","Epoch 322/1000\n"," - 11s - loss: 0.1572 - accuracy: 0.9370 - val_loss: 1.3709e-05 - val_accuracy: 0.5068\n","\n","Epoch 00322: val_loss did not improve from 0.00000\n","Epoch 323/1000\n"," - 11s - loss: 0.1553 - accuracy: 0.9358 - val_loss: 1.0727 - val_accuracy: 0.5065\n","\n","Epoch 00323: val_loss did not improve from 0.00000\n","Epoch 324/1000\n"," - 11s - loss: 0.1594 - accuracy: 0.9355 - val_loss: 5.3818 - val_accuracy: 0.5047\n","\n","Epoch 00324: val_loss did not improve from 0.00000\n","Epoch 325/1000\n"," - 11s - loss: 0.1579 - accuracy: 0.9385 - val_loss: 5.0147 - val_accuracy: 0.5083\n","\n","Epoch 00325: val_loss did not improve from 0.00000\n","Epoch 326/1000\n"," - 11s - loss: 0.1576 - accuracy: 0.9366 - val_loss: 1.1214 - val_accuracy: 0.5089\n","\n","Epoch 00326: val_loss did not improve from 0.00000\n","Epoch 327/1000\n"," - 11s - loss: 0.1555 - accuracy: 0.9374 - val_loss: 0.4208 - val_accuracy: 0.5032\n","\n","Epoch 00327: val_loss did not improve from 0.00000\n","Epoch 328/1000\n"," - 11s - loss: 0.1531 - accuracy: 0.9394 - val_loss: 0.0059 - val_accuracy: 0.5101\n","\n","Epoch 00328: val_loss did not improve from 0.00000\n","Epoch 329/1000\n"," - 11s - loss: 0.1539 - accuracy: 0.9376 - val_loss: 0.0968 - val_accuracy: 0.5128\n","\n","Epoch 00329: val_loss did not improve from 0.00000\n","Epoch 330/1000\n"," - 11s - loss: 0.1500 - accuracy: 0.9405 - val_loss: 7.9281 - val_accuracy: 0.5062\n","\n","Epoch 00330: val_loss did not improve from 0.00000\n","Epoch 331/1000\n"," - 11s - loss: 0.1513 - accuracy: 0.9409 - val_loss: 2.0444 - val_accuracy: 0.5059\n","\n","Epoch 00331: val_loss did not improve from 0.00000\n","Epoch 332/1000\n"," - 11s - loss: 0.1529 - accuracy: 0.9398 - val_loss: 2.3470 - val_accuracy: 0.5128\n","\n","Epoch 00332: val_loss did not improve from 0.00000\n","Epoch 333/1000\n"," - 11s - loss: 0.1468 - accuracy: 0.9408 - val_loss: 3.5780e-04 - val_accuracy: 0.5107\n","\n","Epoch 00333: val_loss did not improve from 0.00000\n","Epoch 334/1000\n"," - 11s - loss: 0.1576 - accuracy: 0.9347 - val_loss: 11.7307 - val_accuracy: 0.5080\n","\n","Epoch 00334: val_loss did not improve from 0.00000\n","Epoch 335/1000\n"," - 11s - loss: 0.1535 - accuracy: 0.9398 - val_loss: 3.5120 - val_accuracy: 0.5083\n","\n","Epoch 00335: val_loss did not improve from 0.00000\n","Epoch 336/1000\n"," - 11s - loss: 0.1514 - accuracy: 0.9380 - val_loss: 8.8592 - val_accuracy: 0.5122\n","\n","Epoch 00336: val_loss did not improve from 0.00000\n","Epoch 337/1000\n"," - 11s - loss: 0.1550 - accuracy: 0.9372 - val_loss: 7.9120 - val_accuracy: 0.5050\n","\n","Epoch 00337: val_loss did not improve from 0.00000\n","Epoch 338/1000\n"," - 11s - loss: 0.1459 - accuracy: 0.9423 - val_loss: 0.4690 - val_accuracy: 0.5110\n","\n","Epoch 00338: val_loss did not improve from 0.00000\n","Epoch 339/1000\n"," - 11s - loss: 0.1436 - accuracy: 0.9420 - val_loss: 3.8872 - val_accuracy: 0.5098\n","\n","Epoch 00339: val_loss did not improve from 0.00000\n","Epoch 340/1000\n"," - 11s - loss: 0.1500 - accuracy: 0.9387 - val_loss: 5.2359 - val_accuracy: 0.5143\n","\n","Epoch 00340: val_loss did not improve from 0.00000\n","Epoch 341/1000\n"," - 11s - loss: 0.1495 - accuracy: 0.9392 - val_loss: 7.5933e-05 - val_accuracy: 0.5137\n","\n","Epoch 00341: val_loss did not improve from 0.00000\n","Epoch 342/1000\n"," - 11s - loss: 0.1431 - accuracy: 0.9440 - val_loss: 3.5566 - val_accuracy: 0.5098\n","\n","Epoch 00342: val_loss did not improve from 0.00000\n","Epoch 343/1000\n"," - 11s - loss: 0.1394 - accuracy: 0.9451 - val_loss: 14.5391 - val_accuracy: 0.5110\n","\n","Epoch 00343: val_loss did not improve from 0.00000\n","Epoch 344/1000\n"," - 11s - loss: 0.1461 - accuracy: 0.9445 - val_loss: 0.3668 - val_accuracy: 0.5074\n","\n","Epoch 00344: val_loss did not improve from 0.00000\n","Epoch 345/1000\n"," - 11s - loss: 0.1454 - accuracy: 0.9399 - val_loss: 7.3631 - val_accuracy: 0.5095\n","\n","Epoch 00345: val_loss did not improve from 0.00000\n","Epoch 346/1000\n"," - 11s - loss: 0.1494 - accuracy: 0.9384 - val_loss: 3.1896 - val_accuracy: 0.5074\n","\n","Epoch 00346: val_loss did not improve from 0.00000\n","Epoch 347/1000\n"," - 11s - loss: 0.1443 - accuracy: 0.9436 - val_loss: 1.3737 - val_accuracy: 0.5107\n","\n","Epoch 00347: val_loss did not improve from 0.00000\n","Epoch 348/1000\n"," - 11s - loss: 0.1491 - accuracy: 0.9402 - val_loss: 0.0000e+00 - val_accuracy: 0.5110\n","\n","Epoch 00348: val_loss did not improve from 0.00000\n","Epoch 349/1000\n"," - 11s - loss: 0.1438 - accuracy: 0.9418 - val_loss: 0.2188 - val_accuracy: 0.5038\n","\n","Epoch 00349: val_loss did not improve from 0.00000\n","Epoch 350/1000\n"," - 11s - loss: 0.1378 - accuracy: 0.9446 - val_loss: 0.0820 - val_accuracy: 0.5074\n","\n","Epoch 00350: val_loss did not improve from 0.00000\n","Epoch 351/1000\n"," - 11s - loss: 0.1432 - accuracy: 0.9430 - val_loss: 0.0044 - val_accuracy: 0.5107\n","\n","Epoch 00351: val_loss did not improve from 0.00000\n","Epoch 352/1000\n"," - 11s - loss: 0.1420 - accuracy: 0.9429 - val_loss: 4.2751e-04 - val_accuracy: 0.5134\n","\n","Epoch 00352: val_loss did not improve from 0.00000\n","Epoch 353/1000\n"," - 11s - loss: 0.1421 - accuracy: 0.9401 - val_loss: 1.6202 - val_accuracy: 0.5128\n","\n","Epoch 00353: val_loss did not improve from 0.00000\n","Epoch 354/1000\n"," - 11s - loss: 0.1447 - accuracy: 0.9402 - val_loss: 1.9280 - val_accuracy: 0.5041\n","\n","Epoch 00354: val_loss did not improve from 0.00000\n","Epoch 355/1000\n"," - 11s - loss: 0.1367 - accuracy: 0.9467 - val_loss: 3.0950 - val_accuracy: 0.5020\n","\n","Epoch 00355: val_loss did not improve from 0.00000\n","Epoch 356/1000\n"," - 11s - loss: 0.1381 - accuracy: 0.9458 - val_loss: 0.5550 - val_accuracy: 0.5101\n","\n","Epoch 00356: val_loss did not improve from 0.00000\n","Epoch 357/1000\n"," - 11s - loss: 0.1420 - accuracy: 0.9428 - val_loss: 6.9616e-05 - val_accuracy: 0.5086\n","\n","Epoch 00357: val_loss did not improve from 0.00000\n","Epoch 358/1000\n"," - 11s - loss: 0.1382 - accuracy: 0.9455 - val_loss: 1.1921e-07 - val_accuracy: 0.5149\n","\n","Epoch 00358: val_loss did not improve from 0.00000\n","Epoch 359/1000\n"," - 11s - loss: 0.1383 - accuracy: 0.9450 - val_loss: 9.2225e-04 - val_accuracy: 0.5101\n","\n","Epoch 00359: val_loss did not improve from 0.00000\n","Epoch 360/1000\n"," - 11s - loss: 0.1341 - accuracy: 0.9474 - val_loss: 6.9044 - val_accuracy: 0.5086\n","\n","Epoch 00360: val_loss did not improve from 0.00000\n","Epoch 361/1000\n"," - 11s - loss: 0.1384 - accuracy: 0.9451 - val_loss: 0.0027 - val_accuracy: 0.5116\n","\n","Epoch 00361: val_loss did not improve from 0.00000\n","Epoch 362/1000\n"," - 11s - loss: 0.1357 - accuracy: 0.9452 - val_loss: 7.2119e-05 - val_accuracy: 0.5092\n","\n","Epoch 00362: val_loss did not improve from 0.00000\n","Epoch 363/1000\n"," - 11s - loss: 0.1372 - accuracy: 0.9417 - val_loss: 14.2264 - val_accuracy: 0.5125\n","\n","Epoch 00363: val_loss did not improve from 0.00000\n","Epoch 364/1000\n"," - 11s - loss: 0.1352 - accuracy: 0.9464 - val_loss: 1.9426 - val_accuracy: 0.5071\n","\n","Epoch 00364: val_loss did not improve from 0.00000\n","Epoch 365/1000\n"," - 11s - loss: 0.1380 - accuracy: 0.9452 - val_loss: 4.4095 - val_accuracy: 0.5098\n","\n","Epoch 00365: val_loss did not improve from 0.00000\n","Epoch 366/1000\n"," - 11s - loss: 0.1380 - accuracy: 0.9461 - val_loss: 2.1300e-04 - val_accuracy: 0.5005\n","\n","Epoch 00366: val_loss did not improve from 0.00000\n","Epoch 367/1000\n"," - 11s - loss: 0.1368 - accuracy: 0.9448 - val_loss: 0.0013 - val_accuracy: 0.5107\n","\n","Epoch 00367: val_loss did not improve from 0.00000\n","Epoch 368/1000\n"," - 11s - loss: 0.1329 - accuracy: 0.9473 - val_loss: 0.0311 - val_accuracy: 0.4989\n","\n","Epoch 00368: val_loss did not improve from 0.00000\n","Epoch 369/1000\n"," - 11s - loss: 0.1383 - accuracy: 0.9464 - val_loss: 3.8699 - val_accuracy: 0.5047\n","\n","Epoch 00369: val_loss did not improve from 0.00000\n","Epoch 370/1000\n"," - 11s - loss: 0.1360 - accuracy: 0.9476 - val_loss: 0.0014 - val_accuracy: 0.5017\n","\n","Epoch 00370: val_loss did not improve from 0.00000\n","Epoch 371/1000\n"," - 11s - loss: 0.1328 - accuracy: 0.9467 - val_loss: 0.5723 - val_accuracy: 0.5071\n","\n","Epoch 00371: val_loss did not improve from 0.00000\n","Epoch 372/1000\n"," - 11s - loss: 0.1276 - accuracy: 0.9501 - val_loss: 6.0588e-04 - val_accuracy: 0.5008\n","\n","Epoch 00372: val_loss did not improve from 0.00000\n","Epoch 373/1000\n"," - 11s - loss: 0.1343 - accuracy: 0.9488 - val_loss: 5.6429 - val_accuracy: 0.5050\n","\n","Epoch 00373: val_loss did not improve from 0.00000\n","Epoch 374/1000\n"," - 11s - loss: 0.1346 - accuracy: 0.9457 - val_loss: 1.0279 - val_accuracy: 0.5068\n","\n","Epoch 00374: val_loss did not improve from 0.00000\n","Epoch 375/1000\n"," - 11s - loss: 0.1341 - accuracy: 0.9485 - val_loss: 1.4263 - val_accuracy: 0.5089\n","\n","Epoch 00375: val_loss did not improve from 0.00000\n","Epoch 376/1000\n"," - 11s - loss: 0.1354 - accuracy: 0.9460 - val_loss: 1.1921e-07 - val_accuracy: 0.5086\n","\n","Epoch 00376: val_loss did not improve from 0.00000\n","Epoch 377/1000\n"," - 11s - loss: 0.1321 - accuracy: 0.9475 - val_loss: 3.0247 - val_accuracy: 0.5092\n","\n","Epoch 00377: val_loss did not improve from 0.00000\n","Epoch 378/1000\n"," - 11s - loss: 0.1293 - accuracy: 0.9499 - val_loss: 3.5970 - val_accuracy: 0.5032\n","\n","Epoch 00378: val_loss did not improve from 0.00000\n","Epoch 379/1000\n"," - 11s - loss: 0.1236 - accuracy: 0.9538 - val_loss: 6.5985 - val_accuracy: 0.5008\n","\n","Epoch 00379: val_loss did not improve from 0.00000\n","Epoch 380/1000\n"," - 11s - loss: 0.1251 - accuracy: 0.9538 - val_loss: 4.8192 - val_accuracy: 0.5101\n","\n","Epoch 00380: val_loss did not improve from 0.00000\n","Epoch 381/1000\n"," - 11s - loss: 0.1313 - accuracy: 0.9499 - val_loss: 1.4820 - val_accuracy: 0.5095\n","\n","Epoch 00381: val_loss did not improve from 0.00000\n","Epoch 382/1000\n"," - 11s - loss: 0.1232 - accuracy: 0.9529 - val_loss: 0.0288 - val_accuracy: 0.5080\n","\n","Epoch 00382: val_loss did not improve from 0.00000\n","Epoch 383/1000\n"," - 11s - loss: 0.1325 - accuracy: 0.9479 - val_loss: 0.2569 - val_accuracy: 0.5119\n","\n","Epoch 00383: val_loss did not improve from 0.00000\n","Epoch 384/1000\n"," - 11s - loss: 0.1291 - accuracy: 0.9507 - val_loss: 6.2882 - val_accuracy: 0.5062\n","\n","Epoch 00384: val_loss did not improve from 0.00000\n","Epoch 385/1000\n"," - 11s - loss: 0.1269 - accuracy: 0.9500 - val_loss: 2.7236e-04 - val_accuracy: 0.5056\n","\n","Epoch 00385: val_loss did not improve from 0.00000\n","Epoch 386/1000\n"," - 11s - loss: 0.1258 - accuracy: 0.9497 - val_loss: 8.7871 - val_accuracy: 0.5047\n","\n","Epoch 00386: val_loss did not improve from 0.00000\n","Epoch 387/1000\n"," - 11s - loss: 0.1276 - accuracy: 0.9501 - val_loss: 28.1102 - val_accuracy: 0.5086\n","\n","Epoch 00387: val_loss did not improve from 0.00000\n","Epoch 388/1000\n"," - 11s - loss: 0.1194 - accuracy: 0.9553 - val_loss: 0.0021 - val_accuracy: 0.5113\n","\n","Epoch 00388: val_loss did not improve from 0.00000\n","Epoch 389/1000\n"," - 11s - loss: 0.1221 - accuracy: 0.9529 - val_loss: 0.3482 - val_accuracy: 0.5062\n","\n","Epoch 00389: val_loss did not improve from 0.00000\n","Epoch 390/1000\n"," - 11s - loss: 0.1255 - accuracy: 0.9538 - val_loss: 10.2383 - val_accuracy: 0.5053\n","\n","Epoch 00390: val_loss did not improve from 0.00000\n","Epoch 391/1000\n"," - 11s - loss: 0.1288 - accuracy: 0.9483 - val_loss: 2.3484e-05 - val_accuracy: 0.5029\n","\n","Epoch 00391: val_loss did not improve from 0.00000\n","Epoch 392/1000\n"," - 11s - loss: 0.1259 - accuracy: 0.9482 - val_loss: 2.8880e-04 - val_accuracy: 0.5137\n","\n","Epoch 00392: val_loss did not improve from 0.00000\n","Epoch 393/1000\n"," - 11s - loss: 0.1240 - accuracy: 0.9526 - val_loss: 2.4447e-04 - val_accuracy: 0.5056\n","\n","Epoch 00393: val_loss did not improve from 0.00000\n","Epoch 394/1000\n"," - 11s - loss: 0.1246 - accuracy: 0.9514 - val_loss: 3.8251 - val_accuracy: 0.5128\n","\n","Epoch 00394: val_loss did not improve from 0.00000\n","Epoch 395/1000\n"," - 11s - loss: 0.1201 - accuracy: 0.9550 - val_loss: 3.4755e-04 - val_accuracy: 0.5083\n","\n","Epoch 00395: val_loss did not improve from 0.00000\n","Epoch 396/1000\n"," - 11s - loss: 0.1270 - accuracy: 0.9504 - val_loss: 1.4971 - val_accuracy: 0.5029\n","\n","Epoch 00396: val_loss did not improve from 0.00000\n","Epoch 397/1000\n"," - 11s - loss: 0.1208 - accuracy: 0.9548 - val_loss: 4.4825 - val_accuracy: 0.5131\n","\n","Epoch 00397: val_loss did not improve from 0.00000\n","Epoch 398/1000\n"," - 11s - loss: 0.1234 - accuracy: 0.9495 - val_loss: 2.3355 - val_accuracy: 0.5107\n","\n","Epoch 00398: val_loss did not improve from 0.00000\n","Epoch 399/1000\n"," - 11s - loss: 0.1235 - accuracy: 0.9525 - val_loss: 2.9721 - val_accuracy: 0.5080\n","\n","Epoch 00399: val_loss did not improve from 0.00000\n","Epoch 400/1000\n"," - 11s - loss: 0.1198 - accuracy: 0.9540 - val_loss: 4.1784 - val_accuracy: 0.5176\n","\n","Epoch 00400: val_loss did not improve from 0.00000\n","Epoch 401/1000\n"," - 11s - loss: 0.1203 - accuracy: 0.9544 - val_loss: 9.7634 - val_accuracy: 0.5086\n","\n","Epoch 00401: val_loss did not improve from 0.00000\n","Epoch 402/1000\n"," - 11s - loss: 0.1198 - accuracy: 0.9524 - val_loss: 9.2862 - val_accuracy: 0.5071\n","\n","Epoch 00402: val_loss did not improve from 0.00000\n","Epoch 403/1000\n"," - 11s - loss: 0.1211 - accuracy: 0.9535 - val_loss: 8.7381 - val_accuracy: 0.5005\n","\n","Epoch 00403: val_loss did not improve from 0.00000\n","Epoch 404/1000\n"," - 11s - loss: 0.1222 - accuracy: 0.9527 - val_loss: 1.4305e-06 - val_accuracy: 0.5041\n","\n","Epoch 00404: val_loss did not improve from 0.00000\n","Epoch 405/1000\n"," - 11s - loss: 0.1196 - accuracy: 0.9537 - val_loss: 2.9350 - val_accuracy: 0.5101\n","\n","Epoch 00405: val_loss did not improve from 0.00000\n","Epoch 406/1000\n"," - 11s - loss: 0.1235 - accuracy: 0.9533 - val_loss: 6.9308 - val_accuracy: 0.5152\n","\n","Epoch 00406: val_loss did not improve from 0.00000\n","Epoch 407/1000\n"," - 11s - loss: 0.1156 - accuracy: 0.9567 - val_loss: 3.2661 - val_accuracy: 0.5044\n","\n","Epoch 00407: val_loss did not improve from 0.00000\n","Epoch 408/1000\n"," - 11s - loss: 0.1161 - accuracy: 0.9554 - val_loss: 16.7484 - val_accuracy: 0.5053\n","\n","Epoch 00408: val_loss did not improve from 0.00000\n","Epoch 409/1000\n"," - 11s - loss: 0.1150 - accuracy: 0.9571 - val_loss: 0.0000e+00 - val_accuracy: 0.5029\n","\n","Epoch 00409: val_loss did not improve from 0.00000\n","Epoch 410/1000\n"," - 11s - loss: 0.1198 - accuracy: 0.9513 - val_loss: 0.0000e+00 - val_accuracy: 0.5092\n","\n","Epoch 00410: val_loss did not improve from 0.00000\n","Epoch 411/1000\n"," - 11s - loss: 0.1181 - accuracy: 0.9565 - val_loss: 0.5797 - val_accuracy: 0.5062\n","\n","Epoch 00411: val_loss did not improve from 0.00000\n","Epoch 412/1000\n"," - 11s - loss: 0.1156 - accuracy: 0.9525 - val_loss: 0.0010 - val_accuracy: 0.5038\n","\n","Epoch 00412: val_loss did not improve from 0.00000\n","Epoch 413/1000\n"," - 11s - loss: 0.1210 - accuracy: 0.9524 - val_loss: 15.6047 - val_accuracy: 0.5050\n","\n","Epoch 00413: val_loss did not improve from 0.00000\n","Epoch 414/1000\n"," - 11s - loss: 0.1140 - accuracy: 0.9569 - val_loss: 9.4184 - val_accuracy: 0.5056\n","\n","Epoch 00414: val_loss did not improve from 0.00000\n","Epoch 415/1000\n"," - 11s - loss: 0.1154 - accuracy: 0.9563 - val_loss: 0.1695 - val_accuracy: 0.5068\n","\n","Epoch 00415: val_loss did not improve from 0.00000\n","Epoch 416/1000\n"," - 11s - loss: 0.1159 - accuracy: 0.9552 - val_loss: 3.0114 - val_accuracy: 0.4992\n","\n","Epoch 00416: val_loss did not improve from 0.00000\n","Epoch 417/1000\n"," - 11s - loss: 0.1178 - accuracy: 0.9531 - val_loss: 3.5763e-07 - val_accuracy: 0.5020\n","\n","Epoch 00417: val_loss did not improve from 0.00000\n","Epoch 418/1000\n"," - 11s - loss: 0.1093 - accuracy: 0.9593 - val_loss: 1.4479 - val_accuracy: 0.5062\n","\n","Epoch 00418: val_loss did not improve from 0.00000\n","Epoch 419/1000\n"," - 11s - loss: 0.1092 - accuracy: 0.9587 - val_loss: 8.8694 - val_accuracy: 0.5047\n","\n","Epoch 00419: val_loss did not improve from 0.00000\n","Epoch 420/1000\n"," - 11s - loss: 0.1192 - accuracy: 0.9526 - val_loss: 4.1225 - val_accuracy: 0.5074\n","\n","Epoch 00420: val_loss did not improve from 0.00000\n","Epoch 421/1000\n"," - 11s - loss: 0.1117 - accuracy: 0.9571 - val_loss: 0.0496 - val_accuracy: 0.5053\n","\n","Epoch 00421: val_loss did not improve from 0.00000\n","Epoch 422/1000\n"," - 11s - loss: 0.1137 - accuracy: 0.9557 - val_loss: 8.2735 - val_accuracy: 0.5104\n","\n","Epoch 00422: val_loss did not improve from 0.00000\n","Epoch 423/1000\n"," - 11s - loss: 0.1144 - accuracy: 0.9561 - val_loss: 1.4082 - val_accuracy: 0.5116\n","\n","Epoch 00423: val_loss did not improve from 0.00000\n","Epoch 424/1000\n"," - 11s - loss: 0.1091 - accuracy: 0.9604 - val_loss: 0.1919 - val_accuracy: 0.5077\n","\n","Epoch 00424: val_loss did not improve from 0.00000\n","Epoch 425/1000\n"," - 11s - loss: 0.1074 - accuracy: 0.9585 - val_loss: 2.8610e-06 - val_accuracy: 0.5146\n","\n","Epoch 00425: val_loss did not improve from 0.00000\n","Epoch 426/1000\n"," - 11s - loss: 0.1124 - accuracy: 0.9559 - val_loss: 1.1822 - val_accuracy: 0.5128\n","\n","Epoch 00426: val_loss did not improve from 0.00000\n","Epoch 427/1000\n"," - 11s - loss: 0.1089 - accuracy: 0.9594 - val_loss: 0.0138 - val_accuracy: 0.5122\n","\n","Epoch 00427: val_loss did not improve from 0.00000\n","Epoch 428/1000\n"," - 11s - loss: 0.1060 - accuracy: 0.9613 - val_loss: 0.0195 - val_accuracy: 0.5119\n","\n","Epoch 00428: val_loss did not improve from 0.00000\n","Epoch 429/1000\n"," - 11s - loss: 0.1105 - accuracy: 0.9586 - val_loss: 0.0043 - val_accuracy: 0.5023\n","\n","Epoch 00429: val_loss did not improve from 0.00000\n","Epoch 430/1000\n"," - 11s - loss: 0.1123 - accuracy: 0.9545 - val_loss: 8.8021e-04 - val_accuracy: 0.5110\n","\n","Epoch 00430: val_loss did not improve from 0.00000\n","Epoch 431/1000\n"," - 11s - loss: 0.1121 - accuracy: 0.9571 - val_loss: 3.6854 - val_accuracy: 0.5068\n","\n","Epoch 00431: val_loss did not improve from 0.00000\n","Epoch 432/1000\n"," - 11s - loss: 0.1119 - accuracy: 0.9550 - val_loss: 3.0972 - val_accuracy: 0.4998\n","\n","Epoch 00432: val_loss did not improve from 0.00000\n","Epoch 433/1000\n"," - 11s - loss: 0.1073 - accuracy: 0.9593 - val_loss: 9.5367e-07 - val_accuracy: 0.5137\n","\n","Epoch 00433: val_loss did not improve from 0.00000\n","Epoch 434/1000\n"," - 11s - loss: 0.1101 - accuracy: 0.9574 - val_loss: 13.6026 - val_accuracy: 0.5041\n","\n","Epoch 00434: val_loss did not improve from 0.00000\n","Epoch 435/1000\n"," - 11s - loss: 0.1016 - accuracy: 0.9639 - val_loss: 0.0172 - val_accuracy: 0.5065\n","\n","Epoch 00435: val_loss did not improve from 0.00000\n","Epoch 436/1000\n"," - 11s - loss: 0.1048 - accuracy: 0.9597 - val_loss: 4.7377 - val_accuracy: 0.5029\n","\n","Epoch 00436: val_loss did not improve from 0.00000\n","Epoch 437/1000\n"," - 11s - loss: 0.1082 - accuracy: 0.9581 - val_loss: 0.0302 - val_accuracy: 0.5086\n","\n","Epoch 00437: val_loss did not improve from 0.00000\n","Epoch 438/1000\n"," - 11s - loss: 0.1066 - accuracy: 0.9597 - val_loss: 0.4983 - val_accuracy: 0.5098\n","\n","Epoch 00438: val_loss did not improve from 0.00000\n","Epoch 439/1000\n"," - 11s - loss: 0.1063 - accuracy: 0.9604 - val_loss: 1.1921e-07 - val_accuracy: 0.5038\n","\n","Epoch 00439: val_loss did not improve from 0.00000\n","Epoch 440/1000\n"," - 11s - loss: 0.1036 - accuracy: 0.9586 - val_loss: 2.9802e-05 - val_accuracy: 0.5068\n","\n","Epoch 00440: val_loss did not improve from 0.00000\n","Epoch 441/1000\n"," - 11s - loss: 0.1078 - accuracy: 0.9580 - val_loss: 3.3547 - val_accuracy: 0.5080\n","\n","Epoch 00441: val_loss did not improve from 0.00000\n","Epoch 442/1000\n"," - 11s - loss: 0.1009 - accuracy: 0.9639 - val_loss: 0.4137 - val_accuracy: 0.5083\n","\n","Epoch 00442: val_loss did not improve from 0.00000\n","Epoch 443/1000\n"," - 11s - loss: 0.1048 - accuracy: 0.9601 - val_loss: 1.7735 - val_accuracy: 0.5041\n","\n","Epoch 00443: val_loss did not improve from 0.00000\n","Epoch 444/1000\n"," - 11s - loss: 0.1062 - accuracy: 0.9596 - val_loss: 1.6965 - val_accuracy: 0.5059\n","\n","Epoch 00444: val_loss did not improve from 0.00000\n","Epoch 445/1000\n"," - 11s - loss: 0.1059 - accuracy: 0.9585 - val_loss: 0.0017 - val_accuracy: 0.5038\n","\n","Epoch 00445: val_loss did not improve from 0.00000\n","Epoch 446/1000\n"," - 11s - loss: 0.1051 - accuracy: 0.9598 - val_loss: 7.7134 - val_accuracy: 0.5059\n","\n","Epoch 00446: val_loss did not improve from 0.00000\n","Epoch 447/1000\n"," - 11s - loss: 0.1018 - accuracy: 0.9622 - val_loss: 7.0333e-06 - val_accuracy: 0.5119\n","\n","Epoch 00447: val_loss did not improve from 0.00000\n","Epoch 448/1000\n"," - 11s - loss: 0.0996 - accuracy: 0.9624 - val_loss: 0.9777 - val_accuracy: 0.4986\n","\n","Epoch 00448: val_loss did not improve from 0.00000\n","Epoch 449/1000\n"," - 11s - loss: 0.1042 - accuracy: 0.9598 - val_loss: 0.0011 - val_accuracy: 0.5113\n","\n","Epoch 00449: val_loss did not improve from 0.00000\n","Epoch 450/1000\n"," - 11s - loss: 0.1033 - accuracy: 0.9602 - val_loss: 4.6492e-06 - val_accuracy: 0.5062\n","\n","Epoch 00450: val_loss did not improve from 0.00000\n","Epoch 451/1000\n"," - 11s - loss: 0.0986 - accuracy: 0.9638 - val_loss: 15.2849 - val_accuracy: 0.5056\n","\n","Epoch 00451: val_loss did not improve from 0.00000\n","Epoch 452/1000\n"," - 11s - loss: 0.1003 - accuracy: 0.9622 - val_loss: 2.2631 - val_accuracy: 0.5113\n","\n","Epoch 00452: val_loss did not improve from 0.00000\n","Epoch 453/1000\n"," - 11s - loss: 0.1070 - accuracy: 0.9571 - val_loss: 5.7354 - val_accuracy: 0.5122\n","\n","Epoch 00453: val_loss did not improve from 0.00000\n","Epoch 454/1000\n"," - 11s - loss: 0.0991 - accuracy: 0.9619 - val_loss: 0.4054 - val_accuracy: 0.5095\n","\n","Epoch 00454: val_loss did not improve from 0.00000\n","Epoch 455/1000\n"," - 11s - loss: 0.0999 - accuracy: 0.9598 - val_loss: 2.6764 - val_accuracy: 0.5053\n","\n","Epoch 00455: val_loss did not improve from 0.00000\n","Epoch 456/1000\n"," - 11s - loss: 0.1047 - accuracy: 0.9589 - val_loss: 1.1506 - val_accuracy: 0.5080\n","\n","Epoch 00456: val_loss did not improve from 0.00000\n","Epoch 457/1000\n"," - 11s - loss: 0.1031 - accuracy: 0.9594 - val_loss: 4.4363 - val_accuracy: 0.5038\n","\n","Epoch 00457: val_loss did not improve from 0.00000\n","Epoch 458/1000\n"," - 11s - loss: 0.0936 - accuracy: 0.9675 - val_loss: 0.0045 - val_accuracy: 0.5056\n","\n","Epoch 00458: val_loss did not improve from 0.00000\n","Epoch 459/1000\n"," - 11s - loss: 0.0981 - accuracy: 0.9654 - val_loss: 1.3113e-06 - val_accuracy: 0.5086\n","\n","Epoch 00459: val_loss did not improve from 0.00000\n","Epoch 460/1000\n"," - 11s - loss: 0.1031 - accuracy: 0.9619 - val_loss: 8.9630 - val_accuracy: 0.5050\n","\n","Epoch 00460: val_loss did not improve from 0.00000\n","Epoch 461/1000\n"," - 11s - loss: 0.0999 - accuracy: 0.9622 - val_loss: 0.9280 - val_accuracy: 0.5068\n","\n","Epoch 00461: val_loss did not improve from 0.00000\n","Epoch 462/1000\n"," - 11s - loss: 0.0961 - accuracy: 0.9625 - val_loss: 4.5265 - val_accuracy: 0.5086\n","\n","Epoch 00462: val_loss did not improve from 0.00000\n","Epoch 463/1000\n"," - 11s - loss: 0.1010 - accuracy: 0.9615 - val_loss: 7.0106e-04 - val_accuracy: 0.5056\n","\n","Epoch 00463: val_loss did not improve from 0.00000\n","Epoch 464/1000\n"," - 11s - loss: 0.0937 - accuracy: 0.9649 - val_loss: 5.9578 - val_accuracy: 0.5095\n","\n","Epoch 00464: val_loss did not improve from 0.00000\n","Epoch 465/1000\n"," - 11s - loss: 0.0981 - accuracy: 0.9625 - val_loss: 3.8109 - val_accuracy: 0.5080\n","\n","Epoch 00465: val_loss did not improve from 0.00000\n","Epoch 466/1000\n"," - 11s - loss: 0.1036 - accuracy: 0.9606 - val_loss: 1.3672e-04 - val_accuracy: 0.5077\n","\n","Epoch 00466: val_loss did not improve from 0.00000\n","Epoch 467/1000\n"," - 11s - loss: 0.0959 - accuracy: 0.9653 - val_loss: 3.5763e-07 - val_accuracy: 0.5086\n","\n","Epoch 00467: val_loss did not improve from 0.00000\n","Epoch 468/1000\n"," - 11s - loss: 0.0940 - accuracy: 0.9653 - val_loss: 1.4873 - val_accuracy: 0.5080\n","\n","Epoch 00468: val_loss did not improve from 0.00000\n","Epoch 469/1000\n"," - 11s - loss: 0.0926 - accuracy: 0.9664 - val_loss: 0.1115 - val_accuracy: 0.4995\n","\n","Epoch 00469: val_loss did not improve from 0.00000\n","Epoch 470/1000\n"," - 11s - loss: 0.0948 - accuracy: 0.9651 - val_loss: 0.0079 - val_accuracy: 0.5116\n","\n","Epoch 00470: val_loss did not improve from 0.00000\n","Epoch 471/1000\n"," - 11s - loss: 0.0951 - accuracy: 0.9635 - val_loss: 2.4134 - val_accuracy: 0.5065\n","\n","Epoch 00471: val_loss did not improve from 0.00000\n","Epoch 472/1000\n"," - 11s - loss: 0.0921 - accuracy: 0.9640 - val_loss: 1.0675 - val_accuracy: 0.5098\n","\n","Epoch 00472: val_loss did not improve from 0.00000\n","Epoch 473/1000\n"," - 11s - loss: 0.0903 - accuracy: 0.9670 - val_loss: 2.2025 - val_accuracy: 0.5047\n","\n","Epoch 00473: val_loss did not improve from 0.00000\n","Epoch 474/1000\n"," - 11s - loss: 0.0921 - accuracy: 0.9658 - val_loss: 1.5371 - val_accuracy: 0.5059\n","\n","Epoch 00474: val_loss did not improve from 0.00000\n","Epoch 475/1000\n"," - 11s - loss: 0.0916 - accuracy: 0.9665 - val_loss: 1.2438 - val_accuracy: 0.5026\n","\n","Epoch 00475: val_loss did not improve from 0.00000\n","Epoch 476/1000\n"," - 11s - loss: 0.0895 - accuracy: 0.9675 - val_loss: 0.0712 - val_accuracy: 0.5008\n","\n","Epoch 00476: val_loss did not improve from 0.00000\n","Epoch 477/1000\n"," - 11s - loss: 0.1028 - accuracy: 0.9611 - val_loss: 5.8226 - val_accuracy: 0.5029\n","\n","Epoch 00477: val_loss did not improve from 0.00000\n","Epoch 478/1000\n"," - 11s - loss: 0.0949 - accuracy: 0.9635 - val_loss: 5.2019 - val_accuracy: 0.5014\n","\n","Epoch 00478: val_loss did not improve from 0.00000\n","Epoch 479/1000\n"," - 11s - loss: 0.0994 - accuracy: 0.9614 - val_loss: 3.2305e-05 - val_accuracy: 0.5038\n","\n","Epoch 00479: val_loss did not improve from 0.00000\n","Epoch 480/1000\n"," - 11s - loss: 0.0932 - accuracy: 0.9641 - val_loss: 0.0545 - val_accuracy: 0.5095\n","\n","Epoch 00480: val_loss did not improve from 0.00000\n","Epoch 481/1000\n"," - 11s - loss: 0.0923 - accuracy: 0.9677 - val_loss: 0.3611 - val_accuracy: 0.5032\n","\n","Epoch 00481: val_loss did not improve from 0.00000\n","Epoch 482/1000\n"," - 11s - loss: 0.0905 - accuracy: 0.9667 - val_loss: 2.8138 - val_accuracy: 0.5104\n","\n","Epoch 00482: val_loss did not improve from 0.00000\n","Epoch 483/1000\n"," - 11s - loss: 0.0944 - accuracy: 0.9654 - val_loss: 8.9487 - val_accuracy: 0.5071\n","\n","Epoch 00483: val_loss did not improve from 0.00000\n","Epoch 484/1000\n"," - 11s - loss: 0.0896 - accuracy: 0.9673 - val_loss: 1.9693 - val_accuracy: 0.5080\n","\n","Epoch 00484: val_loss did not improve from 0.00000\n","Epoch 485/1000\n"," - 11s - loss: 0.0928 - accuracy: 0.9653 - val_loss: 0.7694 - val_accuracy: 0.5104\n","\n","Epoch 00485: val_loss did not improve from 0.00000\n","Epoch 486/1000\n"," - 11s - loss: 0.0909 - accuracy: 0.9675 - val_loss: 7.6814 - val_accuracy: 0.5116\n","\n","Epoch 00486: val_loss did not improve from 0.00000\n","Epoch 487/1000\n"," - 11s - loss: 0.0929 - accuracy: 0.9651 - val_loss: 5.0771 - val_accuracy: 0.5113\n","\n","Epoch 00487: val_loss did not improve from 0.00000\n","Epoch 488/1000\n"," - 11s - loss: 0.0882 - accuracy: 0.9704 - val_loss: 5.5042 - val_accuracy: 0.5071\n","\n","Epoch 00488: val_loss did not improve from 0.00000\n","Epoch 489/1000\n"," - 11s - loss: 0.0922 - accuracy: 0.9655 - val_loss: 11.2624 - val_accuracy: 0.5095\n","\n","Epoch 00489: val_loss did not improve from 0.00000\n","Epoch 490/1000\n"," - 11s - loss: 0.0926 - accuracy: 0.9638 - val_loss: 0.0000e+00 - val_accuracy: 0.5077\n","\n","Epoch 00490: val_loss did not improve from 0.00000\n","Epoch 491/1000\n"," - 11s - loss: 0.0903 - accuracy: 0.9672 - val_loss: 3.3696 - val_accuracy: 0.5125\n","\n","Epoch 00491: val_loss did not improve from 0.00000\n","Epoch 492/1000\n"," - 11s - loss: 0.0879 - accuracy: 0.9674 - val_loss: 8.5163 - val_accuracy: 0.5095\n","\n","Epoch 00492: val_loss did not improve from 0.00000\n","Epoch 493/1000\n"," - 11s - loss: 0.0913 - accuracy: 0.9654 - val_loss: 5.2814 - val_accuracy: 0.5020\n","\n","Epoch 00493: val_loss did not improve from 0.00000\n","Epoch 494/1000\n"," - 11s - loss: 0.0866 - accuracy: 0.9678 - val_loss: 7.1523e-05 - val_accuracy: 0.5086\n","\n","Epoch 00494: val_loss did not improve from 0.00000\n","Epoch 495/1000\n"," - 11s - loss: 0.0871 - accuracy: 0.9664 - val_loss: 0.0000e+00 - val_accuracy: 0.5071\n","\n","Epoch 00495: val_loss did not improve from 0.00000\n","Epoch 496/1000\n"," - 11s - loss: 0.0856 - accuracy: 0.9695 - val_loss: 11.2361 - val_accuracy: 0.5035\n","\n","Epoch 00496: val_loss did not improve from 0.00000\n","Epoch 497/1000\n"," - 11s - loss: 0.0898 - accuracy: 0.9649 - val_loss: 3.6541 - val_accuracy: 0.5068\n","\n","Epoch 00497: val_loss did not improve from 0.00000\n","Epoch 498/1000\n"," - 11s - loss: 0.0934 - accuracy: 0.9627 - val_loss: 7.9445e-04 - val_accuracy: 0.5047\n","\n","Epoch 00498: val_loss did not improve from 0.00000\n","Epoch 499/1000\n"," - 11s - loss: 0.0911 - accuracy: 0.9669 - val_loss: 5.3895 - val_accuracy: 0.5035\n","\n","Epoch 00499: val_loss did not improve from 0.00000\n","Epoch 500/1000\n"," - 11s - loss: 0.0879 - accuracy: 0.9682 - val_loss: 4.6729e-05 - val_accuracy: 0.5098\n","\n","Epoch 00500: val_loss did not improve from 0.00000\n","Epoch 501/1000\n"," - 11s - loss: 0.0930 - accuracy: 0.9638 - val_loss: 11.0764 - val_accuracy: 0.5032\n","\n","Epoch 00501: val_loss did not improve from 0.00000\n","Epoch 502/1000\n"," - 11s - loss: 0.0884 - accuracy: 0.9674 - val_loss: 0.0014 - val_accuracy: 0.5077\n","\n","Epoch 00502: val_loss did not improve from 0.00000\n","Epoch 503/1000\n"," - 11s - loss: 0.0878 - accuracy: 0.9670 - val_loss: 0.0971 - val_accuracy: 0.5056\n","\n","Epoch 00503: val_loss did not improve from 0.00000\n","Epoch 504/1000\n"," - 11s - loss: 0.0865 - accuracy: 0.9683 - val_loss: 0.0000e+00 - val_accuracy: 0.5035\n","\n","Epoch 00504: val_loss did not improve from 0.00000\n","Epoch 505/1000\n"," - 11s - loss: 0.0862 - accuracy: 0.9665 - val_loss: 0.0757 - val_accuracy: 0.5050\n","\n","Epoch 00505: val_loss did not improve from 0.00000\n","Epoch 506/1000\n"," - 11s - loss: 0.0797 - accuracy: 0.9707 - val_loss: 4.0424 - val_accuracy: 0.5071\n","\n","Epoch 00506: val_loss did not improve from 0.00000\n","Epoch 507/1000\n"," - 11s - loss: 0.0823 - accuracy: 0.9698 - val_loss: 1.2159e-05 - val_accuracy: 0.5032\n","\n","Epoch 00507: val_loss did not improve from 0.00000\n","Epoch 508/1000\n"," - 11s - loss: 0.0877 - accuracy: 0.9670 - val_loss: 4.2915e-06 - val_accuracy: 0.5035\n","\n","Epoch 00508: val_loss did not improve from 0.00000\n","Epoch 509/1000\n"," - 11s - loss: 0.0849 - accuracy: 0.9694 - val_loss: 5.8302 - val_accuracy: 0.5146\n","\n","Epoch 00509: val_loss did not improve from 0.00000\n","Epoch 510/1000\n"," - 11s - loss: 0.0809 - accuracy: 0.9694 - val_loss: 0.0793 - val_accuracy: 0.5017\n","\n","Epoch 00510: val_loss did not improve from 0.00000\n","Epoch 511/1000\n"," - 11s - loss: 0.0846 - accuracy: 0.9679 - val_loss: 1.1451 - val_accuracy: 0.5053\n","\n","Epoch 00511: val_loss did not improve from 0.00000\n","Epoch 512/1000\n"," - 11s - loss: 0.0835 - accuracy: 0.9704 - val_loss: 0.2463 - val_accuracy: 0.5107\n","\n","Epoch 00512: val_loss did not improve from 0.00000\n","Epoch 513/1000\n"," - 11s - loss: 0.0877 - accuracy: 0.9673 - val_loss: 1.1921e-06 - val_accuracy: 0.5065\n","\n","Epoch 00513: val_loss did not improve from 0.00000\n","Epoch 514/1000\n"," - 11s - loss: 0.0849 - accuracy: 0.9688 - val_loss: 0.0032 - val_accuracy: 0.5044\n","\n","Epoch 00514: val_loss did not improve from 0.00000\n","Epoch 515/1000\n"," - 11s - loss: 0.0895 - accuracy: 0.9656 - val_loss: 0.0017 - val_accuracy: 0.5068\n","\n","Epoch 00515: val_loss did not improve from 0.00000\n","Epoch 516/1000\n"," - 11s - loss: 0.0777 - accuracy: 0.9713 - val_loss: 0.0000e+00 - val_accuracy: 0.5059\n","\n","Epoch 00516: val_loss did not improve from 0.00000\n","Epoch 517/1000\n"," - 11s - loss: 0.0822 - accuracy: 0.9691 - val_loss: 0.0227 - val_accuracy: 0.5104\n","\n","Epoch 00517: val_loss did not improve from 0.00000\n","Epoch 518/1000\n"," - 11s - loss: 0.0875 - accuracy: 0.9659 - val_loss: 3.3943 - val_accuracy: 0.5101\n","\n","Epoch 00518: val_loss did not improve from 0.00000\n","Epoch 519/1000\n"," - 11s - loss: 0.0824 - accuracy: 0.9690 - val_loss: 13.5710 - val_accuracy: 0.5086\n","\n","Epoch 00519: val_loss did not improve from 0.00000\n","Epoch 520/1000\n"," - 11s - loss: 0.0810 - accuracy: 0.9715 - val_loss: 7.1526e-07 - val_accuracy: 0.5116\n","\n","Epoch 00520: val_loss did not improve from 0.00000\n","Epoch 521/1000\n"," - 11s - loss: 0.0824 - accuracy: 0.9702 - val_loss: 1.0610e-05 - val_accuracy: 0.5071\n","\n","Epoch 00521: val_loss did not improve from 0.00000\n","Epoch 522/1000\n"," - 11s - loss: 0.0841 - accuracy: 0.9687 - val_loss: 5.2836 - val_accuracy: 0.5041\n","\n","Epoch 00522: val_loss did not improve from 0.00000\n","Epoch 523/1000\n"," - 11s - loss: 0.0844 - accuracy: 0.9702 - val_loss: 10.1364 - val_accuracy: 0.5110\n","\n","Epoch 00523: val_loss did not improve from 0.00000\n","Epoch 524/1000\n"," - 11s - loss: 0.0814 - accuracy: 0.9710 - val_loss: 11.9841 - val_accuracy: 0.5011\n","\n","Epoch 00524: val_loss did not improve from 0.00000\n","Epoch 525/1000\n"," - 11s - loss: 0.0779 - accuracy: 0.9728 - val_loss: 0.0868 - val_accuracy: 0.5056\n","\n","Epoch 00525: val_loss did not improve from 0.00000\n","Epoch 526/1000\n"," - 11s - loss: 0.0799 - accuracy: 0.9697 - val_loss: 1.6867e-04 - val_accuracy: 0.5134\n","\n","Epoch 00526: val_loss did not improve from 0.00000\n","Epoch 527/1000\n"," - 11s - loss: 0.0759 - accuracy: 0.9721 - val_loss: 0.4025 - val_accuracy: 0.5116\n","\n","Epoch 00527: val_loss did not improve from 0.00000\n","Epoch 528/1000\n"," - 11s - loss: 0.0774 - accuracy: 0.9732 - val_loss: 1.3529e-04 - val_accuracy: 0.5119\n","\n","Epoch 00528: val_loss did not improve from 0.00000\n","Epoch 529/1000\n"," - 11s - loss: 0.0776 - accuracy: 0.9727 - val_loss: 6.8450 - val_accuracy: 0.5053\n","\n","Epoch 00529: val_loss did not improve from 0.00000\n","Epoch 530/1000\n"," - 11s - loss: 0.0810 - accuracy: 0.9692 - val_loss: 9.3567 - val_accuracy: 0.5074\n","\n","Epoch 00530: val_loss did not improve from 0.00000\n","Epoch 531/1000\n"," - 11s - loss: 0.0806 - accuracy: 0.9707 - val_loss: 7.9055 - val_accuracy: 0.5146\n","\n","Epoch 00531: val_loss did not improve from 0.00000\n","Epoch 532/1000\n"," - 11s - loss: 0.0810 - accuracy: 0.9702 - val_loss: 0.0297 - val_accuracy: 0.5101\n","\n","Epoch 00532: val_loss did not improve from 0.00000\n","Epoch 533/1000\n"," - 11s - loss: 0.0759 - accuracy: 0.9730 - val_loss: 3.7730 - val_accuracy: 0.5143\n","\n","Epoch 00533: val_loss did not improve from 0.00000\n","Epoch 534/1000\n"," - 11s - loss: 0.0792 - accuracy: 0.9707 - val_loss: 5.4954e-05 - val_accuracy: 0.5062\n","\n","Epoch 00534: val_loss did not improve from 0.00000\n","Epoch 535/1000\n"," - 11s - loss: 0.0735 - accuracy: 0.9728 - val_loss: 0.0166 - val_accuracy: 0.5128\n","\n","Epoch 00535: val_loss did not improve from 0.00000\n","Epoch 536/1000\n"," - 11s - loss: 0.0800 - accuracy: 0.9699 - val_loss: 0.1848 - val_accuracy: 0.5104\n","\n","Epoch 00536: val_loss did not improve from 0.00000\n","Epoch 537/1000\n"," - 11s - loss: 0.0782 - accuracy: 0.9706 - val_loss: 1.2302e-04 - val_accuracy: 0.5146\n","\n","Epoch 00537: val_loss did not improve from 0.00000\n","Epoch 538/1000\n"," - 11s - loss: 0.0799 - accuracy: 0.9683 - val_loss: 5.1829 - val_accuracy: 0.5047\n","\n","Epoch 00538: val_loss did not improve from 0.00000\n","Epoch 539/1000\n"," - 11s - loss: 0.0767 - accuracy: 0.9730 - val_loss: 0.0189 - val_accuracy: 0.5092\n","\n","Epoch 00539: val_loss did not improve from 0.00000\n","Epoch 540/1000\n"," - 11s - loss: 0.0812 - accuracy: 0.9704 - val_loss: 7.1526e-07 - val_accuracy: 0.5080\n","\n","Epoch 00540: val_loss did not improve from 0.00000\n","Epoch 541/1000\n"," - 11s - loss: 0.0796 - accuracy: 0.9701 - val_loss: 2.2385 - val_accuracy: 0.5107\n","\n","Epoch 00541: val_loss did not improve from 0.00000\n","Epoch 542/1000\n"," - 11s - loss: 0.0750 - accuracy: 0.9735 - val_loss: 0.0000e+00 - val_accuracy: 0.5101\n","\n","Epoch 00542: val_loss did not improve from 0.00000\n","Epoch 543/1000\n"," - 11s - loss: 0.0798 - accuracy: 0.9702 - val_loss: 13.2878 - val_accuracy: 0.5101\n","\n","Epoch 00543: val_loss did not improve from 0.00000\n","Epoch 544/1000\n"," - 11s - loss: 0.0795 - accuracy: 0.9716 - val_loss: 6.1160 - val_accuracy: 0.5101\n","\n","Epoch 00544: val_loss did not improve from 0.00000\n","Epoch 545/1000\n"," - 11s - loss: 0.0759 - accuracy: 0.9718 - val_loss: 0.0000e+00 - val_accuracy: 0.5053\n","\n","Epoch 00545: val_loss did not improve from 0.00000\n","Epoch 546/1000\n"," - 11s - loss: 0.0780 - accuracy: 0.9739 - val_loss: 1.5772 - val_accuracy: 0.5083\n","\n","Epoch 00546: val_loss did not improve from 0.00000\n","Epoch 547/1000\n"," - 11s - loss: 0.0756 - accuracy: 0.9714 - val_loss: 11.9914 - val_accuracy: 0.5152\n","\n","Epoch 00547: val_loss did not improve from 0.00000\n","Epoch 548/1000\n"," - 11s - loss: 0.0742 - accuracy: 0.9730 - val_loss: 6.9636 - val_accuracy: 0.5032\n","\n","Epoch 00548: val_loss did not improve from 0.00000\n","Epoch 549/1000\n"," - 11s - loss: 0.0770 - accuracy: 0.9718 - val_loss: 6.8628 - val_accuracy: 0.5077\n","\n","Epoch 00549: val_loss did not improve from 0.00000\n","Epoch 550/1000\n"," - 11s - loss: 0.0745 - accuracy: 0.9734 - val_loss: 0.0000e+00 - val_accuracy: 0.5026\n","\n","Epoch 00550: val_loss did not improve from 0.00000\n","Epoch 551/1000\n"," - 11s - loss: 0.0792 - accuracy: 0.9720 - val_loss: 0.0444 - val_accuracy: 0.5083\n","\n","Epoch 00551: val_loss did not improve from 0.00000\n","Epoch 552/1000\n"," - 11s - loss: 0.0751 - accuracy: 0.9741 - val_loss: 0.7112 - val_accuracy: 0.5038\n","\n","Epoch 00552: val_loss did not improve from 0.00000\n","Epoch 553/1000\n"," - 11s - loss: 0.0782 - accuracy: 0.9722 - val_loss: 2.2878 - val_accuracy: 0.5023\n","\n","Epoch 00553: val_loss did not improve from 0.00000\n","Epoch 554/1000\n"," - 11s - loss: 0.0717 - accuracy: 0.9750 - val_loss: 0.5449 - val_accuracy: 0.5065\n","\n","Epoch 00554: val_loss did not improve from 0.00000\n","Epoch 555/1000\n"," - 11s - loss: 0.0733 - accuracy: 0.9741 - val_loss: 4.7650 - val_accuracy: 0.5041\n","\n","Epoch 00555: val_loss did not improve from 0.00000\n","Epoch 556/1000\n"," - 11s - loss: 0.0769 - accuracy: 0.9718 - val_loss: 1.6689e-06 - val_accuracy: 0.5107\n","\n","Epoch 00556: val_loss did not improve from 0.00000\n","Epoch 557/1000\n"," - 11s - loss: 0.0759 - accuracy: 0.9723 - val_loss: 0.0000e+00 - val_accuracy: 0.5119\n","\n","Epoch 00557: val_loss did not improve from 0.00000\n","Epoch 558/1000\n"," - 11s - loss: 0.0718 - accuracy: 0.9737 - val_loss: 0.0164 - val_accuracy: 0.5113\n","\n","Epoch 00558: val_loss did not improve from 0.00000\n","Epoch 559/1000\n"," - 11s - loss: 0.0731 - accuracy: 0.9745 - val_loss: 5.9605e-07 - val_accuracy: 0.5101\n","\n","Epoch 00559: val_loss did not improve from 0.00000\n","Epoch 560/1000\n"," - 11s - loss: 0.0739 - accuracy: 0.9733 - val_loss: 0.3698 - val_accuracy: 0.5068\n","\n","Epoch 00560: val_loss did not improve from 0.00000\n","Epoch 561/1000\n"," - 11s - loss: 0.0734 - accuracy: 0.9736 - val_loss: 0.0000e+00 - val_accuracy: 0.5092\n","\n","Epoch 00561: val_loss did not improve from 0.00000\n","Epoch 562/1000\n"," - 11s - loss: 0.0747 - accuracy: 0.9715 - val_loss: 1.7725e-04 - val_accuracy: 0.5107\n","\n","Epoch 00562: val_loss did not improve from 0.00000\n","Epoch 563/1000\n"," - 11s - loss: 0.0748 - accuracy: 0.9737 - val_loss: 20.3825 - val_accuracy: 0.5128\n","\n","Epoch 00563: val_loss did not improve from 0.00000\n","Epoch 564/1000\n"," - 11s - loss: 0.0665 - accuracy: 0.9775 - val_loss: 0.0015 - val_accuracy: 0.5101\n","\n","Epoch 00564: val_loss did not improve from 0.00000\n","Epoch 565/1000\n"," - 11s - loss: 0.0709 - accuracy: 0.9745 - val_loss: 1.9449 - val_accuracy: 0.5116\n","\n","Epoch 00565: val_loss did not improve from 0.00000\n","Epoch 566/1000\n"," - 11s - loss: 0.0708 - accuracy: 0.9750 - val_loss: 0.0768 - val_accuracy: 0.5125\n","\n","Epoch 00566: val_loss did not improve from 0.00000\n","Epoch 567/1000\n"," - 11s - loss: 0.0720 - accuracy: 0.9735 - val_loss: 0.0146 - val_accuracy: 0.5158\n","\n","Epoch 00567: val_loss did not improve from 0.00000\n","Epoch 568/1000\n"," - 11s - loss: 0.0745 - accuracy: 0.9721 - val_loss: 5.4642 - val_accuracy: 0.5140\n","\n","Epoch 00568: val_loss did not improve from 0.00000\n","Epoch 569/1000\n"," - 11s - loss: 0.0664 - accuracy: 0.9766 - val_loss: 3.0716e-04 - val_accuracy: 0.5074\n","\n","Epoch 00569: val_loss did not improve from 0.00000\n","Epoch 570/1000\n"," - 11s - loss: 0.0660 - accuracy: 0.9768 - val_loss: 0.0058 - val_accuracy: 0.5113\n","\n","Epoch 00570: val_loss did not improve from 0.00000\n","Epoch 571/1000\n"," - 11s - loss: 0.0727 - accuracy: 0.9742 - val_loss: 6.2815e-04 - val_accuracy: 0.5092\n","\n","Epoch 00571: val_loss did not improve from 0.00000\n","Epoch 572/1000\n"," - 11s - loss: 0.0710 - accuracy: 0.9780 - val_loss: 3.3586 - val_accuracy: 0.5149\n","\n","Epoch 00572: val_loss did not improve from 0.00000\n","Epoch 573/1000\n"," - 11s - loss: 0.0715 - accuracy: 0.9744 - val_loss: 2.9802e-06 - val_accuracy: 0.5122\n","\n","Epoch 00573: val_loss did not improve from 0.00000\n","Epoch 574/1000\n"," - 11s - loss: 0.0704 - accuracy: 0.9738 - val_loss: 5.9605e-07 - val_accuracy: 0.5152\n","\n","Epoch 00574: val_loss did not improve from 0.00000\n","Epoch 575/1000\n"," - 11s - loss: 0.0691 - accuracy: 0.9750 - val_loss: 0.9317 - val_accuracy: 0.5095\n","\n","Epoch 00575: val_loss did not improve from 0.00000\n","Epoch 576/1000\n"," - 11s - loss: 0.0709 - accuracy: 0.9749 - val_loss: 12.9371 - val_accuracy: 0.5134\n","\n","Epoch 00576: val_loss did not improve from 0.00000\n","Epoch 577/1000\n"," - 11s - loss: 0.0716 - accuracy: 0.9746 - val_loss: 14.8613 - val_accuracy: 0.5119\n","\n","Epoch 00577: val_loss did not improve from 0.00000\n","Epoch 578/1000\n"," - 11s - loss: 0.0661 - accuracy: 0.9760 - val_loss: 2.0588 - val_accuracy: 0.5047\n","\n","Epoch 00578: val_loss did not improve from 0.00000\n","Epoch 579/1000\n"," - 11s - loss: 0.0683 - accuracy: 0.9787 - val_loss: 2.5987e-05 - val_accuracy: 0.5080\n","\n","Epoch 00579: val_loss did not improve from 0.00000\n","Epoch 580/1000\n"," - 11s - loss: 0.0733 - accuracy: 0.9736 - val_loss: 4.7684e-07 - val_accuracy: 0.5077\n","\n","Epoch 00580: val_loss did not improve from 0.00000\n","Epoch 581/1000\n"," - 11s - loss: 0.0745 - accuracy: 0.9734 - val_loss: 0.0013 - val_accuracy: 0.5134\n","\n","Epoch 00581: val_loss did not improve from 0.00000\n","Epoch 582/1000\n"," - 11s - loss: 0.0688 - accuracy: 0.9751 - val_loss: 9.0066 - val_accuracy: 0.5077\n","\n","Epoch 00582: val_loss did not improve from 0.00000\n","Epoch 583/1000\n"," - 11s - loss: 0.0714 - accuracy: 0.9746 - val_loss: 3.3259e-05 - val_accuracy: 0.5083\n","\n","Epoch 00583: val_loss did not improve from 0.00000\n","Epoch 584/1000\n"," - 11s - loss: 0.0617 - accuracy: 0.9802 - val_loss: 0.0010 - val_accuracy: 0.5107\n","\n","Epoch 00584: val_loss did not improve from 0.00000\n","Epoch 585/1000\n"," - 11s - loss: 0.0664 - accuracy: 0.9760 - val_loss: 0.0030 - val_accuracy: 0.5098\n","\n","Epoch 00585: val_loss did not improve from 0.00000\n","Epoch 586/1000\n"," - 11s - loss: 0.0654 - accuracy: 0.9766 - val_loss: 0.0026 - val_accuracy: 0.5068\n","\n","Epoch 00586: val_loss did not improve from 0.00000\n","Epoch 587/1000\n"," - 11s - loss: 0.0680 - accuracy: 0.9768 - val_loss: 11.3039 - val_accuracy: 0.5095\n","\n","Epoch 00587: val_loss did not improve from 0.00000\n","Epoch 588/1000\n"," - 11s - loss: 0.0650 - accuracy: 0.9769 - val_loss: 75.8377 - val_accuracy: 0.5098\n","\n","Epoch 00588: val_loss did not improve from 0.00000\n","Epoch 589/1000\n"," - 11s - loss: 0.0699 - accuracy: 0.9746 - val_loss: 0.0000e+00 - val_accuracy: 0.5071\n","\n","Epoch 00589: val_loss did not improve from 0.00000\n","Epoch 590/1000\n"," - 11s - loss: 0.0683 - accuracy: 0.9757 - val_loss: 3.3379e-06 - val_accuracy: 0.5053\n","\n","Epoch 00590: val_loss did not improve from 0.00000\n","Epoch 591/1000\n"," - 11s - loss: 0.0621 - accuracy: 0.9782 - val_loss: 14.0280 - val_accuracy: 0.5035\n","\n","Epoch 00591: val_loss did not improve from 0.00000\n","Epoch 592/1000\n"," - 11s - loss: 0.0683 - accuracy: 0.9762 - val_loss: 5.2910 - val_accuracy: 0.5029\n","\n","Epoch 00592: val_loss did not improve from 0.00000\n","Epoch 593/1000\n"," - 11s - loss: 0.0666 - accuracy: 0.9755 - val_loss: 0.0751 - val_accuracy: 0.5092\n","\n","Epoch 00593: val_loss did not improve from 0.00000\n","Epoch 594/1000\n"," - 11s - loss: 0.0668 - accuracy: 0.9759 - val_loss: 2.6100 - val_accuracy: 0.5083\n","\n","Epoch 00594: val_loss did not improve from 0.00000\n","Epoch 595/1000\n"," - 11s - loss: 0.0635 - accuracy: 0.9795 - val_loss: 0.8835 - val_accuracy: 0.5068\n","\n","Epoch 00595: val_loss did not improve from 0.00000\n","Epoch 596/1000\n"," - 11s - loss: 0.0687 - accuracy: 0.9752 - val_loss: 7.4848 - val_accuracy: 0.5092\n","\n","Epoch 00596: val_loss did not improve from 0.00000\n","Epoch 597/1000\n"," - 11s - loss: 0.0658 - accuracy: 0.9765 - val_loss: 0.0058 - val_accuracy: 0.5113\n","\n","Epoch 00597: val_loss did not improve from 0.00000\n","Epoch 598/1000\n"," - 11s - loss: 0.0692 - accuracy: 0.9751 - val_loss: 11.4137 - val_accuracy: 0.5023\n","\n","Epoch 00598: val_loss did not improve from 0.00000\n","Epoch 599/1000\n"," - 11s - loss: 0.0644 - accuracy: 0.9768 - val_loss: 0.0114 - val_accuracy: 0.5161\n","\n","Epoch 00599: val_loss did not improve from 0.00000\n","Epoch 600/1000\n"," - 11s - loss: 0.0664 - accuracy: 0.9772 - val_loss: 1.1921e-07 - val_accuracy: 0.5143\n","\n","Epoch 00600: val_loss did not improve from 0.00000\n","Epoch 601/1000\n"," - 11s - loss: 0.0632 - accuracy: 0.9784 - val_loss: 0.0731 - val_accuracy: 0.5152\n","\n","Epoch 00601: val_loss did not improve from 0.00000\n","Epoch 602/1000\n"," - 11s - loss: 0.0680 - accuracy: 0.9756 - val_loss: 11.8012 - val_accuracy: 0.5101\n","\n","Epoch 00602: val_loss did not improve from 0.00000\n","Epoch 603/1000\n"," - 11s - loss: 0.0608 - accuracy: 0.9791 - val_loss: 0.0000e+00 - val_accuracy: 0.5095\n","\n","Epoch 00603: val_loss did not improve from 0.00000\n","Epoch 604/1000\n"," - 11s - loss: 0.0660 - accuracy: 0.9761 - val_loss: 4.7786 - val_accuracy: 0.5074\n","\n","Epoch 00604: val_loss did not improve from 0.00000\n","Epoch 605/1000\n"," - 11s - loss: 0.0645 - accuracy: 0.9782 - val_loss: 0.0000e+00 - val_accuracy: 0.5113\n","\n","Epoch 00605: val_loss did not improve from 0.00000\n","Epoch 606/1000\n"," - 11s - loss: 0.0626 - accuracy: 0.9776 - val_loss: 0.0000e+00 - val_accuracy: 0.5104\n","\n","Epoch 00606: val_loss did not improve from 0.00000\n","Epoch 607/1000\n"," - 11s - loss: 0.0666 - accuracy: 0.9757 - val_loss: 1.0889 - val_accuracy: 0.5044\n","\n","Epoch 00607: val_loss did not improve from 0.00000\n","Epoch 608/1000\n"," - 11s - loss: 0.0642 - accuracy: 0.9765 - val_loss: 0.2021 - val_accuracy: 0.5068\n","\n","Epoch 00608: val_loss did not improve from 0.00000\n","Epoch 609/1000\n"," - 11s - loss: 0.0630 - accuracy: 0.9778 - val_loss: 1.0729e-06 - val_accuracy: 0.5092\n","\n","Epoch 00609: val_loss did not improve from 0.00000\n","Epoch 610/1000\n"," - 11s - loss: 0.0641 - accuracy: 0.9769 - val_loss: 20.6946 - val_accuracy: 0.5065\n","\n","Epoch 00610: val_loss did not improve from 0.00000\n","Epoch 611/1000\n"," - 11s - loss: 0.0603 - accuracy: 0.9785 - val_loss: 1.6131 - val_accuracy: 0.5086\n","\n","Epoch 00611: val_loss did not improve from 0.00000\n","Epoch 612/1000\n"," - 11s - loss: 0.0602 - accuracy: 0.9803 - val_loss: 0.0000e+00 - val_accuracy: 0.5035\n","\n","Epoch 00612: val_loss did not improve from 0.00000\n","Epoch 613/1000\n"," - 11s - loss: 0.0616 - accuracy: 0.9800 - val_loss: 1.4805e-04 - val_accuracy: 0.5068\n","\n","Epoch 00613: val_loss did not improve from 0.00000\n","Epoch 614/1000\n"," - 11s - loss: 0.0630 - accuracy: 0.9781 - val_loss: 0.9290 - val_accuracy: 0.5062\n","\n","Epoch 00614: val_loss did not improve from 0.00000\n","Epoch 615/1000\n"," - 11s - loss: 0.0616 - accuracy: 0.9788 - val_loss: 4.5832 - val_accuracy: 0.5134\n","\n","Epoch 00615: val_loss did not improve from 0.00000\n","Epoch 616/1000\n"," - 11s - loss: 0.0609 - accuracy: 0.9806 - val_loss: 6.5477 - val_accuracy: 0.5083\n","\n","Epoch 00616: val_loss did not improve from 0.00000\n","Epoch 617/1000\n"," - 11s - loss: 0.0647 - accuracy: 0.9777 - val_loss: 4.4200 - val_accuracy: 0.5035\n","\n","Epoch 00617: val_loss did not improve from 0.00000\n","Epoch 618/1000\n"," - 11s - loss: 0.0599 - accuracy: 0.9807 - val_loss: 3.0096e-04 - val_accuracy: 0.5083\n","\n","Epoch 00618: val_loss did not improve from 0.00000\n","Epoch 619/1000\n"," - 11s - loss: 0.0573 - accuracy: 0.9800 - val_loss: 12.8757 - val_accuracy: 0.5110\n","\n","Epoch 00619: val_loss did not improve from 0.00000\n","Epoch 620/1000\n"," - 11s - loss: 0.0641 - accuracy: 0.9765 - val_loss: 11.9955 - val_accuracy: 0.5125\n","\n","Epoch 00620: val_loss did not improve from 0.00000\n","Epoch 621/1000\n"," - 11s - loss: 0.0652 - accuracy: 0.9773 - val_loss: 2.3657 - val_accuracy: 0.5080\n","\n","Epoch 00621: val_loss did not improve from 0.00000\n","Epoch 622/1000\n"," - 11s - loss: 0.0584 - accuracy: 0.9806 - val_loss: 0.1462 - val_accuracy: 0.5044\n","\n","Epoch 00622: val_loss did not improve from 0.00000\n","Epoch 623/1000\n"," - 11s - loss: 0.0599 - accuracy: 0.9808 - val_loss: 7.3397 - val_accuracy: 0.5113\n","\n","Epoch 00623: val_loss did not improve from 0.00000\n","Epoch 624/1000\n"," - 11s - loss: 0.0588 - accuracy: 0.9786 - val_loss: 0.0032 - val_accuracy: 0.5122\n","\n","Epoch 00624: val_loss did not improve from 0.00000\n","Epoch 625/1000\n"," - 11s - loss: 0.0607 - accuracy: 0.9783 - val_loss: 12.5819 - val_accuracy: 0.5083\n","\n","Epoch 00625: val_loss did not improve from 0.00000\n","Epoch 626/1000\n"," - 11s - loss: 0.0621 - accuracy: 0.9771 - val_loss: 6.2084 - val_accuracy: 0.5122\n","\n","Epoch 00626: val_loss did not improve from 0.00000\n","Epoch 627/1000\n"," - 11s - loss: 0.0599 - accuracy: 0.9803 - val_loss: 0.0000e+00 - val_accuracy: 0.5062\n","\n","Epoch 00627: val_loss did not improve from 0.00000\n","Epoch 628/1000\n"," - 11s - loss: 0.0580 - accuracy: 0.9794 - val_loss: 3.8583 - val_accuracy: 0.5077\n","\n","Epoch 00628: val_loss did not improve from 0.00000\n","Epoch 629/1000\n"," - 11s - loss: 0.0603 - accuracy: 0.9796 - val_loss: 0.0000e+00 - val_accuracy: 0.5128\n","\n","Epoch 00629: val_loss did not improve from 0.00000\n","Epoch 630/1000\n"," - 11s - loss: 0.0592 - accuracy: 0.9799 - val_loss: 14.6366 - val_accuracy: 0.5107\n","\n","Epoch 00630: val_loss did not improve from 0.00000\n","Epoch 631/1000\n"," - 11s - loss: 0.0558 - accuracy: 0.9817 - val_loss: 10.2875 - val_accuracy: 0.5026\n","\n","Epoch 00631: val_loss did not improve from 0.00000\n","Epoch 632/1000\n"," - 11s - loss: 0.0614 - accuracy: 0.9796 - val_loss: 10.8538 - val_accuracy: 0.5065\n","\n","Epoch 00632: val_loss did not improve from 0.00000\n","Epoch 633/1000\n"," - 11s - loss: 0.0580 - accuracy: 0.9800 - val_loss: 0.1121 - val_accuracy: 0.5080\n","\n","Epoch 00633: val_loss did not improve from 0.00000\n","Epoch 634/1000\n"," - 11s - loss: 0.0578 - accuracy: 0.9809 - val_loss: 3.5763e-07 - val_accuracy: 0.5029\n","\n","Epoch 00634: val_loss did not improve from 0.00000\n","Epoch 635/1000\n"," - 11s - loss: 0.0565 - accuracy: 0.9813 - val_loss: 13.3357 - val_accuracy: 0.5143\n","\n","Epoch 00635: val_loss did not improve from 0.00000\n","Epoch 636/1000\n"," - 11s - loss: 0.0584 - accuracy: 0.9804 - val_loss: 0.0000e+00 - val_accuracy: 0.5092\n","\n","Epoch 00636: val_loss did not improve from 0.00000\n","Epoch 637/1000\n"," - 11s - loss: 0.0579 - accuracy: 0.9811 - val_loss: 1.1921e-07 - val_accuracy: 0.5023\n","\n","Epoch 00637: val_loss did not improve from 0.00000\n","Epoch 638/1000\n"," - 11s - loss: 0.0574 - accuracy: 0.9810 - val_loss: 1.2321 - val_accuracy: 0.5074\n","\n","Epoch 00638: val_loss did not improve from 0.00000\n","Epoch 639/1000\n"," - 11s - loss: 0.0588 - accuracy: 0.9802 - val_loss: 0.2676 - val_accuracy: 0.5044\n","\n","Epoch 00639: val_loss did not improve from 0.00000\n","Epoch 640/1000\n"," - 11s - loss: 0.0570 - accuracy: 0.9796 - val_loss: 0.0118 - val_accuracy: 0.5086\n","\n","Epoch 00640: val_loss did not improve from 0.00000\n","Epoch 641/1000\n"," - 11s - loss: 0.0600 - accuracy: 0.9806 - val_loss: 7.5062e-04 - val_accuracy: 0.5062\n","\n","Epoch 00641: val_loss did not improve from 0.00000\n","Epoch 642/1000\n"," - 11s - loss: 0.0576 - accuracy: 0.9801 - val_loss: 0.2147 - val_accuracy: 0.5062\n","\n","Epoch 00642: val_loss did not improve from 0.00000\n","Epoch 643/1000\n"," - 11s - loss: 0.0570 - accuracy: 0.9809 - val_loss: 0.0000e+00 - val_accuracy: 0.5071\n","\n","Epoch 00643: val_loss did not improve from 0.00000\n","Epoch 644/1000\n"," - 11s - loss: 0.0590 - accuracy: 0.9785 - val_loss: 11.9689 - val_accuracy: 0.5098\n","\n","Epoch 00644: val_loss did not improve from 0.00000\n","Epoch 645/1000\n"," - 11s - loss: 0.0560 - accuracy: 0.9808 - val_loss: 0.0127 - val_accuracy: 0.5023\n","\n","Epoch 00645: val_loss did not improve from 0.00000\n","Epoch 646/1000\n"," - 11s - loss: 0.0567 - accuracy: 0.9801 - val_loss: 16.8589 - val_accuracy: 0.5089\n","\n","Epoch 00646: val_loss did not improve from 0.00000\n","Epoch 647/1000\n"," - 11s - loss: 0.0566 - accuracy: 0.9804 - val_loss: 4.5775 - val_accuracy: 0.5071\n","\n","Epoch 00647: val_loss did not improve from 0.00000\n","Epoch 648/1000\n"," - 11s - loss: 0.0570 - accuracy: 0.9794 - val_loss: 5.3059 - val_accuracy: 0.5107\n","\n","Epoch 00648: val_loss did not improve from 0.00000\n","Epoch 649/1000\n"," - 11s - loss: 0.0542 - accuracy: 0.9805 - val_loss: 1.1297 - val_accuracy: 0.5062\n","\n","Epoch 00649: val_loss did not improve from 0.00000\n","Epoch 650/1000\n"," - 11s - loss: 0.0580 - accuracy: 0.9795 - val_loss: 13.4193 - val_accuracy: 0.5098\n","\n","Epoch 00650: val_loss did not improve from 0.00000\n","Epoch 651/1000\n"," - 11s - loss: 0.0618 - accuracy: 0.9783 - val_loss: 0.0899 - val_accuracy: 0.5065\n","\n","Epoch 00651: val_loss did not improve from 0.00000\n","Epoch 652/1000\n"," - 11s - loss: 0.0583 - accuracy: 0.9788 - val_loss: 3.5763e-07 - val_accuracy: 0.5071\n","\n","Epoch 00652: val_loss did not improve from 0.00000\n","Epoch 653/1000\n"," - 11s - loss: 0.0582 - accuracy: 0.9802 - val_loss: 25.1522 - val_accuracy: 0.5005\n","\n","Epoch 00653: val_loss did not improve from 0.00000\n","Epoch 654/1000\n"," - 11s - loss: 0.0561 - accuracy: 0.9812 - val_loss: 14.1650 - val_accuracy: 0.5047\n","\n","Epoch 00654: val_loss did not improve from 0.00000\n","Epoch 655/1000\n"," - 11s - loss: 0.0555 - accuracy: 0.9806 - val_loss: 10.6027 - val_accuracy: 0.5059\n","\n","Epoch 00655: val_loss did not improve from 0.00000\n","Epoch 656/1000\n"," - 11s - loss: 0.0545 - accuracy: 0.9828 - val_loss: 4.3308 - val_accuracy: 0.5098\n","\n","Epoch 00656: val_loss did not improve from 0.00000\n","Epoch 657/1000\n"," - 11s - loss: 0.0572 - accuracy: 0.9796 - val_loss: 10.8982 - val_accuracy: 0.5113\n","\n","Epoch 00657: val_loss did not improve from 0.00000\n","Epoch 658/1000\n"," - 11s - loss: 0.0550 - accuracy: 0.9815 - val_loss: 4.1595 - val_accuracy: 0.5068\n","\n","Epoch 00658: val_loss did not improve from 0.00000\n","Epoch 659/1000\n"," - 11s - loss: 0.0559 - accuracy: 0.9803 - val_loss: 1.9193e-05 - val_accuracy: 0.5092\n","\n","Epoch 00659: val_loss did not improve from 0.00000\n","Epoch 660/1000\n"," - 11s - loss: 0.0595 - accuracy: 0.9797 - val_loss: 0.0918 - val_accuracy: 0.5062\n","\n","Epoch 00660: val_loss did not improve from 0.00000\n","Epoch 661/1000\n"," - 11s - loss: 0.0532 - accuracy: 0.9822 - val_loss: 9.5367e-07 - val_accuracy: 0.5023\n","\n","Epoch 00661: val_loss did not improve from 0.00000\n","Epoch 662/1000\n"," - 11s - loss: 0.0551 - accuracy: 0.9810 - val_loss: 7.5102e-06 - val_accuracy: 0.5062\n","\n","Epoch 00662: val_loss did not improve from 0.00000\n","Epoch 663/1000\n"," - 11s - loss: 0.0582 - accuracy: 0.9786 - val_loss: 12.9168 - val_accuracy: 0.5056\n","\n","Epoch 00663: val_loss did not improve from 0.00000\n","Epoch 664/1000\n"," - 11s - loss: 0.0571 - accuracy: 0.9800 - val_loss: 0.0000e+00 - val_accuracy: 0.5089\n","\n","Epoch 00664: val_loss did not improve from 0.00000\n","Epoch 665/1000\n"," - 11s - loss: 0.0542 - accuracy: 0.9819 - val_loss: 0.0000e+00 - val_accuracy: 0.5026\n","\n","Epoch 00665: val_loss did not improve from 0.00000\n","Epoch 666/1000\n"," - 11s - loss: 0.0508 - accuracy: 0.9825 - val_loss: 0.0121 - val_accuracy: 0.5077\n","\n","Epoch 00666: val_loss did not improve from 0.00000\n","Epoch 667/1000\n"," - 11s - loss: 0.0553 - accuracy: 0.9802 - val_loss: 1.5855e-05 - val_accuracy: 0.5089\n","\n","Epoch 00667: val_loss did not improve from 0.00000\n","Epoch 668/1000\n"," - 11s - loss: 0.0552 - accuracy: 0.9804 - val_loss: 10.1754 - val_accuracy: 0.5104\n","\n","Epoch 00668: val_loss did not improve from 0.00000\n","Epoch 669/1000\n"," - 11s - loss: 0.0531 - accuracy: 0.9814 - val_loss: 0.0812 - val_accuracy: 0.5110\n","\n","Epoch 00669: val_loss did not improve from 0.00000\n","Epoch 670/1000\n"," - 11s - loss: 0.0512 - accuracy: 0.9838 - val_loss: 0.0288 - val_accuracy: 0.5095\n","\n","Epoch 00670: val_loss did not improve from 0.00000\n","Epoch 671/1000\n"," - 11s - loss: 0.0541 - accuracy: 0.9830 - val_loss: 7.3181 - val_accuracy: 0.5038\n","\n","Epoch 00671: val_loss did not improve from 0.00000\n","Epoch 672/1000\n"," - 11s - loss: 0.0522 - accuracy: 0.9820 - val_loss: 4.3762 - val_accuracy: 0.5044\n","\n","Epoch 00672: val_loss did not improve from 0.00000\n","Epoch 673/1000\n"," - 11s - loss: 0.0551 - accuracy: 0.9813 - val_loss: 0.0628 - val_accuracy: 0.5065\n","\n","Epoch 00673: val_loss did not improve from 0.00000\n","Epoch 674/1000\n"," - 11s - loss: 0.0523 - accuracy: 0.9826 - val_loss: 0.0059 - val_accuracy: 0.5026\n","\n","Epoch 00674: val_loss did not improve from 0.00000\n","Epoch 675/1000\n"," - 11s - loss: 0.0497 - accuracy: 0.9835 - val_loss: 1.0180e-04 - val_accuracy: 0.5083\n","\n","Epoch 00675: val_loss did not improve from 0.00000\n","Epoch 676/1000\n"," - 11s - loss: 0.0581 - accuracy: 0.9795 - val_loss: 8.2569 - val_accuracy: 0.5044\n","\n","Epoch 00676: val_loss did not improve from 0.00000\n","Epoch 677/1000\n"," - 11s - loss: 0.0538 - accuracy: 0.9815 - val_loss: 4.3141 - val_accuracy: 0.5074\n","\n","Epoch 00677: val_loss did not improve from 0.00000\n","Epoch 678/1000\n"," - 11s - loss: 0.0513 - accuracy: 0.9827 - val_loss: 0.3656 - val_accuracy: 0.5083\n","\n","Epoch 00678: val_loss did not improve from 0.00000\n","Epoch 679/1000\n"," - 11s - loss: 0.0510 - accuracy: 0.9814 - val_loss: 15.7297 - val_accuracy: 0.5086\n","\n","Epoch 00679: val_loss did not improve from 0.00000\n","Epoch 680/1000\n"," - 11s - loss: 0.0517 - accuracy: 0.9825 - val_loss: 0.4970 - val_accuracy: 0.5071\n","\n","Epoch 00680: val_loss did not improve from 0.00000\n","Epoch 681/1000\n"," - 11s - loss: 0.0540 - accuracy: 0.9810 - val_loss: 0.0000e+00 - val_accuracy: 0.5119\n","\n","Epoch 00681: val_loss did not improve from 0.00000\n","Epoch 682/1000\n"," - 11s - loss: 0.0509 - accuracy: 0.9841 - val_loss: 3.3140e-05 - val_accuracy: 0.5104\n","\n","Epoch 00682: val_loss did not improve from 0.00000\n","Epoch 683/1000\n"," - 11s - loss: 0.0500 - accuracy: 0.9829 - val_loss: 0.6096 - val_accuracy: 0.5086\n","\n","Epoch 00683: val_loss did not improve from 0.00000\n","Epoch 684/1000\n"," - 11s - loss: 0.0558 - accuracy: 0.9804 - val_loss: 0.0029 - val_accuracy: 0.5032\n","\n","Epoch 00684: val_loss did not improve from 0.00000\n","Epoch 685/1000\n"," - 11s - loss: 0.0488 - accuracy: 0.9842 - val_loss: 0.1645 - val_accuracy: 0.5074\n","\n","Epoch 00685: val_loss did not improve from 0.00000\n","Epoch 686/1000\n"," - 11s - loss: 0.0526 - accuracy: 0.9823 - val_loss: 3.4588 - val_accuracy: 0.5044\n","\n","Epoch 00686: val_loss did not improve from 0.00000\n","Epoch 687/1000\n"," - 11s - loss: 0.0491 - accuracy: 0.9837 - val_loss: 12.6207 - val_accuracy: 0.5107\n","\n","Epoch 00687: val_loss did not improve from 0.00000\n","Epoch 688/1000\n"," - 11s - loss: 0.0510 - accuracy: 0.9831 - val_loss: 0.8436 - val_accuracy: 0.5110\n","\n","Epoch 00688: val_loss did not improve from 0.00000\n","Epoch 689/1000\n"," - 11s - loss: 0.0531 - accuracy: 0.9822 - val_loss: 0.0103 - val_accuracy: 0.5098\n","\n","Epoch 00689: val_loss did not improve from 0.00000\n","Epoch 690/1000\n"," - 11s - loss: 0.0459 - accuracy: 0.9859 - val_loss: 27.6355 - val_accuracy: 0.5065\n","\n","Epoch 00690: val_loss did not improve from 0.00000\n","Epoch 691/1000\n"," - 11s - loss: 0.0527 - accuracy: 0.9813 - val_loss: 6.4745e-04 - val_accuracy: 0.5080\n","\n","Epoch 00691: val_loss did not improve from 0.00000\n","Epoch 692/1000\n"," - 11s - loss: 0.0469 - accuracy: 0.9843 - val_loss: 6.9700 - val_accuracy: 0.5068\n","\n","Epoch 00692: val_loss did not improve from 0.00000\n","Epoch 693/1000\n"," - 11s - loss: 0.0485 - accuracy: 0.9832 - val_loss: 15.8464 - val_accuracy: 0.5080\n","\n","Epoch 00693: val_loss did not improve from 0.00000\n","Epoch 694/1000\n"," - 11s - loss: 0.0482 - accuracy: 0.9832 - val_loss: 0.0023 - val_accuracy: 0.5074\n","\n","Epoch 00694: val_loss did not improve from 0.00000\n","Epoch 695/1000\n"," - 11s - loss: 0.0517 - accuracy: 0.9811 - val_loss: 0.0000e+00 - val_accuracy: 0.5041\n","\n","Epoch 00695: val_loss did not improve from 0.00000\n","Epoch 696/1000\n"," - 11s - loss: 0.0522 - accuracy: 0.9818 - val_loss: 3.3379e-06 - val_accuracy: 0.5032\n","\n","Epoch 00696: val_loss did not improve from 0.00000\n","Epoch 697/1000\n"," - 11s - loss: 0.0493 - accuracy: 0.9836 - val_loss: 0.0080 - val_accuracy: 0.5104\n","\n","Epoch 00697: val_loss did not improve from 0.00000\n","Epoch 698/1000\n"," - 11s - loss: 0.0515 - accuracy: 0.9837 - val_loss: 0.0000e+00 - val_accuracy: 0.5068\n","\n","Epoch 00698: val_loss did not improve from 0.00000\n","Epoch 699/1000\n"," - 11s - loss: 0.0515 - accuracy: 0.9828 - val_loss: 13.7403 - val_accuracy: 0.5086\n","\n","Epoch 00699: val_loss did not improve from 0.00000\n","Epoch 700/1000\n"," - 11s - loss: 0.0498 - accuracy: 0.9829 - val_loss: 30.9425 - val_accuracy: 0.5104\n","\n","Epoch 00700: val_loss did not improve from 0.00000\n","Epoch 701/1000\n"," - 11s - loss: 0.0466 - accuracy: 0.9850 - val_loss: 15.6428 - val_accuracy: 0.5053\n","\n","Epoch 00701: val_loss did not improve from 0.00000\n","Epoch 702/1000\n"," - 11s - loss: 0.0483 - accuracy: 0.9845 - val_loss: 3.4212e-05 - val_accuracy: 0.5017\n","\n","Epoch 00702: val_loss did not improve from 0.00000\n","Epoch 703/1000\n"," - 11s - loss: 0.0463 - accuracy: 0.9848 - val_loss: 0.2367 - val_accuracy: 0.5080\n","\n","Epoch 00703: val_loss did not improve from 0.00000\n","Epoch 704/1000\n"," - 11s - loss: 0.0446 - accuracy: 0.9860 - val_loss: 1.0530 - val_accuracy: 0.5056\n","\n","Epoch 00704: val_loss did not improve from 0.00000\n","Epoch 705/1000\n"," - 11s - loss: 0.0494 - accuracy: 0.9834 - val_loss: 8.4248 - val_accuracy: 0.5074\n","\n","Epoch 00705: val_loss did not improve from 0.00000\n","Epoch 706/1000\n"," - 11s - loss: 0.0479 - accuracy: 0.9839 - val_loss: 0.0000e+00 - val_accuracy: 0.5059\n","\n","Epoch 00706: val_loss did not improve from 0.00000\n","Epoch 707/1000\n"," - 11s - loss: 0.0478 - accuracy: 0.9841 - val_loss: 22.0519 - val_accuracy: 0.5080\n","\n","Epoch 00707: val_loss did not improve from 0.00000\n","Epoch 708/1000\n"," - 11s - loss: 0.0518 - accuracy: 0.9822 - val_loss: 17.8808 - val_accuracy: 0.5050\n","\n","Epoch 00708: val_loss did not improve from 0.00000\n","Epoch 709/1000\n"," - 11s - loss: 0.0467 - accuracy: 0.9843 - val_loss: 0.0584 - val_accuracy: 0.5107\n","\n","Epoch 00709: val_loss did not improve from 0.00000\n","Epoch 710/1000\n"," - 11s - loss: 0.0489 - accuracy: 0.9834 - val_loss: 23.6327 - val_accuracy: 0.5065\n","\n","Epoch 00710: val_loss did not improve from 0.00000\n","Epoch 711/1000\n"," - 11s - loss: 0.0438 - accuracy: 0.9858 - val_loss: 0.0092 - val_accuracy: 0.5152\n","\n","Epoch 00711: val_loss did not improve from 0.00000\n","Epoch 712/1000\n"," - 11s - loss: 0.0505 - accuracy: 0.9820 - val_loss: 2.6226e-06 - val_accuracy: 0.5134\n","\n","Epoch 00712: val_loss did not improve from 0.00000\n","Epoch 713/1000\n"," - 11s - loss: 0.0525 - accuracy: 0.9823 - val_loss: 17.9682 - val_accuracy: 0.5029\n","\n","Epoch 00713: val_loss did not improve from 0.00000\n","Epoch 714/1000\n"," - 11s - loss: 0.0459 - accuracy: 0.9855 - val_loss: 29.7109 - val_accuracy: 0.5104\n","\n","Epoch 00714: val_loss did not improve from 0.00000\n","Epoch 715/1000\n"," - 11s - loss: 0.0456 - accuracy: 0.9865 - val_loss: 0.0000e+00 - val_accuracy: 0.5089\n","\n","Epoch 00715: val_loss did not improve from 0.00000\n","Epoch 716/1000\n"," - 11s - loss: 0.0476 - accuracy: 0.9851 - val_loss: 0.0097 - val_accuracy: 0.5092\n","\n","Epoch 00716: val_loss did not improve from 0.00000\n","Epoch 717/1000\n"," - 11s - loss: 0.0507 - accuracy: 0.9829 - val_loss: 4.5643 - val_accuracy: 0.5107\n","\n","Epoch 00717: val_loss did not improve from 0.00000\n","Epoch 718/1000\n"," - 11s - loss: 0.0476 - accuracy: 0.9838 - val_loss: 26.3700 - val_accuracy: 0.5065\n","\n","Epoch 00718: val_loss did not improve from 0.00000\n","Epoch 719/1000\n"," - 11s - loss: 0.0483 - accuracy: 0.9836 - val_loss: 3.5763e-07 - val_accuracy: 0.5071\n","\n","Epoch 00719: val_loss did not improve from 0.00000\n","Epoch 720/1000\n"," - 11s - loss: 0.0443 - accuracy: 0.9859 - val_loss: 5.3182 - val_accuracy: 0.5050\n","\n","Epoch 00720: val_loss did not improve from 0.00000\n","Epoch 721/1000\n"," - 11s - loss: 0.0464 - accuracy: 0.9854 - val_loss: 14.7129 - val_accuracy: 0.5032\n","\n","Epoch 00721: val_loss did not improve from 0.00000\n","Epoch 722/1000\n"," - 11s - loss: 0.0486 - accuracy: 0.9839 - val_loss: 4.4360e-04 - val_accuracy: 0.5044\n","\n","Epoch 00722: val_loss did not improve from 0.00000\n","Epoch 723/1000\n"," - 11s - loss: 0.0449 - accuracy: 0.9857 - val_loss: 6.6031 - val_accuracy: 0.5020\n","\n","Epoch 00723: val_loss did not improve from 0.00000\n","Epoch 724/1000\n"," - 11s - loss: 0.0442 - accuracy: 0.9866 - val_loss: 14.2875 - val_accuracy: 0.5026\n","\n","Epoch 00724: val_loss did not improve from 0.00000\n","Epoch 725/1000\n"," - 11s - loss: 0.0507 - accuracy: 0.9827 - val_loss: 0.0333 - val_accuracy: 0.5044\n","\n","Epoch 00725: val_loss did not improve from 0.00000\n","Epoch 726/1000\n"," - 11s - loss: 0.0443 - accuracy: 0.9857 - val_loss: 12.9220 - val_accuracy: 0.5077\n","\n","Epoch 00726: val_loss did not improve from 0.00000\n","Epoch 727/1000\n"," - 11s - loss: 0.0448 - accuracy: 0.9863 - val_loss: 8.0337 - val_accuracy: 0.5092\n","\n","Epoch 00727: val_loss did not improve from 0.00000\n","Epoch 728/1000\n"," - 11s - loss: 0.0429 - accuracy: 0.9865 - val_loss: 7.6749 - val_accuracy: 0.5080\n","\n","Epoch 00728: val_loss did not improve from 0.00000\n","Epoch 729/1000\n"," - 11s - loss: 0.0444 - accuracy: 0.9858 - val_loss: 9.5367e-07 - val_accuracy: 0.5068\n","\n","Epoch 00729: val_loss did not improve from 0.00000\n","Epoch 730/1000\n"," - 11s - loss: 0.0457 - accuracy: 0.9858 - val_loss: 11.7938 - val_accuracy: 0.5083\n","\n","Epoch 00730: val_loss did not improve from 0.00000\n","Epoch 731/1000\n"," - 11s - loss: 0.0458 - accuracy: 0.9838 - val_loss: 0.0000e+00 - val_accuracy: 0.5068\n","\n","Epoch 00731: val_loss did not improve from 0.00000\n","Epoch 732/1000\n"," - 11s - loss: 0.0483 - accuracy: 0.9834 - val_loss: 6.6706 - val_accuracy: 0.5095\n","\n","Epoch 00732: val_loss did not improve from 0.00000\n","Epoch 733/1000\n"," - 11s - loss: 0.0435 - accuracy: 0.9852 - val_loss: 1.4793e-04 - val_accuracy: 0.5065\n","\n","Epoch 00733: val_loss did not improve from 0.00000\n","Epoch 734/1000\n"," - 11s - loss: 0.0475 - accuracy: 0.9859 - val_loss: 13.4394 - val_accuracy: 0.5017\n","\n","Epoch 00734: val_loss did not improve from 0.00000\n","Epoch 735/1000\n"," - 11s - loss: 0.0458 - accuracy: 0.9841 - val_loss: 6.4802 - val_accuracy: 0.5128\n","\n","Epoch 00735: val_loss did not improve from 0.00000\n","Epoch 736/1000\n"," - 11s - loss: 0.0459 - accuracy: 0.9854 - val_loss: 0.1057 - val_accuracy: 0.5041\n","\n","Epoch 00736: val_loss did not improve from 0.00000\n","Epoch 737/1000\n"," - 11s - loss: 0.0413 - accuracy: 0.9871 - val_loss: 0.1637 - val_accuracy: 0.5065\n","\n","Epoch 00737: val_loss did not improve from 0.00000\n","Epoch 738/1000\n"," - 11s - loss: 0.0397 - accuracy: 0.9880 - val_loss: 30.7079 - val_accuracy: 0.5083\n","\n","Epoch 00738: val_loss did not improve from 0.00000\n","Epoch 739/1000\n"," - 11s - loss: 0.0453 - accuracy: 0.9843 - val_loss: 0.0691 - val_accuracy: 0.5053\n","\n","Epoch 00739: val_loss did not improve from 0.00000\n","Epoch 740/1000\n"," - 11s - loss: 0.0433 - accuracy: 0.9856 - val_loss: 0.0019 - val_accuracy: 0.5083\n","\n","Epoch 00740: val_loss did not improve from 0.00000\n","Epoch 741/1000\n"," - 11s - loss: 0.0501 - accuracy: 0.9829 - val_loss: 6.0652 - val_accuracy: 0.5101\n","\n","Epoch 00741: val_loss did not improve from 0.00000\n","Epoch 742/1000\n"," - 11s - loss: 0.0449 - accuracy: 0.9868 - val_loss: 23.2508 - val_accuracy: 0.5065\n","\n","Epoch 00742: val_loss did not improve from 0.00000\n","Epoch 743/1000\n"," - 11s - loss: 0.0449 - accuracy: 0.9848 - val_loss: 0.0109 - val_accuracy: 0.5098\n","\n","Epoch 00743: val_loss did not improve from 0.00000\n","Epoch 744/1000\n"," - 11s - loss: 0.0423 - accuracy: 0.9851 - val_loss: 2.7418e-06 - val_accuracy: 0.5083\n","\n","Epoch 00744: val_loss did not improve from 0.00000\n","Epoch 745/1000\n"," - 11s - loss: 0.0442 - accuracy: 0.9851 - val_loss: 5.2253 - val_accuracy: 0.5083\n","\n","Epoch 00745: val_loss did not improve from 0.00000\n","Epoch 746/1000\n"," - 11s - loss: 0.0453 - accuracy: 0.9858 - val_loss: 0.0109 - val_accuracy: 0.5002\n","\n","Epoch 00746: val_loss did not improve from 0.00000\n","Epoch 747/1000\n"," - 11s - loss: 0.0429 - accuracy: 0.9866 - val_loss: 1.1921e-07 - val_accuracy: 0.5074\n","\n","Epoch 00747: val_loss did not improve from 0.00000\n","Epoch 748/1000\n"," - 11s - loss: 0.0436 - accuracy: 0.9846 - val_loss: 0.9667 - val_accuracy: 0.5068\n","\n","Epoch 00748: val_loss did not improve from 0.00000\n","Epoch 749/1000\n"," - 11s - loss: 0.0449 - accuracy: 0.9858 - val_loss: 1.8304 - val_accuracy: 0.5035\n","\n","Epoch 00749: val_loss did not improve from 0.00000\n","Epoch 750/1000\n"," - 11s - loss: 0.0462 - accuracy: 0.9834 - val_loss: 26.8164 - val_accuracy: 0.5080\n","\n","Epoch 00750: val_loss did not improve from 0.00000\n","Epoch 751/1000\n"," - 11s - loss: 0.0403 - accuracy: 0.9885 - val_loss: 0.0582 - val_accuracy: 0.5128\n","\n","Epoch 00751: val_loss did not improve from 0.00000\n","Epoch 752/1000\n"," - 11s - loss: 0.0417 - accuracy: 0.9867 - val_loss: 4.5923 - val_accuracy: 0.5086\n","\n","Epoch 00752: val_loss did not improve from 0.00000\n","Epoch 753/1000\n"," - 11s - loss: 0.0428 - accuracy: 0.9845 - val_loss: 11.1004 - val_accuracy: 0.5098\n","\n","Epoch 00753: val_loss did not improve from 0.00000\n","Epoch 754/1000\n"," - 11s - loss: 0.0444 - accuracy: 0.9860 - val_loss: 14.2167 - val_accuracy: 0.5158\n","\n","Epoch 00754: val_loss did not improve from 0.00000\n","Epoch 755/1000\n"," - 11s - loss: 0.0412 - accuracy: 0.9874 - val_loss: 0.1386 - val_accuracy: 0.5053\n","\n","Epoch 00755: val_loss did not improve from 0.00000\n","Epoch 756/1000\n"," - 11s - loss: 0.0435 - accuracy: 0.9851 - val_loss: 0.0000e+00 - val_accuracy: 0.5083\n","\n","Epoch 00756: val_loss did not improve from 0.00000\n","Epoch 757/1000\n"," - 11s - loss: 0.0470 - accuracy: 0.9831 - val_loss: 16.4703 - val_accuracy: 0.5104\n","\n","Epoch 00757: val_loss did not improve from 0.00000\n","Epoch 758/1000\n"," - 11s - loss: 0.0416 - accuracy: 0.9874 - val_loss: 0.0267 - val_accuracy: 0.5110\n","\n","Epoch 00758: val_loss did not improve from 0.00000\n","Epoch 759/1000\n"," - 11s - loss: 0.0394 - accuracy: 0.9882 - val_loss: 2.9329 - val_accuracy: 0.5038\n","\n","Epoch 00759: val_loss did not improve from 0.00000\n","Epoch 760/1000\n"," - 11s - loss: 0.0446 - accuracy: 0.9848 - val_loss: 0.0000e+00 - val_accuracy: 0.5032\n","\n","Epoch 00760: val_loss did not improve from 0.00000\n","Epoch 761/1000\n"," - 11s - loss: 0.0443 - accuracy: 0.9858 - val_loss: 3.1809 - val_accuracy: 0.5065\n","\n","Epoch 00761: val_loss did not improve from 0.00000\n","Epoch 762/1000\n"," - 11s - loss: 0.0409 - accuracy: 0.9873 - val_loss: 8.7917 - val_accuracy: 0.5086\n","\n","Epoch 00762: val_loss did not improve from 0.00000\n","Epoch 763/1000\n"," - 11s - loss: 0.0401 - accuracy: 0.9872 - val_loss: 36.2162 - val_accuracy: 0.5056\n","\n","Epoch 00763: val_loss did not improve from 0.00000\n","Epoch 764/1000\n"," - 11s - loss: 0.0402 - accuracy: 0.9869 - val_loss: 0.0050 - val_accuracy: 0.5098\n","\n","Epoch 00764: val_loss did not improve from 0.00000\n","Epoch 765/1000\n"," - 11s - loss: 0.0420 - accuracy: 0.9858 - val_loss: 2.3924 - val_accuracy: 0.5083\n","\n","Epoch 00765: val_loss did not improve from 0.00000\n","Epoch 766/1000\n"," - 11s - loss: 0.0461 - accuracy: 0.9856 - val_loss: 17.8256 - val_accuracy: 0.5023\n","\n","Epoch 00766: val_loss did not improve from 0.00000\n","Epoch 767/1000\n"," - 11s - loss: 0.0432 - accuracy: 0.9862 - val_loss: 0.0012 - val_accuracy: 0.5095\n","\n","Epoch 00767: val_loss did not improve from 0.00000\n","Epoch 768/1000\n"," - 11s - loss: 0.0407 - accuracy: 0.9871 - val_loss: 10.8217 - val_accuracy: 0.5047\n","\n","Epoch 00768: val_loss did not improve from 0.00000\n","Epoch 769/1000\n"," - 11s - loss: 0.0423 - accuracy: 0.9855 - val_loss: 6.0433e-04 - val_accuracy: 0.5089\n","\n","Epoch 00769: val_loss did not improve from 0.00000\n","Epoch 770/1000\n"," - 11s - loss: 0.0403 - accuracy: 0.9866 - val_loss: 7.7960e-05 - val_accuracy: 0.5032\n","\n","Epoch 00770: val_loss did not improve from 0.00000\n","Epoch 771/1000\n"," - 11s - loss: 0.0397 - accuracy: 0.9864 - val_loss: 3.6104 - val_accuracy: 0.5068\n","\n","Epoch 00771: val_loss did not improve from 0.00000\n","Epoch 772/1000\n"," - 11s - loss: 0.0404 - accuracy: 0.9872 - val_loss: 26.3253 - val_accuracy: 0.5110\n","\n","Epoch 00772: val_loss did not improve from 0.00000\n","Epoch 773/1000\n"," - 11s - loss: 0.0359 - accuracy: 0.9885 - val_loss: 6.2996 - val_accuracy: 0.5131\n","\n","Epoch 00773: val_loss did not improve from 0.00000\n","Epoch 774/1000\n"," - 11s - loss: 0.0402 - accuracy: 0.9869 - val_loss: 7.6151 - val_accuracy: 0.5065\n","\n","Epoch 00774: val_loss did not improve from 0.00000\n","Epoch 775/1000\n"," - 11s - loss: 0.0365 - accuracy: 0.9890 - val_loss: 0.4903 - val_accuracy: 0.5110\n","\n","Epoch 00775: val_loss did not improve from 0.00000\n","Epoch 776/1000\n"," - 11s - loss: 0.0394 - accuracy: 0.9878 - val_loss: 0.0722 - val_accuracy: 0.5101\n","\n","Epoch 00776: val_loss did not improve from 0.00000\n","Epoch 777/1000\n"," - 11s - loss: 0.0416 - accuracy: 0.9860 - val_loss: 30.4993 - val_accuracy: 0.5083\n","\n","Epoch 00777: val_loss did not improve from 0.00000\n","Epoch 778/1000\n"," - 11s - loss: 0.0378 - accuracy: 0.9892 - val_loss: 16.7849 - val_accuracy: 0.5077\n","\n","Epoch 00778: val_loss did not improve from 0.00000\n","Epoch 779/1000\n"," - 11s - loss: 0.0388 - accuracy: 0.9885 - val_loss: 0.0000e+00 - val_accuracy: 0.5068\n","\n","Epoch 00779: val_loss did not improve from 0.00000\n","Epoch 780/1000\n"," - 11s - loss: 0.0439 - accuracy: 0.9875 - val_loss: 3.5763e-07 - val_accuracy: 0.5062\n","\n","Epoch 00780: val_loss did not improve from 0.00000\n","Epoch 781/1000\n"," - 11s - loss: 0.0433 - accuracy: 0.9854 - val_loss: 6.6924 - val_accuracy: 0.5017\n","\n","Epoch 00781: val_loss did not improve from 0.00000\n","Epoch 782/1000\n"," - 11s - loss: 0.0392 - accuracy: 0.9878 - val_loss: 1.1921e-07 - val_accuracy: 0.5059\n","\n","Epoch 00782: val_loss did not improve from 0.00000\n","Epoch 783/1000\n"," - 11s - loss: 0.0406 - accuracy: 0.9873 - val_loss: 1.3113e-06 - val_accuracy: 0.5080\n","\n","Epoch 00783: val_loss did not improve from 0.00000\n","Epoch 784/1000\n"," - 11s - loss: 0.0375 - accuracy: 0.9887 - val_loss: 0.0620 - val_accuracy: 0.5092\n","\n","Epoch 00784: val_loss did not improve from 0.00000\n","Epoch 785/1000\n"," - 11s - loss: 0.0393 - accuracy: 0.9877 - val_loss: 13.1381 - val_accuracy: 0.5089\n","\n","Epoch 00785: val_loss did not improve from 0.00000\n","Epoch 786/1000\n"," - 11s - loss: 0.0378 - accuracy: 0.9893 - val_loss: 0.0011 - val_accuracy: 0.5062\n","\n","Epoch 00786: val_loss did not improve from 0.00000\n","Epoch 787/1000\n"," - 11s - loss: 0.0429 - accuracy: 0.9856 - val_loss: 0.0110 - val_accuracy: 0.5053\n","\n","Epoch 00787: val_loss did not improve from 0.00000\n","Epoch 788/1000\n"," - 11s - loss: 0.0375 - accuracy: 0.9885 - val_loss: 6.2342 - val_accuracy: 0.5062\n","\n","Epoch 00788: val_loss did not improve from 0.00000\n","Epoch 789/1000\n"," - 11s - loss: 0.0394 - accuracy: 0.9880 - val_loss: 0.0452 - val_accuracy: 0.5068\n","\n","Epoch 00789: val_loss did not improve from 0.00000\n","Epoch 790/1000\n"," - 11s - loss: 0.0435 - accuracy: 0.9857 - val_loss: 0.0000e+00 - val_accuracy: 0.5035\n","\n","Epoch 00790: val_loss did not improve from 0.00000\n","Epoch 791/1000\n"," - 11s - loss: 0.0348 - accuracy: 0.9900 - val_loss: 2.4980 - val_accuracy: 0.5074\n","\n","Epoch 00791: val_loss did not improve from 0.00000\n","Epoch 792/1000\n"," - 11s - loss: 0.0367 - accuracy: 0.9886 - val_loss: 5.6593 - val_accuracy: 0.5107\n","\n","Epoch 00792: val_loss did not improve from 0.00000\n","Epoch 793/1000\n"," - 11s - loss: 0.0418 - accuracy: 0.9866 - val_loss: 6.3056 - val_accuracy: 0.5032\n","\n","Epoch 00793: val_loss did not improve from 0.00000\n","Epoch 794/1000\n"," - 11s - loss: 0.0379 - accuracy: 0.9889 - val_loss: 12.2058 - val_accuracy: 0.5032\n","\n","Epoch 00794: val_loss did not improve from 0.00000\n","Epoch 795/1000\n"," - 11s - loss: 0.0353 - accuracy: 0.9897 - val_loss: 0.1160 - val_accuracy: 0.5098\n","\n","Epoch 00795: val_loss did not improve from 0.00000\n","Epoch 796/1000\n"," - 11s - loss: 0.0367 - accuracy: 0.9883 - val_loss: 4.5814 - val_accuracy: 0.5068\n","\n","Epoch 00796: val_loss did not improve from 0.00000\n","Epoch 797/1000\n"," - 11s - loss: 0.0367 - accuracy: 0.9887 - val_loss: 0.3542 - val_accuracy: 0.5068\n","\n","Epoch 00797: val_loss did not improve from 0.00000\n","Epoch 798/1000\n"," - 11s - loss: 0.0378 - accuracy: 0.9881 - val_loss: 13.2126 - val_accuracy: 0.5101\n","\n","Epoch 00798: val_loss did not improve from 0.00000\n","Epoch 799/1000\n"," - 11s - loss: 0.0382 - accuracy: 0.9877 - val_loss: 6.5279 - val_accuracy: 0.5080\n","\n","Epoch 00799: val_loss did not improve from 0.00000\n","Epoch 800/1000\n"," - 11s - loss: 0.0389 - accuracy: 0.9884 - val_loss: 7.7648 - val_accuracy: 0.5065\n","\n","Epoch 00800: val_loss did not improve from 0.00000\n","Epoch 801/1000\n"," - 11s - loss: 0.0381 - accuracy: 0.9867 - val_loss: 0.0000e+00 - val_accuracy: 0.5071\n","\n","Epoch 00801: val_loss did not improve from 0.00000\n","Epoch 802/1000\n"," - 11s - loss: 0.0389 - accuracy: 0.9877 - val_loss: 9.5367e-06 - val_accuracy: 0.5092\n","\n","Epoch 00802: val_loss did not improve from 0.00000\n","Epoch 803/1000\n"," - 11s - loss: 0.0400 - accuracy: 0.9876 - val_loss: 2.0266e-06 - val_accuracy: 0.5107\n","\n","Epoch 00803: val_loss did not improve from 0.00000\n","Epoch 804/1000\n"," - 11s - loss: 0.0380 - accuracy: 0.9874 - val_loss: 0.0000e+00 - val_accuracy: 0.5104\n","\n","Epoch 00804: val_loss did not improve from 0.00000\n","Epoch 805/1000\n"," - 11s - loss: 0.0376 - accuracy: 0.9886 - val_loss: 7.0310 - val_accuracy: 0.5065\n","\n","Epoch 00805: val_loss did not improve from 0.00000\n","Epoch 806/1000\n"," - 11s - loss: 0.0409 - accuracy: 0.9868 - val_loss: 39.4322 - val_accuracy: 0.5086\n","\n","Epoch 00806: val_loss did not improve from 0.00000\n","Epoch 807/1000\n"," - 11s - loss: 0.0365 - accuracy: 0.9876 - val_loss: 2.5442 - val_accuracy: 0.5098\n","\n","Epoch 00807: val_loss did not improve from 0.00000\n","Epoch 808/1000\n"," - 11s - loss: 0.0389 - accuracy: 0.9877 - val_loss: 7.1943 - val_accuracy: 0.5053\n","\n","Epoch 00808: val_loss did not improve from 0.00000\n","Epoch 809/1000\n"," - 11s - loss: 0.0351 - accuracy: 0.9898 - val_loss: 3.2305e-05 - val_accuracy: 0.5089\n","\n","Epoch 00809: val_loss did not improve from 0.00000\n","Epoch 810/1000\n"," - 11s - loss: 0.0325 - accuracy: 0.9911 - val_loss: 17.0033 - val_accuracy: 0.5101\n","\n","Epoch 00810: val_loss did not improve from 0.00000\n","Epoch 811/1000\n"," - 11s - loss: 0.0358 - accuracy: 0.9897 - val_loss: 7.8338 - val_accuracy: 0.5134\n","\n","Epoch 00811: val_loss did not improve from 0.00000\n","Epoch 812/1000\n"," - 11s - loss: 0.0373 - accuracy: 0.9894 - val_loss: 5.5355 - val_accuracy: 0.5092\n","\n","Epoch 00812: val_loss did not improve from 0.00000\n","Epoch 813/1000\n"," - 11s - loss: 0.0383 - accuracy: 0.9878 - val_loss: 24.6252 - val_accuracy: 0.5038\n","\n","Epoch 00813: val_loss did not improve from 0.00000\n","Epoch 814/1000\n"," - 11s - loss: 0.0368 - accuracy: 0.9898 - val_loss: 10.5291 - val_accuracy: 0.5071\n","\n","Epoch 00814: val_loss did not improve from 0.00000\n","Epoch 815/1000\n"," - 11s - loss: 0.0353 - accuracy: 0.9896 - val_loss: 0.0000e+00 - val_accuracy: 0.5014\n","\n","Epoch 00815: val_loss did not improve from 0.00000\n","Epoch 816/1000\n"," - 11s - loss: 0.0344 - accuracy: 0.9890 - val_loss: 0.0000e+00 - val_accuracy: 0.5059\n","\n","Epoch 00816: val_loss did not improve from 0.00000\n","Epoch 817/1000\n"," - 11s - loss: 0.0348 - accuracy: 0.9892 - val_loss: 18.6292 - val_accuracy: 0.4992\n","\n","Epoch 00817: val_loss did not improve from 0.00000\n","Epoch 818/1000\n"," - 11s - loss: 0.0374 - accuracy: 0.9878 - val_loss: 9.2983e-06 - val_accuracy: 0.5101\n","\n","Epoch 00818: val_loss did not improve from 0.00000\n","Epoch 819/1000\n"," - 11s - loss: 0.0368 - accuracy: 0.9885 - val_loss: 3.1705e-04 - val_accuracy: 0.5080\n","\n","Epoch 00819: val_loss did not improve from 0.00000\n","Epoch 820/1000\n"," - 11s - loss: 0.0360 - accuracy: 0.9890 - val_loss: 0.5185 - val_accuracy: 0.5074\n","\n","Epoch 00820: val_loss did not improve from 0.00000\n","Epoch 821/1000\n"," - 11s - loss: 0.0375 - accuracy: 0.9877 - val_loss: 4.7801 - val_accuracy: 0.5053\n","\n","Epoch 00821: val_loss did not improve from 0.00000\n","Epoch 822/1000\n"," - 11s - loss: 0.0366 - accuracy: 0.9880 - val_loss: 0.0541 - val_accuracy: 0.5029\n","\n","Epoch 00822: val_loss did not improve from 0.00000\n","Epoch 823/1000\n"," - 11s - loss: 0.0388 - accuracy: 0.9881 - val_loss: 2.3842e-07 - val_accuracy: 0.5065\n","\n","Epoch 00823: val_loss did not improve from 0.00000\n","Epoch 824/1000\n"," - 11s - loss: 0.0331 - accuracy: 0.9905 - val_loss: 1.6970 - val_accuracy: 0.5071\n","\n","Epoch 00824: val_loss did not improve from 0.00000\n","Epoch 825/1000\n"," - 11s - loss: 0.0327 - accuracy: 0.9900 - val_loss: 11.8354 - val_accuracy: 0.5014\n","\n","Epoch 00825: val_loss did not improve from 0.00000\n","Epoch 826/1000\n"," - 11s - loss: 0.0372 - accuracy: 0.9876 - val_loss: 0.0000e+00 - val_accuracy: 0.5080\n","\n","Epoch 00826: val_loss did not improve from 0.00000\n","Epoch 827/1000\n"," - 11s - loss: 0.0347 - accuracy: 0.9893 - val_loss: 50.9449 - val_accuracy: 0.5050\n","\n","Epoch 00827: val_loss did not improve from 0.00000\n","Epoch 828/1000\n"," - 11s - loss: 0.0361 - accuracy: 0.9880 - val_loss: 12.6964 - val_accuracy: 0.5107\n","\n","Epoch 00828: val_loss did not improve from 0.00000\n","Epoch 829/1000\n"," - 11s - loss: 0.0359 - accuracy: 0.9893 - val_loss: 0.0000e+00 - val_accuracy: 0.5086\n","\n","Epoch 00829: val_loss did not improve from 0.00000\n","Epoch 830/1000\n"," - 11s - loss: 0.0349 - accuracy: 0.9891 - val_loss: 34.2620 - val_accuracy: 0.5068\n","\n","Epoch 00830: val_loss did not improve from 0.00000\n","Epoch 831/1000\n"," - 11s - loss: 0.0347 - accuracy: 0.9914 - val_loss: 8.8650 - val_accuracy: 0.5059\n","\n","Epoch 00831: val_loss did not improve from 0.00000\n","Epoch 832/1000\n"," - 11s - loss: 0.0378 - accuracy: 0.9878 - val_loss: 7.1430 - val_accuracy: 0.5098\n","\n","Epoch 00832: val_loss did not improve from 0.00000\n","Epoch 833/1000\n"," - 11s - loss: 0.0345 - accuracy: 0.9904 - val_loss: 21.5732 - val_accuracy: 0.5140\n","\n","Epoch 00833: val_loss did not improve from 0.00000\n","Epoch 834/1000\n"," - 11s - loss: 0.0352 - accuracy: 0.9891 - val_loss: 16.5054 - val_accuracy: 0.5113\n","\n","Epoch 00834: val_loss did not improve from 0.00000\n","Epoch 835/1000\n"," - 11s - loss: 0.0384 - accuracy: 0.9877 - val_loss: 0.3621 - val_accuracy: 0.5137\n","\n","Epoch 00835: val_loss did not improve from 0.00000\n","Epoch 836/1000\n"," - 11s - loss: 0.0344 - accuracy: 0.9903 - val_loss: 9.7200 - val_accuracy: 0.5125\n","\n","Epoch 00836: val_loss did not improve from 0.00000\n","Epoch 837/1000\n"," - 11s - loss: 0.0320 - accuracy: 0.9913 - val_loss: 12.8441 - val_accuracy: 0.5080\n","\n","Epoch 00837: val_loss did not improve from 0.00000\n","Epoch 838/1000\n"," - 11s - loss: 0.0329 - accuracy: 0.9895 - val_loss: 0.0000e+00 - val_accuracy: 0.5041\n","\n","Epoch 00838: val_loss did not improve from 0.00000\n","Epoch 839/1000\n"," - 11s - loss: 0.0368 - accuracy: 0.9884 - val_loss: 12.9651 - val_accuracy: 0.5080\n","\n","Epoch 00839: val_loss did not improve from 0.00000\n","Epoch 840/1000\n"," - 11s - loss: 0.0345 - accuracy: 0.9894 - val_loss: 0.0000e+00 - val_accuracy: 0.5059\n","\n","Epoch 00840: val_loss did not improve from 0.00000\n","Epoch 841/1000\n"," - 11s - loss: 0.0321 - accuracy: 0.9907 - val_loss: 11.8231 - val_accuracy: 0.5137\n","\n","Epoch 00841: val_loss did not improve from 0.00000\n","Epoch 842/1000\n"," - 11s - loss: 0.0337 - accuracy: 0.9896 - val_loss: 0.5845 - val_accuracy: 0.5092\n","\n","Epoch 00842: val_loss did not improve from 0.00000\n","Epoch 843/1000\n"," - 11s - loss: 0.0342 - accuracy: 0.9908 - val_loss: 1.1921e-07 - val_accuracy: 0.5032\n","\n","Epoch 00843: val_loss did not improve from 0.00000\n","Epoch 844/1000\n"," - 11s - loss: 0.0362 - accuracy: 0.9894 - val_loss: 0.0033 - val_accuracy: 0.5086\n","\n","Epoch 00844: val_loss did not improve from 0.00000\n","Epoch 845/1000\n"," - 11s - loss: 0.0326 - accuracy: 0.9905 - val_loss: 13.3596 - val_accuracy: 0.5095\n","\n","Epoch 00845: val_loss did not improve from 0.00000\n","Epoch 846/1000\n"," - 11s - loss: 0.0347 - accuracy: 0.9886 - val_loss: 0.0000e+00 - val_accuracy: 0.5119\n","\n","Epoch 00846: val_loss did not improve from 0.00000\n","Epoch 847/1000\n"," - 11s - loss: 0.0368 - accuracy: 0.9885 - val_loss: 4.2295 - val_accuracy: 0.5077\n","\n","Epoch 00847: val_loss did not improve from 0.00000\n","Epoch 848/1000\n"," - 11s - loss: 0.0332 - accuracy: 0.9905 - val_loss: 0.0145 - val_accuracy: 0.5158\n","\n","Epoch 00848: val_loss did not improve from 0.00000\n","Epoch 849/1000\n"," - 11s - loss: 0.0358 - accuracy: 0.9889 - val_loss: 7.6229e-04 - val_accuracy: 0.5092\n","\n","Epoch 00849: val_loss did not improve from 0.00000\n","Epoch 850/1000\n"," - 11s - loss: 0.0331 - accuracy: 0.9904 - val_loss: 8.5627 - val_accuracy: 0.5089\n","\n","Epoch 00850: val_loss did not improve from 0.00000\n","Epoch 851/1000\n"," - 11s - loss: 0.0343 - accuracy: 0.9899 - val_loss: 15.4261 - val_accuracy: 0.5131\n","\n","Epoch 00851: val_loss did not improve from 0.00000\n","Epoch 852/1000\n"," - 11s - loss: 0.0312 - accuracy: 0.9899 - val_loss: 0.0000e+00 - val_accuracy: 0.5062\n","\n","Epoch 00852: val_loss did not improve from 0.00000\n","Epoch 853/1000\n"," - 11s - loss: 0.0361 - accuracy: 0.9886 - val_loss: 4.2029 - val_accuracy: 0.5119\n","\n","Epoch 00853: val_loss did not improve from 0.00000\n","Epoch 854/1000\n"," - 11s - loss: 0.0345 - accuracy: 0.9886 - val_loss: 17.8364 - val_accuracy: 0.5083\n","\n","Epoch 00854: val_loss did not improve from 0.00000\n","Epoch 855/1000\n"," - 11s - loss: 0.0358 - accuracy: 0.9879 - val_loss: 3.1059 - val_accuracy: 0.5104\n","\n","Epoch 00855: val_loss did not improve from 0.00000\n","Epoch 856/1000\n"," - 11s - loss: 0.0349 - accuracy: 0.9876 - val_loss: 0.0132 - val_accuracy: 0.5086\n","\n","Epoch 00856: val_loss did not improve from 0.00000\n","Epoch 857/1000\n"," - 11s - loss: 0.0340 - accuracy: 0.9896 - val_loss: 36.4678 - val_accuracy: 0.5122\n","\n","Epoch 00857: val_loss did not improve from 0.00000\n","Epoch 858/1000\n"," - 11s - loss: 0.0319 - accuracy: 0.9896 - val_loss: 0.2274 - val_accuracy: 0.5068\n","\n","Epoch 00858: val_loss did not improve from 0.00000\n","Epoch 859/1000\n"," - 11s - loss: 0.0301 - accuracy: 0.9922 - val_loss: 3.9339e-06 - val_accuracy: 0.5059\n","\n","Epoch 00859: val_loss did not improve from 0.00000\n","Epoch 860/1000\n"," - 11s - loss: 0.0320 - accuracy: 0.9900 - val_loss: 5.7576e-05 - val_accuracy: 0.5119\n","\n","Epoch 00860: val_loss did not improve from 0.00000\n","Epoch 861/1000\n"," - 11s - loss: 0.0312 - accuracy: 0.9911 - val_loss: 12.3691 - val_accuracy: 0.5086\n","\n","Epoch 00861: val_loss did not improve from 0.00000\n","Epoch 862/1000\n"," - 11s - loss: 0.0342 - accuracy: 0.9900 - val_loss: 0.0000e+00 - val_accuracy: 0.5083\n","\n","Epoch 00862: val_loss did not improve from 0.00000\n","Epoch 863/1000\n"," - 11s - loss: 0.0327 - accuracy: 0.9905 - val_loss: 0.0000e+00 - val_accuracy: 0.5080\n","\n","Epoch 00863: val_loss did not improve from 0.00000\n","Epoch 864/1000\n"," - 11s - loss: 0.0322 - accuracy: 0.9896 - val_loss: 16.7607 - val_accuracy: 0.5107\n","\n","Epoch 00864: val_loss did not improve from 0.00000\n","Epoch 865/1000\n"," - 11s - loss: 0.0358 - accuracy: 0.9889 - val_loss: 0.0048 - val_accuracy: 0.5119\n","\n","Epoch 00865: val_loss did not improve from 0.00000\n","Epoch 866/1000\n"," - 11s - loss: 0.0317 - accuracy: 0.9905 - val_loss: 0.0000e+00 - val_accuracy: 0.5104\n","\n","Epoch 00866: val_loss did not improve from 0.00000\n","Epoch 867/1000\n"," - 11s - loss: 0.0325 - accuracy: 0.9897 - val_loss: 1.5496e-04 - val_accuracy: 0.5137\n","\n","Epoch 00867: val_loss did not improve from 0.00000\n","Epoch 868/1000\n"," - 11s - loss: 0.0355 - accuracy: 0.9882 - val_loss: 8.9145 - val_accuracy: 0.5116\n","\n","Epoch 00868: val_loss did not improve from 0.00000\n","Epoch 869/1000\n"," - 11s - loss: 0.0290 - accuracy: 0.9924 - val_loss: 0.0319 - val_accuracy: 0.5080\n","\n","Epoch 00869: val_loss did not improve from 0.00000\n","Epoch 870/1000\n"," - 11s - loss: 0.0337 - accuracy: 0.9894 - val_loss: 0.1023 - val_accuracy: 0.5083\n","\n","Epoch 00870: val_loss did not improve from 0.00000\n","Epoch 871/1000\n"," - 11s - loss: 0.0311 - accuracy: 0.9905 - val_loss: 0.0000e+00 - val_accuracy: 0.5113\n","\n","Epoch 00871: val_loss did not improve from 0.00000\n","Epoch 872/1000\n"," - 11s - loss: 0.0316 - accuracy: 0.9896 - val_loss: 30.4721 - val_accuracy: 0.5176\n","\n","Epoch 00872: val_loss did not improve from 0.00000\n","Epoch 873/1000\n"," - 11s - loss: 0.0321 - accuracy: 0.9894 - val_loss: 17.3745 - val_accuracy: 0.5122\n","\n","Epoch 00873: val_loss did not improve from 0.00000\n","Epoch 874/1000\n"," - 11s - loss: 0.0337 - accuracy: 0.9903 - val_loss: 9.7050 - val_accuracy: 0.5110\n","\n","Epoch 00874: val_loss did not improve from 0.00000\n","Epoch 875/1000\n"," - 11s - loss: 0.0335 - accuracy: 0.9883 - val_loss: 5.1260e-06 - val_accuracy: 0.5095\n","\n","Epoch 00875: val_loss did not improve from 0.00000\n","Epoch 876/1000\n"," - 11s - loss: 0.0296 - accuracy: 0.9928 - val_loss: 5.5547 - val_accuracy: 0.5080\n","\n","Epoch 00876: val_loss did not improve from 0.00000\n","Epoch 877/1000\n"," - 11s - loss: 0.0315 - accuracy: 0.9903 - val_loss: 0.3135 - val_accuracy: 0.5095\n","\n","Epoch 00877: val_loss did not improve from 0.00000\n","Epoch 878/1000\n"," - 11s - loss: 0.0328 - accuracy: 0.9891 - val_loss: 8.0710 - val_accuracy: 0.5077\n","\n","Epoch 00878: val_loss did not improve from 0.00000\n","Epoch 879/1000\n"," - 11s - loss: 0.0290 - accuracy: 0.9910 - val_loss: 40.6940 - val_accuracy: 0.5128\n","\n","Epoch 00879: val_loss did not improve from 0.00000\n","Epoch 880/1000\n"," - 11s - loss: 0.0316 - accuracy: 0.9901 - val_loss: 4.4107e-06 - val_accuracy: 0.5074\n","\n","Epoch 00880: val_loss did not improve from 0.00000\n","Epoch 881/1000\n"," - 11s - loss: 0.0315 - accuracy: 0.9914 - val_loss: 2.2535 - val_accuracy: 0.5101\n","\n","Epoch 00881: val_loss did not improve from 0.00000\n","Epoch 882/1000\n"," - 11s - loss: 0.0320 - accuracy: 0.9894 - val_loss: 0.0000e+00 - val_accuracy: 0.5083\n","\n","Epoch 00882: val_loss did not improve from 0.00000\n","Epoch 883/1000\n"," - 11s - loss: 0.0301 - accuracy: 0.9902 - val_loss: 6.5565e-06 - val_accuracy: 0.5119\n","\n","Epoch 00883: val_loss did not improve from 0.00000\n","Epoch 884/1000\n"," - 11s - loss: 0.0296 - accuracy: 0.9909 - val_loss: 5.4700 - val_accuracy: 0.5086\n","\n","Epoch 00884: val_loss did not improve from 0.00000\n","Epoch 885/1000\n"," - 11s - loss: 0.0329 - accuracy: 0.9903 - val_loss: 2.1677 - val_accuracy: 0.5098\n","\n","Epoch 00885: val_loss did not improve from 0.00000\n","Epoch 886/1000\n"," - 11s - loss: 0.0321 - accuracy: 0.9900 - val_loss: 0.1005 - val_accuracy: 0.5068\n","\n","Epoch 00886: val_loss did not improve from 0.00000\n","Epoch 887/1000\n"," - 11s - loss: 0.0294 - accuracy: 0.9911 - val_loss: 0.2341 - val_accuracy: 0.5074\n","\n","Epoch 00887: val_loss did not improve from 0.00000\n","Epoch 888/1000\n"," - 11s - loss: 0.0286 - accuracy: 0.9916 - val_loss: 4.6492e-06 - val_accuracy: 0.5077\n","\n","Epoch 00888: val_loss did not improve from 0.00000\n","Epoch 889/1000\n"," - 11s - loss: 0.0312 - accuracy: 0.9902 - val_loss: 5.9605e-07 - val_accuracy: 0.5068\n","\n","Epoch 00889: val_loss did not improve from 0.00000\n","Epoch 890/1000\n"," - 11s - loss: 0.0330 - accuracy: 0.9896 - val_loss: 6.1432 - val_accuracy: 0.5101\n","\n","Epoch 00890: val_loss did not improve from 0.00000\n","Epoch 891/1000\n"," - 11s - loss: 0.0311 - accuracy: 0.9908 - val_loss: 6.8445 - val_accuracy: 0.5035\n","\n","Epoch 00891: val_loss did not improve from 0.00000\n","Epoch 892/1000\n"," - 11s - loss: 0.0274 - accuracy: 0.9929 - val_loss: 9.7480 - val_accuracy: 0.5101\n","\n","Epoch 00892: val_loss did not improve from 0.00000\n","Epoch 893/1000\n"," - 11s - loss: 0.0286 - accuracy: 0.9923 - val_loss: 0.5031 - val_accuracy: 0.5080\n","\n","Epoch 00893: val_loss did not improve from 0.00000\n","Epoch 894/1000\n"," - 11s - loss: 0.0310 - accuracy: 0.9910 - val_loss: 0.0000e+00 - val_accuracy: 0.5092\n","\n","Epoch 00894: val_loss did not improve from 0.00000\n","Epoch 895/1000\n"," - 11s - loss: 0.0306 - accuracy: 0.9906 - val_loss: 0.9267 - val_accuracy: 0.5113\n","\n","Epoch 00895: val_loss did not improve from 0.00000\n","Epoch 896/1000\n"," - 11s - loss: 0.0307 - accuracy: 0.9893 - val_loss: 18.8540 - val_accuracy: 0.5143\n","\n","Epoch 00896: val_loss did not improve from 0.00000\n","Epoch 897/1000\n"," - 11s - loss: 0.0281 - accuracy: 0.9927 - val_loss: 3.2849e-04 - val_accuracy: 0.5086\n","\n","Epoch 00897: val_loss did not improve from 0.00000\n","Epoch 898/1000\n"," - 11s - loss: 0.0308 - accuracy: 0.9904 - val_loss: 0.0000e+00 - val_accuracy: 0.5104\n","\n","Epoch 00898: val_loss did not improve from 0.00000\n","Epoch 899/1000\n"," - 11s - loss: 0.0273 - accuracy: 0.9920 - val_loss: 0.0000e+00 - val_accuracy: 0.5092\n","\n","Epoch 00899: val_loss did not improve from 0.00000\n","Epoch 900/1000\n"," - 11s - loss: 0.0289 - accuracy: 0.9917 - val_loss: 13.9810 - val_accuracy: 0.5056\n","\n","Epoch 00900: val_loss did not improve from 0.00000\n","Epoch 901/1000\n"," - 11s - loss: 0.0303 - accuracy: 0.9903 - val_loss: 9.4948 - val_accuracy: 0.5071\n","\n","Epoch 00901: val_loss did not improve from 0.00000\n","Epoch 902/1000\n"," - 11s - loss: 0.0288 - accuracy: 0.9909 - val_loss: 33.0092 - val_accuracy: 0.5107\n","\n","Epoch 00902: val_loss did not improve from 0.00000\n","Epoch 903/1000\n"," - 11s - loss: 0.0280 - accuracy: 0.9913 - val_loss: 18.0500 - val_accuracy: 0.5068\n","\n","Epoch 00903: val_loss did not improve from 0.00000\n","Epoch 904/1000\n"," - 11s - loss: 0.0316 - accuracy: 0.9899 - val_loss: 0.0000e+00 - val_accuracy: 0.5077\n","\n","Epoch 00904: val_loss did not improve from 0.00000\n","Epoch 905/1000\n"," - 11s - loss: 0.0294 - accuracy: 0.9917 - val_loss: 0.0381 - val_accuracy: 0.5086\n","\n","Epoch 00905: val_loss did not improve from 0.00000\n","Epoch 906/1000\n"," - 11s - loss: 0.0327 - accuracy: 0.9903 - val_loss: 18.6150 - val_accuracy: 0.5056\n","\n","Epoch 00906: val_loss did not improve from 0.00000\n","Epoch 907/1000\n"," - 11s - loss: 0.0301 - accuracy: 0.9917 - val_loss: 16.9692 - val_accuracy: 0.5059\n","\n","Epoch 00907: val_loss did not improve from 0.00000\n","Epoch 908/1000\n"," - 11s - loss: 0.0280 - accuracy: 0.9924 - val_loss: 1.5259e-05 - val_accuracy: 0.5062\n","\n","Epoch 00908: val_loss did not improve from 0.00000\n","Epoch 909/1000\n"," - 11s - loss: 0.0320 - accuracy: 0.9905 - val_loss: 0.0000e+00 - val_accuracy: 0.5125\n","\n","Epoch 00909: val_loss did not improve from 0.00000\n","Epoch 910/1000\n"," - 11s - loss: 0.0297 - accuracy: 0.9912 - val_loss: 0.0000e+00 - val_accuracy: 0.5083\n","\n","Epoch 00910: val_loss did not improve from 0.00000\n","Epoch 911/1000\n"," - 11s - loss: 0.0263 - accuracy: 0.9919 - val_loss: 0.0061 - val_accuracy: 0.5095\n","\n","Epoch 00911: val_loss did not improve from 0.00000\n","Epoch 912/1000\n"," - 11s - loss: 0.0289 - accuracy: 0.9918 - val_loss: 0.0000e+00 - val_accuracy: 0.5059\n","\n","Epoch 00912: val_loss did not improve from 0.00000\n","Epoch 913/1000\n"," - 11s - loss: 0.0296 - accuracy: 0.9904 - val_loss: 9.5648 - val_accuracy: 0.5137\n","\n","Epoch 00913: val_loss did not improve from 0.00000\n","Epoch 914/1000\n"," - 11s - loss: 0.0261 - accuracy: 0.9939 - val_loss: 9.9513 - val_accuracy: 0.5077\n","\n","Epoch 00914: val_loss did not improve from 0.00000\n","Epoch 915/1000\n"," - 11s - loss: 0.0280 - accuracy: 0.9913 - val_loss: 0.0223 - val_accuracy: 0.5089\n","\n","Epoch 00915: val_loss did not improve from 0.00000\n","Epoch 916/1000\n"," - 11s - loss: 0.0302 - accuracy: 0.9909 - val_loss: 5.5838 - val_accuracy: 0.5089\n","\n","Epoch 00916: val_loss did not improve from 0.00000\n","Epoch 917/1000\n"," - 11s - loss: 0.0298 - accuracy: 0.9919 - val_loss: 9.4122 - val_accuracy: 0.5128\n","\n","Epoch 00917: val_loss did not improve from 0.00000\n","Epoch 918/1000\n"," - 11s - loss: 0.0281 - accuracy: 0.9918 - val_loss: 0.0000e+00 - val_accuracy: 0.5050\n","\n","Epoch 00918: val_loss did not improve from 0.00000\n","Epoch 919/1000\n"," - 11s - loss: 0.0281 - accuracy: 0.9924 - val_loss: 0.0000e+00 - val_accuracy: 0.5068\n","\n","Epoch 00919: val_loss did not improve from 0.00000\n","Epoch 920/1000\n"," - 11s - loss: 0.0260 - accuracy: 0.9933 - val_loss: 19.1805 - val_accuracy: 0.5107\n","\n","Epoch 00920: val_loss did not improve from 0.00000\n","Epoch 921/1000\n"," - 11s - loss: 0.0280 - accuracy: 0.9902 - val_loss: 0.0000e+00 - val_accuracy: 0.5086\n","\n","Epoch 00921: val_loss did not improve from 0.00000\n","Epoch 922/1000\n"," - 11s - loss: 0.0278 - accuracy: 0.9921 - val_loss: 28.6141 - val_accuracy: 0.5083\n","\n","Epoch 00922: val_loss did not improve from 0.00000\n","Epoch 923/1000\n"," - 11s - loss: 0.0292 - accuracy: 0.9916 - val_loss: 12.1877 - val_accuracy: 0.5116\n","\n","Epoch 00923: val_loss did not improve from 0.00000\n","Epoch 924/1000\n"," - 11s - loss: 0.0274 - accuracy: 0.9916 - val_loss: 14.6581 - val_accuracy: 0.5143\n","\n","Epoch 00924: val_loss did not improve from 0.00000\n","Epoch 925/1000\n"," - 11s - loss: 0.0267 - accuracy: 0.9931 - val_loss: 0.0350 - val_accuracy: 0.5071\n","\n","Epoch 00925: val_loss did not improve from 0.00000\n","Epoch 926/1000\n"," - 11s - loss: 0.0249 - accuracy: 0.9931 - val_loss: 0.2931 - val_accuracy: 0.5074\n","\n","Epoch 00926: val_loss did not improve from 0.00000\n","Epoch 927/1000\n"," - 11s - loss: 0.0275 - accuracy: 0.9923 - val_loss: 3.9600 - val_accuracy: 0.5098\n","\n","Epoch 00927: val_loss did not improve from 0.00000\n","Epoch 928/1000\n"," - 11s - loss: 0.0284 - accuracy: 0.9907 - val_loss: 54.8854 - val_accuracy: 0.5104\n","\n","Epoch 00928: val_loss did not improve from 0.00000\n","Epoch 929/1000\n"," - 11s - loss: 0.0257 - accuracy: 0.9929 - val_loss: 8.1430 - val_accuracy: 0.5110\n","\n","Epoch 00929: val_loss did not improve from 0.00000\n","Epoch 930/1000\n"," - 11s - loss: 0.0280 - accuracy: 0.9910 - val_loss: 0.0307 - val_accuracy: 0.5062\n","\n","Epoch 00930: val_loss did not improve from 0.00000\n","Epoch 931/1000\n"," - 11s - loss: 0.0298 - accuracy: 0.9906 - val_loss: 10.2821 - val_accuracy: 0.5134\n","\n","Epoch 00931: val_loss did not improve from 0.00000\n","Epoch 932/1000\n"," - 11s - loss: 0.0283 - accuracy: 0.9915 - val_loss: 0.0229 - val_accuracy: 0.5098\n","\n","Epoch 00932: val_loss did not improve from 0.00000\n","Epoch 933/1000\n"," - 11s - loss: 0.0248 - accuracy: 0.9926 - val_loss: 8.6308 - val_accuracy: 0.5149\n","\n","Epoch 00933: val_loss did not improve from 0.00000\n","Epoch 934/1000\n"," - 11s - loss: 0.0252 - accuracy: 0.9933 - val_loss: 1.4311 - val_accuracy: 0.5065\n","\n","Epoch 00934: val_loss did not improve from 0.00000\n","Epoch 935/1000\n"," - 11s - loss: 0.0245 - accuracy: 0.9928 - val_loss: 0.0000e+00 - val_accuracy: 0.5131\n","\n","Epoch 00935: val_loss did not improve from 0.00000\n","Epoch 936/1000\n"," - 11s - loss: 0.0283 - accuracy: 0.9926 - val_loss: 7.8799 - val_accuracy: 0.5086\n","\n","Epoch 00936: val_loss did not improve from 0.00000\n","Epoch 937/1000\n"," - 11s - loss: 0.0290 - accuracy: 0.9913 - val_loss: 0.0000e+00 - val_accuracy: 0.5086\n","\n","Epoch 00937: val_loss did not improve from 0.00000\n","Epoch 938/1000\n"," - 11s - loss: 0.0299 - accuracy: 0.9912 - val_loss: 6.3846 - val_accuracy: 0.5131\n","\n","Epoch 00938: val_loss did not improve from 0.00000\n","Epoch 939/1000\n"," - 11s - loss: 0.0302 - accuracy: 0.9897 - val_loss: 0.0000e+00 - val_accuracy: 0.5047\n","\n","Epoch 00939: val_loss did not improve from 0.00000\n","Epoch 940/1000\n"," - 11s - loss: 0.0282 - accuracy: 0.9916 - val_loss: 22.0295 - val_accuracy: 0.5029\n","\n","Epoch 00940: val_loss did not improve from 0.00000\n","Epoch 941/1000\n"," - 11s - loss: 0.0272 - accuracy: 0.9913 - val_loss: 1.0842 - val_accuracy: 0.5089\n","\n","Epoch 00941: val_loss did not improve from 0.00000\n","Epoch 942/1000\n"," - 11s - loss: 0.0272 - accuracy: 0.9917 - val_loss: 0.0707 - val_accuracy: 0.5080\n","\n","Epoch 00942: val_loss did not improve from 0.00000\n","Epoch 943/1000\n"," - 11s - loss: 0.0259 - accuracy: 0.9920 - val_loss: 7.7528 - val_accuracy: 0.5071\n","\n","Epoch 00943: val_loss did not improve from 0.00000\n","Epoch 944/1000\n"," - 11s - loss: 0.0311 - accuracy: 0.9906 - val_loss: 0.1099 - val_accuracy: 0.5032\n","\n","Epoch 00944: val_loss did not improve from 0.00000\n","Epoch 945/1000\n"," - 11s - loss: 0.0281 - accuracy: 0.9913 - val_loss: 0.0167 - val_accuracy: 0.5080\n","\n","Epoch 00945: val_loss did not improve from 0.00000\n","Epoch 946/1000\n"," - 11s - loss: 0.0280 - accuracy: 0.9911 - val_loss: 13.5997 - val_accuracy: 0.5062\n","\n","Epoch 00946: val_loss did not improve from 0.00000\n","Epoch 947/1000\n"," - 11s - loss: 0.0263 - accuracy: 0.9921 - val_loss: 0.0000e+00 - val_accuracy: 0.5083\n","\n","Epoch 00947: val_loss did not improve from 0.00000\n","Epoch 948/1000\n"," - 11s - loss: 0.0262 - accuracy: 0.9923 - val_loss: 11.3067 - val_accuracy: 0.5101\n","\n","Epoch 00948: val_loss did not improve from 0.00000\n","Epoch 949/1000\n"," - 11s - loss: 0.0271 - accuracy: 0.9913 - val_loss: 2.5938 - val_accuracy: 0.5032\n","\n","Epoch 00949: val_loss did not improve from 0.00000\n","Epoch 950/1000\n"," - 11s - loss: 0.0255 - accuracy: 0.9931 - val_loss: 0.0163 - val_accuracy: 0.5089\n","\n","Epoch 00950: val_loss did not improve from 0.00000\n","Epoch 951/1000\n"," - 11s - loss: 0.0241 - accuracy: 0.9948 - val_loss: 0.0345 - val_accuracy: 0.5068\n","\n","Epoch 00951: val_loss did not improve from 0.00000\n","Epoch 952/1000\n"," - 11s - loss: 0.0287 - accuracy: 0.9916 - val_loss: 1.6309 - val_accuracy: 0.5095\n","\n","Epoch 00952: val_loss did not improve from 0.00000\n","Epoch 953/1000\n"," - 11s - loss: 0.0281 - accuracy: 0.9921 - val_loss: 0.2264 - val_accuracy: 0.5146\n","\n","Epoch 00953: val_loss did not improve from 0.00000\n","Epoch 954/1000\n"," - 11s - loss: 0.0265 - accuracy: 0.9921 - val_loss: 0.0260 - val_accuracy: 0.5080\n","\n","Epoch 00954: val_loss did not improve from 0.00000\n","Epoch 955/1000\n"," - 11s - loss: 0.0253 - accuracy: 0.9924 - val_loss: 0.6118 - val_accuracy: 0.5092\n","\n","Epoch 00955: val_loss did not improve from 0.00000\n","Epoch 956/1000\n"," - 11s - loss: 0.0291 - accuracy: 0.9911 - val_loss: 20.0629 - val_accuracy: 0.5086\n","\n","Epoch 00956: val_loss did not improve from 0.00000\n","Epoch 957/1000\n"," - 11s - loss: 0.0258 - accuracy: 0.9921 - val_loss: 0.7030 - val_accuracy: 0.5041\n","\n","Epoch 00957: val_loss did not improve from 0.00000\n","Epoch 958/1000\n"," - 11s - loss: 0.0243 - accuracy: 0.9937 - val_loss: 2.3063 - val_accuracy: 0.5017\n","\n","Epoch 00958: val_loss did not improve from 0.00000\n","Epoch 959/1000\n"," - 11s - loss: 0.0277 - accuracy: 0.9928 - val_loss: 0.1611 - val_accuracy: 0.5119\n","\n","Epoch 00959: val_loss did not improve from 0.00000\n","Epoch 960/1000\n"," - 11s - loss: 0.0249 - accuracy: 0.9932 - val_loss: 0.7056 - val_accuracy: 0.5104\n","\n","Epoch 00960: val_loss did not improve from 0.00000\n","Epoch 961/1000\n"," - 11s - loss: 0.0230 - accuracy: 0.9933 - val_loss: 0.0376 - val_accuracy: 0.5107\n","\n","Epoch 00961: val_loss did not improve from 0.00000\n","Epoch 962/1000\n"," - 11s - loss: 0.0268 - accuracy: 0.9923 - val_loss: 3.5673 - val_accuracy: 0.5098\n","\n","Epoch 00962: val_loss did not improve from 0.00000\n","Epoch 963/1000\n"," - 11s - loss: 0.0267 - accuracy: 0.9928 - val_loss: 2.7858 - val_accuracy: 0.5038\n","\n","Epoch 00963: val_loss did not improve from 0.00000\n","Epoch 964/1000\n"," - 11s - loss: 0.0217 - accuracy: 0.9936 - val_loss: 1.7762e-05 - val_accuracy: 0.5110\n","\n","Epoch 00964: val_loss did not improve from 0.00000\n","Epoch 965/1000\n"," - 11s - loss: 0.0263 - accuracy: 0.9926 - val_loss: 4.3172 - val_accuracy: 0.5080\n","\n","Epoch 00965: val_loss did not improve from 0.00000\n","Epoch 966/1000\n"," - 11s - loss: 0.0290 - accuracy: 0.9915 - val_loss: 5.4238 - val_accuracy: 0.5038\n","\n","Epoch 00966: val_loss did not improve from 0.00000\n","Epoch 967/1000\n"," - 11s - loss: 0.0265 - accuracy: 0.9923 - val_loss: 0.0034 - val_accuracy: 0.5086\n","\n","Epoch 00967: val_loss did not improve from 0.00000\n","Epoch 968/1000\n"," - 11s - loss: 0.0232 - accuracy: 0.9942 - val_loss: 9.1044 - val_accuracy: 0.5116\n","\n","Epoch 00968: val_loss did not improve from 0.00000\n","Epoch 969/1000\n"," - 11s - loss: 0.0259 - accuracy: 0.9933 - val_loss: 7.0594 - val_accuracy: 0.5092\n","\n","Epoch 00969: val_loss did not improve from 0.00000\n","Epoch 970/1000\n"," - 11s - loss: 0.0247 - accuracy: 0.9937 - val_loss: 0.0224 - val_accuracy: 0.5074\n","\n","Epoch 00970: val_loss did not improve from 0.00000\n","Epoch 971/1000\n"," - 11s - loss: 0.0249 - accuracy: 0.9928 - val_loss: 0.0000e+00 - val_accuracy: 0.5089\n","\n","Epoch 00971: val_loss did not improve from 0.00000\n","Epoch 972/1000\n"," - 11s - loss: 0.0274 - accuracy: 0.9923 - val_loss: 1.1921e-07 - val_accuracy: 0.5092\n","\n","Epoch 00972: val_loss did not improve from 0.00000\n","Epoch 973/1000\n"," - 11s - loss: 0.0237 - accuracy: 0.9935 - val_loss: 3.8016 - val_accuracy: 0.5098\n","\n","Epoch 00973: val_loss did not improve from 0.00000\n","Epoch 974/1000\n"," - 11s - loss: 0.0241 - accuracy: 0.9922 - val_loss: 0.0025 - val_accuracy: 0.5107\n","\n","Epoch 00974: val_loss did not improve from 0.00000\n","Epoch 975/1000\n"," - 11s - loss: 0.0278 - accuracy: 0.9915 - val_loss: 0.6835 - val_accuracy: 0.5017\n","\n","Epoch 00975: val_loss did not improve from 0.00000\n","Epoch 976/1000\n"," - 11s - loss: 0.0292 - accuracy: 0.9914 - val_loss: 9.2626 - val_accuracy: 0.5065\n","\n","Epoch 00976: val_loss did not improve from 0.00000\n","Epoch 977/1000\n"," - 11s - loss: 0.0258 - accuracy: 0.9923 - val_loss: 8.2883 - val_accuracy: 0.5050\n","\n","Epoch 00977: val_loss did not improve from 0.00000\n","Epoch 978/1000\n"," - 11s - loss: 0.0235 - accuracy: 0.9938 - val_loss: 4.4828 - val_accuracy: 0.5014\n","\n","Epoch 00978: val_loss did not improve from 0.00000\n","Epoch 979/1000\n"," - 11s - loss: 0.0223 - accuracy: 0.9946 - val_loss: 0.9657 - val_accuracy: 0.5098\n","\n","Epoch 00979: val_loss did not improve from 0.00000\n","Epoch 980/1000\n"," - 11s - loss: 0.0247 - accuracy: 0.9939 - val_loss: 21.9922 - val_accuracy: 0.5080\n","\n","Epoch 00980: val_loss did not improve from 0.00000\n","Epoch 981/1000\n"," - 11s - loss: 0.0252 - accuracy: 0.9925 - val_loss: 6.4729e-05 - val_accuracy: 0.5071\n","\n","Epoch 00981: val_loss did not improve from 0.00000\n","Epoch 982/1000\n"," - 11s - loss: 0.0232 - accuracy: 0.9936 - val_loss: 0.0000e+00 - val_accuracy: 0.5062\n","\n","Epoch 00982: val_loss did not improve from 0.00000\n","Epoch 983/1000\n"," - 11s - loss: 0.0267 - accuracy: 0.9923 - val_loss: 16.7440 - val_accuracy: 0.5038\n","\n","Epoch 00983: val_loss did not improve from 0.00000\n","Epoch 984/1000\n"," - 11s - loss: 0.0238 - accuracy: 0.9929 - val_loss: 5.4100 - val_accuracy: 0.5092\n","\n","Epoch 00984: val_loss did not improve from 0.00000\n","Epoch 985/1000\n"," - 11s - loss: 0.0233 - accuracy: 0.9935 - val_loss: 3.6358e-05 - val_accuracy: 0.5110\n","\n","Epoch 00985: val_loss did not improve from 0.00000\n","Epoch 986/1000\n"," - 11s - loss: 0.0260 - accuracy: 0.9917 - val_loss: 0.0000e+00 - val_accuracy: 0.5107\n","\n","Epoch 00986: val_loss did not improve from 0.00000\n","Epoch 987/1000\n"," - 11s - loss: 0.0238 - accuracy: 0.9927 - val_loss: 0.0086 - val_accuracy: 0.5062\n","\n","Epoch 00987: val_loss did not improve from 0.00000\n","Epoch 988/1000\n"," - 11s - loss: 0.0243 - accuracy: 0.9934 - val_loss: 7.0487e-04 - val_accuracy: 0.5020\n","\n","Epoch 00988: val_loss did not improve from 0.00000\n","Epoch 989/1000\n"," - 11s - loss: 0.0246 - accuracy: 0.9932 - val_loss: 0.0000e+00 - val_accuracy: 0.5044\n","\n","Epoch 00989: val_loss did not improve from 0.00000\n","Epoch 990/1000\n"," - 11s - loss: 0.0240 - accuracy: 0.9931 - val_loss: 16.7729 - val_accuracy: 0.5041\n","\n","Epoch 00990: val_loss did not improve from 0.00000\n","Epoch 991/1000\n"," - 11s - loss: 0.0239 - accuracy: 0.9934 - val_loss: 7.5999 - val_accuracy: 0.5014\n","\n","Epoch 00991: val_loss did not improve from 0.00000\n","Epoch 992/1000\n"," - 11s - loss: 0.0241 - accuracy: 0.9930 - val_loss: 0.0000e+00 - val_accuracy: 0.5074\n","\n","Epoch 00992: val_loss did not improve from 0.00000\n","Epoch 993/1000\n"," - 11s - loss: 0.0220 - accuracy: 0.9943 - val_loss: 3.1661 - val_accuracy: 0.5071\n","\n","Epoch 00993: val_loss did not improve from 0.00000\n","Epoch 994/1000\n"," - 11s - loss: 0.0223 - accuracy: 0.9937 - val_loss: 1.1241e-04 - val_accuracy: 0.5110\n","\n","Epoch 00994: val_loss did not improve from 0.00000\n","Epoch 995/1000\n"," - 11s - loss: 0.0234 - accuracy: 0.9930 - val_loss: 8.9545 - val_accuracy: 0.5047\n","\n","Epoch 00995: val_loss did not improve from 0.00000\n","Epoch 996/1000\n"," - 11s - loss: 0.0269 - accuracy: 0.9916 - val_loss: 11.8899 - val_accuracy: 0.5125\n","\n","Epoch 00996: val_loss did not improve from 0.00000\n","Epoch 997/1000\n"," - 11s - loss: 0.0249 - accuracy: 0.9925 - val_loss: 33.3875 - val_accuracy: 0.5098\n","\n","Epoch 00997: val_loss did not improve from 0.00000\n","Epoch 998/1000\n"," - 11s - loss: 0.0240 - accuracy: 0.9923 - val_loss: 2.6622 - val_accuracy: 0.5077\n","\n","Epoch 00998: val_loss did not improve from 0.00000\n","Epoch 999/1000\n"," - 11s - loss: 0.0206 - accuracy: 0.9948 - val_loss: 0.3688 - val_accuracy: 0.5140\n","\n","Epoch 00999: val_loss did not improve from 0.00000\n","Epoch 1000/1000\n"," - 11s - loss: 0.0227 - accuracy: 0.9937 - val_loss: 3.2741e-04 - val_accuracy: 0.5110\n","\n","Epoch 01000: val_loss did not improve from 0.00000\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"pfPMGMvm0rGu","executionInfo":{"status":"ok","timestamp":1619015861402,"user_tz":-540,"elapsed":4241,"user":{"displayName":"7th June","photoUrl":"","userId":"08178289703395036410"}}},"source":["ckpt_path = current_path + 'ckpt/'\n","board_path = current_path + 'graph/'\n","\n","# model_name = 'classifier_%s_close_updown_pr_theta_non_shuffle_ex_ma7.h5'\n","# model_name = 'classifier_45_close_updown_pr_theta.h5'\n","\n","model = keras.models.load_model(ckpt_path + model_name)"],"execution_count":13,"outputs":[]},{"cell_type":"code","metadata":{"id":"zhRu2BIK792m","colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1nZmVnRiclNOGlfgtxS6MjsSCEuDtpIwG"},"executionInfo":{"status":"error","timestamp":1619016367805,"user_tz":-540,"elapsed":205589,"user":{"displayName":"7th June","photoUrl":"","userId":"08178289703395036410"}},"outputId":"cf861e25-fb67-4a14-8748-bb5c1318033b"},"source":["test_result = model.predict(x_test)\n","# test_result = model.predict(test_set)\n","\n","print('test_result.shape :', test_result.shape)\n","print('pr_val.shape :', pr_val.shape)\n","\n","y_score = test_result[:, [1]]\n","print('y_test[:5] :', y_test.reshape(-1,)[:5])\n","# print('np.unique(y_test) :', np.unique(y_test, return_counts=True))\n","print('y_score[:5] :', y_score[:5])\n","# print('np.unique(y_score) :', np.unique(y_score, return_counts=True))\n","\n","print('y_test.shape :', y_test.shape)\n","print('y_score.shape :', y_score.shape)\n","\n","print('len(y_test) :', len(y_test))\n","\n","#     precision recall curve   #\n","precision, recall, threshold = precision_recall_curve(y_test, y_score)\n","precision, recall = precision[:-1], recall[:-1]\n","\n","plt.plot(threshold, precision, label='precision')\n","plt.plot(threshold, recall, label='recall')\n","plt.legend()\n","plt.title('precision recall')\n","plt.show()\n","# print(y_pred)\n","\n","\n","# threshold = [0.65]\n","# print('threshold :', threshold)\n","# break\n","new_thresh = []\n","acc_pr_bythr = []\n","for thresh in threshold:\n","\n","  if thresh < 0.55:\n","    continue\n","\n","  y_pred = np.where(y_score[:, -1] > thresh, 1, 0)\n","  print('y_pred.shape :', y_pred.shape)\n","  # print('y_pred :', y_pred)\n","\n","  #     compare precision     #\n","\n","  print('precision :', precision_score(y_test, y_pred))\n","  print('recall :', recall_score(y_test, y_pred))\n","  print()\n","\n","  print('np.isnan(np.sum(x_test)) :', np.isnan(np.sum(x_test)))\n","  print('np.isnan(np.sum(y_test)) :', np.isnan(np.sum(y_test)))\n","\n","  # plot_confusion_matrix(best_model, x_test, y_test, normalize=None)\n","  # plt.show()  \n","  print()\n","\n","  #     check win-ratio improvement     #\n","  cmat = confusion_matrix(y_test, y_pred)\n","  # print(cmat)\n","  # print(np.sum(cmat, axis=1))\n","\n","  test_size = len(y_test)\n","  test_pr_list = pr_test\n","  print('origin ac_pr :', np.cumprod(test_pr_list)[-1])\n","\n","  org_wr = np.sum(cmat, axis=1)[-1] / sum(np.sum(cmat, axis=1))\n","  ml_wr = cmat[1][1] / np.sum(cmat, axis=0)[-1]\n","  print('win ratio improvement %.2f --> %.2f' % (org_wr, ml_wr))\n","\n","  # print('pr_test.shape :', pr_test.shape)\n","\n","  # print(y_pred)\n","  # print(test_pr_list)\n","  pred_pr_list = np.where(y_pred == 1, test_pr_list.reshape(-1, ), 1.0)\n","  # print('pred_pr_list.shape :', pred_pr_list.shape)\n","\n","  if np.cumprod(test_pr_list)[-1] < np.cumprod(pred_pr_list)[-1]:\n","    print('accum_pr increased ! : %.3f --> %.3f' % (np.cumprod(test_pr_list)[-1], np.cumprod(pred_pr_list)[-1]))\n","    print('thresh :', thresh)\n","    \n","  # if len(threshold) == 1:\n","    plt.figure(figsize=(10, 5))\n","    plt.subplot(121)\n","    plt.plot(np.cumprod(test_pr_list))\n","    plt.title('%.3f' % (np.cumprod(test_pr_list)[-1]))\n","  # plt.show()\n","\n","    plt.subplot(122)\n","    plt.plot(np.cumprod(pred_pr_list))\n","    plt.title('%.3f' % (np.cumprod(pred_pr_list)[-1]))\n","    plt.show()\n","\n","\n","  acc_pr_bythr.append(np.cumprod(pred_pr_list)[-1])\n","  new_thresh.append(thresh)\n","\n","print('acc_pr_bythr :', acc_pr_bythr)\n","\n","plt.figure(figsize=(10, 5))\n","plt.subplot(121)\n","plt.plot(threshold, precision, label='precision')\n","plt.plot(threshold, recall, label='recall')\n","plt.legend()\n","plt.title('precision recall')\n","# plt.show()\n","plt.subplot(122)\n","plt.plot(threshold, acc_pr_bythr)\n","plt.axhline(np.cumprod(test_pr_list)[-1], linestyle='--', color='r')\n","plt.show()"],"execution_count":17,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"markdown","metadata":{"id":"-jo3k5MdhFyg"},"source":["#### **clustering output**"]},{"cell_type":"code","metadata":{"id":"njxxm-TJ-RP-"},"source":["# x_train_for_k = test_result.flatten().reshape(-1, 1)\n","x_train_for_k = test_result\n","print(x_train_for_k[:10])\n","# x_train_for_k = test_result[:, [1]]\n","pr_train = pr_test\n","\n","print('x_train_for_k.shape :', x_train_for_k.shape)\n","print('pr_train.shape :', pr_train.shape)\n","\n","K = range(2, 10)\n","s_dist = []\n","sil = []\n","for k in K:\n","  # if cen_data.shape[0] < k:\n","  #   break\n","\n","  km = KMeans(n_clusters=k)\n","  km = km.fit(x_train_for_k)\n","\n","  labels = km.labels_\n","  # print('len(labels) :', len(labels))\n","  # print('labels[:10] :', labels[:10])\n","  sil.append(silhouette_score(x_train_for_k, labels, metric='euclidean'))\n","\n","  # inertia = km.inertia_\n","  # s_dist.append(inertia)\n","\n","best_k = K[np.argmax(np.array(sil))]\n","\n","plt.figure(figsize=(10, 5))\n","plt.plot(K, sil)\n","plt.axvline(best_k, linestyle='--')\n","# plt.plot(K, s_dist)\n","plt.show()\n","\n","\n","\n","\n","\n","#   with best_k, label 별 pr_list 확인\n","km = KMeans(n_clusters=best_k)\n","km = km.fit(x_train_for_k)\n","\n","labels = km.labels_\n","\n","print(km.score(x_train_for_k))\n","print(len(labels), len(pr_train))\n","\n","\n","\n","\n","\n","#   label 별로 profit 을 저장, 승률을 확인한다\n","label_types = np.unique(labels, return_counts=False)\n","\n","label_pr_dict = {}\n","#   init dict   #\n","for label in label_types:\n","  label_pr_dict[label] = []\n","print(label_pr_dict)\n","# break\n","\n","for i, (label, pr) in enumerate(zip(labels, pr_train)):\n","  label_pr_dict[label].append(pr[0])\n","\n","  \n","# for label in label_types:\n","print(label_pr_dict)\n","\n","\n","\n","\n","\n","def win_ratio(list_x):\n","\n","  win_cnt = np.sum(np.array(list_x) > 1)\n","  return win_cnt / len(list_x)\n","\n","\n","def acc_pr(list_x):\n","\n","  return np.cumprod(np.array(list_x))[-1]\n","\n","\n","for key in label_pr_dict:\n","  \n","  print(key, ':', 'win_ratio : %.2f' % (win_ratio(label_pr_dict[key])), 'acc_pr : %.2f' % (acc_pr(label_pr_dict[key])))\n","\n","\n","\n","\n","\n","#     predict test && test 의 라벨에 따른 win_ratio 확인\n","# test_labels = km.predict(x_test)\n","# # print(test_labels)\n","\n","# label_pr_dict = {}\n","# #   init dict   #\n","# for label in label_types:\n","#   label_pr_dict[label] = []\n","# print(label_pr_dict)\n","# # break\n","\n","# for i, (label, pr) in enumerate(zip(test_labels, pr_test)):\n","#   label_pr_dict[label].append(pr[0])\n","\n","# for key in label_pr_dict:\n","\n","#   print(key, ':', 'win_ratio : %.2f' % (win_ratio(label_pr_dict[key])), 'acc_pr : %.2f' % (acc_pr(label_pr_dict[key])))\n","\n"],"execution_count":null,"outputs":[]}]}