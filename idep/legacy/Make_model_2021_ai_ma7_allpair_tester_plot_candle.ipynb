{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python [conda env:tensorflow2_p36]","language":"python","name":"conda-env-tensorflow2_p36-py"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.5"},"colab":{"name":"Make_model_2021_ai_ma7_allpair_tester_plot_candle.ipynb","provenance":[{"file_id":"1z4z_KLPzc6RWsxo3X_dHbpByxUAgjoMJ","timestamp":1583754134002}],"collapsed_sections":[],"machine_shape":"hm"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"AK9FjWwLOyay"},"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","import os, sys\n","\n","current_path = '/content/drive/My Drive/Colab Notebooks/Project_Stock/'\n","\n","os.chdir(current_path)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"V0OVS8AozdDk"},"source":["import pandas as pd\n","print(pd.__version__)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"y7bVjhlwPI_-"},"source":["### **ARIMA**"]},{"cell_type":"code","metadata":{"id":"NvdpArctN_6l"},"source":["from statsmodels.tsa.arima_model import ARIMA\n","from datetime import datetime\n","\n","\n","def arima_test(close, use_rows=None):\n","\n","  size = int(len(close) * 0.66)\n","  train, test = close[0:size].values, close[size:len(close)]\n","  test_shift = test.shift(1).values\n","  test = test.values\n","  # break\n","\n","  history = list(train)\n","  predictions = list()\n","  err_ranges = list()\n","  for t in range(len(test)):\n","    \n","      if use_rows is not None:\n","        history = history[-use_rows:]\n","        \n","      model = ARIMA(history, order=(0, 2, 1))\n","      model_fit = model.fit(trend='c', disp=0)\n","      output = model_fit.forecast()\n","      # print(output)\n","      # break\n","      yhat = output[0]\n","      predictions.append(yhat)\n","      err_ranges.append(output[1])\n","      obs = test[t]\n","      # print('obs :', obs)\n","      history.append(obs)\n","      # break\n","      print('\\r %.2f%%' % (t / len(test) * 100), end='')\n","\n","  print(len(test), len(predictions))\n","\n","  return predictions, err_ranges\n","\n","\n","# print(high)\n","\n","\n","def get_back_result(ohlcv, predictions, err_ranges, tp=0.04, sl=None, leverage=1, show_detail=False, show_plot=False, return_pr=False, cumsum=False, \n","                    close_ver=False, reverse_short=False):\n","\n","  \n","  high, low, test = np.split(ohlcv.values[-len(predictions):, [1, 2, 3]], 3, axis=1)\n","\n","  if close_ver:\n","    predictions = ohlcv['close'].shift(1).values[-len(test):]\n","\n","  fee = 0.0006\n","  long_profits = []\n","  short_profits = []\n","  liquidations = []\n","  win_cnt = 0\n","  for i in range(len(test)):\n","\n","    long_ep = predictions[i]\n","    # long_ep = (predictions[i] - err_ranges[i]) * (1 / (1 + tp))\n","    if sl is not None:\n","      long_sl = long_ep * (1 / (sl + 1))\n","\n","    # assert long_ep < long_exit, 'long_exit < long_ep !, %s, %s' % (long_exit, long_ep)\n","    \n","    short_ep = (predictions[i] + err_ranges[i]) * (1 + tp)\n","    # short_ep = (predictions[i] + err_ranges[i]) * (1 / (1 - tp))\n","    if sl is not None:\n","      short_sl = short_ep * (1 / (1 - sl))\n","\n","    # print((low[i]))\n","\n","    #    long 우선   # <-- long & short 둘다 체결된 상황에서는 long 체결을 우선으로 한다.\n","    if low[i] < long_ep:\n","      \n","      liquidation = low[i] / long_ep - fee\n","      l_liquidation = 1 + (liquidation - 1) * leverage\n","      liquidations.append(l_liquidation)\n","\n","      if max(l_liquidation, 0) == 0:\n","        l_profit = 0\n","        # print('low[i], long_ep, l_liquidation :', low[i], long_ep, l_liquidation)\n","      else:\n","\n","        if sl is not None:\n","          if low[i] < long_sl:\n","            profit = long_sl / long_ep - fee\n","          else:\n","            profit = test[i] / long_ep - fee\n","\n","        else:\n","          profit = test[i] / long_ep - fee\n","\n","        l_profit = 1 + (profit - 1) * leverage\n","        l_profit = max(l_profit, 0)\n","        \n","        if profit >= 1:\n","          win_cnt += 1\n","\n","      long_profits.append(l_profit)\n","      short_profits.append(1.0)\n","\n","      if show_detail:\n","        print(test[i], predictions[i], long_ep)\n","\n","    # if high[i] > short_ep > low[i]: # 지정 대기가 아니라, 해당 price 가 지나면, long 한다.\n","\n","    #   if not reverse_short:\n","    #     liquidation = short_ep / high[i]  - fee\n","    #   else:\n","    #     liquidation = low[i] / short_ep  - fee\n","    #   l_liquidation = 1 + (liquidation - 1) * leverage\n","\n","    #   if max(l_liquidation, 0) == 0:\n","    #     l_profit = 0\n","    #   else:\n","\n","    #     if sl is not None:\n","    #       if high[i] > short_sl:\n","\n","    #         if not reverse_short:\n","    #           profit = short_ep / short_sl - fee\n","    #         else:\n","    #           profit = short_sl / short_ep - fee\n","\n","    #       else:\n","    #         if not reverse_short:\n","    #           profit = short_ep / test[i] - fee\n","    #         else:\n","    #           profit = test[i] / short_ep - fee\n","\n","    #     else:\n","\n","    #       if not reverse_short:\n","    #         profit = short_ep / test[i] - fee\n","    #       else:\n","    #         profit = test[i] / short_ep - fee\n","\n","    #     l_profit = 1 + (profit - 1) * leverage\n","    #     l_profit = max(l_profit, 0)\n","\n","    #     if profit >= 1:\n","    #       win_cnt += 1\n","\n","    #   short_profits.append(l_profit)\n","    #   long_profits.append(1.0)\n","\n","    #   if show_detail:\n","    #     print(test[i], predictions[i], short_ep)\n","    \n","    else:\n","      long_profits.append(1.0)\n","      short_profits.append(1.0)\n","      liquidations.append(1.0)\n","\n","\n","  long_win_ratio = sum(np.array(long_profits) > 1.0) / sum(np.array(long_profits) != 1.0)\n","  short_win_ratio = sum(np.array(short_profits) > 1.0) / sum(np.array(short_profits) != 1.0)\n","  long_frequency = sum(np.array(long_profits) != 1.0) / len(test)\n","  short_frequency = sum(np.array(short_profits) != 1.0) / len(test)\n","  if not cumsum:\n","    long_accum_profit = np.array(long_profits).cumprod()\n","    short_accum_profit = np.array(short_profits).cumprod()\n","  else:\n","    long_accum_profit = (np.array(long_profits) - 1.0).cumsum()\n","    short_accum_profit = (np.array(short_profits) - 1.0).cumsum()\n","\n","  # print(win_ratio)\n","\n","  if show_plot:\n","\n","    plt.figure(figsize=(10, 5))\n","    plt.suptitle('tp=%.4f, lvrg=%d' % (tp, leverage))\n","\n","    plt.subplot(151)\n","    plt.plot(liquidations)\n","    plt.title('liquidations')\n","\n","    plt.subplot(152)\n","    plt.plot(long_profits)\n","    plt.title('Win Ratio : %.2f %%\\nrequency : %.2f %%' % (long_win_ratio * 100, long_frequency * 100), color='black')\n","    # plt.show()\n","\n","    # print()\n","    plt.subplot(153)\n","    plt.plot(long_accum_profit)\n","    plt.title('Accum_profit : %.2f' % long_accum_profit[-1], color='black')\n","\n","    plt.subplot(154)\n","    plt.plot(short_profits)\n","    plt.title('Win Ratio : %.2f %%\\nrequency : %.2f %%' % (short_win_ratio * 100, short_frequency * 100), color='black')\n","    # plt.show()\n","\n","    # print()\n","    plt.subplot(155)\n","    plt.plot(short_accum_profit)\n","    plt.title('Accum_profit : %.2f' % short_accum_profit[-1], color='black')\n","    plt.show()\n","\n","  return [long_win_ratio, short_win_ratio], [long_frequency, short_frequency], [long_accum_profit[-1], short_accum_profit[-1]], [long_profits, short_profits]\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aDkU3tMiM2lO"},"source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","interval = '30m'\n","date_path = './candlestick_concated/%s/2021-02-11/' % interval\n","file_list = os.listdir(date_path)\n","\n","print((file_list))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mw1p_kdaQduk"},"source":["### **Keep connection**"]},{"cell_type":"code","metadata":{"id":"z4omXOkuGKqw"},"source":["while 1 : 1"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0en4ihETQ32K"},"source":["### **Model tester**"]},{"cell_type":"code","metadata":{"id":"SvZuk1rPrUMe"},"source":["from sklearn.preprocessing import MinMaxScaler, MaxAbsScaler\n","import keras\n","from sklearn.metrics import confusion_matrix\n","import pickle\n","\n","\n","with open('./arima_result/arima_ma7_profit_ls_only_long_result2_%s.pickle' % interval, 'rb') as f:\n","  load_dict = pickle.load(f)\n","\n","ckpt_path = current_path + 'ckpt/'\n","\n","model_name = 'classifier_45_ma7_pr3_03766.h5'\n","model = keras.models.load_model(ckpt_path + model_name)\n","\n","long_index = 0\n","leverage = 5\n","thresh = 0.5154\n","\n","candis = list(load_dict.keys())\n","prev_x = None\n","for i in range(len(candis)):\n","\n","  # if i >= 2:\n","  #   break\n","\n","  keys = [candis[i]]\n","  \n","  # if 'algo'.upper() not in candis[i]:\n","  #   continue\n","  if '2021-03-02 DOTUSDT.xlsx' in candis[i]:\n","    # print('')\n","    continue\n","\n","  # if '02-11' not in candis[i]:\n","  # # if '2021-02-11 BTCUSDT.xlsx' not in candis[i]:\n","  #   continue\n","\n","  if '04-08' not in candis[i]:\n","# if '2021-02-11 BTCUSDT.xlsx' not in candis[i]:\n","    continue\n","\n","  # plt.figure(figsize=(35, 10))\n","  # plt.suptitle('%s %s' % (interval, keys))\n","\n","\n","  #         get tp parameter        #\n","\n","  # plt.subplot(1,10,3)\n","  # for key in keys:  \n","  #   # plt.plot(profit_result_dict[key]['tp_list'], profit_result_dict[key]['ap_list'])\n","  #   argmax = np.argmax(profit_result_dict[key]['ap_list'][:, [long_index]])\n","  #   peak_tp = profit_result_dict[key]['tp_list'][argmax]\n","  #   # plt.axvline(peak_tp, linestyle='--')\n","  #   # plt.title('acc profit, max at %.4f' % (peak_tp))  \n","\n","  # plt.subplot(1,10,4)\n","  # plt.title('max acc profit by leverage')  \n","  # for key in keys:  \n","  #   # plt.plot(profit_result_dict[key]['tp_list'], profit_result_dict[key]['max_ap_list'], label=key)\n","  #   argmax = np.argmax(profit_result_dict[key]['max_ap_list'][:, [long_index]])\n","  #   max_peak_tp = profit_result_dict[key]['tp_list'][argmax]\n","  #   # plt.axvline(max_peak_tp, linestyle='--')\n","  #   # plt.title('max acc profit, max at %.4f' % (max_peak_tp))  \n","\n","\n","  for key in keys:  \n","    # print(profit_result_dict[key]['leverage_ap_list'])\n","\n","    # for tp in [max_peak_tp]:\n","\n","      # if tp == peak_tp:\n","      #   plt.subplot(1,10,5)\n","      # else:\n","      #   plt.subplot(1,10,6)\n","\n","      #     leverage analysis     #\n","      ohlcv = load_dict[key]['ohlcv']\n","      print('len(ohlcv) :', len(ohlcv))\n","      # ohlcv = ohlcv.iloc[-int(len(ohlcv) * 0.34):]\n","      predictions = load_dict[key]['predictions']\n","      err_ranges = load_dict[key]['err_ranges']\n","\n","      # predictions = ohlcv['close'].shift(1).values\n","      # err_ranges = np.zeros_like(predictions)\n","\n","      # leverage_list = profit_result_dict[key]['leverage_list']\n","      # temp_ap_list = list()\n","      # temp_pr_list = list()\n","\n","      try:\n","        print('-------------- %s --------------' % key)\n","        result = get_back_result(ohlcv, predictions, err_ranges, tp=0, leverage=leverage, show_plot=True, reverse_short=False, show_detail=False)\n","        # temp_ap_list.append(result[2])\n","        # temp_pr_list.append(result[3])\n","\n","        # if round(leverage) == 1:\n","        #   temp_pr_list = result[3]\n","        pr_list = result[3][long_index]\n","\n","      except Exception as e:\n","        print(e)\n","        break    \n","  # break\n","\n","\n","      pd.set_option('display.max_rows', 500)\n","      pd.set_option('display.max_columns', 500)\n","      pd.set_option('display.width', 1000)\n","\n","      #         clustering zone           #\n","\n","      #       set data features : ohlc, v, ep\n","      ohlc = ohlcv.iloc[-len(predictions):, :4]\n","      vol = ohlcv.iloc[-len(predictions):, [4]]\n","      long_ep = np.array(predictions)\n","      # long_ep = ohlc.iloc[:, [-1]].values <-- 이건 이전 종가가 아니라 그냥 종가임\n","      long_ep = long_ep.reshape(-1, 1)\n","\n","      ohlcv['u_wick'] = ohlcv['high'] / np.maximum(ohlcv['close'] , ohlcv['open'])\n","      ohlcv['d_wick'] = np.minimum(ohlcv['close'] , ohlcv['open']) / ohlcv['low']\n","      ohlcv['body'] = ohlcv['close'] / ohlcv['open']\n","\n","      candle = ohlcv.iloc[-len(predictions):, -3:]\n","\n","\n","      print('len(ohlc) :', len(ohlc))\n","      print('long_ep.shape :', long_ep.shape)\n","      print('len(pr_list) :', len(pr_list))\n","\n","\n","      #       set params    #\n","      period = 45\n","      data_x, data_pr, data_updown = [], [], []\n","      key_i = i\n","\n","      for i in range(period, len(predictions)):\n","\n","        #   pr_list != 1 인 데이터만 사용한다\n","        # if 1:\n","        if pr_list[i] != 1:\n","          \n","          #   prediction 을 제외한 이전 데이터를 사용해야한다\n","          temp_ohlc = ohlc.iloc[i - period : i].values\n","          temp_long_ep = long_ep[i - period : i]\n","          temp_vol = vol.iloc[i - period : i].values\n","          temp_candle = candle.iloc[i - period : i].values\n","\n","          # print(temp_ohlc.shape)\n","          # print(temp_long_ep.shape)\n","          # print(temp_vol.shape)\n","          # print(temp_candle.shape)\n","          # break\n","\n","          #   stacking  \n","          # temp_data = np.hstack((temp_ohlc, temp_long_ep, temp_vol, temp_candle))\n","          temp_data = np.hstack((temp_ohlc, temp_long_ep, temp_vol))\n","          # temp_data = np.hstack((temp_ohlc, temp_vol))\n","\n","          # temp_data = np.hstack((temp_ohlc, temp_long_ep))\n","          # temp_data = temp_vol\n","\n","          #   scaler 설정\n","\n","          #   ohlc & ep -> max_abs\n","          # max_abs = MaxAbsScaler()\n","          # temp_data[:, :5] = max_abs.fit_transform(temp_data[:, :5])\n","\n","\n","          min_max = MinMaxScaler()\n","          temp_data[:, :5] = min_max.fit_transform(temp_data[:, :5])\n","\n","\n","          #   vol -> min_max\n","          min_max = MinMaxScaler()\n","          temp_data[:, [5]] = min_max.fit_transform(temp_data[:, [5]])\n","\n","\n","          #   candle -> max_abs    \n","          # max_abs = MaxAbsScaler()\n","          # temp_data[:, -3:] = max_abs.fit_transform(temp_data[:, -3:])\n","\n","          # min_max = MinMaxScaler()\n","          # temp_data[:, -3:] = min_max.fit_transform(temp_data[:, -3:])\n","\n","          if np.isnan(np.sum(temp_data)):\n","            continue\n","\n","          data_x.append(temp_data)\n","          #     append result data    #\n","          data_pr.append(pr_list[i])\n","          data_updown.append(ohlc['close'].iloc[i] / ohlc['open'].iloc[i])\n","\n","\n","      print('np.array(data_x).shape :', np.array(data_x).shape)\n","      # print(data_x[0])\n","\n","\n","      #       Reshape data for image deep - learning     #\n","      _, row, col = np.array(data_x).shape\n","\n","      input_x = np.array(data_x).reshape(-1, row, col, 1).astype(np.float32)\n","\n","      #     1c to 3c    #\n","      input_x = input_x * np.ones(3, dtype=np.float32)[None, None, None, :]\n","\n","      input_pr = np.array(data_pr).reshape(-1, 1).astype(np.float32)\n","      input_ud = np.array(data_updown).reshape(-1, 1).astype(np.float32)\n","      print('input_x.shape :', input_x.shape)\n","      print('input_x.dtype :', input_x.dtype)\n","      print('input_pr.shape :', input_pr.shape)\n","      print('input_ud.shape :', input_ud.shape)\n","\n","\n","      #         reshape data     #\n","      temp_x = list()\n","      for d_i, data in enumerate(input_x):\n","        # resized_data = cv2.resize(data, (row * 2, col * 2)) --> input image 홰손된다\n","        resized_data = data.repeat(2, axis=0).repeat(2, axis=1)\n","        # cmapped = plt.cm.Set1(resized_data)[:, :, :3]  # Drop Alpha Channel\n","        \n","        # if d_i == 0:\n","        #   plt.imshow(data)\n","        #   plt.show()\n","        #   plt.imshow(resized_data)\n","        #   plt.show()\n","        # print('resized_data.shape :', resized_data.shape)\n","        # break\n","        temp_x.append(resized_data)\n","\n","      re_input_x = np.array(temp_x)\n","      y_test = np.where(input_pr > 1, 1, 0)\n","      # y_test = np.where(input_ud > 1, 1, 0)\n","\n","      #             ai tester phase              #\n","      test_result = model.predict(re_input_x)\n","      \n","      y_score = test_result[:, [1]]\n","      y_pred = np.where(y_score[:, -1] > thresh, 1, 0)\n","\n","\n","      \n","      #         plot result       #\n","      test_size = len(y_test)\n","      test_pr_list = input_pr\n","      print('origin ac_pr :', np.cumprod(test_pr_list)[-1])\n","\n","      \n","      cmat = confusion_matrix(y_test, y_pred)\n","\n","      org_wr = np.sum(cmat, axis=1)[-1] / sum(np.sum(cmat, axis=1))\n","      ml_wr = cmat[1][1] / np.sum(cmat, axis=0)[-1]\n","      print('win ratio improvement %.2f --> %.2f' % (org_wr, ml_wr))\n","\n","      # print('pr_test.shape :', pr_test.shape)\n","\n","      # print(y_pred)\n","      # print(test_pr_list)\n","      pred_pr_list = np.where(y_pred == 1, test_pr_list.reshape(-1, ), 1.0)\n","      # print('pred_pr_list.shape :', pred_pr_list.shape)\n","\n","      if np.cumprod(test_pr_list)[-1] < np.cumprod(pred_pr_list)[-1]:\n","        print('accum_pr increased ! : %.3f --> %.3f' % (np.cumprod(test_pr_list)[-1], np.cumprod(pred_pr_list)[-1]))\n","        print('thresh :', thresh)\n","        \n","      # if len(threshold) == 1:\n","      plt.figure(figsize=(10, 5))\n","\n","      plt.suptitle(key)\n","      plt.subplot(121)\n","      plt.plot(np.cumprod(test_pr_list))\n","      plt.title('%.3f' % (np.cumprod(test_pr_list)[-1]))\n","    # plt.show()\n","\n","      plt.subplot(122)\n","      plt.plot(np.cumprod(pred_pr_list))\n","      plt.title('%.3f' % (np.cumprod(pred_pr_list)[-1]))\n","      plt.show()\n","\n","#     save acc_pr result for comparing pairs --> find best pair   #\n","      #     save improved win_ratio, acc_pr   #\n","      temp_dict = load_dict[key]\n","      temp_dict['improved_wr'] = ml_wr\n","      temp_dict['improved_ap_list'] = np.cumprod(pred_pr_list)\n","      \n","      break\n","  break\n","\n","\n","      #         save dict       #\n","      # with open('./arima_result/arima_ma7_profit_ls_only_long_result_%s.pickle' % interval, 'wb') as f:\n","      #   pickle.dump(load_dict, f)\n","\n","\n","      #     do stacking   #\n","      # if prev_x is None:\n","      #   prev_x = input_x\n","      #   prev_pr = input_pr\n","      #   prev_ud = input_ud\n","      # else:\n","      #   total_x = np.vstack((prev_x, input_x))\n","      #   total_pr = np.vstack((prev_pr, input_pr))\n","      #   total_ud = np.vstack((prev_ud, input_ud))\n","\n","      #   prev_x = total_x\n","      #   prev_pr = total_pr\n","      #   prev_ud = total_ud\n","\n","      #   print('total_x.shape :', total_x.shape)\n","      #   print('total_pr.shape :', total_pr.shape)\n","      #   print('total_ud.shape :', total_ud.shape)\n","        "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"W82NmMIGUU1o"},"source":["!pip install mpl_finance"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bzb9ku2lUKCB"},"source":["import mpl_finance as mf\n","\n","plot_data_x = np.array(data_x)\n","print(\"len(plot_data_x) :\", len(plot_data_x))\n","print(\"len(pred_pr_list) :\", len(pred_pr_list))\n","\n","# print(plot_data_x[12, :, :4])\n","# break\n","    \n","for pred_i in range(len(pred_pr_list)):\n","\n","  # if pred_i == 12:\n","    print('pred_i: ', pred_i)\n","    fig = plt.figure(figsize=(7, 7))\n","    ax = fig.add_subplot(111)\n","    print('plot_data_x.shape :', plot_data_x.shape)\n","    # ohlc = plot_data_x[pred_i, -5:, :4]\n","    ohlc = plot_data_x[pred_i, :, :4]\n","    print('ohlc.shape :', ohlc.shape)\n","    long_ep = plot_data_x[pred_i, :, 4]\n","    # long_ep = plot_data_x[pred_i, -5:, 4]\n","    c_open, c_high, c_low, c_close = np.split(ohlc, 4, axis=1)\n","\n","    # ohlc = df2.iloc[:, :4]\n","    index = np.arange(len(ohlc))\n","    ohlc = np.hstack((np.reshape(index, (-1, 1)), ohlc))\n","    mf.candlestick_ohlc(ax, ohlc, width=0.5, colorup='r', colordown='b')\n","\n","    plt.plot(long_ep)\n","    plt.title(y_pred[pred_i - 1])\n","    plt.show()\n","    # break\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bHs61VjPcW21"},"source":["with open('./arima_result/arima_ma7_profit_ls_only_long_result_%s.pickle' % interval, 'rb') as f:\n","  pair_comp_dict = pickle.load(f)\n","\n","total_pair_list, total_wr_list, total_pr_list = [], [], []\n","\n","keys = list(pair_comp_dict.keys())\n","for key in keys:\n","\n","  if '04-08' in key:\n","    total_pair_list.append(key)\n","    total_wr_list.append(pair_comp_dict[key]['improved_wr'])\n","    total_pr_list.append(pair_comp_dict[key]['improved_ap_list'][-1])\n","    \n","    \n","result_df = pd.DataFrame(index=total_pair_list)\n","\n","\n","# result_df['index'] = pairs\n","result_df['improved_wr'] = total_wr_list\n","result_df['improved_ap_list']= total_pr_list\n","# print(result_df)\n","\n","#     sort by values    #\n","print(result_df.sort_values(by='improved_wr', ascending=False))\n","print(result_df.sort_values(by='improved_ap_list', ascending=False))\n","\n","#     extract candidates    #\n","# candis = result_df.sort_values(by=['max_lv_profit'], ascending=False).index"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GmmgsEUMqUjN"},"source":["### **Model**"]},{"cell_type":"code","metadata":{"id":"mcDUjgQzqUSr"},"source":["import os\n","# os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   \n","# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","\n","%tensorflow_version 1.x\n","\n","import keras\n","import tensorflow as tf\n","\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import scipy.misc \n","from math import sqrt \n","import itertools\n","from IPython.display import display\n","\n","%matplotlib inline\n","\n","from keras.utils import plot_model\n","import keras.backend as K\n","from keras.models import Model, Sequential\n","import keras.layers as layers\n","from keras.optimizers import Adam, SGD\n","from keras.regularizers import l1, l2\n","\n","from sklearn.cluster import KMeans\n","from sklearn.metrics import silhouette_score\n","from sklearn.metrics import precision_score, recall_score, precision_recall_curve\n","from sklearn.metrics import confusion_matrix\n","from sklearn.metrics import plot_confusion_matrix\n","\n","\n","gdrive_path = current_path\n","\n","num_classes = 2\n","\n","def FER_Model(input_shape=(row, col, 3)):\n","    # first input model\n","    visible = layers.Input(shape=input_shape, name='input')\n","    \n","    net = layers.Conv2D(32, kernel_size=3, padding='same', kernel_initializer='he_normal')(visible)\n","    # net = layers.Conv2D(256, kernel_size=3, padding='same', kernel_initializer='he_normal')(visible)\n","    # net = layers.BatchNormalization()(net)\n","    net = layers.LeakyReLU()(net)\n","\n","    net = layers.Conv2D(64, kernel_size=3, padding='same', kernel_initializer='he_normal')(net)\n","    # net = layers.Conv2D(128, kernel_size=3, padding='same', kernel_initializer='he_normal')(net)\n","    # net = layers.BatchNormalization()(net)\n","    # net = layers.Activation('relu')(net)\n","    net = layers.LeakyReLU()(net)\n","    # net = layers.MaxPool2D(pool_size=2)(net)\n","    # net = layers.AveragePooling2D(padding='same')(net)\n","\n","    shortcut_1 = net\n","\n","    # net = layers.Conv2D(64, kernel_size=3, padding='same', kernel_initializer='he_normal')(net)\n","    net = layers.Conv2D(128, kernel_size=3, padding='same', kernel_initializer='he_normal')(net)\n","    net = layers.LeakyReLU()(net)\n","\n","    net = layers.Conv2D(256, kernel_size=3, padding='same', kernel_initializer='he_normal')(net)\n","    # net = layers.BatchNormalization()(net)\n","    # net = layers.Activation('relu')(net)\n","    net = layers.LeakyReLU()(net)\n","    # net = layers.MaxPool2D(pool_size=2)(net)\n","\n","    shortcut_2 = net\n","\n","#     net = layers.Conv2D(256, kernel_size=3, padding='same')(net)\n","#     # net = layers.Activation('relu')(net)\n","#     net = layers.LeakyReLU()(net)\n","#     net = layers.MaxPool2D(pool_size=2)(net)\n","\n","#     shortcut_3 = net\n","\n","#     net = layers.Conv2D(128, kernel_size=1, padding='same')(net)\n","#     # net = layers.Activation('relu')(net)\n","#     net = layers.LeakyReLU()(net)\n","#     net = layers.MaxPool2D(pool_size=2)(net)\n","\n","    net = layers.Flatten()(net)\n","    net = layers.Dense(128)(net)\n","    net = layers.LeakyReLU()(net)\n","\n","    net = layers.Dense(64)(net)\n","    net = layers.LeakyReLU()(net)\n","\n","    net = layers.Dense(num_classes, activation='softmax')(net)\n","\n","    # create model \n","    model = Model(inputs=visible, outputs=net)\n","    # summary layers\n","    # print(model.summary())\n","    \n","    return model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zscZynIgMbAq"},"source":["keras.__version__\n","tf.__version__"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fWUEyjzF21cJ"},"source":["### **Data Split**"]},{"cell_type":"code","metadata":{"scrolled":false,"id":"2iYLNSeSEp7p"},"source":["from sklearn.model_selection import train_test_split\n","from keras.utils import np_utils\n","from keras.preprocessing.image import ImageDataGenerator \n","from sklearn.utils import class_weight\n","# import cv2\n","\n","\n","\n","seed = 1\n","random_state = 20\n","np.random.seed(seed)\n","# tf.random.set_seed(seed)\n","\n","#         resize total_x shape  (, 5, 6, 3)      #\n","# .repeat(2, axis=0).repeat(2, axis=1)\n","temp_x = list()\n","for d_i, data in enumerate(total_x):\n","  # resized_data = cv2.resize(data, (row * 2, col * 2)) --> input image 홰손된다\n","  resized_data = data.repeat(2, axis=0).repeat(2, axis=1)\n","  # cmapped = plt.cm.Set1(resized_data)[:, :, :3]  # Drop Alpha Channel\n","  \n","  if d_i == 0:\n","    plt.imshow(data)\n","    plt.show()\n","    plt.imshow(resized_data)\n","    plt.show()\n","  # print('resized_data.shape :', resized_data.shape)\n","  # break\n","  temp_x.append(resized_data)\n","\n","re_total_x = np.array(temp_x)\n","print('re_total_x.shape :', re_total_x.shape)\n","# break\n","\n","#         train / test split      #\n","x_train, x_test_, pr_train, pr_test_, ud_train, ud_test_ = train_test_split(re_total_x, total_pr, total_ud, test_size=0.4, shuffle=True, random_state=random_state)\n","x_test, x_val, pr_test, pr_val, ud_test, ud_val = train_test_split(x_test_, pr_test_, ud_test_, test_size=0.5, shuffle=True, random_state=random_state)\n","\n","\n","#         pr label   #\n","y_train = np.where(pr_train > 1, 1, 0)\n","y_test = np.where(pr_test > 1, 1, 0)\n","y_val = np.where(pr_val > 1, 1, 0)\n","\n","#         up label      #\n","# y_train = np.where(ud_train > 1, 1, 0)\n","# y_test = np.where(ud_test > 1, 1, 0)\n","# y_val = np.where(ud_val > 1, 1, 0)\n","\n","print('pr_train[:5] :', pr_train[:5])\n","print('ud_train[:5] :', ud_train[:5])\n","print('y_train[:5] :', y_train[:5])\n","print('y_train.dtype :', y_train.dtype)\n","\n","print('x_train.shape :', x_train.shape)\n","print('x_test.shape :', x_test.shape)\n","print('x_val.shape :', x_val.shape)\n","print('y_train.shape :', y_train.shape)\n","print('y_test.shape :', y_test.shape)\n","print('y_val.shape :', y_val.shape)\n","\n","def class_ratio(in_list):\n","\n","  return in_list / in_list[1]\n","\n","print('np.unique(y_train, return_counts=True :', np.unique(y_train, return_counts=True), class_ratio(np.unique(y_train, return_counts=True)[1]))\n","print('np.unique(y_val, return_counts=True :', np.unique(y_val, return_counts=True), class_ratio(np.unique(y_val, return_counts=True)[1]))\n","print('np.unique(y_test, return_counts=True :', np.unique(y_test, return_counts=True), class_ratio(np.unique(y_test, return_counts=True)[1]))\n","\n","label = y_train.reshape(-1, )\n","class_weights = class_weight.compute_class_weight('balanced', \n","                                                    classes=np.unique(label),\n","                                                    y=label)\n","class_weights = dict(enumerate(class_weights))\n","print('class_weights :', class_weights)\n","\n","# sample_weight = np.ones(shape=(len(y_train),))\n","# sample_weight[(y_train == 1).reshape(-1,)] = 1.5\n","# print('sample_weight[:20] :', sample_weight[:20])\n","\n","\n","print('np.isnan(np.sum(x_train)) :', np.isnan(np.sum(x_train)))\n","print('np.isnan(np.sum(x_val)) :', np.isnan(np.sum(x_val)))\n","print('np.isnan(np.sum(x_test)) :', np.isnan(np.sum(x_test)))\n","\n","print('np.isnan(np.sum(y_train)) :', np.isnan(np.sum(y_train)))\n","print('np.isnan(np.sum(y_val)) :', np.isnan(np.sum(y_val)))\n","print('np.isnan(np.sum(y_test)) :', np.isnan(np.sum(y_test)))\n","\n","y_train_ohe = np_utils.to_categorical(y_train, num_classes)\n","y_val_ohe = np_utils.to_categorical(y_val, num_classes)\n","y_test_ohe = np_utils.to_categorical(y_test, num_classes)\n","print('y_train_ohe.shape :', y_train_ohe.shape)\n","print('y_val_ohe.shape :', y_val_ohe.shape)\n","print('y_test_ohe.shape :', y_test_ohe.shape)\n","\n","datagen = ImageDataGenerator( \n","    rotation_range = 45,\n","    # zoom_range = 0.5,\n","    # shear_range = 0.5,\n","    # horizontal_flip = True,\n","    # vertical_flip = True,\n","    # width_shift_range=0.5,\n","    # height_shift_range=0.5,\n","    # fill_mode = 'nearest'\n","    )\n","\n","valgen = ImageDataGenerator( \n","    )\n","\n","datagen.fit(x_train)\n","valgen.fit(x_val)\n","\n","batch_size = 16\n","\n","for x_batch, _ in datagen.flow(x_train, y_train_ohe, batch_size=9):\n","\n","    plt.suptitle(\"train x_batch\")\n","\n","    for i in range(0, 9): \n","        plt.subplot(330 + 1 + i) \n","        # resized = cv2.resize(x_batch[i].reshape(row, col), (row * 2, col * 10))\n","        # cmapped = plt.cm.Set1(resized)\n","        # plt.imshow(cmapped)\n","        # plt.imshow(x_batch[i].reshape(row, col))\n","        plt.imshow(x_batch[i])\n","        plt.axis('off') \n","    plt.show() \n","    break\n","\n","for x_batch, _ in valgen.flow(x_val, y_val_ohe, batch_size=9):\n","\n","    plt.suptitle(\"val x_batch\")\n","\n","    for i in range(0, 9): \n","        plt.subplot(330 + 1 + i) \n","        # resized = cv2.resize(x_batch[i].reshape(row, col), (row * 2, col * 10))\n","        # cmapped = plt.cm.Set1(resized)\n","        # plt.imshow(cmapped)\n","        # plt.imshow(x_batch[i].reshape(row, col))\n","        plt.imshow(x_batch[i])\n","        plt.axis('off') \n","    plt.show() \n","    break\n","    \n","train_flow = datagen.flow(x_train, y_train_ohe, batch_size=batch_size) \n","val_flow = valgen.flow(x_val, y_val_ohe, batch_size=batch_size) \n","# break\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"s-W6LL5c2VN2"},"source":["### **Training**"]},{"cell_type":"code","metadata":{"id":"EkVg1hVI2TNP"},"source":["(_, row, col, _) = x_train.shape\n","\n","model = FER_Model(input_shape=(row, col, 3))\n","opt = Adam(lr=0.00001, decay=0.000005)\n","model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n","  \n","from keras.callbacks import ModelCheckpoint, EarlyStopping, TensorBoard\n","\n","ckpt_path = current_path + 'ckpt/'\n","board_path = current_path + 'graph/'\n","model_name = 'classifier_%s_min_pr_re2.h5' % period\n","\n","checkpoint = ModelCheckpoint(ckpt_path + model_name, monitor='val_loss', verbose=1, save_best_only=True, mode='auto')\n","checkpoint2 = TensorBoard(log_dir=board_path,\n","                          histogram_freq=0,\n","                          write_graph=True,\n","                          write_images=True)\n","checkpoint3 = EarlyStopping(monitor='val_loss', patience=30)\n","callbacks_list = [checkpoint, checkpoint2, checkpoint3]\n","\n","# keras.callbacks.Callback 로 부터 log 를 받아와 history log 를 작성할 수 있다.\n","\n","# we iterate 200 times over the entire training set\n","num_epochs = 300\n","history = model.fit_generator(train_flow, \n","                    steps_per_epoch=len(x_train) / batch_size, \n","                    epochs=num_epochs,  \n","                    verbose=2,  \n","                    callbacks=callbacks_list,\n","                    class_weight=class_weights,\n","                    validation_data=val_flow,  \n","                    validation_steps=len(x_val) / batch_size,\n","                    shuffle=False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pfPMGMvm0rGu"},"source":["model_name = \"classifier_45_min_pr_re2.h5\"  # <-- specifying model name\n","model = keras.models.load_model(ckpt_path + model_name)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zhRu2BIK792m"},"source":["test_result = model.predict(x_test)\n","# test_result = model.predict(test_set)\n","\n","print('test_result.shape :', test_result.shape)\n","print('pr_val.shape :', pr_val.shape)\n","\n","y_score = test_result[:, [1]]\n","\n","print('y_test[:5] :', y_test.reshape(-1,)[:5])\n","# print('np.unique(y_test) :', np.unique(y_test, return_counts=True))\n","print('y_score[:5] :', y_score[:5])\n","# print('np.unique(y_score) :', np.unique(y_score, return_counts=True))\n","\n","print('y_test.shape :', y_test.shape)\n","print('y_score.shape :', y_score.shape)\n","\n","print('len(y_test) :', len(y_test))\n","\n","#     precision recall curve   #\n","precision, recall, threshold = precision_recall_curve(y_test, y_score)\n","precision, recall = precision[:-1], recall[:-1]\n","\n","plt.plot(threshold, precision, label='precision')\n","plt.plot(threshold, recall, label='recall')\n","plt.legend()\n","plt.title('precision recall')\n","plt.show()\n","# print(y_pred)\n","\n","# thresh = 0.19\n","# threshold = [thresh]\n","print('threshold :', threshold)\n","\n","acc_pr_bythr = []\n","for thresh in threshold:\n","\n","  y_pred = np.where(y_score[:, -1] > thresh, 1, 0)\n","  print('y_pred.shape :', y_pred.shape)\n","  # print('y_pred :', y_pred)\n","\n","  #     compare precision     #\n","\n","  print('precision :', precision_score(y_test, y_pred))\n","  print('recall :', recall_score(y_test, y_pred))\n","  print()\n","\n","  print('np.isnan(np.sum(x_test)) :', np.isnan(np.sum(x_test)))\n","  print('np.isnan(np.sum(y_test)) :', np.isnan(np.sum(y_test)))\n","\n","  # plot_confusion_matrix(best_model, x_test, y_test, normalize=None)\n","  # plt.show()  \n","  print()\n","\n","  #     check win-ratio improvement     #\n","  cmat = confusion_matrix(y_test, y_pred)\n","  # print(cmat)\n","  # print(np.sum(cmat, axis=1))\n","\n","  test_size = len(y_test)\n","  test_pr_list = pr_test\n","  print('origin ac_pr :', np.cumprod(test_pr_list)[-1])\n","\n","  org_wr = np.sum(cmat, axis=1)[-1] / sum(np.sum(cmat, axis=1))\n","  ml_wr = cmat[1][1] / np.sum(cmat, axis=0)[-1]\n","  print('win ratio improvement %.2f --> %.2f' % (org_wr, ml_wr))\n","\n","  # print('pr_test.shape :', pr_test.shape)\n","\n","  # print(y_pred)\n","  # print(test_pr_list)\n","  pred_pr_list = np.where(y_pred == 1, test_pr_list.reshape(-1, ), 1.0)\n","  # print('pred_pr_list.shape :', pred_pr_list.shape)\n","\n","  if np.cumprod(test_pr_list)[-1] < np.cumprod(pred_pr_list)[-1]:\n","    print('accum_pr increased ! : %.3f --> %.3f' % (np.cumprod(test_pr_list)[-1], np.cumprod(pred_pr_list)[-1]))\n","    print('thresh :', thresh)\n","    \n","  # if len(threshold) == 1:\n","    plt.figure(figsize=(10, 5))\n","    plt.subplot(121)\n","    plt.plot(np.cumprod(test_pr_list))\n","    plt.title('%.3f' % (np.cumprod(test_pr_list)[-1]))\n","  # plt.show()\n","\n","    plt.subplot(122)\n","    plt.plot(np.cumprod(pred_pr_list))\n","    plt.title('%.3f' % (np.cumprod(pred_pr_list)[-1]))\n","    plt.show()\n","\n","\n","  acc_pr_bythr.append(np.cumprod(pred_pr_list)[-1])\n","\n","print('acc_pr_bythr :', acc_pr_bythr)\n","\n","plt.figure(figsize=(10, 5))\n","plt.subplot(121)\n","plt.plot(threshold, precision, label='precision')\n","plt.plot(threshold, recall, label='recall')\n","plt.legend()\n","plt.title('precision recall')\n","# plt.show()\n","plt.subplot(122)\n","plt.plot(threshold, acc_pr_bythr)\n","plt.axhline(np.cumprod(test_pr_list)[-1], linestyle='--', color='r')\n","plt.axvline(threshold[np.argmax(acc_pr_bythr)], linestyle='--', color='b')\n","plt.title('best thr : %.4f' % threshold[np.argmax(acc_pr_bythr)])\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"t354bLLO8CL6"},"source":["\n","plt.subplot(122)\n","plt.plot(threshold, acc_pr_bythr)\n","plt.axhline(np.cumprod(test_pr_list)[-1], linestyle='--', color='r')\n","plt.axvline(threshold[np.argmax(acc_pr_bythr)], linestyle='--', color='b')\n","plt.title('best thr : %.4f' % threshold[np.argmax(acc_pr_bythr)])\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-jo3k5MdhFyg"},"source":["#### **clustering output**"]},{"cell_type":"code","metadata":{"id":"njxxm-TJ-RP-"},"source":["# x_train_for_k = test_result.flatten().reshape(-1, 1)\n","x_train_for_k = test_result\n","print(x_train_for_k[:10])\n","# x_train_for_k = test_result[:, [1]]\n","pr_train = pr_test\n","\n","print('x_train_for_k.shape :', x_train_for_k.shape)\n","print('pr_train.shape :', pr_train.shape)\n","\n","K = range(2, 10)\n","s_dist = []\n","sil = []\n","for k in K:\n","  # if cen_data.shape[0] < k:\n","  #   break\n","\n","  km = KMeans(n_clusters=k)\n","  km = km.fit(x_train_for_k)\n","\n","  labels = km.labels_\n","  # print('len(labels) :', len(labels))\n","  # print('labels[:10] :', labels[:10])\n","  sil.append(silhouette_score(x_train_for_k, labels, metric='euclidean'))\n","\n","  # inertia = km.inertia_\n","  # s_dist.append(inertia)\n","\n","best_k = K[np.argmax(np.array(sil))]\n","\n","plt.figure(figsize=(10, 5))\n","plt.plot(K, sil)\n","plt.axvline(best_k, linestyle='--')\n","# plt.plot(K, s_dist)\n","plt.show()\n","\n","\n","\n","\n","\n","#   with best_k, label 별 pr_list 확인\n","km = KMeans(n_clusters=best_k)\n","km = km.fit(x_train_for_k)\n","\n","labels = km.labels_\n","\n","print(km.score(x_train_for_k))\n","print(len(labels), len(pr_train))\n","\n","\n","\n","\n","\n","#   label 별로 profit 을 저장, 승률을 확인한다\n","label_types = np.unique(labels, return_counts=False)\n","\n","label_pr_dict = {}\n","#   init dict   #\n","for label in label_types:\n","  label_pr_dict[label] = []\n","print(label_pr_dict)\n","# break\n","\n","for i, (label, pr) in enumerate(zip(labels, pr_train)):\n","  label_pr_dict[label].append(pr[0])\n","\n","  \n","# for label in label_types:\n","print(label_pr_dict)\n","\n","\n","\n","\n","\n","def win_ratio(list_x):\n","\n","  win_cnt = np.sum(np.array(list_x) > 1)\n","  return win_cnt / len(list_x)\n","\n","\n","def acc_pr(list_x):\n","\n","  return np.cumprod(np.array(list_x))[-1]\n","\n","\n","for key in label_pr_dict:\n","  \n","  print(key, ':', 'win_ratio : %.2f' % (win_ratio(label_pr_dict[key])), 'acc_pr : %.2f' % (acc_pr(label_pr_dict[key])))\n","\n","\n","\n","\n","\n","#     predict test && test 의 라벨에 따른 win_ratio 확인\n","# test_labels = km.predict(x_test)\n","# # print(test_labels)\n","\n","# label_pr_dict = {}\n","# #   init dict   #\n","# for label in label_types:\n","#   label_pr_dict[label] = []\n","# print(label_pr_dict)\n","# # break\n","\n","# for i, (label, pr) in enumerate(zip(test_labels, pr_test)):\n","#   label_pr_dict[label].append(pr[0])\n","\n","# for key in label_pr_dict:\n","\n","#   print(key, ':', 'win_ratio : %.2f' % (win_ratio(label_pr_dict[key])), 'acc_pr : %.2f' % (acc_pr(label_pr_dict[key])))\n","\n"],"execution_count":null,"outputs":[]}]}