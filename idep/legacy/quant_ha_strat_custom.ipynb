{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python [conda env:tensorflow2_p36]","language":"python","name":"conda-env-tensorflow2_p36-py"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.5"},"colab":{"name":"quant_ha_strat_custom.ipynb","provenance":[{"file_id":"1z4z_KLPzc6RWsxo3X_dHbpByxUAgjoMJ","timestamp":1583754134002}],"collapsed_sections":[],"machine_shape":"hm"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"AK9FjWwLOyay"},"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","import os, sys\n","\n","current_path = '/content/drive/My Drive/Colab Notebooks/300/'\n","\n","os.chdir(current_path)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8uqYv5StTazo"},"source":["### **Requirements**"]},{"cell_type":"code","metadata":{"id":"9qGt60DKTZmf"},"source":["# !pip install statsmodels==0.12.2\n","\n","# import statsmodels\n","# statsmodels.__version__"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"y7bVjhlwPI_-"},"source":["### **ARIMA**"]},{"cell_type":"code","metadata":{"id":"NvdpArctN_6l"},"source":["from statsmodels.tsa.arima_model import ARIMA\n","# from statsmodels.tsa.arima.model import ARIMA\n","\n","from datetime import datetime\n","from funcs_indicator import *\n","\n","\n","\n","def arima_close(target, use_rows=None):\n","\n","  size = int(len(target) * 0.66)\n","  train, test = target[:size].values, target[size:]\n","  test_shift = test.shift(1).values\n","  test = test.values\n","  # break\n","\n","  history = list(train)\n","  predictions = list()\n","  err_ranges = list()\n","  for t in range(len(test)):\n","    \n","      if use_rows is not None:\n","        history = history[-use_rows:]\n","        \n","      model = ARIMA(history, order=(0, 2, 4))\n","      model_fit = model.fit()\n","      output = model_fit.forecast()\n","      # print(output)\n","      # break\n","\n","      predictions.append(output[0])\n","      err_ranges.append(output[1])\n","      obs = test[t]\n","      # print('obs :', obs)\n","      history.append(obs)\n","      # break\n","      print('\\r %.2f%%' % (t / len(test) * 100), end='')\n","\n","  print(len(test), len(predictions))\n","\n","  return predictions, err_ranges\n","\n","\n","# print(high)\n","\n","\n","def get_back_result(ohlcv, predictions, err_ranges, indicator, tp=0.04, sl=None, leverage=1, show_detail=False, show_plot=False, return_pr=False, cumsum=False, \n","                    close_ver=False, reverse_short=False, show_chart=False):\n","\n","  \n","  high, low, close = np.split(ohlcv.values[-len(predictions):, [1, 2, 3]], 3, axis=1)\n","  indicator = indicator.values\n","\n","  if close_ver:\n","    predictions = ohlcv['close'].shift(1).values[-len(close):]\n","\n","  fee = 0.0006\n","  long_profits = []\n","  short_profits = []\n","  liquidations = []\n","  win_cnt = 0\n","  for i in range(len(close)):\n","\n","    long_ep = predictions[i]\n","    if sl is not None:\n","      long_sl = long_ep * (1 / (sl + 1))\n","\n","    # assert long_ep < long_exit, 'long_exit < long_ep !, %s, %s' % (long_exit, long_ep)\n","    \n","    short_ep = (predictions[i] + err_ranges[i]) * (1 + tp)\n","    # short_ep = (predictions[i] + err_ranges[i]) * (1 / (1 - tp))\n","    if sl is not None:\n","      short_sl = short_ep * (1 / (1 - sl))\n","\n","    # print((low[i]))\n","\n","    #    long 우선   # <-- long & short 둘다 체결된 상황에서는 long 체결을 우선으로 한다.\n","    #         tick range 내에 ep 가 포함되어야함      #\n","    # if low[i] < long_ep:\n","\n","    #       new constraint    #\n","    # if indicator[i - 1] < close[i - 1]:   # sar, ema1\n","    if indicator[i - 2] < 50 and indicator[i - 1] > 50:   # stoch_d\n","      pass\n","    else:\n","      long_profits.append(1.0)\n","      short_profits.append(1.0)\n","      liquidations.append(1.0)\n","      continue\n","\n","    if low[i] < long_ep < high[i]:\n","      \n","      liquidation = low[i] / long_ep - fee\n","      l_liquidation = 1 + (liquidation - 1) * leverage\n","      liquidations.append(l_liquidation)\n","\n","      if max(l_liquidation, 0) == 0:\n","        l_profit = 0\n","        # print('low[i], long_ep, l_liquidation :', low[i], long_ep, l_liquidation)\n","      else:\n","\n","        if sl is not None:\n","          if low[i] < long_sl:\n","            profit = long_sl / long_ep - fee\n","          else:\n","            profit = close[i] / long_ep - fee\n","\n","        else:\n","          profit = close[i] / long_ep - fee\n","\n","        l_profit = 1 + (profit - 1) * leverage\n","        l_profit = max(l_profit, 0)\n","        \n","        if profit >= 1:\n","          win_cnt += 1\n","\n","      long_profits.append(l_profit)\n","      short_profits.append(1.0)\n","\n","      if show_detail:\n","        print(close[i], predictions[i], long_ep)\n","\n","      if show_chart:\n","        if i > 10:\n","          plt.plot(close[i + 1 - 10: i + 1], label='close')\n","          # plt.plot(predictions[i + 1 - 10: i + 1], label='ep')\n","          # plt.plot(high[i + 1 - 10: i + 1], label='high')\n","          # plt.plot(low[i + 1 - 10: i + 1], label='low')\n","          plt.plot(indicator[i + 1 - 10: i + 1], label='indicator')\n","          plt.legend()\n","          plt.show()\n","\n","    # if high[i] > short_ep > low[i]: # 지정 대기가 아니라, 해당 price 가 지나면, long 한다.\n","\n","    #   if not reverse_short:\n","    #     liquidation = short_ep / high[i]  - fee\n","    #   else:\n","    #     liquidation = low[i] / short_ep  - fee\n","    #   l_liquidation = 1 + (liquidation - 1) * leverage\n","\n","    #   if max(l_liquidation, 0) == 0:\n","    #     l_profit = 0\n","    #   else:\n","\n","    #     if sl is not None:\n","    #       if high[i] > short_sl:\n","\n","    #         if not reverse_short:\n","    #           profit = short_ep / short_sl - fee\n","    #         else:\n","    #           profit = short_sl / short_ep - fee\n","\n","    #       else:\n","    #         if not reverse_short:\n","    #           profit = short_ep / close[i] - fee\n","    #         else:\n","    #           profit = close[i] / short_ep - fee\n","\n","    #     else:\n","\n","    #       if not reverse_short:\n","    #         profit = short_ep / close[i] - fee\n","    #       else:\n","    #         profit = close[i] / short_ep - fee\n","\n","    #     l_profit = 1 + (profit - 1) * leverage\n","    #     l_profit = max(l_profit, 0)\n","\n","    #     if profit >= 1:\n","    #       win_cnt += 1\n","\n","    #   short_profits.append(l_profit)\n","    #   long_profits.append(1.0)\n","\n","    #   if show_detail:\n","    #     print(close[i], predictions[i], short_ep)\n","    \n","    else:\n","      long_profits.append(1.0)\n","      short_profits.append(1.0)\n","      liquidations.append(1.0)\n","\n","\n","  long_win_ratio = sum(np.array(long_profits) > 1.0) / sum(np.array(long_profits) != 1.0)\n","  short_win_ratio = sum(np.array(short_profits) > 1.0) / sum(np.array(short_profits) != 1.0)\n","  long_frequency = sum(np.array(long_profits) != 1.0) / len(close)\n","  short_frequency = sum(np.array(short_profits) != 1.0) / len(close)\n","  if not cumsum:\n","    long_accum_profit = np.array(long_profits).cumprod()\n","    short_accum_profit = np.array(short_profits).cumprod()\n","  else:\n","    long_accum_profit = (np.array(long_profits) - 1.0).cumsum()\n","    short_accum_profit = (np.array(short_profits) - 1.0).cumsum()\n","\n","  # print(win_ratio)\n","\n","  if show_plot:\n","\n","    plt.figure(figsize=(10, 5))\n","    plt.suptitle('tp=%.4f, lvrg=%d' % (tp, leverage))\n","\n","    plt.subplot(151)\n","    plt.plot(liquidations)\n","    plt.title('liquidations')\n","\n","    plt.subplot(152)\n","    plt.plot(long_profits)\n","    plt.title('Win Ratio : %.2f %%\\nrequency : %.2f %%' % (long_win_ratio * 100, long_frequency * 100), color='black')\n","    # plt.show()\n","\n","    # print()\n","    plt.subplot(153)\n","    plt.plot(long_accum_profit)\n","    plt.title('Accum_profit : %.2f' % long_accum_profit[-1], color='black')\n","\n","    plt.subplot(154)\n","    plt.plot(short_profits)\n","    plt.title('Win Ratio : %.2f %%\\nrequency : %.2f %%' % (short_win_ratio * 100, short_frequency * 100), color='black')\n","    # plt.show()\n","\n","    # print()\n","    plt.subplot(155)\n","    plt.plot(short_accum_profit)\n","    plt.title('Accum_profit : %.2f' % short_accum_profit[-1], color='black')\n","    plt.show()\n","\n","  return [long_win_ratio, short_win_ratio], [long_frequency, short_frequency], [long_accum_profit[-1], short_accum_profit[-1]], [long_profits, short_profits]\n","\n","\n","# get_back_result(tp=0.04, leverage=1, show_plot=True)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aDkU3tMiM2lO"},"source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","interval = '30m'\n","date_path = './candlestick_concated/%s/2021-04-27/' % interval\n","file_list = os.listdir(date_path)\n","\n","print((file_list))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GmmgsEUMqUjN"},"source":["### **Model**"]},{"cell_type":"code","metadata":{"id":"SvZuk1rPrUMe"},"source":["from sklearn.preprocessing import MinMaxScaler, MaxAbsScaler\n","import pickle\n","from sklearn.model_selection import train_test_split\n","from datetime import datetime\n","\n","from sklearn.model_selection import train_test_split\n","from keras.utils import np_utils\n","from keras.preprocessing.image import ImageDataGenerator \n","from sklearn.utils import class_weight\n","\n","from keras.callbacks import ModelCheckpoint, EarlyStopping, TensorBoard\n","\n","from funcs_indicator import *\n","\n","pd.set_option('display.max_rows', 500)\n","pd.set_option('display.max_columns', 500)\n","pd.set_option('display.width', 1000)\n","\n","\n","start_stamp = 0\n","# start_stamp = datetime.timestamp(pd.to_datetime('2021-02-12'))\n","print(\"start_stamp :\", start_stamp)\n","# break\n","\n","np.random.shuffle(file_list)\n","candis = file_list\n","\n","long_index = 0\n","leverage = 5\n","prev_x = None\n","total_x = None\n","\n","seed = 1\n","random_state = 201\n","np.random.seed(seed)\n","\n","result_df = pd.DataFrame(index=candis)\n","pairs_pr, pairs_wr = [], []\n","\n","for i in range(len(candis)):\n","\n","  keys = [candis[i]]\n","  \n","  # if 'algo'.upper() not in candis[i]:\n","  #   continue\n","\n","\n","  # if '02-11' not in candis[i]:  # <-- 04-08 includes all timestamp range\n","  #   continue  \n","\n","  if 'eth'.upper() not in candis[i]:\n","    continue\n","\n","  # plt.figure(figsize=(35, 10))\n","  # plt.suptitle('%s %s' % (interval, keys))\n","\n","\n","  #         get tp parameter        #\n","\n","  # plt.subplot(1,10,3)\n","  # for key in keys:  \n","  #   # plt.plot(profit_result_dict[key]['tp_list'], profit_result_dict[key]['ap_list'])\n","  #   argmax = np.argmax(profit_result_dict[key]['ap_list'][:, [long_index]])\n","  #   peak_tp = profit_result_dict[key]['tp_list'][argmax]\n","  #   # plt.axvline(peak_tp, linestyle='--')\n","  #   # plt.title('acc profit, max at %.4f' % (peak_tp))  \n","\n","  # plt.subplot(1,10,4)\n","  # plt.title('max acc profit by leverage')  \n","  # for key in keys:  \n","  #   # plt.plot(profit_result_dict[key]['tp_list'], profit_result_dict[key]['max_ap_list'], label=key)\n","  #   argmax = np.argmax(profit_result_dict[key]['max_ap_list'][:, [long_index]])\n","  #   max_peak_tp = profit_result_dict[key]['tp_list'][argmax]\n","  #   # plt.axvline(max_peak_tp, linestyle='--')\n","  #   # plt.title('max acc profit, max at %.4f' % (max_peak_tp))  \n","\n","\n","  for key in keys:  \n","\n","    # print(profit_result_dict[key]['leverage_ap_list'])\n","\n","    # for tp in [max_peak_tp]:\n","\n","      # if tp == peak_tp:\n","      #   plt.subplot(1,10,5)\n","      # else:\n","      #   plt.subplot(1,10,6)\n","\n","      #     leverage analysis     #\n","      # ohlcv = load_dict[key]['ohlcv']\n","\n","    # if 'eth'.upper() not in key:\n","    #   continue\n","    \n","    ohlcv = pd.read_excel(date_path + key, index_col=0)\n","    print('len(ohlcv) :', len(ohlcv))\n","\n","\n","    #       select timestamp range      #\n","    # time_index = ohlcv.index\n","    # total_stamp = list(map(lambda x: datetime.timestamp(x), time_index)) \n","\n","    # rm_index_amt = np.sum(np.array(total_stamp) < start_stamp)\n","\n","    # ohlcv = ohlcv.iloc[rm_index_amt:]\n","    # print(ohlcv.head())\n","\n","    # ohlcv = ohlcv.iloc[:-int(len(ohlcv) * 0.3)]  # exclude back_range\n","    # predictions = load_dict[key]['predictions']\n","    # err_ranges = load_dict[key]['err_ranges']\n","    print(\"ohlcv.index[0] :\", ohlcv.index[0])\n","    print(\"ohlcv.index[-1] :\", ohlcv.index[-1])\n","\n","    predictions = ohlcv['close'].shift(1).values\n","    err_ranges = np.zeros_like(predictions)\n","    \n","    #         clustering zone           #\n","\n","    #       set data features : ohlc, v, ep\n","    time_index = ohlcv.index[-len(predictions):]\n","\n","    sliced_ohlcv = ohlcv[-len(predictions):]\n","\n","    #       scale with price    #\n","    ohlc = ohlcv.iloc[-len(predictions):, :4]     \n","\n","    ha_ohlc = heikinashi(sliced_ohlcv).iloc[:, :4]\n","    # sar = lucid_sar(sliced_ohlcv)\n","    # stoch_d = stoch(sliced_ohlcv)\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ORt5nkSoeHCx"},"source":["positions = np.full_like(predictions, np.nan)\n","highvslow = np.zeros_like(predictions)\n","total_pr = np.ones_like(predictions)\n","\n","\n","\n","#                       strategy part                       #\n","for i in range(1, len(ha_ohlc)):\n","\n","  if ha_ohlc['close'].iloc[i] > ha_ohlc['close'].iloc[i - 1]:\n","    highvslow[i] = 1\n","  else:\n","    highvslow[i] = -1\n","\n","\n","\n","#               upward              #\n","for i in range(3, len(ha_ohlc)):  # <-- for 문을 돌면서, positions 가 overwrite 되지는 않을까..? / overwrite 을 하는게 맞을 수도\n","\n","  #   앞에 연속된 3 개의 highvslow == -1 가 overwrite 을 막는다고 보는데\n","  \n","  if highvslow[i - 1] == -1 and  highvslow[i - 2] == -1 and  \\\n","    highvslow[i - 3] == -1 and  highvslow[i] == 1:\n","\n","    for j in range(i, len(ha_ohlc)):\n","\n","      if highvslow[j] == -1:\n","        positions[j] = 0\n","        break\n","\n","      else:\n","        positions[j] = 1\n","        break\n","\n","  else:\n","    pass # positions(i,1)= positions(i,1);\n","\n","\n","\n","#               downward, positions values can be overwritten             #\n","for i in range(3, len(ha_ohlc)):\n","\n","  if highvslow[i - 1] == 1 and  highvslow[i - 2] == 1 and  \\\n","    highvslow[i - 3] == 1 and  highvslow[i] == -1:\n","\n","    for j in range(i, len(ha_ohlc)):\n","\n","      if highvslow[j] == 1:\n","        positions[j] = 0\n","        break\n","\n","      else:\n","        positions[j] = -1 \n","        break\n","\n","  else:\n","    pass # positions(i,1)= positions(i,1);\n","\n","#               buy              #\n","# for i in range(3, len(ha_ohlc)):\n","  \n","#   if highvslow[i - 1] == -1 and  highvslow[i - 2] == -1 and  \\\n","#     highvslow[i - 3] == -1 and  highvslow[i] == 1:\n","\n","#     for j in range(i, len(ha_ohlc)):\n","\n","#       if highvslow[j] == -1:\n","#         positions[j] = 0\n","#         break\n","\n","#       else:\n","#         positions[j] = 1\n","\n","#   else:\n","#     pass # positions(i,1)= positions(i,1);\n","\n","#     1. positions 는 np.nan 으로 초기화됨    \n","#     2. 현재 ha 종가는 이전 ha 종가보다 높지 않고, 현재 이전으로 3 개의 연속된 ha 종가 \"상향 갱신\"이 선행되면\n","#     3. 진입하고 보유한다 (positions = -1)\n","#     4. ha 종가가 상향 갱신되면, positions = 0 하고 나온다\n","#     5. 다음 trigger condition 까지 positions = np.nan\n","#     6. 결과적으로, 아래의 trade logic 은 --> sell 조건의 종가에 매수 + ha 종가 상향 갱신 다음 틱의 종가에 매도\n","\n","\n","\n","#                   이 파트를 이해해보자                  #\n","for i in range(len(positions)):\n","\n","  # if positions[i] == -1:\n","  if positions[i] == 1:\n","\n","    g = i # positions 이 -1 이면, 진입한다 (position on, positions 를 -1 로 만든 종가에 진입)\n","\n","    for n in range(i, len(positions)):\n","\n","      # if positions[n] == 0: # --> customized strategy : we exit on posotion[n] == 0\n","      \n","      if np.isnan(positions[n]): # positions 이 np.nan 이면, positions 을 내려놓는다 (내려놓은 종가에 팜 = 종가가 완성되자마자 팜)\n","        h = n\n","        break\n","\n","      else:\n","        pass\n","    \n","    #     positions 를 -1 과 np.nan 으로 만드는 trigger 는 ?    #\n","\n","    pr = ohlc['close'].iloc[h] / ohlc['close'].iloc[g] - 0.0006\n","    # pr = ohlc['close'].iloc[g] / ohlc['close'].iloc[h] - 0.0006\n","    total_pr[h] = 1 + (pr - 1) * 5\n","\n","win_ratio = np.sum(total_pr > 1) / np.sum(total_pr != 1) * 100\n","\n","plt.plot(np.cumprod(total_pr))\n","plt.title(\"%s\\nacc_pr : %s\\nwinratio : %.2f\" % (key, np.cumprod(total_pr)[-1], win_ratio))\n","plt.show()\n","\n","pairs_pr.append(np.cumprod(total_pr)[-1])\n","pairs_wr.append(win_ratio)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dJOrX45MHN4n"},"source":["### short"]},{"cell_type":"code","metadata":{"id":"azqkmFuCHPSR"},"source":["positions = np.full_like(predictions, np.nan)\n","highvslow = np.zeros_like(predictions)\n","total_pr = np.ones_like(predictions)\n","\n","\n","\n","#                       strategy part                       #\n","for i in range(1, len(ha_ohlc)):\n","\n","  if ha_ohlc['close'].iloc[i] > ha_ohlc['close'].iloc[i - 1]:\n","    highvslow[i] = 1\n","  else:\n","    highvslow[i] = -1\n","\n","\n","# #               buy              #\n","for i in range(3, len(ha_ohlc)):\n","  \n","  if highvslow[i - 1] == -1 and  highvslow[i - 2] == -1 and  \\\n","    highvslow[i - 3] == -1 and  highvslow[i] == 1:\n","\n","    for j in range(i, len(ha_ohlc)):\n","\n","      if highvslow[j] == -1:\n","        positions[j] = 0\n","        break\n","\n","      else:\n","        positions[j] = 1\n","\n","  else:\n","    pass # positions(i,1)= positions(i,1);\n","\n","\n","#               sell, positions values can be overwritten             #\n","# for i in range(3, len(ha_ohlc)):\n","\n","#   if highvslow[i - 1] == 1 and  highvslow[i - 2] == 1 and  \\\n","#     highvslow[i - 3] == 1 and  highvslow[i] == -1:\n","\n","#     for j in range(i, len(ha_ohlc)):\n","\n","#       if highvslow[j] == 1:\n","#         positions[j] = 0\n","#         break\n","\n","#       else:\n","#         positions[j] = -1 \n","\n","#   else:\n","#     pass # positions(i,1)= positions(i,1);\n","\n","\n","# #               buy              #\n","# for i in range(3, len(ha_ohlc)):\n","  \n","#   if highvslow[i - 1] == -1 and  highvslow[i - 2] == -1 and  \\\n","#     highvslow[i - 3] == -1 and  highvslow[i] == 1:\n","\n","#     for j in range(i, len(ha_ohlc)):\n","\n","#       if highvslow[j] == -1:\n","#         positions[j] = 0\n","#         break\n","\n","#       else:\n","#         positions[j] = 1\n","\n","#   else:\n","#     pass # positions(i,1)= positions(i,1);\n","\n","\n","\n","#     1. positions 는 np.nan 으로 초기화됨    \n","#     2. 현재 ha 종가는 이전 ha 종가보다 높지 않고, 현재 이전으로 3 개의 연속된 ha 종가 \"상향 갱신\"이 선행되면\n","#     3. 진입하고 보유한다 (positions = -1)\n","#     4. ha 종가가 상향 갱신되면, positions = 0 하고 나온다\n","#     5. 다음 trigger condition 까지 positions = np.nan\n","#     6. 결과적으로, 아래의 trade logic 은 --> sell 조건의 종가에 매수 + ha 종가 상향 갱신 다음 틱의 종가에 매도\n","\n","\n","\n","#                   이 파트를 이해해보자                  #\n","show_detail = False\n","pyramid_entry = []\n","pyramid_cnt = np.zeros_like(positions)\n","\n","entry_time = np.zeros_like(positions)\n","pr_check = np.zeros_like(positions)\n","\n","lvrg = 5\n","\n","i = 1\n","while 1:\n","# for i in range(len(positions)):\n","\n","  if show_detail: \n","      print(i, positions[i], total_pr[i])\n","\n","  # if positions[i] == -1:\n","  # if positions[i] == 0:\n","  # if positions[i - 1] == -1 and positions[i] == 0:\n","  \n","  # if positions[i] == 1:\n","  if positions[i - 1] == 1 and positions[i] == 0:\n","\n","\n","    g = i # positions 이 -1 이면, 진입한다 (position on, positions 를 -1 로 만든 종가에 진입)\n","    if show_detail: print(i, positions[i], total_pr[i])\n","\n","\n","    for n in range(i + 1, len(positions)):\n","\n","      # if positions[n] == 0:      \n","      # if n == i + 1:\n","      if np.isnan(positions[n]): # <-- positions 이 np.nan 이면, positions 을 내려놓는다 (내려놓은 종가에 팜 = 종가가 완성되자마자 팜)      \n","        h = n\n","        break\n","    \n","    #     positions 를 -1 과 np.nan 으로 만드는 trigger 는 ?    #\n","\n","    #     g + 1 ~ n - 1 까지의 tick 중에서 entry 신호는 최대 몇개 ?   # --> n tick = exit tick 에는 entry 가 가능해진다\n","    signal_cnt = np.sum(positions[g + 1 : n] == -1)\n","    pyramid_entry.append(signal_cnt)\n","    # print(signal_cnt)\n","\n","    fee = 0.0006\n","    # fee = 0.000\n","    \n","    #     long    #\n","    pr = ohlc['close'].iloc[h] / ohlc['close'].iloc[g] - fee\n","    #     short   #\n","    # pr = ohlc['close'].iloc[g] / ohlc['close'].iloc[h] - fee\n","\n","    pyramid_cnt[h] += 1\n","\n","    #         issue : total_pr[h] 가 마지막 진입 기준으로 overwrite 된다    #\n","    # if pyramid_cnt[h] > 1: #  overwrite 하게 되는 경우라면, sum pr 한다 = pyramid     #\n","    #   back_pr = 1 + (total_pr[h] - 1) / lvrg \n","    #   total_pr[h] =  1 + ((back_pr - 1) + pr - 1) * lvrg\n","    #   # print(\"%.2f %.2f %.2f\" % (total_pr[h], back_pr, pr))\n","    # else:\n","    #   total_pr[h] = 1 + (pr - 1) * lvrg\n","\n","    # if pyramid_cnt[h] == 3:\n","    #   total_pr[h] = 1 + (pr - 1) * lvrg\n","\n","    # if pyramid_cnt[h] == 1:\n","    #   total_pr[h] = 1 + (pr - 1) * lvrg\n","    # else:\n","    #   if pyramid_cnt[h] > 2:\n","    #     if total_pr[h] > 1:    \n","    #       total_pr[h] = 1 + (pr - 1) * lvrg\n","    \n","    # if pyramid_cnt[h] == 1:\n","    #   total_pr[h] = 1 + (pr - 1) * lvrg\n","\n","    total_pr[h] = 1 + (pr - 1) * lvrg\n","\n","    if pr > 1:\n","      pr_check[h] = 1\n","    else:\n","      pr_check[h] = -1\n","\n","    entry_time[g] = 1\n","\n","    #       resume trade after exit     #\n","    # if i <= h:\n","    #   i = h\n","\n","  # print(positions[i - 1], positions[i], total_pr[i])\n","  \n","  \n","  #     end trade   #\n","  # else:\n","  i += 1    # <-- exit 한 자리에서 entry 가능함\n","  if i >= len(positions):\n","    break\n","\n","  # print(i, len(positions))\n","\n","win_ratio = np.sum(total_pr > 1) / np.sum(total_pr != 1) * 100\n","\n","plt.plot(np.cumprod(total_pr))\n","plt.title(\"%s\\nacc_pr : %s\\nwinratio : %.2f\\nmax_pyramid : %s\" % (key, np.cumprod(total_pr)[-1], win_ratio, np.max(pyramid_entry)))\n","plt.show()\n","\n","pairs_pr.append(np.cumprod(total_pr)[-1])\n","pairs_wr.append(win_ratio)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ckghv33acLGr"},"source":["non_pira_positions = positions\n","non_pira_pr_check = pr_check\n","non_pira_entry_time = entry_time"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nz3AXVkrb1iF"},"source":["plot_size = 200\n","\n","plt.figure(figsize=(15,3))\n","\n","plt.subplot(211)\n","plt.plot(positions[:plot_size], 'fuchsia')\n","plt.plot(pr_check[:plot_size], 'b')\n","plt.plot(entry_time[:plot_size], 'g')\n","\n","# plt.show()\n","\n","plt.subplot(212)\n","plt.plot(non_pira_positions[:plot_size], 'fuchsia')\n","plt.plot(non_pira_pr_check[:plot_size], 'r')\n","plt.plot(non_pira_entry_time[:plot_size], 'y')\n","\n","plt.show()\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZvwBNvRbaZPP"},"source":["print(\"mean : \", np.array(pyramid_entry).mean())\n","print(\"var : \", np.array(pyramid_entry).var())\n","print(\"std : \", np.array(pyramid_entry).std())\n","plt.plot(pyramid_entry)\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Pshys093i9M4"},"source":["print(np.max(np.cumprod(total_pr)))\n","print(np.min(np.cumprod(total_pr)))\n","plt.plot(np.cumprod(total_pr)[:1000])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UAkP-nG8ZtFt"},"source":["# result_df['index'] = pairs\n","result_df['profit']= pairs_pr\n","result_df['win_ratio']= pairs_wr\n","# print(result_df)\n","\n","#     sort by values    #\n","print(result_df.sort_values(by='profit', ascending=False))\n","print(result_df.sort_values(by=['win_ratio'], ascending=False))\n","\n","#     extract candidates    #\n","# candis = result_df.sort_values(by=['profit'], ascending=False).index\n","# print(candis)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"z0hFkP-hPOrZ"},"source":["### plotting analysis"]},{"cell_type":"code","metadata":{"id":"1BZWqDY4POL_"},"source":["#           \"진입\" 부터 \"이탈\" 까지 plot        #\n","\n","plot_size = 4\n","fontsize = 10\n","color = 'black'\n","\n","i = 0\n","while 1:\n","# for i in range(plot_size, len(positions)):\n","\n","  if positions[i] == 1:\n","\n","    g = i # positions 이 -1 이면, 진입한다 (position on, positions 를 -1 로 만든 종가에 진입)\n","\n","    for n in range(i, len(positions)):\n","\n","      if positions[n] == 0:\n","        \n","        # should use ohlc for checking true profit\n","        plt.plot(ohlc['close'].iloc[g:n + 1].values)  \n","\n","        # plt.title(\"positions = %s\\nacc_pr %.3f\" % (positions[n], np.cumprod(total_pr[:n + 1])[-1]), fontsize=fontsize, color=color)\n","        # plt.\n","        calc_pr = 1 + (ohlc['close'].iloc[g] / ohlc['close'].iloc[n] - 1) * 5\n","\n","        # plt.title(\"positions = %s\\nprofit %.3f\" % (positions[n], total_pr[n]), fontsize=fontsize, color=color)\n","        plt.title(\"positions = %s\\nprofit %.3f\" % (positions[n], calc_pr), fontsize=fontsize, color=color)\n","        plt.show()\n","        print(ohlc['close'].iloc[g], ohlc['close'].iloc[n])\n","\n","        # i = n\n","        break\n","\n","\n","  #    end trade   #\n","  i += 1\n","  if i >= len(positions):\n","    break\n","\n","\n","  # if positions[i] == -1:\n","  #   fontsize = 13\n","  #   color = 'red'\n","\n","  # elif positions[i - 1] == 0 and np.isnan(positions[i]):\n","  #   fontsize = 13\n","  #   color = 'blue'\n","\n","  \n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SprSOMuuDvWj"},"source":["plot_size = 10\n","\n","plt.figure(figsize=(10, 5))\n","\n","plt.subplot(311)\n","plt.plot(highvslow[:plot_size])\n","plt.title(\"highvslow\")\n","\n","plt.subplot(312)\n","plt.plot(positions[:plot_size])\n","plt.xlim(0, len(positions[:plot_size]))\n","plt.title(\"positions\")\n","\n","plt.subplot(313)\n","plt.plot(np.cumprod(total_pr)[:plot_size])\n","plt.title(\"accum pr\")\n","\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"krQZps7i_KCh"},"source":["\n","    # ohlcv['u_wick'] = ohlcv['high'] / np.maximum(ohlcv['close'] , ohlcv['open'])\n","    # ohlcv['d_wick'] = np.minimum(ohlcv['close'] , ohlcv['open']) / ohlcv['low']\n","    # ohlcv['body'] = ohlcv['close'] / ohlcv['open']\n","    # candle = ohlcv.iloc[-len(predictions):, -3:]\n","\n","\n","    # print('len(ohlc) :', len(ohlc))\n","    # print('long_ep.shape :', long_ep.shape)\n","    # print('len(ha_ohlc) :', len(ha_ohlc))\n","    # print('len(sar) :', len(sar))\n","    # print('len(ema1) :', len(ema1))\n","    # print('len(senkou1) :', len(senkou1))\n","    # print('len(cbo) :', len(cbo))\n","    # print('len(ema_cbo) :', len(ema_cbo))\n","    # print('len(bbw) :', len(bbw))\n","    # print('len(fish) :', len(fish))\n","    # print('len(trix) :', len(trix))\n","    # print('len(rsi_) :', len(rsi_))\n","    # print('len(macd_hist) :', len(macd_hist))\n","\n","\n","    # break\n","\n","\n","    #       set params    #\n","    period = 45\n","    key_i = i\n","\n","    # plt.plot(cbo)\n","    # # plt.plot(bbw)\n","    # plt.plot(fish, color='b')\n","    # # plt.plot(trix)\n","    # # plt.plot(rsi_)\n","    # # plt.plot(macd_hist)\n","    # plt.show()\n","\n","    # #      global scaling   #\n","    # min_max = MinMaxScaler()\n","    # cbo = min_max.fit_transform(cbo.values.reshape(-1, 1))\n","    # ema_cbo = min_max.fit_transform(ema_cbo.values.reshape(-1, 1))\n","    # bbw = min_max.fit_transform(bbw.values.reshape(-1, 1))\n","    # fish = min_max.fit_transform(fish.values.reshape(-1, 1))\n","    # trix = min_max.fit_transform(trix.values.reshape(-1, 1))\n","    # rsi_ = min_max.fit_transform(rsi_.values.reshape(-1, 1))\n","    # macd_hist = min_max.fit_transform(macd_hist.values.reshape(-1, 1))\n","\n","    # # plt.plot(cbo)\n","    # # plt.plot(bbw)\n","    # plt.plot(fish, color='r')\n","    # # plt.plot(trix)\n","    # # plt.plot(rsi_)\n","    # # plt.plot(macd_hist)\n","    # plt.show()\n","    # # break\n","\n","    plotting = True\n","\n","    for trial_number in range(1):\n","\n","      data_x, data_pr, data_updown = [], [], []\n","      data_index = []\n","\n","      for i in range(period, len(predictions)):\n","\n","        #   pr_list != 1 인 데이터만 사용한다\n","        # if 1:\n","        if pr_list[i] != 1:\n","\n","          min_max = MinMaxScaler()\n","          \n","          #   prediction 을 제외한 이전 데이터를 사용해야한다\n","          temp_ohlc = ohlc.iloc[i - period : i].values\n","          temp_long_ep = long_ep[i - period : i]          \n","          temp_ha_ohlc = ha_ohlc.iloc[i - period : i].values\n","          temp_sar = sar.iloc[i - period : i].values.reshape(-1, 1)\n","          temp_ema1 = ema1.iloc[i - period : i].values.reshape(-1, 1)\n","          temp_ema2 = ema2.iloc[i - period : i].values.reshape(-1, 1)\n","          temp_ema3 = ema3.iloc[i - period : i].values.reshape(-1, 1)\n","          temp_senkou1 = senkou1.iloc[i - period : i].values.reshape(-1, 1)\n","          temp_senkou2 = senkou2.iloc[i - period : i].values.reshape(-1, 1)          \n","\n","          price_data = np.hstack((temp_ohlc, temp_long_ep, temp_ha_ohlc, temp_sar, temp_ema1, temp_ema2, temp_ema3, temp_senkou1, temp_senkou2))\n","\n","          if np.isnan(np.sum(price_data)):\n","            continue\n","\n","          # print(\"price_data[:10] :\", price_data[:10])\n","          # print(\"temp_ohlc.shape :\", temp_ohlc.shape)\n","          # print(\"temp_long_ep.shape :\", temp_long_ep.shape)\n","          # print(\"temp_ha_ohlc.shape :\", temp_ha_ohlc.shape)\n","          # print(\"price_data.shape :\", price_data.shape)\n","\n","          if plotting:\n","            plt.plot(price_data)\n","            plt.show()\n","\n","          temp_price_data = min_max_scale(price_data)\n","          # temp_price_data = (price_data - np.min(price_data)) / (np.max(price_data) - np.min(price_data))\n","\n","          temp_ohlc = temp_price_data[:, :4]\n","          temp_long_ep = temp_price_data[:, [4]]\n","          temp_ha_ohlc = temp_price_data[:, 5:9]\n","          temp_sar, temp_ema1, temp_ema2, temp_ema3, temp_senkou1, temp_senkou2 = np.split(temp_price_data[:, 9:], 6, axis=1)\n","\n","          if plotting:\n","\n","            plt.plot(temp_price_data)\n","            plt.show()\n","\n","            plotting = False\n","          # break\n","\n","          #   vol -> min_max\n","          temp_vol = min_max.fit_transform(vol.iloc[i - period : i].values.reshape(-1, 1))\n","          \n","          # temp_candle = candle.iloc[i - period : i].values\n","\n","          temp_cbo = cbo[i - period : i]\n","          temp_ema_cbo = ema_cbo[i - period : i]\n","          temp_bbw = bbw[i - period : i]\n","          temp_fish = fish[i - period : i]\n","          temp_trix = trix[i - period : i]\n","          temp_rsi_ = rsi_[i - period : i]\n","          temp_macd_hist = macd_hist[i - period : i]\n","\n","          # print(temp_ohlc.shape)\n","          # print(temp_long_ep.shape)\n","          # print(temp_vol.shape)\n","          # print(temp_candle.shape)\n","          # break\n","\n","          trial_list = [temp_ha_ohlc, temp_sar, temp_ema1, temp_ema2, temp_ema3, temp_senkou1, temp_senkou2,\n","                        temp_cbo, temp_ema_cbo, temp_bbw, temp_fish, temp_trix, temp_rsi_, temp_macd_hist]\n","\n","          # trial_list = [temp_ohlc[:, [0], temp_ohlc[:, [1], temp_ohlc[:, [2], temp_ohlc[:, [3]], temp_vol, temp_ema_cbo]\n","\n","          #                   feature selection                   #  \n","          # temp_data = np.hstack((temp_ohlc, temp_long_ep, temp_vol, temp_candle))\n","          # temp_data = trial_list[trial_number]\n","          # temp_data = np.hstack((temp_ohlc[:, [3]], temp_ohlc[:, [1]], temp_bbw))\n","          temp_data = temp_ohlc[:, [3]]\n","          # temp_data = np.hstack((temp_ohlc, temp_vol))\n","\n","          #     only close    #\n","          # temp_data = temp_ohlc[:, [-1]]\n","\n","          # temp_data = temp_ohlc\n","\n","          #     only volume    #\n","          # temp_data = temp_vol\n","\n","          # temp_data = np.hstack((temp_ohlc, temp_long_ep))\n","          # temp_data = temp_vol\n","\n","          #   scaler 설정\n","\n","          #   ohlc & ep -> max_abs\n","          # max_abs = MaxAbsScaler()\n","          # temp_data[:, :-1] = max_abs.fit_transform(temp_data[:, :-1])\n","\n","          \n","          # min_max = MinMaxScaler()\n","          # temp_data = min_max.fit_transform(temp_data)\n","\n","          #   candle -> max_abs    \n","          # max_abs = MaxAbsScaler()\n","          # temp_data[:, -3:] = max_abs.fit_transform(temp_data[:, -3:])\n","\n","          # min_max = MinMaxScaler()\n","          # temp_data[:, -3:] = min_max.fit_transform(temp_data[:, -3:])\n","\n","          if np.isnan(np.sum(temp_data)):\n","            continue\n","\n","          data_x.append(temp_data)\n","          data_pr.append(pr_list[i])\n","          data_index.append(time_index[i])\n","          data_updown.append(ohlc['close'].iloc[i] / ohlc['open'].iloc[i])\n","\n","\n","      print('np.array(data_x).shape :', np.array(data_x).shape)\n","      # print(data_x[0])\n","\n","\n","      #       Reshape data for image deep - learning     #\n","      _, row, col = np.array(data_x).shape\n","\n","      # input_x = np.array(data_x).reshape(-1, row, col, 1).astype(np.float32)\n","      input_x = np.array(data_x).reshape(-1, row, col).astype(np.float32)\n","\n","      #     1c to 3c    #\n","      # input_x = input_x * np.ones(3, dtype=np.float32)[None, None, None, :]\n","      # input_x = np.array(resize_npy(input_x))\n","\n","\n","      input_pr = np.array(data_pr).reshape(-1, 1).astype(np.float32)\n","      input_ud = np.array(data_updown).reshape(-1, 1).astype(np.float32)\n","      input_index = np.array(data_index).reshape(-1, 1)\n","      print('input_x.shape :', input_x.shape)\n","      print('input_x.dtype :', input_x.dtype)\n","      print('input_pr.shape :', input_pr.shape)\n","      print('input_ud.shape :', input_ud.shape)\n","      print('input_index.shape :', input_index.shape)\n","\n","\n","      # x_train_, x_test, pr_train_, pr_test, ud_train_, ud_test = train_test_split(input_x, input_pr, input_ud, test_size=0.4, shuffle=False, random_state=random_state)\n","      # x_train, x_val, pr_train, pr_val, ud_train, ud_val = train_test_split(x_train_, pr_train_, ud_train_, test_size=0.25, shuffle=False, random_state=random_state)\n","\n","      #     do stacking   #\n","      # if prev_x is None:\n","      prev_x = input_x\n","      prev_pr = input_pr\n","      prev_ud = input_ud\n","      prev_index = input_index\n","\n","      total_x = input_x\n","      total_pr = input_pr\n","      total_ud = input_ud\n","      total_index = input_index\n","\n","      # else:\n","      #   total_x = np.vstack((prev_x, input_x))\n","      #   total_pr = np.vstack((prev_pr, input_pr))\n","      #   total_ud = np.vstack((prev_ud, input_ud)) \n","      #   total_index = np.vstack((prev_index, input_index)) \n","\n","      #   prev_x = total_x\n","      #   prev_pr = total_pr\n","      #   prev_ud = total_ud\n","      #   prev_index = total_index\n","\n","      print('total_x.shape :', total_x.shape)\n","      print('total_pr.shape :', total_pr.shape)\n","      print('total_ud.shape :', total_ud.shape)\n","      print('prev_index.shape :', prev_index.shape)\n","\n","      \n","      # _, row, col, _ = input_x.shape\n","      _, row, col = input_x.shape\n","\n","      #       split new test      #\n","\n","      seed = 1\n","      random_state = 201\n","      np.random.seed(seed)\n","      from sklearn.model_selection import train_test_split\n","\n","\n","      #         get unique timestamp      #\n","      print(np.unique(total_index, return_counts=True))\n","      uniq_stamp = np.unique(total_index)\n","\n","      stamp_train_, stamp_test = train_test_split(uniq_stamp, test_size=0.2, shuffle=False, random_state=random_state)\n","      stamp_train, stamp_val = train_test_split(stamp_train_, test_size=0.25, shuffle=True, random_state=random_state)\n","\n","      print(\"stamp_train.shape :\", stamp_train.shape)\n","      print(\"stamp_val.shape :\", stamp_val.shape)\n","      print(\"stamp_test.shape :\", stamp_test.shape)\n","      # break\n","\n","\n","      #         split data by stamp     #\n","      x_train, x_val, x_test = [], [], []\n","      pr_train, pr_val, pr_test = [], [], []\n","      index_train, index_val, index_test = [], [], []\n","\n","\n","      from tqdm.notebook import tqdm\n","\n","      np.random.shuffle(total_index)\n","\n","      for i in tqdm(range(len(total_index))):\n","\n","        if total_index[i] in stamp_train:\n","          x_train.append(total_x[i])\n","          pr_train.append(total_pr[i])\n","          index_train.append(total_index[i])\n","\n","        elif total_index[i] in stamp_val:\n","          x_val.append(total_x[i])\n","          pr_val.append(total_pr[i])\n","          index_val.append(total_index[i])\n","        \n","        elif total_index[i] in stamp_test:\n","          x_test.append(total_x[i])\n","          pr_test.append(total_pr[i])\n","          index_test.append(total_index[i])\n","\n","\n","      x_train = np.array(x_train)\n","      x_val = np.array(x_val)\n","      x_test = np.array(x_test)\n","\n","      pr_train = np.array(pr_train)\n","      pr_val = np.array(pr_val)\n","      pr_test = np.array(pr_test)\n","\n","      index_train = np.array(index_train)\n","      index_val = np.array(index_val)\n","      index_test = np.array(index_test)\n","        \n","      print(\"x_train.shape :\", x_train.shape) # x_train.shape : (3807, 90, 12, 3)\n","      print(\"x_val.shape :\", x_val.shape) # x_train.shape : (3807, 90, 12, 3)\n","      print(\"x_test.shape :\", x_test.shape) # x_train.shape : (3807, 90, 12, 3)\n","\n","  \n","      symbol_name = key.split(' ')[1].split('.')[0]\n","\n","      x_save_path = current_path + 'npy/' + '%s_rnn_close_updown_x_train_robust_trial_%s_timesplit.npy' % (period, symbol_name)\n","      np.save(x_save_path, x_train)\n","      np.save(x_save_path.replace('x_train', 'x_val'), x_val)\n","      np.save(x_save_path.replace('x_train', 'x_test'), x_test)\n","      # np.save(x_save_path.replace('x_train', 'new_input_x'), new_input_x)\n","      print('x series saved !')\n","\n","      pr_save_path = current_path + 'npy/' + '%s_rnn_close_updown_pr_train_robust_trial_%s_timesplit.npy' % (period, symbol_name)\n","      np.save(pr_save_path, pr_train)\n","      np.save(pr_save_path.replace('pr_train', 'pr_val'), pr_val)\n","      np.save(pr_save_path.replace('pr_train', 'pr_test'), pr_test)\n","      # np.save(pr_save_path.replace('pr_train', 'new_input_pr'), new_input_pr)\n","      print('pr series saved !')\n","\n","      \n","      _, row, col, = x_train.shape\n","\n","      #         pr label   #\n","      y_train = np.where(pr_train > 1, 1, 0)\n","      y_test = np.where(pr_test > 1, 1, 0)\n","      y_val = np.where(pr_val > 1, 1, 0)\n","\n","\n","      seed = 1\n","      random_state = 20\n","      np.random.seed(seed)\n","      # tf.random.set_seed(seed)\n","\n","      #         up label      #\n","      # y_train = np.where(ud_train > 1, 1, 0)\n","      # y_test = np.where(ud_test > 1, 1, 0)\n","      # y_val = np.where(ud_val > 1, 1, 0)\n","\n","      # print('pr_train[:5] :', pr_train[:5])\n","      # print('ud_train[:5] :', ud_train[:5])\n","      print('y_train[:5] :', y_train[:5])\n","      print('y_train.dtype :', y_train.dtype)\n","\n","      print('x_train.shape :', x_train.shape)\n","      print('x_test.shape :', x_test.shape)\n","      print('x_val.shape :', x_val.shape)\n","      print('y_train.shape :', y_train.shape)\n","      print('y_test.shape :', y_test.shape)\n","      print('y_val.shape :', y_val.shape)\n","\n","      def class_ratio(in_list):\n","\n","        return in_list / in_list[1]\n","\n","      print('np.unique(y_train, return_counts=True :', np.unique(y_train, return_counts=True), class_ratio(np.unique(y_train, return_counts=True)[1]))\n","      print('np.unique(y_val, return_counts=True :', np.unique(y_val, return_counts=True), class_ratio(np.unique(y_val, return_counts=True)[1]))\n","      print('np.unique(y_test, return_counts=True :', np.unique(y_test, return_counts=True), class_ratio(np.unique(y_test, return_counts=True)[1]))\n","\n","      label = y_train.reshape(-1, )\n","      class_weights = class_weight.compute_class_weight('balanced', \n","                                                          classes=np.unique(label),\n","                                                          y=label)\n","      class_weights = dict(enumerate(class_weights))\n","      print('class_weights :', class_weights)\n","\n","      # sample_weight = np.ones(shape=(len(y_train),))\n","      # sample_weight[(y_train == 1).reshape(-1,)] = 1.5\n","      # print('sample_weight[:20] :', sample_weight[:20])\n","\n","\n","      print('np.isnan(np.sum(x_train)) :', np.isnan(np.sum(x_train)))\n","      print('np.isnan(np.sum(x_val)) :', np.isnan(np.sum(x_val)))\n","      print('np.isnan(np.sum(x_test)) :', np.isnan(np.sum(x_test)))\n","\n","      print('np.isnan(np.sum(y_train)) :', np.isnan(np.sum(y_train)))\n","      print('np.isnan(np.sum(y_val)) :', np.isnan(np.sum(y_val)))\n","      print('np.isnan(np.sum(y_test)) :', np.isnan(np.sum(y_test)))\n","\n","      y_train_ohe = np_utils.to_categorical(y_train, num_classes)\n","      y_val_ohe = np_utils.to_categorical(y_val, num_classes)\n","      y_test_ohe = np_utils.to_categorical(y_test, num_classes)\n","      print('y_train_ohe.shape :', y_train_ohe.shape)\n","      print('y_val_ohe.shape :', y_val_ohe.shape)\n","      print('y_test_ohe.shape :', y_test_ohe.shape)\n","\n","\n","      ckpt_path = current_path + 'ckpt/'\n","      board_path = current_path + 'graph/'\n","      # model_name = 'classifier_%s_close_updown_pr_theta_shuffle_without_back_03.h5' % period\n","      # model_name = 'classifier_%s_close_updown_pr_theta_train_prevdata_005.h5' % period\n","\n","      # model_name = 'classifier_%s_close_updown_pr_theta_train_evendata_shuffle2_300k.h5' % period\n","      # model_name = 'classifier_%s_lstm_close_updown_pr_neo_re_tf1_timesplit.h5' % period\n","      # model = keras.models.load_model(ckpt_path + model_name)\n","\n","      batch_size = 512\n","      # _, row, col, = x_train.shape\n","      # print(\"row, col :\", row, col)\n","\n","      model = FER_Model(input_shape=(row, col))\n","      opt = Adam(lr=0.00001, decay=0.000005)\n","      # opt = Adam(lr=0.001, decay=0.0005)\n","      model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n","\n","      # model_name = 'classifier_%s_close_updown_pr_all_pair_shuffle_300k_timesplit.h5' % period\n","      # model_name = 'classifier_%s_lstm_close_updown_pr_neo_re_tf1_timesplit.h5' % period\n","      # model_name = 'classifier_%s_lstm_close_updown_pr_eth_abs_timesplit.h5' % period\n","      # model_name = 'classifier_%s_lstm_close_updown_pr_neo_abs_timesplit.h5' % period\n","      model_name = 'classifier_%s_lstm_close_updown_pr_robust_trial_%s_timesplit.h5' % (period, symbol_name)\n","\n","\n","\n","      checkpoint = ModelCheckpoint(ckpt_path + model_name, monitor='val_loss', verbose=1, save_best_only=True, mode='auto')\n","      checkpoint2 = TensorBoard(log_dir=board_path,\n","                                histogram_freq=0,\n","                                write_graph=True,\n","                                write_images=True)\n","      checkpoint3 = EarlyStopping(monitor='val_loss', patience=250)\n","      callbacks_list = [checkpoint, checkpoint2, checkpoint3]\n","      # callbacks_list = [checkpoint, checkpoint2]\n","\n","      # keras.callbacks.Callback 로 부터 log 를 받아와 history log 를 작성할 수 있다.\n","\n","      # we iterate 200 times over the entire training set\n","      num_epochs = 1000                    \n","\n","      history = model.fit(x_train, y_train_ohe,\n","                          steps_per_epoch=int(len(x_train) / batch_size), \n","                          epochs=num_epochs,  \n","                          verbose=2,  \n","                          callbacks=callbacks_list,\n","                          class_weight=class_weights,\n","                          validation_data=(x_val, y_val_ohe),  \n","                          validation_steps=int(len(x_val) / batch_size),\n","                          shuffle=False)\n","      \n","      ckpt_path = current_path + 'ckpt/'\n","      board_path = current_path + 'graph/'\n","      # model_name = 'classifier_45_close_updown_pr_theta_non_shuffle_ex_02008.h5'\n","      # model_name = 'classifier_45_close_updown_pr_theta.h5'\n","      # model_name = 'classifier_45_close_updown_pr_theta_train_evendata_shuffle2_300k.h5'\n","      # model_name = 'classifier_45_lstm_close_updown_pr_neo_timesplit.h5'\n","\n","      # model_name = 'classifier_45_lstm_close_updown_pr_eth_timesplit.h5'\n","\n","      model = keras.models.load_model(ckpt_path + model_name)\n","\n","      # x_test = new_input_x\n","      # y_test = np.where(new_input_pr > 1, 1, 0)\n","      # pr_test = new_input_pr\n","\n","      # x_test = org_x_test\n","      # y_test = org_y_test\n","      # pr_test = org_pr_test\n","\n","      # x_test = x_train\n","      # y_test = y_train\n","      # pr_test = pr_train\n","\n","      # x_test = x_val\n","      # y_test = y_val\n","      # pr_test = pr_val\n","\n","      # x_test = concat_x\n","      # y_test = np.where(concat_pr > 1, 1, 0)\n","      # pr_test = concat_pr\n","\n","      test_result = model.predict(x_test)\n","      # test_result = model.predict(test_set)\n","\n","      print('test_result.shape :', test_result.shape)\n","      # print('pr_val.shape :', pr_val.shape)\n","\n","      y_score = test_result[:, [1]]\n","      print('y_test[:5] :', y_test.reshape(-1,)[:5])\n","      # print('np.unique(y_test) :', np.unique(y_test, return_counts=True))\n","      print('y_score[:5] :', y_score[:5])\n","      # print('np.unique(y_score) :', np.unique(y_score, return_counts=True))\n","\n","      print('y_test.shape :', y_test.shape)\n","      print('y_score.shape :', y_score.shape)\n","\n","      print('len(y_test) :', len(y_test))\n","\n","      #     precision recall curve   #\n","      precision, recall, threshold = precision_recall_curve(y_test, y_score)\n","      precision, recall = precision[:-1], recall[:-1]\n","\n","      plt.plot(threshold, precision, label='precision')\n","      plt.plot(threshold, recall, label='recall')\n","      plt.legend()\n","      plt.title('precision recall')\n","      plt.show()\n","      # print(y_pred)\n","\n","\n","      # threshold = [0.65]\n","      # print('threshold :', threshold)\n","      # break\n","\n","      acc_pr_bythr = []\n","      new_thresh = []\n","\n","      for thresh in threshold:\n","        \n","        # if thresh < 0.5:\n","        #     continue\n","\n","        y_pred = np.where(y_score[:, -1] > thresh, 1, 0)\n","        # print('y_pred.shape :', y_pred.shape)\n","        # print('y_pred :', y_pred)\n","\n","        #     compare precision     #\n","\n","        # print('precision :', precision_score(y_test, y_pred))\n","        # print('recall :', recall_score(y_test, y_pred))\n","        # print()\n","\n","        # print('np.isnan(np.sum(x_test)) :', np.isnan(np.sum(x_test)))\n","        # print('np.isnan(np.sum(y_test)) :', np.isnan(np.sum(y_test)))\n","\n","        # # plot_confusion_matrix(best_model, x_test, y_test, normalize=None)\n","        # # plt.show()  \n","        # print()\n","\n","        #     check win-ratio improvement     #\n","        cmat = confusion_matrix(y_test, y_pred)\n","        # print(cmat)\n","        # print(np.sum(cmat, axis=1))\n","\n","        test_size = len(y_test)\n","        test_pr_list = pr_test\n","        # print('origin ac_pr :', np.cumprod(test_pr_list)[-1])\n","\n","        org_wr = np.sum(cmat, axis=1)[-1] / sum(np.sum(cmat, axis=1))\n","        ml_wr = cmat[1][1] / np.sum(cmat, axis=0)[-1]\n","        # print('win ratio improvement %.2f --> %.2f' % (org_wr, ml_wr))\n","\n","        # print('pr_test.shape :', pr_test.shape)\n","\n","        # print(y_pred)\n","        # print(test_pr_list)\n","        pred_pr_list = np.where(y_pred == 1, test_pr_list.reshape(-1, ), 1.0)\n","        pred_pr_list = np.where(np.isnan(pred_pr_list), 1.0, pred_pr_list)\n","        pred_pr_list = np.where(pred_pr_list == 0.0, 1.0, pred_pr_list)\n","        # print('pred_pr_list.shape :', pred_pr_list.shape)\n","\n","        # if np.cumprod(test_pr_list)[-1] < np.cumprod(pred_pr_list)[-1]:\n","        #   print('accum_pr increased ! : %.3f --> %.3f' % (np.cumprod(test_pr_list)[-1], np.cumprod(pred_pr_list)[-1]))\n","        #   print('thresh :', thresh)\n","          \n","        # if len(threshold) == 1:\n","      #   plt.figure(figsize=(10, 5))\n","      #   plt.subplot(121)\n","      #   plt.plot(np.cumprod(test_pr_list))\n","      #   plt.title('%.3f' % (np.cumprod(test_pr_list)[-1]))\n","      # # plt.show()\n","\n","      #   plt.subplot(122)\n","      #   plt.plot(np.cumprod(pred_pr_list))\n","      #   plt.title('%.3f' % (np.cumprod(pred_pr_list)[-1]))\n","      #   # plt.axvline(len(org_pr_test), linestyle='--', color='r')\n","      #   plt.show()\n","\n","\n","        acc_pr_bythr.append(np.cumprod(pred_pr_list)[-1])\n","        new_thresh.append(thresh)\n","\n","\n","      print('acc_pr_bythr :', acc_pr_bythr)\n","\n","      plt.figure(figsize=(10, 5))\n","      plt.subplot(121)\n","      plt.plot(threshold, precision, label='precision')\n","      plt.plot(threshold, recall, label='recall')\n","      plt.legend()\n","      plt.title('precision recall')\n","      # plt.show()\n","      plt.subplot(122)\n","      plt.plot(new_thresh, acc_pr_bythr)\n","      plt.axhline(np.cumprod(test_pr_list)[-1], linestyle='--', color='r')\n","      plt.show()\n","\n","\n","  # break # --> use only one pair dataset\n","\n","  #         chunks 로 나누지 않아도, generator 에서 batch_size 만큼만 load 할 것   #\n","  try:\n","    if len(total_x) > 300000:\n","      break\n","  except:\n","    pass\n","\n","  \n","        "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mcDUjgQzqUSr"},"source":["import os\n","# os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   \n","# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","\n","%tensorflow_version 1.x\n","\n","import keras\n","import tensorflow as tf\n","\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import scipy.misc \n","from math import sqrt \n","import itertools\n","from IPython.display import display\n","\n","%matplotlib inline\n","\n","from keras.utils import plot_model\n","import keras.backend as K\n","from keras.models import Model, Sequential\n","import keras.layers as layers\n","from keras.optimizers import Adam, SGD\n","from keras.regularizers import l1, l2\n","\n","from sklearn.cluster import KMeans\n","from sklearn.metrics import silhouette_score\n","from sklearn.metrics import precision_score, recall_score, precision_recall_curve\n","from sklearn.metrics import confusion_matrix\n","from sklearn.metrics import plot_confusion_matrix\n","\n","\n","gdrive_path = current_path\n","\n","num_classes = 2\n","\n","def FER_Model(input_shape):\n","    # first input model\n","    visible = layers.Input(shape=input_shape, name='input')\n","    \n","    # net = layers.LSTM(32, return_sequences=False)(visible)\n","    net = layers.LSTM(10, return_sequences=False)(visible)\n","\n","    # net = layers.Dense(32)(visible)\n","    # net = layers.Conv2D(32, kernel_size=3, padding='same', kernel_initializer='he_normal')(visible)\n","    # net = layers.Conv2D(256, kernel_size=3, padding='same', kernel_initializer='he_normal')(visible)\n","    # net = layers.BatchNormalization()(net)\n","    net = layers.LeakyReLU()(net)\n","\n","    net = layers.Dense(64)(net)\n","    # net = layers.Conv2D(64, kernel_size=3, padding='same', kernel_initializer='he_normal')(net)\n","    # net = layers.Conv2D(128, kernel_size=3, padding='same', kernel_initializer='he_normal')(net)\n","    # net = layers.BatchNormalization()(net)\n","    # net = layers.Activation('relu')(net)\n","    net = layers.LeakyReLU()(net)\n","    net = layers.BatchNormalization()(net)\n","\n","    # net = layers.MaxPool2D(pool_size=2)(net)\n","    # net = layers.AveragePooling2D(padding='same')(net)\n","\n","    shortcut_1 = net\n","\n","    net = layers.Dense(128)(net)\n","    # net = layers.Conv2D(64, kernel_size=3, padding='same', kernel_initializer='he_normal')(net)\n","    # net = layers.Conv2D(128, kernel_size=3, padding='same', kernel_initializer='he_normal')(net)\n","    net = layers.LeakyReLU()(net)\n","\n","    net = layers.Dense(256)(net)\n","    # net = layers.Conv2D(256, kernel_size=3, padding='same', kernel_initializer='he_normal')(net)\n","    # net = layers.Activation('relu')(net)\n","    net = layers.LeakyReLU()(net)\n","    net = layers.BatchNormalization()(net)\n","\n","\n","    # net = layers.MaxPool2D(pool_size=2)(net)\n","\n","    shortcut_2 = net\n","\n","#     net = layers.Conv2D(256, kernel_size=3, padding='same')(net)\n","#     # net = layers.Activation('relu')(net)\n","#     net = layers.LeakyReLU()(net)\n","#     net = layers.MaxPool2D(pool_size=2)(net)\n","\n","#     shortcut_3 = net\n","\n","#     net = layers.Conv2D(128, kernel_size=1, padding='same')(net)\n","#     # net = layers.Activation('relu')(net)\n","#     net = layers.LeakyReLU()(net)\n","#     net = layers.MaxPool2D(pool_size=2)(net)\n","\n","    # net = layers.Flatten()(net)\n","    net = layers.Dense(128)(net)\n","    net = layers.LeakyReLU()(net)\n","\n","    net = layers.Dense(64)(net)\n","    net = layers.LeakyReLU()(net)\n","\n","    net = layers.Dropout(0.3)(net)\n","\n","    net = layers.Dense(num_classes, activation='softmax')(net)\n","\n","    # create model \n","    model = Model(inputs=visible, outputs=net)\n","    # summary layers\n","    print(model.summary())\n","    \n","    return model"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0en4ihETQ32K"},"source":["### **Data Stacking**"]},{"cell_type":"code","metadata":{"id":"OgZyYJPg3RJa"},"source":["def resize_npy(x):\n","\n","  temp_x = []\n","\n","  for d_i, data in enumerate(x):\n","    # resized_data = cv2.resize(data, (row * 2, col * 2)) --> input image 홰손된다\n","    # resized_data = data.repeat(2, axis=0).repeat(2, axis=1)\n","    data = data.repeat(2, axis=0).repeat(2, axis=1)\n","    # resized_data = data.repeat(1, axis=0).repeat(1, axis=1)\n","    # cmapped = plt.cm.Set1(resized_data)[:, :, :3]  # Drop Alpha Channel\n","    \n","    if d_i == 0:\n","      plt.imshow(data)\n","      plt.show()\n","      # plt.imshow(resized_data)\n","      # plt.show()\n","    # print('resized_data.shape :', resized_data.shape)\n","    # break\n","    temp_x.append(data)\n","\n","  return temp_x"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZZtZP5tqenWh"},"source":["def min_max_scale(npy_x):\n","\n","  return (npy_x - np.min(npy_x)) / (np.max(npy_x) - np.min(npy_x))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QYwWJxCSLF62"},"source":["min_max = MinMaxScaler()\n","cbo1 = min_max.fit_transform(cbo.values.reshape(-1, 1))\n","bbw1 = min_max.fit_transform(bbw.values.reshape(-1, 1))\n","\n","min_max = MinMaxScaler()\n","bbw2 = min_max.fit_transform(bbw.values.reshape(-1, 1))\n","\n","print(bbw1[-10:])\n","print(bbw2[-10:])\n","plt.plot(bbw1)\n","plt.plot(bbw2)\n","\n","plt.show()\n","\n","print(len(bbw1))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6XibtKgphXyQ"},"source":["### Check shuffled index"]},{"cell_type":"code","metadata":{"id":"KH8eEW8ChZtV"},"source":["# print(index_val)\n","from datetime import datetime\n","\n","# print(index_test)\n","# print(index_train)\n","# print(index_val)\n","total_stamp = list(map(lambda x: datetime.timestamp(x[0]), input_index)) \n","timestamp_train = list(map(lambda x: datetime.timestamp(x[0]), index_train)) \n","timestamp_val = list(map(lambda x: datetime.timestamp(x[0]), index_val)) \n","timestamp_test = list(map(lambda x: datetime.timestamp(x[0]), index_test)) \n","# print(total_stamp)\n","# print(timestamp_train)\n","plt.figure(figsize=(40, 4))\n","plt.scatter(range(len(timestamp_train)), timestamp_train, label='train')\n","plt.scatter(range(len(timestamp_val)), timestamp_val, color='orange', label='val')\n","plt.scatter(range(len(timestamp_test)), timestamp_test, color='red', label='test')\n","plt.ylim(min(total_stamp), max(total_stamp))\n","plt.legend(fontsize=20)\n","\n","# print(new_input_index)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"b1UEFg1GVSLS"},"source":["### Load Data"]},{"cell_type":"code","metadata":{"id":"oa0CYY1zKH0l"},"source":["period = 45\n","\n","x_save_path = current_path + 'npy/' + '%s_rnn_close_updown_x_train_neo_timesplit.npy' % period\n","x_train = np.load(x_save_path)\n","x_val = np.load(x_save_path.replace('x_train', 'x_val'))\n","x_test = np.load(x_save_path.replace('x_train', 'x_test'))\n","print('x series loaded !')\n","\n","pr_save_path = current_path + 'npy/' + '%s_rnn_close_updown_pr_train_neo_timesplit.npy' % period\n","pr_train = np.load(pr_save_path)\n","pr_val = np.load(pr_save_path.replace('pr_train', 'pr_val'))\n","pr_test = np.load(pr_save_path.replace('pr_train', 'pr_test'))\n","print('y series loaded !')\n","\n","_, row, col = x_train.shape\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zscZynIgMbAq"},"source":["print(keras.__version__)\n","print(tf.__version__)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fWUEyjzF21cJ"},"source":["### **Data Split**"]},{"cell_type":"markdown","metadata":{"id":"s-W6LL5c2VN2"},"source":["### **Training**"]},{"cell_type":"code","metadata":{"id":"pfPMGMvm0rGu"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QlxBTT1tEKkB"},"source":["org_x_test = x_test\n","org_y_test = y_test\n","org_pr_test = pr_test"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_JS3iA80ExQ6"},"source":["x_test = org_x_test\n","y_test = org_y_test\n","pr_test = org_pr_test"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"d3NzZSrbFgrB"},"source":["concat_x = np.vstack((org_x_test, new_input_x))\n","concat_y = np.vstack((org_y_test, np.where(new_input_pr > 1, 1, 0)))\n","concat_pr = np.vstack((org_pr_test, new_input_pr))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BwKeKaE_Q6XZ"},"source":["x_test = new_input_x\n","y_test = np.where(new_input_pr > 1, 1, 0)\n","pr_test = new_input_pr"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zhRu2BIK792m"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0ATtzBO53HfN"},"source":["# print(pred_pr_list)\n","for pr in pred_pr_list:\n","  if pr != 1:\n","    print(pr)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TP6lH9u8PXPY"},"source":["plt.figure(figsize=(50, 5))\n","plt.plot(np.cumprod(pred_pr_list)[len(org_pr_test):len(org_pr_test) + 10])\n","# plt.plot(np.cumprod(pred_pr_list)[len(org_pr_test) - 10:len(org_pr_test) + 10])\n","plt.title('%.3f' % (np.cumprod(pred_pr_list)[-1]))\n","# plt.axvline(len(org_pr_test), linestyle='--', color='r')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yWU2mDnQMVlB"},"source":["# plt.show()\n","# plt.subplot(122)\n","plt.plot(new_thresh, acc_pr_bythr)\n","plt.axhline(np.cumprod(test_pr_list)[-1], linestyle='--', color='r')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-jo3k5MdhFyg"},"source":["#### **clustering output**"]},{"cell_type":"code","metadata":{"id":"njxxm-TJ-RP-"},"source":["# x_train_for_k = test_result.flatten().reshape(-1, 1)\n","x_train_for_k = test_result\n","print(x_train_for_k[:10])\n","# x_train_for_k = test_result[:, [1]]\n","pr_train = pr_test\n","\n","print('x_train_for_k.shape :', x_train_for_k.shape)\n","print('pr_train.shape :', pr_train.shape)\n","\n","K = range(2, 10)\n","s_dist = []\n","sil = []\n","for k in K:\n","  # if cen_data.shape[0] < k:\n","  #   break\n","\n","  km = KMeans(n_clusters=k)\n","  km = km.fit(x_train_for_k)\n","\n","  labels = km.labels_\n","  # print('len(labels) :', len(labels))\n","  # print('labels[:10] :', labels[:10])\n","  sil.append(silhouette_score(x_train_for_k, labels, metric='euclidean'))\n","\n","  # inertia = km.inertia_\n","  # s_dist.append(inertia)\n","\n","best_k = K[np.argmax(np.array(sil))]\n","\n","plt.figure(figsize=(10, 5))\n","plt.plot(K, sil)\n","plt.axvline(best_k, linestyle='--')\n","# plt.plot(K, s_dist)\n","plt.show()\n","\n","\n","\n","\n","\n","#   with best_k, label 별 pr_list 확인\n","km = KMeans(n_clusters=best_k)\n","km = km.fit(x_train_for_k)\n","\n","labels = km.labels_\n","\n","print(km.score(x_train_for_k))\n","print(len(labels), len(pr_train))\n","\n","\n","\n","\n","\n","#   label 별로 profit 을 저장, 승률을 확인한다\n","label_types = np.unique(labels, return_counts=False)\n","\n","label_pr_dict = {}\n","#   init dict   #\n","for label in label_types:\n","  label_pr_dict[label] = []\n","print(label_pr_dict)\n","# break\n","\n","for i, (label, pr) in enumerate(zip(labels, pr_train)):\n","  label_pr_dict[label].append(pr[0])\n","\n","  \n","# for label in label_types:\n","print(label_pr_dict)\n","\n","\n","\n","\n","\n","def win_ratio(list_x):\n","\n","  win_cnt = np.sum(np.array(list_x) > 1)\n","  return win_cnt / len(list_x)\n","\n","\n","def acc_pr(list_x):\n","\n","  return np.cumprod(np.array(list_x))[-1]\n","\n","\n","for key in label_pr_dict:\n","  \n","  print(key, ':', 'win_ratio : %.2f' % (win_ratio(label_pr_dict[key])), 'acc_pr : %.2f' % (acc_pr(label_pr_dict[key])))\n","\n","\n","\n","\n","\n","#     predict test && test 의 라벨에 따른 win_ratio 확인\n","# test_labels = km.predict(x_test)\n","# # print(test_labels)\n","\n","# label_pr_dict = {}\n","# #   init dict   #\n","# for label in label_types:\n","#   label_pr_dict[label] = []\n","# print(label_pr_dict)\n","# # break\n","\n","# for i, (label, pr) in enumerate(zip(test_labels, pr_test)):\n","#   label_pr_dict[label].append(pr[0])\n","\n","# for key in label_pr_dict:\n","\n","#   print(key, ':', 'win_ratio : %.2f' % (win_ratio(label_pr_dict[key])), 'acc_pr : %.2f' % (acc_pr(label_pr_dict[key])))\n","\n"],"execution_count":null,"outputs":[]}]}