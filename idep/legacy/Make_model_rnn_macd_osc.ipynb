{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python [conda env:tensorflow2_p36]","language":"python","name":"conda-env-tensorflow2_p36-py"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.5"},"colab":{"name":"Make_model_rnn_macd_osc.ipynb","provenance":[{"file_id":"1YUiI2oP1aMpqDt9OGGf6f38IVQ2hR4MP","timestamp":1584365704742},{"file_id":"1YF8_MnvES9U5ImcMOeF0v4QBeWL1Yqem","timestamp":1584115546819},{"file_id":"1W9HzvtL-1kYHCyYbDJxFsA7uKJNOCwuv","timestamp":1584004491586},{"file_id":"1WEMU-VCj-p8mZvMxqBpQAdViCsHXpo20","timestamp":1583840352407},{"file_id":"1z4z_KLPzc6RWsxo3X_dHbpByxUAgjoMJ","timestamp":1583754134002}],"collapsed_sections":[],"machine_shape":"hm"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"8s5fopqwFUf9","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1600487050234,"user_tz":-540,"elapsed":903,"user":{"displayName":"JJANGJAE","photoUrl":"","userId":"08178289703395036410"}},"outputId":"d27775dd-106e-4024-89b4-3ce9e4cf4b13"},"source":["from google.colab import drive\n","\n","drive.mount('/content/gdrive')"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"scrolled":false,"id":"2iYLNSeSEp7p","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":172},"executionInfo":{"status":"error","timestamp":1600487088790,"user_tz":-540,"elapsed":6205,"user":{"displayName":"JJANGJAE","photoUrl":"","userId":"08178289703395036410"}},"outputId":"7569ea2b-ccbc-4ece-f3db-44f13b64dc55"},"source":["import os\n","# os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   \n","# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","\n","import pandas as pd\n","import numpy as np\n","from matplotlib import pyplot\n","import scipy.misc \n","from math import sqrt \n","import itertools\n","from IPython.display import display\n","%matplotlib inline\n","\n","# input_data_length = int(input('input_data_length : '))\n","input_data_length = 60\n","model_num = 132\n","num_classes = 2\n","\n","gdrive_path = '/content/gdrive/My Drive/Colab Notebooks/Project_Stock/'\n","\n","Made_X = np.load(gdrive_path + 'Made_X/Made_X %s_%s.npy' % (input_data_length, model_num))\n","Made_Y = np.load(gdrive_path + 'Made_X/Made_Y %s_%s.npy' % (input_data_length, model_num)).reshape(-1, 1)\n","\n","\n","#       dataset 분리      #\n","# dataX 구성 : VOLUME, MA, CMO, OBV, RSI, MACD, MACD_SIGNAL, MACD_OSC \n","# dataX 구성 : VOLUME, EMA1, EMA2, CMO, OBV, RSI, MACD, MACD_SIGNAL, MACD_OSC \n","# Made_X = Made_X[:, :, [-4]]\n","print(Made_X.shape)\n","print(Made_Y.shape)\n","# break\n","\n","row = Made_X.shape[1]\n","col = Made_X.shape[2]\n","\n","from sklearn.model_selection import train_test_split\n","X_train, X_val, Y_train, Y_val = train_test_split(Made_X, Made_Y, test_size=0.3,\n","                                                   shuffle=False)\n","\n","X_train = X_train.astype('float32')#.reshape(-1, input_data_length, col, 1)\n","X_val = X_val.astype('float32')#.reshape(-1, input_data_length, col, 1)\n","print(X_train.shape)\n","print(X_val.shape)\n","\n","from keras.utils import np_utils\n","from keras.preprocessing.image import ImageDataGenerator \n","\n","# Data Class Weight\n","from sklearn.utils import class_weight\n","\n","print(Y_train[:, 0])\n","class_weights = class_weight.compute_class_weight('balanced', \n","                                                  np.unique(Y_train[:, 0]),\n","                                                  Y_train[:, 0])\n","class_weights = dict(enumerate(class_weights))\n","print(class_weights)\n","# quit()\n","\n","Y_train = Y_train.astype('float32')\n","Y_val = Y_val.astype('float32')\n","Y_train = np_utils.to_categorical(Y_train, num_classes)\n","Y_val = np_utils.to_categorical(Y_val, num_classes)\n","print(Y_train.shape)\n","print(Y_val.shape)\n","\n","# datagen = ImageDataGenerator( \n","# #     rotation_range = 60,\n","# #     zoom_range = 0.6,\n","# #     shear_range = 0.6,\n","# #     horizontal_flip = True,\n","# #     width_shift_range=0.6,\n","# #     height_shift_range=0.6,\n","#     fill_mode = 'nearest'\n","#     )\n","\n","# testgen = ImageDataGenerator( \n","#     )\n","# datagen.fit(X_train)\n","batch_size = 16\n","\n","# for X_batch, _ in datagen.flow(X_train, Y_train, batch_size=9):\n","#     for i in range(0, 9): \n","#         pyplot.axis('off') \n","#         pyplot.subplot(330 + 1 + i) \n","#         pyplot.imshow(X_batch[i].reshape(input_data_length, col))\n","#     pyplot.axis('off') \n","#     pyplot.show() \n","#     break\n","    \n","    \n","# train_flow = datagen.flow(X_train, Y_train, batch_size=batch_size) \n","# val_flow = testgen.flow(X_val, Y_val, batch_size=batch_size) \n","\n","\n","from keras.utils import plot_model\n","import keras.backend as K\n","from keras.models import Model, Sequential\n","import keras.layers as layers\n","from keras.optimizers import Adam, SGD\n","from keras.regularizers import l1, l2\n","from sklearn.metrics import confusion_matrix\n","\n","def FER_Model(input_shape=(row, col)):\n","    # first input model\n","    visible = layers.Input(shape=input_shape, name='input')\n","    \n","    #             LSTM Version            #\n","    net = layers.LSTM(50, return_sequences=True)(visible)\n","    net = layers.Dropout(0.2)\n","\n","    net = layers.LSTM(50, return_sequences=True)(visible)\n","    net = layers.Dropout(0.2)\n","\n","    net = layers.LSTM(50, return_sequences=True)(visible)\n","    net = layers.Dropout(0.2)\n","\n","    net = layers.LSTM(50, return_sequences=True)(visible)\n","    net = layers.Dropout(0.2)\n","\n","    # net = layers.Conv2D(64, kernel_size=3, padding='same')(visible)\n","    # # net = layers.Activation('relu')(net)\n","    # net = layers.LeakyReLU()(net)\n","    # net = layers.MaxPool2D(pool_size=2)(net)\n","\n","    # shortcut_1 = net\n","\n","    # net = layers.Conv2D(128, kernel_size=3, padding='same')(net)\n","    # # net = layers.Activation('relu')(net)\n","    # net = layers.LeakyReLU()(net)\n","    # # net = layers.MaxPool2D(pool_size=2)(net)\n","\n","    # shortcut_2 = net\n","\n","#     net = layers.Conv2D(256, kernel_size=3, padding='same')(net)\n","#     # net = layers.Activation('relu')(net)\n","#     net = layers.LeakyReLU()(net)\n","#     net = layers.MaxPool2D(pool_size=2)(net)\n","\n","#     shortcut_3 = net\n","\n","#     net = layers.Conv2D(128, kernel_size=1, padding='same')(net)\n","#     # net = layers.Activation('relu')(net)\n","#     net = layers.LeakyReLU()(net)\n","#     net = layers.MaxPool2D(pool_size=2)(net)\n","\n","    # net = layers.Flatten()(net)\n","    # net = layers.Dense(64)(net)\n","    # net = layers.LeakyReLU()(net)\n","    net = layers.Dense(num_classes, activation='softmax')(net)\n","\n","    # create model \n","    model = Model(inputs =visible, outputs = net)\n","    # summary layers\n","    print(model.summary())\n","    \n","    return model\n","\n","model = FER_Model()\n","# from keras.models import load_model\n","# model = load_model(gdrive_path + 'model/rapid_ascending %s_%s_rnn_macdonly.hdf5' % (input_data_length, model_num))\n","opt = Adam(lr=0.0001, decay=1e-6)\n","model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n","  \n","    \n","from keras.callbacks import ModelCheckpoint, EarlyStopping, TensorBoard\n","filepath = gdrive_path + \"model/rapid_ascending %s_%s_rnn_macdonly.hdf5\" % (input_data_length, model_num)\n","checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='auto')\n","checkpoint2 = TensorBoard(log_dir='Tensorboard_graph',\n","                          histogram_freq=0,\n","                          write_graph=True,\n","                          write_images=True)\n","checkpoint3 = EarlyStopping(monitor='val_acc', patience=200)\n","callbacks_list = [checkpoint, checkpoint2, checkpoint3]\n","\n","# keras.callbacks.Callback 로 부터 log 를 받아와 history log 를 작성할 수 있다.\n","\n","# we iterate 200 times over the entire training set\n","num_epochs = 1000\n","history = model.fit(X_train, Y_train,\n","                    steps_per_epoch=int(len(X_train) / batch_size), \n","                    epochs=num_epochs,  \n","                    verbose=2,  \n","                    callbacks=callbacks_list,\n","                    class_weight=class_weights,\n","                    validation_data=(X_val, Y_val),  \n","                    validation_steps=int(len(X_val) / batch_size),\n","                    shuffle=False)\n","\n"],"execution_count":5,"outputs":[{"output_type":"stream","text":["(1409, 60, 14)\n","(1409, 1)\n"],"name":"stdout"},{"output_type":"error","ename":"SyntaxError","evalue":"ignored","traceback":["\u001b[0;36m  File \u001b[0;32m\"<ipython-input-5-5cb7a051ec85>\"\u001b[0;36m, line \u001b[0;32m31\u001b[0m\n\u001b[0;31m    break\u001b[0m\n\u001b[0m         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m 'break' outside loop\n"]}]},{"cell_type":"code","metadata":{"id":"a01LFE7QEp70","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]}]}