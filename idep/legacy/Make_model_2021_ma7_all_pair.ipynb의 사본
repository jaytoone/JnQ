{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python [conda env:tensorflow2_p36]","language":"python","name":"conda-env-tensorflow2_p36-py"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.5"},"colab":{"name":"Make_model_2021_ma7_all_pair.ipynb의 사본","provenance":[{"file_id":"1z4z_KLPzc6RWsxo3X_dHbpByxUAgjoMJ","timestamp":1583754134002}],"collapsed_sections":[],"toc_visible":true,"machine_shape":"hm"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AK9FjWwLOyay","executionInfo":{"status":"ok","timestamp":1618488249552,"user_tz":-540,"elapsed":19520,"user":{"displayName":"7th June","photoUrl":"","userId":"08178289703395036410"}},"outputId":"514383f7-aba1-4cf9-b865-45b6990a98e3"},"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","import os, sys\n","\n","current_path = '/content/drive/My Drive/Colab Notebooks/Project_Stock/'\n","\n","os.chdir(current_path)"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"8uqYv5StTazo"},"source":["### **Requirements**"]},{"cell_type":"code","metadata":{"id":"9qGt60DKTZmf"},"source":["# !pip install statsmodels==0.12.2\n","\n","# import statsmodels\n","# statsmodels.__version__"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"y7bVjhlwPI_-"},"source":["### **ARIMA**"]},{"cell_type":"code","metadata":{"id":"NvdpArctN_6l","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618488251949,"user_tz":-540,"elapsed":7514,"user":{"displayName":"7th June","photoUrl":"","userId":"08178289703395036410"}},"outputId":"2e9f7267-c0f6-4cd5-c08c-0b253572b183"},"source":["from statsmodels.tsa.arima_model import ARIMA\n","# from statsmodels.tsa.arima.model import ARIMA\n","\n","from datetime import datetime\n","\n","\n","def arima_test(target, use_rows=None):\n","\n","  size = int(len(target) * 0.66)\n","  train, test = target[:size].values, target[size:]\n","  test_shift = test.shift(1).values\n","  test = test.values\n","  # break\n","\n","  history = list(train)\n","  predictions = list()\n","  err_ranges = list()\n","  for t in range(len(test)):\n","    \n","      if use_rows is not None:\n","        history = history[-use_rows:]\n","        \n","      model = ARIMA(history, order=(0, 2, 4))\n","      model_fit = model.fit()\n","      output = model_fit.forecast()\n","      # print(output)\n","      # break\n","\n","      predictions.append(output[0])\n","      err_ranges.append(output[1])\n","      obs = test[t]\n","      # print('obs :', obs)\n","      history.append(obs)\n","      # break\n","      print('\\r %.2f%%' % (t / len(test) * 100), end='')\n","\n","  print(len(test), len(predictions))\n","\n","  return predictions, err_ranges\n","\n","\n","# print(high)\n","\n","\n","def get_back_result(ohlcv, predictions, err_ranges, tp=0.04, sl=None, leverage=1, show_detail=False, show_plot=False, return_pr=False, cumsum=False, \n","                    close_ver=False, reverse_short=False):\n","\n","  \n","  high, low, test = np.split(ohlcv.values[-len(predictions):, [1, 2, 3]], 3, axis=1)\n","\n","  if close_ver:\n","    predictions = ohlcv['close'].shift(1).values[-len(test):]\n","\n","  fee = 0.0006\n","  long_profits = []\n","  short_profits = []\n","  liquidations = []\n","  win_cnt = 0\n","  for i in range(len(test)):\n","\n","    long_ep = predictions[i]\n","    if sl is not None:\n","      long_sl = long_ep * (1 / (sl + 1))\n","\n","    # assert long_ep < long_exit, 'long_exit < long_ep !, %s, %s' % (long_exit, long_ep)\n","    \n","    short_ep = (predictions[i] + err_ranges[i]) * (1 + tp)\n","    # short_ep = (predictions[i] + err_ranges[i]) * (1 / (1 - tp))\n","    if sl is not None:\n","      short_sl = short_ep * (1 / (1 - sl))\n","\n","    # print((low[i]))\n","\n","    #    long 우선   # <-- long & short 둘다 체결된 상황에서는 long 체결을 우선으로 한다.\n","    if low[i] < long_ep:\n","      \n","      liquidation = low[i] / long_ep - fee\n","      l_liquidation = 1 + (liquidation - 1) * leverage\n","      liquidations.append(l_liquidation)\n","\n","      if max(l_liquidation, 0) == 0:\n","        l_profit = 0\n","        # print('low[i], long_ep, l_liquidation :', low[i], long_ep, l_liquidation)\n","      else:\n","\n","        if sl is not None:\n","          if low[i] < long_sl:\n","            profit = long_sl / long_ep - fee\n","          else:\n","            profit = test[i] / long_ep - fee\n","\n","        else:\n","          profit = test[i] / long_ep - fee\n","\n","        l_profit = 1 + (profit - 1) * leverage\n","        l_profit = max(l_profit, 0)\n","        \n","        if profit >= 1:\n","          win_cnt += 1\n","\n","      long_profits.append(l_profit)\n","      short_profits.append(1.0)\n","\n","      if show_detail:\n","        print(test[i], predictions[i], long_ep)\n","\n","    # if high[i] > short_ep > low[i]: # 지정 대기가 아니라, 해당 price 가 지나면, long 한다.\n","\n","    #   if not reverse_short:\n","    #     liquidation = short_ep / high[i]  - fee\n","    #   else:\n","    #     liquidation = low[i] / short_ep  - fee\n","    #   l_liquidation = 1 + (liquidation - 1) * leverage\n","\n","    #   if max(l_liquidation, 0) == 0:\n","    #     l_profit = 0\n","    #   else:\n","\n","    #     if sl is not None:\n","    #       if high[i] > short_sl:\n","\n","    #         if not reverse_short:\n","    #           profit = short_ep / short_sl - fee\n","    #         else:\n","    #           profit = short_sl / short_ep - fee\n","\n","    #       else:\n","    #         if not reverse_short:\n","    #           profit = short_ep / test[i] - fee\n","    #         else:\n","    #           profit = test[i] / short_ep - fee\n","\n","    #     else:\n","\n","    #       if not reverse_short:\n","    #         profit = short_ep / test[i] - fee\n","    #       else:\n","    #         profit = test[i] / short_ep - fee\n","\n","    #     l_profit = 1 + (profit - 1) * leverage\n","    #     l_profit = max(l_profit, 0)\n","\n","    #     if profit >= 1:\n","    #       win_cnt += 1\n","\n","    #   short_profits.append(l_profit)\n","    #   long_profits.append(1.0)\n","\n","    #   if show_detail:\n","    #     print(test[i], predictions[i], short_ep)\n","    \n","    else:\n","      long_profits.append(1.0)\n","      short_profits.append(1.0)\n","      liquidations.append(1.0)\n","\n","\n","  long_win_ratio = sum(np.array(long_profits) > 1.0) / sum(np.array(long_profits) != 1.0)\n","  short_win_ratio = sum(np.array(short_profits) > 1.0) / sum(np.array(short_profits) != 1.0)\n","  long_frequency = sum(np.array(long_profits) != 1.0) / len(test)\n","  short_frequency = sum(np.array(short_profits) != 1.0) / len(test)\n","  if not cumsum:\n","    long_accum_profit = np.array(long_profits).cumprod()\n","    short_accum_profit = np.array(short_profits).cumprod()\n","  else:\n","    long_accum_profit = (np.array(long_profits) - 1.0).cumsum()\n","    short_accum_profit = (np.array(short_profits) - 1.0).cumsum()\n","\n","  # print(win_ratio)\n","\n","  if show_plot:\n","\n","    plt.figure(figsize=(10, 5))\n","    plt.suptitle('tp=%.4f, lvrg=%d' % (tp, leverage))\n","\n","    plt.subplot(151)\n","    plt.plot(liquidations)\n","    plt.title('liquidations')\n","\n","    plt.subplot(152)\n","    plt.plot(long_profits)\n","    plt.title('Win Ratio : %.2f %%\\nrequency : %.2f %%' % (long_win_ratio * 100, long_frequency * 100), color='black')\n","    # plt.show()\n","\n","    # print()\n","    plt.subplot(153)\n","    plt.plot(long_accum_profit)\n","    plt.title('Accum_profit : %.2f' % long_accum_profit[-1], color='black')\n","\n","    plt.subplot(154)\n","    plt.plot(short_profits)\n","    plt.title('Win Ratio : %.2f %%\\nrequency : %.2f %%' % (short_win_ratio * 100, short_frequency * 100), color='black')\n","    # plt.show()\n","\n","    # print()\n","    plt.subplot(155)\n","    plt.plot(short_accum_profit)\n","    plt.title('Accum_profit : %.2f' % short_accum_profit[-1], color='black')\n","    plt.show()\n","\n","  return [long_win_ratio, short_win_ratio], [long_frequency, short_frequency], [long_accum_profit[-1], short_accum_profit[-1]], [long_profits, short_profits]\n","\n","\n","# get_back_result(tp=0.04, leverage=1, show_plot=True)\n"],"execution_count":2,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n","  import pandas.util.testing as tm\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"aDkU3tMiM2lO","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618488253620,"user_tz":-540,"elapsed":8581,"user":{"displayName":"7th June","photoUrl":"","userId":"08178289703395036410"}},"outputId":"740fdebd-93d5-41e6-e26b-f1376c2bee01"},"source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","interval = '30m'\n","date_path = './candlestick_concated/%s/2021-02-11/' % interval\n","file_list = os.listdir(date_path)\n","\n","print((file_list))"],"execution_count":3,"outputs":[{"output_type":"stream","text":["['2021-02-11 BTCUSDT.xlsx', '2021-02-11 ETHUSDT.xlsx', '2021-02-11 BCHUSDT.xlsx', '2021-02-11 XRPUSDT.xlsx', '2021-02-11 EOSUSDT.xlsx', '2021-02-11 LTCUSDT.xlsx', '2021-02-11 ETCUSDT.xlsx', '2021-02-11 LINKUSDT.xlsx', '2021-02-11 XLMUSDT.xlsx', '2021-02-11 ADAUSDT.xlsx', '2021-02-11 XMRUSDT.xlsx', '2021-02-11 SXPUSDT.xlsx', '2021-02-11 KAVAUSDT.xlsx', '2021-02-11 BANDUSDT.xlsx', '2021-02-11 DASHUSDT.xlsx', '2021-02-11 ZECUSDT.xlsx', '2021-02-11 XTZUSDT.xlsx', '2021-02-11 BNBUSDT.xlsx', '2021-02-11 ATOMUSDT.xlsx', '2021-02-11 ONTUSDT.xlsx', '2021-02-11 IOTAUSDT.xlsx', '2021-02-11 BATUSDT.xlsx', '2021-02-11 NEOUSDT.xlsx', '2021-02-11 QTUMUSDT.xlsx', '2021-02-11 WAVESUSDT.xlsx', '2021-02-11 MKRUSDT.xlsx', '2021-02-11 SNXUSDT.xlsx', '2021-02-11 DOTUSDT.xlsx', '2021-02-11 THETAUSDT.xlsx', '2021-02-11 ALGOUSDT.xlsx', '2021-02-11 KNCUSDT.xlsx', '2021-02-11 ZRXUSDT.xlsx', '2021-02-11 COMPUSDT.xlsx', '2021-02-11 OMGUSDT.xlsx']\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"0en4ihETQ32K"},"source":["### **Data Stacking**"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1Ks84tI6gEcDbGgWvCH9CieXL1GrqpSMH"},"id":"SvZuk1rPrUMe","executionInfo":{"status":"ok","timestamp":1618488550006,"user_tz":-540,"elapsed":303960,"user":{"displayName":"7th June","photoUrl":"","userId":"08178289703395036410"}},"outputId":"7a41bfc0-243f-4626-8451-82b74699e06e"},"source":["from sklearn.preprocessing import MinMaxScaler, MaxAbsScaler\n","import pickle\n","\n","with open('./arima_result/arima_ma7_profit_ls_only_long_result_%s.pickle' % interval, 'rb') as f:\n","  load_dict = pickle.load(f)\n","\n","candis = list(load_dict.keys())\n","long_index = 0\n","leverage = 5\n","prev_x = None\n","for i in range(len(candis)):\n","\n","  keys = [candis[i]]\n","  \n","  # if 'algo'.upper() not in candis[i]:\n","  #   continue\n","  if '2021-03-02 DOTUSDT.xlsx' in candis[i]:\n","    # print('')\n","    continue\n","\n","  # plt.figure(figsize=(35, 10))\n","  # plt.suptitle('%s %s' % (interval, keys))\n","\n","\n","  #         get tp parameter        #\n","\n","  # plt.subplot(1,10,3)\n","  # for key in keys:  \n","  #   # plt.plot(profit_result_dict[key]['tp_list'], profit_result_dict[key]['ap_list'])\n","  #   argmax = np.argmax(profit_result_dict[key]['ap_list'][:, [long_index]])\n","  #   peak_tp = profit_result_dict[key]['tp_list'][argmax]\n","  #   # plt.axvline(peak_tp, linestyle='--')\n","  #   # plt.title('acc profit, max at %.4f' % (peak_tp))  \n","\n","  # plt.subplot(1,10,4)\n","  # plt.title('max acc profit by leverage')  \n","  # for key in keys:  \n","  #   # plt.plot(profit_result_dict[key]['tp_list'], profit_result_dict[key]['max_ap_list'], label=key)\n","  #   argmax = np.argmax(profit_result_dict[key]['max_ap_list'][:, [long_index]])\n","  #   max_peak_tp = profit_result_dict[key]['tp_list'][argmax]\n","  #   # plt.axvline(max_peak_tp, linestyle='--')\n","  #   # plt.title('max acc profit, max at %.4f' % (max_peak_tp))  \n","\n","\n","  for key in keys:  \n","    # print(profit_result_dict[key]['leverage_ap_list'])\n","\n","    # for tp in [max_peak_tp]:\n","\n","      # if tp == peak_tp:\n","      #   plt.subplot(1,10,5)\n","      # else:\n","      #   plt.subplot(1,10,6)\n","\n","      #     leverage analysis     #\n","      ohlcv = load_dict[key]['ohlcv']\n","      predictions = load_dict[key]['predictions']\n","      err_ranges = load_dict[key]['err_ranges']\n","\n","      # predictions = ohlcv['close'].shift(1).values\n","      # err_ranges = np.zeros_like(predictions)\n","\n","      # leverage_list = profit_result_dict[key]['leverage_list']\n","      # temp_ap_list = list()\n","      # temp_pr_list = list()\n","\n","      try:\n","        print('-------------- %s --------------' % key)\n","        result = get_back_result(ohlcv, predictions, err_ranges, tp=0, leverage=leverage, show_plot=True, reverse_short=False, show_detail=False)\n","        # temp_ap_list.append(result[2])\n","        # temp_pr_list.append(result[3])\n","\n","        # if round(leverage) == 1:\n","        #   temp_pr_list = result[3]\n","        pr_list = result[3][long_index]\n","\n","      except Exception as e:\n","        print(e)\n","        break    \n","  # break\n","\n","\n","      pd.set_option('display.max_rows', 500)\n","      pd.set_option('display.max_columns', 500)\n","      pd.set_option('display.width', 1000)\n","\n","      #         clustering zone           #\n","\n","      #       set data features : ohlc, v, ep\n","      ohlc = ohlcv.iloc[-len(predictions):, :4]\n","      vol = ohlcv.iloc[-len(predictions):, [4]]\n","      long_ep = np.array(predictions)\n","      long_ep = long_ep.reshape(-1, 1)\n","\n","      ohlcv['u_wick'] = ohlcv['high'] / np.maximum(ohlcv['close'] , ohlcv['open'])\n","      ohlcv['d_wick'] = np.minimum(ohlcv['close'] , ohlcv['open']) / ohlcv['low']\n","      ohlcv['body'] = ohlcv['close'] / ohlcv['open']\n","\n","      candle = ohlcv.iloc[-len(predictions):, -3:]\n","\n","\n","      print('len(ohlc) :', len(ohlc))\n","      print('long_ep.shape :', long_ep.shape)\n","      print('len(pr_list) :', len(pr_list))\n","\n","\n","      #       set params    #\n","      period = 45\n","      data_x, data_pr, data_updown = [], [], []\n","      key_i = i\n","\n","      for i in range(period, len(predictions)):\n","\n","        #   pr_list != 1 인 데이터만 사용한다\n","        # if 1:\n","        if pr_list[i] != 1:\n","          \n","          #   prediction 을 제외한 이전 데이터를 사용해야한다\n","          temp_ohlc = ohlc.iloc[i - period : i].values\n","          temp_long_ep = long_ep[i - period : i]\n","          temp_vol = vol.iloc[i - period : i].values\n","          temp_candle = candle.iloc[i - period : i].values\n","\n","          # print(temp_ohlc.shape)\n","          # print(temp_long_ep.shape)\n","          # print(temp_vol.shape)\n","          # print(temp_candle.shape)\n","          # break\n","\n","          #   stacking  \n","          # temp_data = np.hstack((temp_ohlc, temp_long_ep, temp_vol, temp_candle))\n","          temp_data = np.hstack((temp_ohlc, temp_long_ep, temp_vol))\n","          # temp_data = np.hstack((temp_ohlc, temp_vol))\n","\n","          # temp_data = np.hstack((temp_ohlc, temp_long_ep))\n","          # temp_data = temp_vol\n","\n","          #   scaler 설정\n","\n","          #   ohlc & ep -> max_abs\n","          # max_abs = MaxAbsScaler()\n","          # temp_data[:, :5] = max_abs.fit_transform(temp_data[:, :5])\n","\n","\n","          min_max = MinMaxScaler()\n","          temp_data[:, :5] = min_max.fit_transform(temp_data[:, :5])\n","\n","\n","          #   vol -> min_max\n","          min_max = MinMaxScaler()\n","          temp_data[:, [5]] = min_max.fit_transform(temp_data[:, [5]])\n","\n","\n","          #   candle -> max_abs    \n","          # max_abs = MaxAbsScaler()\n","          # temp_data[:, -3:] = max_abs.fit_transform(temp_data[:, -3:])\n","\n","          # min_max = MinMaxScaler()\n","          # temp_data[:, -3:] = min_max.fit_transform(temp_data[:, -3:])\n","\n","          if np.isnan(np.sum(temp_data)):\n","            continue\n","\n","          data_x.append(temp_data)\n","          data_pr.append(pr_list[i])\n","          data_updown.append(ohlc['close'].iloc[i] / ohlc['open'].iloc[i])\n","\n","\n","      print('np.array(data_x).shape :', np.array(data_x).shape)\n","      # print(data_x[0])\n","\n","\n","      #       Reshape data for image deep - learning     #\n","      _, row, col = np.array(data_x).shape\n","\n","      input_x = np.array(data_x).reshape(-1, row, col, 1).astype(np.float32)\n","\n","      #     1c to 3c    #\n","      input_x = input_x * np.ones(3, dtype=np.float32)[None, None, None, :]\n","\n","      input_pr = np.array(data_pr).reshape(-1, 1).astype(np.float32)\n","      input_ud = np.array(data_updown).reshape(-1, 1).astype(np.float32)\n","      print('input_x.shape :', input_x.shape)\n","      print('input_x.dtype :', input_x.dtype)\n","      print('input_pr.shape :', input_pr.shape)\n","      print('input_ud.shape :', input_ud.shape)\n","\n","      #     do stacking   #\n","      if prev_x is None:\n","        prev_x = input_x\n","        prev_pr = input_pr\n","        prev_ud = input_ud\n","      else:\n","        total_x = np.vstack((prev_x, input_x))\n","        total_pr = np.vstack((prev_pr, input_pr))\n","        total_ud = np.vstack((prev_ud, input_ud))\n","\n","        prev_x = total_x\n","        prev_pr = total_pr\n","        prev_ud = total_ud\n","\n","        print('total_x.shape :', total_x.shape)\n","        print('total_pr.shape :', total_pr.shape)\n","        print('total_ud.shape :', total_ud.shape)\n","        "],"execution_count":4,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"markdown","metadata":{"id":"GmmgsEUMqUjN"},"source":["### **Model**"]},{"cell_type":"code","metadata":{"id":"mcDUjgQzqUSr","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618488553184,"user_tz":-540,"elapsed":305810,"user":{"displayName":"7th June","photoUrl":"","userId":"08178289703395036410"}},"outputId":"4d15416c-9cc9-42d4-a128-2e8f0a8f9a8a"},"source":["import os\n","# os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   \n","# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","\n","%tensorflow_version 1.x\n","\n","import keras\n","import tensorflow as tf\n","\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import scipy.misc \n","from math import sqrt \n","import itertools\n","from IPython.display import display\n","\n","%matplotlib inline\n","\n","from keras.utils import plot_model\n","import keras.backend as K\n","from keras.models import Model, Sequential\n","import keras.layers as layers\n","from keras.optimizers import Adam, SGD\n","from keras.regularizers import l1, l2\n","\n","from sklearn.cluster import KMeans\n","from sklearn.metrics import silhouette_score\n","from sklearn.metrics import precision_score, recall_score, precision_recall_curve\n","from sklearn.metrics import confusion_matrix\n","from sklearn.metrics import plot_confusion_matrix\n","\n","\n","gdrive_path = current_path\n","\n","num_classes = 2\n","\n","def FER_Model(input_shape=(row, col, 3)):\n","    # first input model\n","    visible = layers.Input(shape=input_shape, name='input')\n","    \n","    net = layers.Conv2D(32, kernel_size=3, padding='same', kernel_initializer='he_normal')(visible)\n","    # net = layers.Conv2D(256, kernel_size=3, padding='same', kernel_initializer='he_normal')(visible)\n","    # net = layers.BatchNormalization()(net)\n","    net = layers.LeakyReLU()(net)\n","\n","    net = layers.Conv2D(64, kernel_size=3, padding='same', kernel_initializer='he_normal')(net)\n","    # net = layers.Conv2D(128, kernel_size=3, padding='same', kernel_initializer='he_normal')(net)\n","    # net = layers.BatchNormalization()(net)\n","    # net = layers.Activation('relu')(net)\n","    net = layers.LeakyReLU()(net)\n","    # net = layers.MaxPool2D(pool_size=2)(net)\n","    # net = layers.AveragePooling2D(padding='same')(net)\n","\n","    shortcut_1 = net\n","\n","    # net = layers.Conv2D(64, kernel_size=3, padding='same', kernel_initializer='he_normal')(net)\n","    net = layers.Conv2D(128, kernel_size=3, padding='same', kernel_initializer='he_normal')(net)\n","    net = layers.LeakyReLU()(net)\n","\n","    net = layers.Conv2D(256, kernel_size=3, padding='same', kernel_initializer='he_normal')(net)\n","    # net = layers.BatchNormalization()(net)\n","    # net = layers.Activation('relu')(net)\n","    net = layers.LeakyReLU()(net)\n","    # net = layers.MaxPool2D(pool_size=2)(net)\n","\n","    shortcut_2 = net\n","\n","#     net = layers.Conv2D(256, kernel_size=3, padding='same')(net)\n","#     # net = layers.Activation('relu')(net)\n","#     net = layers.LeakyReLU()(net)\n","#     net = layers.MaxPool2D(pool_size=2)(net)\n","\n","#     shortcut_3 = net\n","\n","#     net = layers.Conv2D(128, kernel_size=1, padding='same')(net)\n","#     # net = layers.Activation('relu')(net)\n","#     net = layers.LeakyReLU()(net)\n","#     net = layers.MaxPool2D(pool_size=2)(net)\n","\n","    net = layers.Flatten()(net)\n","    net = layers.Dense(128)(net)\n","    net = layers.LeakyReLU()(net)\n","\n","    net = layers.Dense(64)(net)\n","    net = layers.LeakyReLU()(net)\n","\n","    net = layers.Dense(num_classes, activation='softmax')(net)\n","\n","    # create model \n","    model = Model(inputs=visible, outputs=net)\n","    # summary layers\n","    # print(model.summary())\n","    \n","    return model"],"execution_count":5,"outputs":[{"output_type":"stream","text":["TensorFlow 1.x selected.\n"],"name":"stdout"},{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"zscZynIgMbAq","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618488553185,"user_tz":-540,"elapsed":305268,"user":{"displayName":"7th June","photoUrl":"","userId":"08178289703395036410"}},"outputId":"f2e642b3-13fd-46c8-a3da-bf978210c359"},"source":["print(keras.__version__)\n","print(tf.__version__)"],"execution_count":6,"outputs":[{"output_type":"stream","text":["2.3.1\n","1.15.2\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"fWUEyjzF21cJ"},"source":["### **Data Split**"]},{"cell_type":"code","metadata":{"scrolled":false,"id":"2iYLNSeSEp7p","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1618488561384,"user_tz":-540,"elapsed":311881,"user":{"displayName":"7th June","photoUrl":"","userId":"08178289703395036410"}},"outputId":"570167e7-5f83-426c-ce5f-ee51b50c69d6"},"source":["from sklearn.model_selection import train_test_split\n","from keras.utils import np_utils\n","from keras.preprocessing.image import ImageDataGenerator \n","from sklearn.utils import class_weight\n","# import cv2\n","\n","\n","\n","seed = 1\n","random_state = 20\n","np.random.seed(seed)\n","# tf.random.set_seed(seed)\n","\n","#         resize total_x shape  (, 5, 6, 3)      #\n","# .repeat(2, axis=0).repeat(2, axis=1)\n","temp_x = list()\n","for d_i, data in enumerate(total_x):\n","  # resized_data = cv2.resize(data, (row * 2, col * 2)) --> input image 홰손된다\n","  resized_data = data.repeat(2, axis=0).repeat(2, axis=1)\n","  # resized_data = data.repeat(1, axis=0).repeat(1, axis=1)\n","  # cmapped = plt.cm.Set1(resized_data)[:, :, :3]  # Drop Alpha Channel\n","  \n","  if d_i == 0:\n","    plt.imshow(data)\n","    plt.show()\n","    plt.imshow(resized_data)\n","    plt.show()\n","  # print('resized_data.shape :', resized_data.shape)\n","  # break\n","  temp_x.append(resized_data)\n","\n","re_total_x = np.array(temp_x)\n","print('re_total_x.shape :', re_total_x.shape)\n","# break\n","\n","#         train / test split      #\n","x_train, x_test_, pr_train, pr_test_, ud_train, ud_test_ = train_test_split(re_total_x, total_pr, total_ud, test_size=0.4, shuffle=True, random_state=random_state)\n","x_test, x_val, pr_test, pr_val, ud_test, ud_val = train_test_split(x_test_, pr_test_, ud_test_, test_size=0.5, shuffle=True, random_state=random_state)\n","\n","\n","#         pr label   #\n","y_train = np.where(pr_train > 1, 1, 0)\n","y_test = np.where(pr_test > 1, 1, 0)\n","y_val = np.where(pr_val > 1, 1, 0)\n","\n","#         up label      #\n","# y_train = np.where(ud_train > 1, 1, 0)\n","# y_test = np.where(ud_test > 1, 1, 0)\n","# y_val = np.where(ud_val > 1, 1, 0)\n","\n","print('pr_train[:5] :', pr_train[:5])\n","print('ud_train[:5] :', ud_train[:5])\n","print('y_train[:5] :', y_train[:5])\n","print('y_train.dtype :', y_train.dtype)\n","\n","print('x_train.shape :', x_train.shape)\n","print('x_test.shape :', x_test.shape)\n","print('x_val.shape :', x_val.shape)\n","print('y_train.shape :', y_train.shape)\n","print('y_test.shape :', y_test.shape)\n","print('y_val.shape :', y_val.shape)\n","\n","def class_ratio(in_list):\n","\n","  return in_list / in_list[1]\n","\n","print('np.unique(y_train, return_counts=True :', np.unique(y_train, return_counts=True), class_ratio(np.unique(y_train, return_counts=True)[1]))\n","print('np.unique(y_val, return_counts=True :', np.unique(y_val, return_counts=True), class_ratio(np.unique(y_val, return_counts=True)[1]))\n","print('np.unique(y_test, return_counts=True :', np.unique(y_test, return_counts=True), class_ratio(np.unique(y_test, return_counts=True)[1]))\n","\n","label = y_train.reshape(-1, )\n","class_weights = class_weight.compute_class_weight('balanced', \n","                                                    classes=np.unique(label),\n","                                                    y=label)\n","class_weights = dict(enumerate(class_weights))\n","print('class_weights :', class_weights)\n","\n","# sample_weight = np.ones(shape=(len(y_train),))\n","# sample_weight[(y_train == 1).reshape(-1,)] = 1.5\n","# print('sample_weight[:20] :', sample_weight[:20])\n","\n","\n","print('np.isnan(np.sum(x_train)) :', np.isnan(np.sum(x_train)))\n","print('np.isnan(np.sum(x_val)) :', np.isnan(np.sum(x_val)))\n","print('np.isnan(np.sum(x_test)) :', np.isnan(np.sum(x_test)))\n","\n","print('np.isnan(np.sum(y_train)) :', np.isnan(np.sum(y_train)))\n","print('np.isnan(np.sum(y_val)) :', np.isnan(np.sum(y_val)))\n","print('np.isnan(np.sum(y_test)) :', np.isnan(np.sum(y_test)))\n","\n","y_train_ohe = np_utils.to_categorical(y_train, num_classes)\n","y_val_ohe = np_utils.to_categorical(y_val, num_classes)\n","y_test_ohe = np_utils.to_categorical(y_test, num_classes)\n","print('y_train_ohe.shape :', y_train_ohe.shape)\n","print('y_val_ohe.shape :', y_val_ohe.shape)\n","print('y_test_ohe.shape :', y_test_ohe.shape)\n","\n","datagen = ImageDataGenerator( \n","    rotation_range = 45,\n","    # zoom_range = 0.5,\n","    # shear_range = 0.5,\n","    # horizontal_flip = True,\n","    # vertical_flip = True,\n","    # width_shift_range=0.5,\n","    # height_shift_range=0.5,\n","    # fill_mode = 'nearest'\n","    )\n","\n","valgen = ImageDataGenerator( \n","    )\n","\n","datagen.fit(x_train)\n","valgen.fit(x_val)\n","\n","batch_size = 256\n","\n","for x_batch, _ in datagen.flow(x_train, y_train_ohe, batch_size=9):\n","\n","    plt.suptitle(\"train x_batch\")\n","\n","    for i in range(0, 9): \n","        plt.subplot(330 + 1 + i) \n","        # resized = cv2.resize(x_batch[i].reshape(row, col), (row * 2, col * 10))\n","        # cmapped = plt.cm.Set1(resized)\n","        # plt.imshow(cmapped)\n","        # plt.imshow(x_batch[i].reshape(row, col))\n","        plt.imshow(x_batch[i])\n","        plt.axis('off') \n","    plt.show() \n","    break\n","\n","for x_batch, _ in valgen.flow(x_val, y_val_ohe, batch_size=9):\n","\n","    plt.suptitle(\"val x_batch\")\n","\n","    for i in range(0, 9): \n","        plt.subplot(330 + 1 + i) \n","        # resized = cv2.resize(x_batch[i].reshape(row, col), (row * 2, col * 10))\n","        # cmapped = plt.cm.Set1(resized)\n","        # plt.imshow(cmapped)\n","        # plt.imshow(x_batch[i].reshape(row, col))\n","        plt.imshow(x_batch[i])\n","        plt.axis('off') \n","    plt.show() \n","    break\n","    \n","train_flow = datagen.flow(x_train, y_train_ohe, batch_size=batch_size) \n","val_flow = valgen.flow(x_val, y_val_ohe, batch_size=batch_size) \n","# break\n","\n"],"execution_count":7,"outputs":[{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAEEAAAD7CAYAAAA1pvlWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAX+ElEQVR4nO2da4wkV3XHf6e7+jU9O7Mzu15nbbxry2seKyMWx3YCiSIMeWysBPMhQjhRSCIk8iUJJIoCJB8gUiI5UpSEDxGJpRD4QGweBgURQiCIh5CszYJhHXttsNl4lzG73ud4Hj39rJMP1bf23ls109U9O91tU3+pNX3v1PP0rVPnnv8554qq8pOOwqQvYBqQC4FcCEAuBCAXApALAdimEETkqIh8X0SeFZH3X6uLGjdkVDtBRIrAD4BfApaA48D9qnry2l3eeBBsY9+7gWdV9RSAiDwM3AdsKoRSqaTVajVuiwhB4F6CiGQ6ebfbJQzDuK2qtNttut2us52qXlTV67Y61naEcCPwI6u9BPzMVjtUq1WOHDkSt2u1Grt376ZYLMZ9hUKBQqHgCMMfrWEYsry8zPr6etzXarU4ffo0y8vLzrYbGxunB93IdoSQCSLybuDdAJVKZadPNxK2oxifB26y2q/o9zlQ1QdV9U5VvbNUKm3jdDuH7YyE48BtInIL0c2/A/jNrXYQEWydUC6XCYKAQuHqbxEEAb6wwjB0nv8wDKlUKk6fiCAiiUcnC0YWgqp2ReQPgP8CisBHVfXJLU8WBCwsLMTtYrFIpVJxnv9qtcrMzIzT1263abVa9rnjbQ3W19cpFouOYLJiWzpBVb8IfHGYfWwlWCwW41/QoFAoUCwW49GhqrGytM6buOEgCEYeCbnFSC4EYAyvSBu+cVQqlajVas5QL5VKjp0gIlQqFcrlcrxNGIYJoyoIAorFovO4AXQ6nYHXNXYh2DdTqVTYtWtX4sLtGxQRarVaqhDs57/T6cSCGBZjF4J/g751mGY2b7ZdWjur2W0j1wmMeSRAcqj7fcMeY6u+rBirEIrFYsJYMorQoFQqJQyoMAxRVUcHzMzMOM9/EATMzc2xurrqnHNtbW3gdY1VCEEQsLi4GLdVNWHhzczMMD8/7whhfX3dmTGa7WZmZpxjz83NsbKy4mx39uzZwdc11F1cA2QZtlkV3FaKchjkipEJjAQb/nNu+nxjyGxnb+u/Ms12o2DsQrB1QLvdptFoOH2dToder+fc4OrqKmtra/FNFgoFdu3aRa1Wi7cxwhv7LHJY+Iqw2+3SaDTo9Xpxn3kT2EJYWVlxtH6hUKBcLjtTaTNSpl4I20WW4b4jU2kR+aiInBeRJ6y+RRH5iog80/+7sNUxph1Z3g4fA456fe8HvqqqtwFf7bcHQlXpdrvOp9frxc9yGIZ0u106nY7zMduZj2mnHWcUDHwcVPWbInKz130f8Kb+948DXwfeN+hY3W6XF154IW53Op2EYjSeJRvNZpNms+koRhGh2WzG2zQaDRqNRqaps49RdcL1qmpMsXPA9Vl26vV6jhnb7Xadm4Pka1NV6XQ6tNvtuK9QKLC2tuYoTyOAUUbDto0lja54U20kIu8WkW+LyLdtZ+k0YVQhvCAi+wH6f89vtqHNO0wr+TLq4/B54HeAB/p//z3LTmZoG5hhbg/hXq/nbGMrU4NCoUC9Xk8wWjfccIMzQQP48Y9/PPC6BgpBRB4iUoJ7RWQJ+CDRzX9KRN4FnAbePvBMRDrg0qVLcbvdbrO+vu4IYXV1lcuXL6cSK/FFB0FCTywsLPDOd76TQ4cOOee8++67B15XlrfD/Zv86y0Dj548lkOitNttms2mc8Nra2tcuXLF6fMdqMVikUajga9jDh06xF133TXsZeWzSMiFAIx57tDr9Zz4gV6vR7vdTtj7tVrN6Uuj6lqtFi+++GLcVywWOXHihKMnsmKsQmi1Wjz77LNXTx4EVCqVBCu9b98+56bN28EWzJUrV7hw4ULcLpVKnDt3jtnZ2aGva6xCCMOQjY2NuF0qlRzy1fSVy+UtHSZmjmEfq9VqsbS0lKD1syDXCeRCAMb8OBQKBccblEbIVioVSqWS8ziYYA6DMAzp9XoJ5Wker2ExdvJlfn4+bpdKJWZmZgbqBJ9oNY5Y/4bTnK9ZMHb3mn3DxhK0+8yNpEWvGBgB2PuZ/pyQHRFTy0CNE2PXCXNzc1dPHgRUq1VnWJfLZYdPAGJfo4GqJvYLw5D19fVEWG8WTJyV9pVgrVZjdnY27lNVlpeXE264mZkZxzrsdDqxL3JYTDxSZbM+O4TP/mtvZ4+E7TxCuWIkG/lyk4h8TUROisiTIvKefv81IWBsstX+2FyE35e2ff+aEp8sGJj00Xek7lfVx0RkF/Ad4G3A7wKXVfWBftbLgqpuyT3Mzs7q7bffvuX5/FB/VU0lZOfn550gDVVlY2MjoRgff/zx76jqnVudM4t77Sxwtv99VUSeIsp1GJqAMU5TA+NUtX8Iw0DZfRsbG86MsVAoxGE9BiLC7OxswoDKgqEUY5+Jej1wjIwEjJ3vYMcibgWffEn7fi2RWWwiMgs8ArxXVZ3AoK0IGJt38FN9pgWZrkpESkQC+ISqfrbf/YKI7FfVs4MIGANVdWIRDMHqO0zSqLj+dTjb+VzEjk2gJDrqvwBPqerfWf8amoDpdDqcO3cubqeF66RFm/htVeXFF1+k0WhcvZEgYH5+3pmqZ0WWkfBzwG8D/ysi3+v3/TkjEDC9Xs9xjgKpiswXTNqMsdFoONsZRbkjilFVvwVsNsaGJmCmEbnFyIRjlnz738B/HNIMKEjXFWNNBLsWKBaL1Gq1RMyif3NBEMQ5TmabZrPpEC0ikmC0s2LiI8FPCUx7O5RKJUcIYRimvg6NA3ZY5DqBXAjABJwqfnK4P/T9ZDEz5O1hbiJe7e3MsaZeJxQKhcT015/6VqtVx70GV8PzbJTLZWcWaQJA7NlmVkyUd7CdJAbmF/ZdbvZIMA4TeySEYUir1RrJ0ZrrBHIhABPOd0iLVCmXy3S7Xcfb7PMOaY9M2nQ7K8ae72BbeSbQwhaCsSJtITSbTUcxSj871jeMCoVCokZLFox9JPgOFF8xbuZP8L3Kvnk9ao4l5DoByMY7VEXkf0TkRJ93+Mt+/y0ickyiQlOfFJFsXtQpRJbHoQW8WVXX+r7Gb4nIfwJ/Avy9qj4sIv8EvAv4yFYH8mObVTURaOVPliA9ZsGPZjN5UaMgi2dJAZOkUOp/FHgzVwvJfBz4EAOEYAya+OT9ED775iqVijOzVNU4UsUvFWAr2bSAj6zItIeIFPv+xfPAV4AfAsuqasyzJSJCJm3fON/B/5+ZA2xFn6WRtfb+aXTcsMgkBFXtqeoRolpKdwOvznoCm3cY6QrHgKHGjqouA18D3gDsFhHzOKUWmsp4zFF2u6bI8na4TkR297/XiKruPUUkjN/obzZU4of9sbPa7Iw2e+inPSImmMt8RMQJ1BgmYCPL22E/8HGJSg8WgE+p6hdE5CTwsIj8FfBdIoImkxAMjDvMZ5Y2C9zw4ff55nVWZHk7PE5Ewvr9p4j0w1RhlMcrtxiZAm9z1uo6vp2QNscYNW5pokIol8ssLi46Mz/fBS8izM/PO6F/YRhy6dIlJ9FU+rHNvrHkc59pmKgQSqUSc3NzjumcRr7Mzs46bHOv16PRaKQKwTfDswgh1wnkQgAmXHvNzBhtneDPAWyuwjaegiBwjmW8SjseuLVdVCoVbr755rhdq9USirFYLCZcZK1Wi1arFQunWCyyd+9edu3aFW9jpulTX07Ej22uVCrU63WHlfKTPlSVlZWVBANdr9edQPAwDEeun5DrBHIhAFNQbCqtHLlxtph2q9VKzAjTvEijxidMNPOl2+0mCsb5ITwmXM9OLy4UCiwuLjqK0bjufOI2CyYqhM0IWRsmcNsmaYrFYuovboJDh0WuExgutrkoIt8VkS/02z9RvIPBe4jcamY69zcMyTuEYejUUMvqJW42m07cgeEn7UKVYRgSBEEiiSwLMq30ISKvIOIW/pqIdPl14ALwUxrVdH8D8CFV/ZWtjhMEgdpT4v6xtzy38UPageCFQiFRxthU/fRjmx999NHtJ3308Q/AnwFGHe9hCN6Bfr6DiDjky2a+Qx9ppK1vGapqnAwyLLJ4m38NOK+q3xn66Li8w6ien51G1ij3t4rIvUCVSCd8mD7v0B8NI/MO04As3uYPAB8AEJE3AX+qqr8lIp8m4h0eJiPvkJbMkRbH7GfDG71gxyekRbRubGyMPVLlfYzAO/jxiL5xk+Yi8yNc7PBeA2N97nhhGVX9OlHW2zXhHbZ6PdpzB3+btFeraU+kCt/LARNPTzPxR1v9P+tz/pLId/B1gB+VCpG3yV/4xiR92DdoiFyDMAxpNpsvjam0fZHGn2hPnavVKvV63YlUMcyzEYKqsr6+7vgYjNCmvrBMGvyhvlW0yk4ZW7liZMJCyPLL7vQogAkXm0pTjEZP2Dft+xNVo5oqaZbl1L8dRGRgGq8dfmOQtiKYqcxlkOc7kNdU2TYmHqmS1me73Y2zZLNXqcF2QgEn6nJPU4K1Wi1Rmc+3NA356vsdXxIZsmlGkF920BCy9kgw02vbYvRjlkxfzjuMiKzlRJ4DVoEe0O37CxeBTwI3A88Bb1fVKztzmTuLYUbCPap6xHJfj7TQhXnnG2WXFqW+WcS7DaM87WNNIoRv6DpLvsWY5l6z8xtMO20pRb9a56i5DpBdCAp8WUQU+GdVfZAR6iz58Ulp1XXAtRBtmt7fxsaoS51AdiH8vKo+LyL7gK+IyNP2P1VV+wJKoC+wBwEqlcrk4/pTkDXp4/n+3/PA54gcrJkXuph2ZGGg6hIVnkNE6sAvA09wtc4SDJHvMAi219j+2Kv/pD1C/evLpFR9ZHkcrgc+1z9gAPybqn5JRI4zZJ2lLHkMJoTH1glmHQgbZnVy+1hp4X/XZCHdPr/wupT+S2yzztJmv5T9ukyzDjfzUI/6mswtRnIhABPgHeyZXxAELCwsOEPYlCK2fYumCqf9iDQaDUdPhGFItVpN6AR7oZ3NMPb1HewLX1hY4MCBA05tFBOzaJvQN9xwA3v27IkF0+l0ePLJJ51chkKhwO7duxNk7tQJAUgoODN1NvCz48w2dmUun4wxSHs7ZEGuE5gCIUxDCM/Y8x1e+cpXxu1du3axuLjoPMe1Wo1qteoM9XK57ARfhGHI3r17E5Mov5xAVoxVCDMzMxw5ciRupy1yYaxD+2ZM+q+BiLB//35uuukmZ5uzZ89mWlHcx9h9jH7632ZES1q8s91nlmg32MwrnQUT1wnTgInnO2zmNEn79X1ydjtcg42xG0t2PLJxt/v12HyF57vlzXZ2dGyr1WJlZSVTMqiPsQvBTsow/ILvK/T1hK9LTA6EPU1uNpusra1NvxA2gx+KN8p+w+5rI2thmd0i8hkReVpEnhKRN8jLaIHtrG+HDwNfUtVXEzlYnmJE3sF3ffm8Q5p7zZ49bsZNbMfyzFLLfR74BaJFLVDVNtAWkZF4h3q9HrdVNbEEatrNpIX6+ryDOf5OWYy3ECV4/KuIvI5opY/3MMIC2z750ul0EgvppoXcGJbaZ7RtIdh+yWGRZa8AuAP4iKq+HljHG/oaXXXqTyBWsalR6qeOA1mEsAQsqeqxfvszRELJxDvYSR+j5CeNAwOFoKrngB+JyKv6XW8BTjIi7+ArQF+52SSrT7YOUqo+N5E1ViGrnfCHwCckSvs7Bfwe/ZpLw/AOvo/RTHps+N4h41nyFaO/0ken09lZY0lVvwekZZSNtMC2/d03m/3ZoSFV0ghYP+Dbn3JnRT6LJBcCMAXrRfrDOq0kSBAECZ3gW4mmpsooeZETLWNu6jbbN+3zDhDNNu3ptIiwsLDgLJVYLpeZnZ1l9+7dzvmmsthU2kiwX2WdTidRy9l/3RWLxbhan93ncxhZkesEciEAEyBk7flDGk9gFJzvWfYVYbvddlx1zWYz1TWXBWMVQqfT4eLFi3E7rfa6WVzXRporfWVlxanHYsqbTv3bwQRmG6T5EyEZnpfmNLHrucLVUZXzDiMiFwIToOGyrDWfxjv4lNvGxkaiKke73Z7+OktpXGQaseKjXq9Tr9fj7cIw5MKFC4mp9MbGhlO4JiumgncYJARD0PhzBR87VjpARF4lIt+zPisi8t6fKN5BVb/fz3M4Avw00CCKb94275DmTktzpZnt/O/+cdNcc1kw7OPwFuCHqnp6FN7BL0p53XXXcfjwYcc4unjxIktLS05N94MHD3LgwAEneu3EiRM899xzzrFf85rXJCZQDzzwwMCbGlYI7wAe6n8fmncIgoA9e/bE7dtuu417772X+fn5uO/kyZM8+uijcXhOoVDgyJEj3HHHHfEva4rV26uOz83Ncc8993Dw4EHnnNdUCH0n61vpV9qxsVW+g1hJH3Nzc1s6QkyWi/0qNcParrNgXqu2JWkKTe30VPpXgcdU9YV++2XDOwzzONzP1UcBRlhX2l+cxtgMfjSKnQO5mYLzvdJBENDr9RLcZhZkTQmsEy1u8ftW99DrSs/NzXH06NG4bXSBPb1eXFzkrrvuchTj9ddfn0ggfe1rX8u+ffvivna7zdNPP82xY8cYFll5h3WionN239D5DrOzs7zxjW+M28bKs3+9ubm5xE37KBaL3Hrrrdx6661x3/nz5/nGN77BY489NswlAVPgY9xsm1GmxDsaqfJyx0QXyIPkYhWGY0yzCP32oG2yYqLFpjqdDq1Wy+lrNpsOV2C4SDsgQ0SYnZ118iSMbfGSqLNkjwRTYdOMBCOkTqfjbOevF2ciXvwl1Xx/RVbkOoFcCMCEvc2mjrutE7rdrrOurMjV8gIGhUKBdrvt6ARTXmSUEkNjFUK323USs7rdLo1Gw7lBs6CNbTH6bLNZTdxOBLl8+TLLy8tOzfesGPtIsK1D88vZQmi32wlCtlQqOb9woVCg2Ww68xCzGsgo5UlznUAuBGACof7+4+AHZKytrXHp0iVHJ/jGUrFYjAM3DDqdDgsLC9x4Y2oh8S0xViH0ej0uX74ct83i2rbZvLS0xDPPPOPoAN8cLpfLtFotZ792u83hw4c5cOCAs+0jjzwy8Lom/or0Y5SazSarq6tbvurK5TIbGxuOEuz1etTr9Z1zr4nIH0u0pvQTIvKQRGtN3yIvk/UdspAvNwJ/BNypqrcDRSKvs1nf4RBwhWh9h5cksj4OAVATkQ4wA5xlhHWlW60Wp0+fjtvGJ2g/DsbY8ZdLtCEiXLlyhTNnzjjHWl1d3Zky5v3SQn8LnAE2gC8T5TxkWt/BRqPR4Pjx43E7bZ0G28Xu99k4c+aMIwRTy31HHK19jvE+ouSPZeDTwNEtd3L3j3mHer3uOFX9IG2IrEM/XMd3oBgFa+/b6/XY2NjYsdjmXwT+T1UvqGoH+CzRmg+Z1pW2eYdB9VknhSxCOAP8rIjMSPRzmHyHkdaVHgXXKhN2M2TRCcdE5DPAY0CXaC2HB4H/YMj1HQyxYmCCr/zoVT9NyNcJ5vGw9UYYhqytre1ckIaqfhD4oNc99PoOvhBEJLE2nDGgbKQJoV6vO/6EXq/HysoKFy5cGOaSgAnzDpt5h9MyXzcrLuVvly9yMSIm6m22XWg2jEvNbqeNoFFzrH2MfSrtrwXt36BfWA6SiaDmWH5ss/areg6LiZYOMHHMaZrfbqetOLy6uuq8RYy5PPXky2YKzseodNqoyBUjEw7rzUrB+68+O4rFPpb9v2Ew9kUuZmZm4nYad2huxr6pZrOZmCKXy2VnDVmjM14StdxtKy9NR6QpRpPlYo+ASqXizDaN+Z3Xch8RuRCY8GJYaUsRpClLP9XHDHvfqRIEwUje5rHzDnYUiole8wtVpgVa+HpidXXVcaX1ej0WFhYcxQtw6tSpgdc1UQaq3W47DDSkC8HPfDHK0h9F1Wp14OpCach1ArkQgFwIANkW175mJxO5QFSi6OKgbbeBvd7xD6rqdVte1ziFACAi39YBK36P+/j540AuBGAyQnhw2o4/dp0wjcgfB8YsBBE5KiLf70e3ZEomHXC8j4rIeRF5wuobOnN3bEIQkSLwj0RZdYeB+0Xk8DYP+zGSYQJDZ+6OcyTcDTyrqqf61T0fJop7GBmq+k3gstd9H1HkDP2/bxt0nHEK4UbgR1Y7U3TLCBi+YugOXMTUYKuKoTbGKYTngZus9qbRLdvE0CuVjVMIx4Hb+vGPZaIwwM/vwHmGrxjql/jcyQ9wL/AD4IfAX1yD4z1EFE7YIdIx7yJKYv0q8Azw38DioOPkFiMvc8WYFbkQyIUA5EIAciEAuRCAXAhALgQA/h/jbto2m6QZXAAAAABJRU5ErkJggg==\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}},{"output_type":"stream","text":["re_total_x.shape : (316770, 90, 12, 3)\n","pr_train[:5] : [[0.98122984]\n"," [0.9547001 ]\n"," [1.0308641 ]\n"," [1.0407867 ]\n"," [0.95465577]]\n","ud_train[:5] : [[0.9964743 ]\n"," [0.99160945]\n"," [1.0065402 ]\n"," [1.0090867 ]\n"," [0.9918472 ]]\n","y_train[:5] : [[0]\n"," [0]\n"," [1]\n"," [1]\n"," [0]]\n","y_train.dtype : int64\n","x_train.shape : (190062, 90, 12, 3)\n","x_test.shape : (63354, 90, 12, 3)\n","x_val.shape : (63354, 90, 12, 3)\n","y_train.shape : (190062, 1)\n","y_test.shape : (63354, 1)\n","y_val.shape : (63354, 1)\n","np.unique(y_train, return_counts=True : (array([0, 1]), array([101886,  88176])) [1.15548449 1.        ]\n","np.unique(y_val, return_counts=True : (array([0, 1]), array([34177, 29177])) [1.17136786 1.        ]\n","np.unique(y_test, return_counts=True : (array([0, 1]), array([34033, 29321])) [1.16070393 1.        ]\n","class_weights : {0: 0.9327189211471645, 1: 1.077742242787153}\n","np.isnan(np.sum(x_train)) : False\n","np.isnan(np.sum(x_val)) : False\n","np.isnan(np.sum(x_test)) : False\n","np.isnan(np.sum(y_train)) : False\n","np.isnan(np.sum(y_val)) : False\n","np.isnan(np.sum(y_test)) : False\n","y_train_ohe.shape : (190062, 2)\n","y_val_ohe.shape : (63354, 2)\n","y_test_ohe.shape : (63354, 2)\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 9 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 9 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"markdown","metadata":{"id":"s-W6LL5c2VN2"},"source":["### **Training**"]},{"cell_type":"code","metadata":{"id":"EkVg1hVI2TNP","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"error","timestamp":1618538396523,"user_tz":-540,"elapsed":49833631,"user":{"displayName":"7th June","photoUrl":"","userId":"08178289703395036410"}},"outputId":"8dfe2b84-74de-4430-a9d1-07c48d081e67"},"source":["(_, row, col, _) = x_train.shape\n","\n","# model = FER_Model(input_shape=(row, col, 3))\n","# opt = Adam(lr=0.00001, decay=0.000005)\n","# model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n","\n","# model_name = \"classifier_45_min_pr_re2.h5\"  # <-- specifying model name\n","ckpt_path = current_path + 'ckpt/'\n","model_name = 'classifier_%s_ma7_pr3.h5' % period\n","\n","model = keras.models.load_model(ckpt_path + model_name)\n","  \n","from keras.callbacks import ModelCheckpoint, EarlyStopping, TensorBoard\n","\n","ckpt_path = current_path + 'ckpt/'\n","board_path = current_path + 'graph/'\n","model_name = 'classifier_%s_ma7_pr3.h5' % period\n","\n","checkpoint = ModelCheckpoint(ckpt_path + model_name, monitor='val_loss', verbose=1, save_best_only=True, mode='auto')\n","checkpoint2 = TensorBoard(log_dir=board_path,\n","                          histogram_freq=0,\n","                          write_graph=True,\n","                          write_images=True)\n","checkpoint3 = EarlyStopping(monitor='val_loss', patience=40)\n","# callbacks_list = [checkpoint, checkpoint2, checkpoint3]\n","callbacks_list = [checkpoint, checkpoint2]\n","\n","# keras.callbacks.Callback 로 부터 log 를 받아와 history log 를 작성할 수 있다.\n","\n","# we iterate 200 times over the entire training set\n","num_epochs = 1000\n","history = model.fit_generator(train_flow, \n","                    steps_per_epoch=len(x_train) / batch_size, \n","                    epochs=num_epochs,  \n","                    verbose=2,  \n","                    callbacks=callbacks_list,\n","                    class_weight=class_weights,\n","                    validation_data=val_flow,  \n","                    validation_steps=len(x_val) / batch_size,\n","                    shuffle=False)"],"execution_count":8,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n","Instructions for updating:\n","If using Keras pass *_constraint arguments to layers.\n","WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n","\n","WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/keras/callbacks/tensorboard_v1.py:200: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.\n","\n","WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/keras/callbacks/tensorboard_v1.py:203: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n","\n","Epoch 1/1000\n"," - 80s - loss: 0.4776 - accuracy: 0.7632 - val_loss: 0.5329 - val_accuracy: 0.7416\n","\n","Epoch 00001: val_loss improved from inf to 0.53289, saving model to /content/drive/My Drive/Colab Notebooks/Project_Stock/ckpt/classifier_45_ma7_pr3.h5\n","WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/keras/callbacks/tensorboard_v1.py:343: The name tf.Summary is deprecated. Please use tf.compat.v1.Summary instead.\n","\n","Epoch 2/1000\n"," - 77s - loss: 0.4775 - accuracy: 0.7648 - val_loss: 0.6420 - val_accuracy: 0.7452\n","\n","Epoch 00002: val_loss did not improve from 0.53289\n","Epoch 3/1000\n"," - 76s - loss: 0.4773 - accuracy: 0.7635 - val_loss: 0.6326 - val_accuracy: 0.7438\n","\n","Epoch 00003: val_loss did not improve from 0.53289\n","Epoch 4/1000\n"," - 76s - loss: 0.4757 - accuracy: 0.7656 - val_loss: 0.6213 - val_accuracy: 0.7387\n","\n","Epoch 00004: val_loss did not improve from 0.53289\n","Epoch 5/1000\n"," - 76s - loss: 0.4772 - accuracy: 0.7649 - val_loss: 0.5607 - val_accuracy: 0.7408\n","\n","Epoch 00005: val_loss did not improve from 0.53289\n","Epoch 6/1000\n"," - 76s - loss: 0.4770 - accuracy: 0.7650 - val_loss: 0.6229 - val_accuracy: 0.7445\n","\n","Epoch 00006: val_loss did not improve from 0.53289\n","Epoch 7/1000\n"," - 76s - loss: 0.4752 - accuracy: 0.7657 - val_loss: 0.4934 - val_accuracy: 0.7417\n","\n","Epoch 00007: val_loss improved from 0.53289 to 0.49336, saving model to /content/drive/My Drive/Colab Notebooks/Project_Stock/ckpt/classifier_45_ma7_pr3.h5\n","Epoch 8/1000\n"," - 77s - loss: 0.4752 - accuracy: 0.7664 - val_loss: 0.4856 - val_accuracy: 0.7418\n","\n","Epoch 00008: val_loss improved from 0.49336 to 0.48557, saving model to /content/drive/My Drive/Colab Notebooks/Project_Stock/ckpt/classifier_45_ma7_pr3.h5\n","Epoch 9/1000\n"," - 77s - loss: 0.4753 - accuracy: 0.7655 - val_loss: 0.5434 - val_accuracy: 0.7406\n","\n","Epoch 00009: val_loss did not improve from 0.48557\n","Epoch 10/1000\n"," - 76s - loss: 0.4748 - accuracy: 0.7667 - val_loss: 0.5966 - val_accuracy: 0.7443\n","\n","Epoch 00010: val_loss did not improve from 0.48557\n","Epoch 11/1000\n"," - 76s - loss: 0.4742 - accuracy: 0.7672 - val_loss: 0.5373 - val_accuracy: 0.7419\n","\n","Epoch 00011: val_loss did not improve from 0.48557\n","Epoch 12/1000\n"," - 76s - loss: 0.4737 - accuracy: 0.7666 - val_loss: 0.5285 - val_accuracy: 0.7442\n","\n","Epoch 00012: val_loss did not improve from 0.48557\n","Epoch 13/1000\n"," - 76s - loss: 0.4724 - accuracy: 0.7674 - val_loss: 0.6362 - val_accuracy: 0.7389\n","\n","Epoch 00013: val_loss did not improve from 0.48557\n","Epoch 14/1000\n"," - 78s - loss: 0.4722 - accuracy: 0.7678 - val_loss: 0.5429 - val_accuracy: 0.7414\n","\n","Epoch 00014: val_loss did not improve from 0.48557\n","Epoch 15/1000\n"," - 77s - loss: 0.4717 - accuracy: 0.7679 - val_loss: 0.5163 - val_accuracy: 0.7422\n","\n","Epoch 00015: val_loss did not improve from 0.48557\n","Epoch 16/1000\n"," - 78s - loss: 0.4722 - accuracy: 0.7684 - val_loss: 0.4690 - val_accuracy: 0.7437\n","\n","Epoch 00016: val_loss improved from 0.48557 to 0.46900, saving model to /content/drive/My Drive/Colab Notebooks/Project_Stock/ckpt/classifier_45_ma7_pr3.h5\n","Epoch 17/1000\n"," - 79s - loss: 0.4726 - accuracy: 0.7673 - val_loss: 0.5257 - val_accuracy: 0.7445\n","\n","Epoch 00017: val_loss did not improve from 0.46900\n","Epoch 18/1000\n"," - 77s - loss: 0.4690 - accuracy: 0.7704 - val_loss: 0.5738 - val_accuracy: 0.7402\n","\n","Epoch 00018: val_loss did not improve from 0.46900\n","Epoch 19/1000\n"," - 77s - loss: 0.4721 - accuracy: 0.7678 - val_loss: 0.6862 - val_accuracy: 0.7435\n","\n","Epoch 00019: val_loss did not improve from 0.46900\n","Epoch 20/1000\n"," - 77s - loss: 0.4693 - accuracy: 0.7697 - val_loss: 0.5874 - val_accuracy: 0.7431\n","\n","Epoch 00020: val_loss did not improve from 0.46900\n","Epoch 21/1000\n"," - 77s - loss: 0.4699 - accuracy: 0.7693 - val_loss: 0.4862 - val_accuracy: 0.7421\n","\n","Epoch 00021: val_loss did not improve from 0.46900\n","Epoch 22/1000\n"," - 77s - loss: 0.4704 - accuracy: 0.7686 - val_loss: 0.5135 - val_accuracy: 0.7448\n","\n","Epoch 00022: val_loss did not improve from 0.46900\n","Epoch 23/1000\n"," - 76s - loss: 0.4699 - accuracy: 0.7695 - val_loss: 0.5141 - val_accuracy: 0.7416\n","\n","Epoch 00023: val_loss did not improve from 0.46900\n","Epoch 24/1000\n"," - 76s - loss: 0.4684 - accuracy: 0.7705 - val_loss: 0.6086 - val_accuracy: 0.7442\n","\n","Epoch 00024: val_loss did not improve from 0.46900\n","Epoch 25/1000\n"," - 76s - loss: 0.4675 - accuracy: 0.7708 - val_loss: 0.5784 - val_accuracy: 0.7381\n","\n","Epoch 00025: val_loss did not improve from 0.46900\n","Epoch 26/1000\n"," - 76s - loss: 0.4679 - accuracy: 0.7712 - val_loss: 0.4995 - val_accuracy: 0.7428\n","\n","Epoch 00026: val_loss did not improve from 0.46900\n","Epoch 27/1000\n"," - 76s - loss: 0.4688 - accuracy: 0.7707 - val_loss: 0.5691 - val_accuracy: 0.7434\n","\n","Epoch 00027: val_loss did not improve from 0.46900\n","Epoch 28/1000\n"," - 76s - loss: 0.4683 - accuracy: 0.7706 - val_loss: 0.5255 - val_accuracy: 0.7434\n","\n","Epoch 00028: val_loss did not improve from 0.46900\n","Epoch 29/1000\n"," - 76s - loss: 0.4673 - accuracy: 0.7719 - val_loss: 0.4976 - val_accuracy: 0.7431\n","\n","Epoch 00029: val_loss did not improve from 0.46900\n","Epoch 30/1000\n"," - 76s - loss: 0.4662 - accuracy: 0.7730 - val_loss: 0.4307 - val_accuracy: 0.7445\n","\n","Epoch 00030: val_loss improved from 0.46900 to 0.43069, saving model to /content/drive/My Drive/Colab Notebooks/Project_Stock/ckpt/classifier_45_ma7_pr3.h5\n","Epoch 31/1000\n"," - 77s - loss: 0.4670 - accuracy: 0.7715 - val_loss: 0.4962 - val_accuracy: 0.7423\n","\n","Epoch 00031: val_loss did not improve from 0.43069\n","Epoch 32/1000\n"," - 77s - loss: 0.4674 - accuracy: 0.7728 - val_loss: 0.4934 - val_accuracy: 0.7452\n","\n","Epoch 00032: val_loss did not improve from 0.43069\n","Epoch 33/1000\n"," - 76s - loss: 0.4675 - accuracy: 0.7722 - val_loss: 0.5650 - val_accuracy: 0.7442\n","\n","Epoch 00033: val_loss did not improve from 0.43069\n","Epoch 34/1000\n"," - 76s - loss: 0.4653 - accuracy: 0.7734 - val_loss: 0.5086 - val_accuracy: 0.7433\n","\n","Epoch 00034: val_loss did not improve from 0.43069\n","Epoch 35/1000\n"," - 78s - loss: 0.4652 - accuracy: 0.7728 - val_loss: 0.6060 - val_accuracy: 0.7448\n","\n","Epoch 00035: val_loss did not improve from 0.43069\n","Epoch 36/1000\n"," - 77s - loss: 0.4648 - accuracy: 0.7719 - val_loss: 0.6232 - val_accuracy: 0.7432\n","\n","Epoch 00036: val_loss did not improve from 0.43069\n","Epoch 37/1000\n"," - 76s - loss: 0.4652 - accuracy: 0.7723 - val_loss: 0.6445 - val_accuracy: 0.7444\n","\n","Epoch 00037: val_loss did not improve from 0.43069\n","Epoch 38/1000\n"," - 76s - loss: 0.4636 - accuracy: 0.7748 - val_loss: 0.5338 - val_accuracy: 0.7410\n","\n","Epoch 00038: val_loss did not improve from 0.43069\n","Epoch 39/1000\n"," - 76s - loss: 0.4634 - accuracy: 0.7739 - val_loss: 0.5413 - val_accuracy: 0.7423\n","\n","Epoch 00039: val_loss did not improve from 0.43069\n","Epoch 40/1000\n"," - 76s - loss: 0.4640 - accuracy: 0.7743 - val_loss: 0.6117 - val_accuracy: 0.7436\n","\n","Epoch 00040: val_loss did not improve from 0.43069\n","Epoch 41/1000\n"," - 76s - loss: 0.4642 - accuracy: 0.7745 - val_loss: 0.5393 - val_accuracy: 0.7389\n","\n","Epoch 00041: val_loss did not improve from 0.43069\n","Epoch 42/1000\n"," - 76s - loss: 0.4626 - accuracy: 0.7744 - val_loss: 0.6096 - val_accuracy: 0.7439\n","\n","Epoch 00042: val_loss did not improve from 0.43069\n","Epoch 43/1000\n"," - 76s - loss: 0.4623 - accuracy: 0.7744 - val_loss: 0.5557 - val_accuracy: 0.7433\n","\n","Epoch 00043: val_loss did not improve from 0.43069\n","Epoch 44/1000\n"," - 76s - loss: 0.4612 - accuracy: 0.7758 - val_loss: 0.5765 - val_accuracy: 0.7434\n","\n","Epoch 00044: val_loss did not improve from 0.43069\n","Epoch 45/1000\n"," - 76s - loss: 0.4631 - accuracy: 0.7735 - val_loss: 0.5115 - val_accuracy: 0.7429\n","\n","Epoch 00045: val_loss did not improve from 0.43069\n","Epoch 46/1000\n"," - 76s - loss: 0.4626 - accuracy: 0.7737 - val_loss: 0.4925 - val_accuracy: 0.7392\n","\n","Epoch 00046: val_loss did not improve from 0.43069\n","Epoch 47/1000\n"," - 76s - loss: 0.4617 - accuracy: 0.7757 - val_loss: 0.5050 - val_accuracy: 0.7438\n","\n","Epoch 00047: val_loss did not improve from 0.43069\n","Epoch 48/1000\n"," - 76s - loss: 0.4609 - accuracy: 0.7753 - val_loss: 0.5749 - val_accuracy: 0.7440\n","\n","Epoch 00048: val_loss did not improve from 0.43069\n","Epoch 49/1000\n"," - 76s - loss: 0.4612 - accuracy: 0.7756 - val_loss: 0.5180 - val_accuracy: 0.7471\n","\n","Epoch 00049: val_loss did not improve from 0.43069\n","Epoch 50/1000\n"," - 76s - loss: 0.4614 - accuracy: 0.7753 - val_loss: 0.4733 - val_accuracy: 0.7474\n","\n","Epoch 00050: val_loss did not improve from 0.43069\n","Epoch 51/1000\n"," - 75s - loss: 0.4614 - accuracy: 0.7747 - val_loss: 0.6427 - val_accuracy: 0.7420\n","\n","Epoch 00051: val_loss did not improve from 0.43069\n","Epoch 52/1000\n"," - 76s - loss: 0.4607 - accuracy: 0.7746 - val_loss: 0.5835 - val_accuracy: 0.7479\n","\n","Epoch 00052: val_loss did not improve from 0.43069\n","Epoch 53/1000\n"," - 76s - loss: 0.4590 - accuracy: 0.7766 - val_loss: 0.5527 - val_accuracy: 0.7471\n","\n","Epoch 00053: val_loss did not improve from 0.43069\n","Epoch 54/1000\n"," - 76s - loss: 0.4596 - accuracy: 0.7756 - val_loss: 0.4801 - val_accuracy: 0.7474\n","\n","Epoch 00054: val_loss did not improve from 0.43069\n","Epoch 55/1000\n"," - 75s - loss: 0.4601 - accuracy: 0.7755 - val_loss: 0.6354 - val_accuracy: 0.7424\n","\n","Epoch 00055: val_loss did not improve from 0.43069\n","Epoch 56/1000\n"," - 76s - loss: 0.4588 - accuracy: 0.7765 - val_loss: 0.5382 - val_accuracy: 0.7456\n","\n","Epoch 00056: val_loss did not improve from 0.43069\n","Epoch 57/1000\n"," - 76s - loss: 0.4575 - accuracy: 0.7780 - val_loss: 0.5495 - val_accuracy: 0.7369\n","\n","Epoch 00057: val_loss did not improve from 0.43069\n","Epoch 58/1000\n"," - 76s - loss: 0.4591 - accuracy: 0.7773 - val_loss: 0.6665 - val_accuracy: 0.7423\n","\n","Epoch 00058: val_loss did not improve from 0.43069\n","Epoch 59/1000\n"," - 75s - loss: 0.4568 - accuracy: 0.7781 - val_loss: 0.5147 - val_accuracy: 0.7437\n","\n","Epoch 00059: val_loss did not improve from 0.43069\n","Epoch 60/1000\n"," - 76s - loss: 0.4569 - accuracy: 0.7783 - val_loss: 0.4664 - val_accuracy: 0.7439\n","\n","Epoch 00060: val_loss did not improve from 0.43069\n","Epoch 61/1000\n"," - 76s - loss: 0.4562 - accuracy: 0.7788 - val_loss: 0.6179 - val_accuracy: 0.7420\n","\n","Epoch 00061: val_loss did not improve from 0.43069\n","Epoch 62/1000\n"," - 76s - loss: 0.4577 - accuracy: 0.7773 - val_loss: 0.6238 - val_accuracy: 0.7435\n","\n","Epoch 00062: val_loss did not improve from 0.43069\n","Epoch 63/1000\n"," - 76s - loss: 0.4566 - accuracy: 0.7786 - val_loss: 0.4854 - val_accuracy: 0.7445\n","\n","Epoch 00063: val_loss did not improve from 0.43069\n","Epoch 64/1000\n"," - 76s - loss: 0.4553 - accuracy: 0.7789 - val_loss: 0.5642 - val_accuracy: 0.7464\n","\n","Epoch 00064: val_loss did not improve from 0.43069\n","Epoch 65/1000\n"," - 76s - loss: 0.4543 - accuracy: 0.7797 - val_loss: 0.6114 - val_accuracy: 0.7447\n","\n","Epoch 00065: val_loss did not improve from 0.43069\n","Epoch 66/1000\n"," - 75s - loss: 0.4567 - accuracy: 0.7780 - val_loss: 0.4377 - val_accuracy: 0.7435\n","\n","Epoch 00066: val_loss did not improve from 0.43069\n","Epoch 67/1000\n"," - 75s - loss: 0.4550 - accuracy: 0.7800 - val_loss: 0.5981 - val_accuracy: 0.7398\n","\n","Epoch 00067: val_loss did not improve from 0.43069\n","Epoch 68/1000\n"," - 76s - loss: 0.4545 - accuracy: 0.7801 - val_loss: 0.5655 - val_accuracy: 0.7454\n","\n","Epoch 00068: val_loss did not improve from 0.43069\n","Epoch 69/1000\n"," - 76s - loss: 0.4547 - accuracy: 0.7794 - val_loss: 0.6499 - val_accuracy: 0.7428\n","\n","Epoch 00069: val_loss did not improve from 0.43069\n","Epoch 70/1000\n"," - 76s - loss: 0.4546 - accuracy: 0.7801 - val_loss: 0.6714 - val_accuracy: 0.7442\n","\n","Epoch 00070: val_loss did not improve from 0.43069\n","Epoch 71/1000\n"," - 76s - loss: 0.4531 - accuracy: 0.7803 - val_loss: 0.5293 - val_accuracy: 0.7460\n","\n","Epoch 00071: val_loss did not improve from 0.43069\n","Epoch 72/1000\n"," - 76s - loss: 0.4525 - accuracy: 0.7805 - val_loss: 0.5329 - val_accuracy: 0.7442\n","\n","Epoch 00072: val_loss did not improve from 0.43069\n","Epoch 73/1000\n"," - 76s - loss: 0.4535 - accuracy: 0.7804 - val_loss: 0.5412 - val_accuracy: 0.7474\n","\n","Epoch 00073: val_loss did not improve from 0.43069\n","Epoch 74/1000\n"," - 76s - loss: 0.4527 - accuracy: 0.7806 - val_loss: 0.5046 - val_accuracy: 0.7457\n","\n","Epoch 00074: val_loss did not improve from 0.43069\n","Epoch 75/1000\n"," - 76s - loss: 0.4526 - accuracy: 0.7817 - val_loss: 0.4386 - val_accuracy: 0.7429\n","\n","Epoch 00075: val_loss did not improve from 0.43069\n","Epoch 76/1000\n"," - 75s - loss: 0.4514 - accuracy: 0.7826 - val_loss: 0.6845 - val_accuracy: 0.7467\n","\n","Epoch 00076: val_loss did not improve from 0.43069\n","Epoch 77/1000\n"," - 76s - loss: 0.4521 - accuracy: 0.7798 - val_loss: 0.5894 - val_accuracy: 0.7476\n","\n","Epoch 00077: val_loss did not improve from 0.43069\n","Epoch 78/1000\n"," - 76s - loss: 0.4520 - accuracy: 0.7815 - val_loss: 0.5237 - val_accuracy: 0.7416\n","\n","Epoch 00078: val_loss did not improve from 0.43069\n","Epoch 79/1000\n"," - 76s - loss: 0.4503 - accuracy: 0.7816 - val_loss: 0.6167 - val_accuracy: 0.7438\n","\n","Epoch 00079: val_loss did not improve from 0.43069\n","Epoch 80/1000\n"," - 76s - loss: 0.4509 - accuracy: 0.7826 - val_loss: 0.7189 - val_accuracy: 0.7458\n","\n","Epoch 00080: val_loss did not improve from 0.43069\n","Epoch 81/1000\n"," - 76s - loss: 0.4505 - accuracy: 0.7824 - val_loss: 0.5135 - val_accuracy: 0.7415\n","\n","Epoch 00081: val_loss did not improve from 0.43069\n","Epoch 82/1000\n"," - 76s - loss: 0.4519 - accuracy: 0.7801 - val_loss: 0.6616 - val_accuracy: 0.7450\n","\n","Epoch 00082: val_loss did not improve from 0.43069\n","Epoch 83/1000\n"," - 76s - loss: 0.4486 - accuracy: 0.7835 - val_loss: 0.5076 - val_accuracy: 0.7440\n","\n","Epoch 00083: val_loss did not improve from 0.43069\n","Epoch 84/1000\n"," - 76s - loss: 0.4508 - accuracy: 0.7822 - val_loss: 0.6687 - val_accuracy: 0.7457\n","\n","Epoch 00084: val_loss did not improve from 0.43069\n","Epoch 85/1000\n"," - 76s - loss: 0.4479 - accuracy: 0.7838 - val_loss: 0.4067 - val_accuracy: 0.7452\n","\n","Epoch 00085: val_loss improved from 0.43069 to 0.40675, saving model to /content/drive/My Drive/Colab Notebooks/Project_Stock/ckpt/classifier_45_ma7_pr3.h5\n","Epoch 86/1000\n"," - 77s - loss: 0.4488 - accuracy: 0.7836 - val_loss: 0.6984 - val_accuracy: 0.7455\n","\n","Epoch 00086: val_loss did not improve from 0.40675\n","Epoch 87/1000\n"," - 76s - loss: 0.4477 - accuracy: 0.7832 - val_loss: 0.5908 - val_accuracy: 0.7430\n","\n","Epoch 00087: val_loss did not improve from 0.40675\n","Epoch 88/1000\n"," - 76s - loss: 0.4484 - accuracy: 0.7834 - val_loss: 0.5305 - val_accuracy: 0.7469\n","\n","Epoch 00088: val_loss did not improve from 0.40675\n","Epoch 89/1000\n"," - 76s - loss: 0.4482 - accuracy: 0.7834 - val_loss: 0.5294 - val_accuracy: 0.7480\n","\n","Epoch 00089: val_loss did not improve from 0.40675\n","Epoch 90/1000\n"," - 76s - loss: 0.4477 - accuracy: 0.7846 - val_loss: 0.5014 - val_accuracy: 0.7475\n","\n","Epoch 00090: val_loss did not improve from 0.40675\n","Epoch 91/1000\n"," - 76s - loss: 0.4471 - accuracy: 0.7846 - val_loss: 0.4168 - val_accuracy: 0.7463\n","\n","Epoch 00091: val_loss did not improve from 0.40675\n","Epoch 92/1000\n"," - 75s - loss: 0.4475 - accuracy: 0.7845 - val_loss: 0.5611 - val_accuracy: 0.7448\n","\n","Epoch 00092: val_loss did not improve from 0.40675\n","Epoch 93/1000\n"," - 76s - loss: 0.4464 - accuracy: 0.7849 - val_loss: 0.6112 - val_accuracy: 0.7468\n","\n","Epoch 00093: val_loss did not improve from 0.40675\n","Epoch 94/1000\n"," - 76s - loss: 0.4469 - accuracy: 0.7846 - val_loss: 0.5076 - val_accuracy: 0.7399\n","\n","Epoch 00094: val_loss did not improve from 0.40675\n","Epoch 95/1000\n"," - 76s - loss: 0.4461 - accuracy: 0.7860 - val_loss: 0.6389 - val_accuracy: 0.7466\n","\n","Epoch 00095: val_loss did not improve from 0.40675\n","Epoch 96/1000\n"," - 76s - loss: 0.4432 - accuracy: 0.7869 - val_loss: 0.3914 - val_accuracy: 0.7470\n","\n","Epoch 00096: val_loss improved from 0.40675 to 0.39144, saving model to /content/drive/My Drive/Colab Notebooks/Project_Stock/ckpt/classifier_45_ma7_pr3.h5\n","Epoch 97/1000\n"," - 77s - loss: 0.4455 - accuracy: 0.7862 - val_loss: 0.5533 - val_accuracy: 0.7478\n","\n","Epoch 00097: val_loss did not improve from 0.39144\n","Epoch 98/1000\n"," - 76s - loss: 0.4451 - accuracy: 0.7858 - val_loss: 0.6405 - val_accuracy: 0.7469\n","\n","Epoch 00098: val_loss did not improve from 0.39144\n","Epoch 99/1000\n"," - 76s - loss: 0.4449 - accuracy: 0.7854 - val_loss: 0.4734 - val_accuracy: 0.7455\n","\n","Epoch 00099: val_loss did not improve from 0.39144\n","Epoch 100/1000\n"," - 76s - loss: 0.4433 - accuracy: 0.7864 - val_loss: 0.4622 - val_accuracy: 0.7432\n","\n","Epoch 00100: val_loss did not improve from 0.39144\n","Epoch 101/1000\n"," - 76s - loss: 0.4438 - accuracy: 0.7850 - val_loss: 0.5629 - val_accuracy: 0.7476\n","\n","Epoch 00101: val_loss did not improve from 0.39144\n","Epoch 102/1000\n"," - 76s - loss: 0.4433 - accuracy: 0.7866 - val_loss: 0.5039 - val_accuracy: 0.7475\n","\n","Epoch 00102: val_loss did not improve from 0.39144\n","Epoch 103/1000\n"," - 76s - loss: 0.4419 - accuracy: 0.7879 - val_loss: 0.6338 - val_accuracy: 0.7420\n","\n","Epoch 00103: val_loss did not improve from 0.39144\n","Epoch 104/1000\n"," - 77s - loss: 0.4422 - accuracy: 0.7873 - val_loss: 0.5648 - val_accuracy: 0.7442\n","\n","Epoch 00104: val_loss did not improve from 0.39144\n","Epoch 105/1000\n"," - 75s - loss: 0.4441 - accuracy: 0.7863 - val_loss: 0.5014 - val_accuracy: 0.7473\n","\n","Epoch 00105: val_loss did not improve from 0.39144\n","Epoch 106/1000\n"," - 76s - loss: 0.4433 - accuracy: 0.7876 - val_loss: 0.5537 - val_accuracy: 0.7445\n","\n","Epoch 00106: val_loss did not improve from 0.39144\n","Epoch 107/1000\n"," - 75s - loss: 0.4435 - accuracy: 0.7862 - val_loss: 0.5025 - val_accuracy: 0.7451\n","\n","Epoch 00107: val_loss did not improve from 0.39144\n","Epoch 108/1000\n"," - 76s - loss: 0.4420 - accuracy: 0.7882 - val_loss: 0.5479 - val_accuracy: 0.7451\n","\n","Epoch 00108: val_loss did not improve from 0.39144\n","Epoch 109/1000\n"," - 76s - loss: 0.4409 - accuracy: 0.7881 - val_loss: 0.6246 - val_accuracy: 0.7462\n","\n","Epoch 00109: val_loss did not improve from 0.39144\n","Epoch 110/1000\n"," - 76s - loss: 0.4425 - accuracy: 0.7867 - val_loss: 0.4373 - val_accuracy: 0.7468\n","\n","Epoch 00110: val_loss did not improve from 0.39144\n","Epoch 111/1000\n"," - 76s - loss: 0.4415 - accuracy: 0.7880 - val_loss: 0.6062 - val_accuracy: 0.7453\n","\n","Epoch 00111: val_loss did not improve from 0.39144\n","Epoch 112/1000\n"," - 76s - loss: 0.4400 - accuracy: 0.7888 - val_loss: 0.7570 - val_accuracy: 0.7468\n","\n","Epoch 00112: val_loss did not improve from 0.39144\n","Epoch 113/1000\n"," - 76s - loss: 0.4406 - accuracy: 0.7875 - val_loss: 0.5775 - val_accuracy: 0.7468\n","\n","Epoch 00113: val_loss did not improve from 0.39144\n","Epoch 114/1000\n"," - 77s - loss: 0.4416 - accuracy: 0.7877 - val_loss: 0.4463 - val_accuracy: 0.7427\n","\n","Epoch 00114: val_loss did not improve from 0.39144\n","Epoch 115/1000\n"," - 76s - loss: 0.4390 - accuracy: 0.7894 - val_loss: 0.4724 - val_accuracy: 0.7480\n","\n","Epoch 00115: val_loss did not improve from 0.39144\n","Epoch 116/1000\n"," - 76s - loss: 0.4401 - accuracy: 0.7890 - val_loss: 0.5433 - val_accuracy: 0.7488\n","\n","Epoch 00116: val_loss did not improve from 0.39144\n","Epoch 117/1000\n"," - 76s - loss: 0.4395 - accuracy: 0.7882 - val_loss: 0.5278 - val_accuracy: 0.7465\n","\n","Epoch 00117: val_loss did not improve from 0.39144\n","Epoch 118/1000\n"," - 76s - loss: 0.4382 - accuracy: 0.7899 - val_loss: 0.6663 - val_accuracy: 0.7478\n","\n","Epoch 00118: val_loss did not improve from 0.39144\n","Epoch 119/1000\n"," - 76s - loss: 0.4386 - accuracy: 0.7887 - val_loss: 0.6930 - val_accuracy: 0.7468\n","\n","Epoch 00119: val_loss did not improve from 0.39144\n","Epoch 120/1000\n"," - 75s - loss: 0.4381 - accuracy: 0.7898 - val_loss: 0.5724 - val_accuracy: 0.7470\n","\n","Epoch 00120: val_loss did not improve from 0.39144\n","Epoch 121/1000\n"," - 76s - loss: 0.4384 - accuracy: 0.7896 - val_loss: 0.6896 - val_accuracy: 0.7476\n","\n","Epoch 00121: val_loss did not improve from 0.39144\n","Epoch 122/1000\n"," - 76s - loss: 0.4374 - accuracy: 0.7903 - val_loss: 0.5313 - val_accuracy: 0.7481\n","\n","Epoch 00122: val_loss did not improve from 0.39144\n","Epoch 123/1000\n"," - 76s - loss: 0.4373 - accuracy: 0.7903 - val_loss: 0.5189 - val_accuracy: 0.7483\n","\n","Epoch 00123: val_loss did not improve from 0.39144\n","Epoch 124/1000\n"," - 76s - loss: 0.4373 - accuracy: 0.7899 - val_loss: 0.6286 - val_accuracy: 0.7476\n","\n","Epoch 00124: val_loss did not improve from 0.39144\n","Epoch 125/1000\n"," - 76s - loss: 0.4352 - accuracy: 0.7921 - val_loss: 0.5091 - val_accuracy: 0.7464\n","\n","Epoch 00125: val_loss did not improve from 0.39144\n","Epoch 126/1000\n"," - 76s - loss: 0.4363 - accuracy: 0.7903 - val_loss: 0.6161 - val_accuracy: 0.7464\n","\n","Epoch 00126: val_loss did not improve from 0.39144\n","Epoch 127/1000\n"," - 76s - loss: 0.4366 - accuracy: 0.7907 - val_loss: 0.6755 - val_accuracy: 0.7486\n","\n","Epoch 00127: val_loss did not improve from 0.39144\n","Epoch 128/1000\n"," - 77s - loss: 0.4358 - accuracy: 0.7907 - val_loss: 0.6112 - val_accuracy: 0.7470\n","\n","Epoch 00128: val_loss did not improve from 0.39144\n","Epoch 129/1000\n"," - 76s - loss: 0.4347 - accuracy: 0.7916 - val_loss: 0.3769 - val_accuracy: 0.7483\n","\n","Epoch 00129: val_loss improved from 0.39144 to 0.37686, saving model to /content/drive/My Drive/Colab Notebooks/Project_Stock/ckpt/classifier_45_ma7_pr3.h5\n","Epoch 130/1000\n"," - 78s - loss: 0.4356 - accuracy: 0.7911 - val_loss: 0.6609 - val_accuracy: 0.7445\n","\n","Epoch 00130: val_loss did not improve from 0.37686\n","Epoch 131/1000\n"," - 76s - loss: 0.4341 - accuracy: 0.7928 - val_loss: 0.5752 - val_accuracy: 0.7446\n","\n","Epoch 00131: val_loss did not improve from 0.37686\n","Epoch 132/1000\n"," - 76s - loss: 0.4350 - accuracy: 0.7905 - val_loss: 0.5236 - val_accuracy: 0.7474\n","\n","Epoch 00132: val_loss did not improve from 0.37686\n","Epoch 133/1000\n"," - 76s - loss: 0.4338 - accuracy: 0.7922 - val_loss: 0.3935 - val_accuracy: 0.7483\n","\n","Epoch 00133: val_loss did not improve from 0.37686\n","Epoch 134/1000\n"," - 76s - loss: 0.4330 - accuracy: 0.7926 - val_loss: 0.4705 - val_accuracy: 0.7487\n","\n","Epoch 00134: val_loss did not improve from 0.37686\n","Epoch 135/1000\n"," - 75s - loss: 0.4327 - accuracy: 0.7934 - val_loss: 0.6535 - val_accuracy: 0.7468\n","\n","Epoch 00135: val_loss did not improve from 0.37686\n","Epoch 136/1000\n"," - 76s - loss: 0.4330 - accuracy: 0.7935 - val_loss: 0.5963 - val_accuracy: 0.7481\n","\n","Epoch 00136: val_loss did not improve from 0.37686\n","Epoch 137/1000\n"," - 76s - loss: 0.4327 - accuracy: 0.7933 - val_loss: 0.5132 - val_accuracy: 0.7455\n","\n","Epoch 00137: val_loss did not improve from 0.37686\n","Epoch 138/1000\n"," - 76s - loss: 0.4322 - accuracy: 0.7932 - val_loss: 0.5635 - val_accuracy: 0.7497\n","\n","Epoch 00138: val_loss did not improve from 0.37686\n","Epoch 139/1000\n"," - 76s - loss: 0.4331 - accuracy: 0.7942 - val_loss: 0.7381 - val_accuracy: 0.7468\n","\n","Epoch 00139: val_loss did not improve from 0.37686\n","Epoch 140/1000\n"," - 76s - loss: 0.4316 - accuracy: 0.7943 - val_loss: 0.5791 - val_accuracy: 0.7482\n","\n","Epoch 00140: val_loss did not improve from 0.37686\n","Epoch 141/1000\n"," - 76s - loss: 0.4323 - accuracy: 0.7930 - val_loss: 0.6531 - val_accuracy: 0.7481\n","\n","Epoch 00141: val_loss did not improve from 0.37686\n","Epoch 142/1000\n"," - 76s - loss: 0.4303 - accuracy: 0.7943 - val_loss: 0.5250 - val_accuracy: 0.7472\n","\n","Epoch 00142: val_loss did not improve from 0.37686\n","Epoch 143/1000\n"," - 76s - loss: 0.4301 - accuracy: 0.7949 - val_loss: 0.6066 - val_accuracy: 0.7467\n","\n","Epoch 00143: val_loss did not improve from 0.37686\n","Epoch 144/1000\n"," - 76s - loss: 0.4304 - accuracy: 0.7952 - val_loss: 0.5665 - val_accuracy: 0.7468\n","\n","Epoch 00144: val_loss did not improve from 0.37686\n","Epoch 145/1000\n"," - 76s - loss: 0.4311 - accuracy: 0.7953 - val_loss: 0.5075 - val_accuracy: 0.7482\n","\n","Epoch 00145: val_loss did not improve from 0.37686\n","Epoch 146/1000\n"," - 76s - loss: 0.4305 - accuracy: 0.7943 - val_loss: 0.5932 - val_accuracy: 0.7441\n","\n","Epoch 00146: val_loss did not improve from 0.37686\n","Epoch 147/1000\n"," - 76s - loss: 0.4292 - accuracy: 0.7959 - val_loss: 0.6412 - val_accuracy: 0.7488\n","\n","Epoch 00147: val_loss did not improve from 0.37686\n","Epoch 148/1000\n"," - 76s - loss: 0.4305 - accuracy: 0.7948 - val_loss: 0.6077 - val_accuracy: 0.7474\n","\n","Epoch 00148: val_loss did not improve from 0.37686\n","Epoch 149/1000\n"," - 76s - loss: 0.4304 - accuracy: 0.7945 - val_loss: 0.4805 - val_accuracy: 0.7498\n","\n","Epoch 00149: val_loss did not improve from 0.37686\n","Epoch 150/1000\n"," - 76s - loss: 0.4303 - accuracy: 0.7939 - val_loss: 0.4931 - val_accuracy: 0.7489\n","\n","Epoch 00150: val_loss did not improve from 0.37686\n","Epoch 151/1000\n"," - 76s - loss: 0.4299 - accuracy: 0.7946 - val_loss: 0.4974 - val_accuracy: 0.7453\n","\n","Epoch 00151: val_loss did not improve from 0.37686\n","Epoch 152/1000\n"," - 76s - loss: 0.4276 - accuracy: 0.7961 - val_loss: 0.5670 - val_accuracy: 0.7481\n","\n","Epoch 00152: val_loss did not improve from 0.37686\n","Epoch 153/1000\n"," - 76s - loss: 0.4280 - accuracy: 0.7965 - val_loss: 0.6143 - val_accuracy: 0.7476\n","\n","Epoch 00153: val_loss did not improve from 0.37686\n","Epoch 154/1000\n"," - 76s - loss: 0.4282 - accuracy: 0.7959 - val_loss: 0.4792 - val_accuracy: 0.7501\n","\n","Epoch 00154: val_loss did not improve from 0.37686\n","Epoch 155/1000\n"," - 76s - loss: 0.4277 - accuracy: 0.7968 - val_loss: 0.5840 - val_accuracy: 0.7475\n","\n","Epoch 00155: val_loss did not improve from 0.37686\n","Epoch 156/1000\n"," - 76s - loss: 0.4274 - accuracy: 0.7960 - val_loss: 0.6135 - val_accuracy: 0.7466\n","\n","Epoch 00156: val_loss did not improve from 0.37686\n","Epoch 157/1000\n"," - 76s - loss: 0.4273 - accuracy: 0.7973 - val_loss: 0.5814 - val_accuracy: 0.7485\n","\n","Epoch 00157: val_loss did not improve from 0.37686\n","Epoch 158/1000\n"," - 76s - loss: 0.4271 - accuracy: 0.7969 - val_loss: 0.6732 - val_accuracy: 0.7491\n","\n","Epoch 00158: val_loss did not improve from 0.37686\n","Epoch 159/1000\n"," - 76s - loss: 0.4282 - accuracy: 0.7958 - val_loss: 0.5652 - val_accuracy: 0.7439\n","\n","Epoch 00159: val_loss did not improve from 0.37686\n","Epoch 160/1000\n"," - 76s - loss: 0.4271 - accuracy: 0.7968 - val_loss: 0.4288 - val_accuracy: 0.7489\n","\n","Epoch 00160: val_loss did not improve from 0.37686\n","Epoch 161/1000\n"," - 75s - loss: 0.4259 - accuracy: 0.7975 - val_loss: 0.5238 - val_accuracy: 0.7472\n","\n","Epoch 00161: val_loss did not improve from 0.37686\n","Epoch 162/1000\n"," - 76s - loss: 0.4262 - accuracy: 0.7967 - val_loss: 0.5245 - val_accuracy: 0.7485\n","\n","Epoch 00162: val_loss did not improve from 0.37686\n","Epoch 163/1000\n"," - 76s - loss: 0.4245 - accuracy: 0.7971 - val_loss: 0.5670 - val_accuracy: 0.7490\n","\n","Epoch 00163: val_loss did not improve from 0.37686\n","Epoch 164/1000\n"," - 76s - loss: 0.4252 - accuracy: 0.7974 - val_loss: 0.7050 - val_accuracy: 0.7482\n","\n","Epoch 00164: val_loss did not improve from 0.37686\n","Epoch 165/1000\n"," - 76s - loss: 0.4239 - accuracy: 0.7981 - val_loss: 0.5213 - val_accuracy: 0.7504\n","\n","Epoch 00165: val_loss did not improve from 0.37686\n","Epoch 166/1000\n"," - 76s - loss: 0.4237 - accuracy: 0.7989 - val_loss: 0.4022 - val_accuracy: 0.7488\n","\n","Epoch 00166: val_loss did not improve from 0.37686\n","Epoch 167/1000\n"," - 76s - loss: 0.4234 - accuracy: 0.7991 - val_loss: 0.5893 - val_accuracy: 0.7498\n","\n","Epoch 00167: val_loss did not improve from 0.37686\n","Epoch 168/1000\n"," - 76s - loss: 0.4212 - accuracy: 0.8007 - val_loss: 0.4439 - val_accuracy: 0.7490\n","\n","Epoch 00168: val_loss did not improve from 0.37686\n","Epoch 169/1000\n"," - 76s - loss: 0.4243 - accuracy: 0.7991 - val_loss: 0.5269 - val_accuracy: 0.7484\n","\n","Epoch 00169: val_loss did not improve from 0.37686\n","Epoch 170/1000\n"," - 76s - loss: 0.4242 - accuracy: 0.7987 - val_loss: 0.4763 - val_accuracy: 0.7488\n","\n","Epoch 00170: val_loss did not improve from 0.37686\n","Epoch 171/1000\n"," - 76s - loss: 0.4232 - accuracy: 0.7992 - val_loss: 0.5925 - val_accuracy: 0.7450\n","\n","Epoch 00171: val_loss did not improve from 0.37686\n","Epoch 172/1000\n"," - 76s - loss: 0.4236 - accuracy: 0.7981 - val_loss: 0.5537 - val_accuracy: 0.7486\n","\n","Epoch 00172: val_loss did not improve from 0.37686\n","Epoch 173/1000\n"," - 76s - loss: 0.4218 - accuracy: 0.8003 - val_loss: 0.5230 - val_accuracy: 0.7483\n","\n","Epoch 00173: val_loss did not improve from 0.37686\n","Epoch 174/1000\n"," - 76s - loss: 0.4226 - accuracy: 0.7992 - val_loss: 0.5336 - val_accuracy: 0.7502\n","\n","Epoch 00174: val_loss did not improve from 0.37686\n","Epoch 175/1000\n"," - 76s - loss: 0.4224 - accuracy: 0.7989 - val_loss: 0.5842 - val_accuracy: 0.7473\n","\n","Epoch 00175: val_loss did not improve from 0.37686\n","Epoch 176/1000\n"," - 76s - loss: 0.4221 - accuracy: 0.8005 - val_loss: 0.5356 - val_accuracy: 0.7502\n","\n","Epoch 00176: val_loss did not improve from 0.37686\n","Epoch 177/1000\n"," - 76s - loss: 0.4206 - accuracy: 0.8011 - val_loss: 0.4397 - val_accuracy: 0.7476\n","\n","Epoch 00177: val_loss did not improve from 0.37686\n","Epoch 178/1000\n"," - 77s - loss: 0.4210 - accuracy: 0.8003 - val_loss: 0.5998 - val_accuracy: 0.7503\n","\n","Epoch 00178: val_loss did not improve from 0.37686\n","Epoch 179/1000\n"," - 76s - loss: 0.4208 - accuracy: 0.8003 - val_loss: 0.6443 - val_accuracy: 0.7483\n","\n","Epoch 00179: val_loss did not improve from 0.37686\n","Epoch 180/1000\n"," - 76s - loss: 0.4211 - accuracy: 0.8011 - val_loss: 0.5207 - val_accuracy: 0.7467\n","\n","Epoch 00180: val_loss did not improve from 0.37686\n","Epoch 181/1000\n"," - 76s - loss: 0.4214 - accuracy: 0.8006 - val_loss: 0.5514 - val_accuracy: 0.7496\n","\n","Epoch 00181: val_loss did not improve from 0.37686\n","Epoch 182/1000\n"," - 76s - loss: 0.4209 - accuracy: 0.8017 - val_loss: 0.5518 - val_accuracy: 0.7503\n","\n","Epoch 00182: val_loss did not improve from 0.37686\n","Epoch 183/1000\n"," - 76s - loss: 0.4198 - accuracy: 0.8015 - val_loss: 0.4906 - val_accuracy: 0.7492\n","\n","Epoch 00183: val_loss did not improve from 0.37686\n","Epoch 184/1000\n"," - 76s - loss: 0.4195 - accuracy: 0.8016 - val_loss: 0.5389 - val_accuracy: 0.7510\n","\n","Epoch 00184: val_loss did not improve from 0.37686\n","Epoch 185/1000\n"," - 76s - loss: 0.4197 - accuracy: 0.8015 - val_loss: 0.6183 - val_accuracy: 0.7478\n","\n","Epoch 00185: val_loss did not improve from 0.37686\n","Epoch 186/1000\n"," - 76s - loss: 0.4194 - accuracy: 0.8019 - val_loss: 0.4642 - val_accuracy: 0.7476\n","\n","Epoch 00186: val_loss did not improve from 0.37686\n","Epoch 187/1000\n"," - 76s - loss: 0.4190 - accuracy: 0.8009 - val_loss: 0.5999 - val_accuracy: 0.7498\n","\n","Epoch 00187: val_loss did not improve from 0.37686\n","Epoch 188/1000\n"," - 76s - loss: 0.4176 - accuracy: 0.8020 - val_loss: 0.4653 - val_accuracy: 0.7464\n","\n","Epoch 00188: val_loss did not improve from 0.37686\n","Epoch 189/1000\n"," - 76s - loss: 0.4202 - accuracy: 0.8000 - val_loss: 0.6391 - val_accuracy: 0.7488\n","\n","Epoch 00189: val_loss did not improve from 0.37686\n","Epoch 190/1000\n"," - 77s - loss: 0.4186 - accuracy: 0.8017 - val_loss: 0.6208 - val_accuracy: 0.7454\n","\n","Epoch 00190: val_loss did not improve from 0.37686\n","Epoch 191/1000\n"," - 76s - loss: 0.4185 - accuracy: 0.8034 - val_loss: 0.6264 - val_accuracy: 0.7496\n","\n","Epoch 00191: val_loss did not improve from 0.37686\n","Epoch 192/1000\n"," - 76s - loss: 0.4180 - accuracy: 0.8025 - val_loss: 0.6037 - val_accuracy: 0.7504\n","\n","Epoch 00192: val_loss did not improve from 0.37686\n","Epoch 193/1000\n"," - 76s - loss: 0.4170 - accuracy: 0.8034 - val_loss: 0.4639 - val_accuracy: 0.7468\n","\n","Epoch 00193: val_loss did not improve from 0.37686\n","Epoch 194/1000\n"," - 76s - loss: 0.4163 - accuracy: 0.8031 - val_loss: 0.6766 - val_accuracy: 0.7467\n","\n","Epoch 00194: val_loss did not improve from 0.37686\n","Epoch 195/1000\n"," - 75s - loss: 0.4157 - accuracy: 0.8028 - val_loss: 0.5736 - val_accuracy: 0.7492\n","\n","Epoch 00195: val_loss did not improve from 0.37686\n","Epoch 196/1000\n"," - 76s - loss: 0.4156 - accuracy: 0.8039 - val_loss: 0.6037 - val_accuracy: 0.7490\n","\n","Epoch 00196: val_loss did not improve from 0.37686\n","Epoch 197/1000\n"," - 76s - loss: 0.4151 - accuracy: 0.8037 - val_loss: 0.4383 - val_accuracy: 0.7501\n","\n","Epoch 00197: val_loss did not improve from 0.37686\n","Epoch 198/1000\n"," - 76s - loss: 0.4144 - accuracy: 0.8044 - val_loss: 0.5999 - val_accuracy: 0.7488\n","\n","Epoch 00198: val_loss did not improve from 0.37686\n","Epoch 199/1000\n"," - 76s - loss: 0.4163 - accuracy: 0.8036 - val_loss: 0.6403 - val_accuracy: 0.7498\n","\n","Epoch 00199: val_loss did not improve from 0.37686\n","Epoch 200/1000\n"," - 76s - loss: 0.4160 - accuracy: 0.8040 - val_loss: 0.4444 - val_accuracy: 0.7468\n","\n","Epoch 00200: val_loss did not improve from 0.37686\n","Epoch 201/1000\n"," - 76s - loss: 0.4158 - accuracy: 0.8035 - val_loss: 0.7016 - val_accuracy: 0.7494\n","\n","Epoch 00201: val_loss did not improve from 0.37686\n","Epoch 202/1000\n"," - 76s - loss: 0.4153 - accuracy: 0.8039 - val_loss: 0.6428 - val_accuracy: 0.7464\n","\n","Epoch 00202: val_loss did not improve from 0.37686\n","Epoch 203/1000\n"," - 75s - loss: 0.4145 - accuracy: 0.8049 - val_loss: 0.7913 - val_accuracy: 0.7472\n","\n","Epoch 00203: val_loss did not improve from 0.37686\n","Epoch 204/1000\n"," - 76s - loss: 0.4156 - accuracy: 0.8034 - val_loss: 0.5527 - val_accuracy: 0.7486\n","\n","Epoch 00204: val_loss did not improve from 0.37686\n","Epoch 205/1000\n"," - 76s - loss: 0.4122 - accuracy: 0.8054 - val_loss: 0.6429 - val_accuracy: 0.7452\n","\n","Epoch 00205: val_loss did not improve from 0.37686\n","Epoch 206/1000\n"," - 76s - loss: 0.4146 - accuracy: 0.8053 - val_loss: 0.5075 - val_accuracy: 0.7476\n","\n","Epoch 00206: val_loss did not improve from 0.37686\n","Epoch 207/1000\n"," - 76s - loss: 0.4135 - accuracy: 0.8046 - val_loss: 0.4536 - val_accuracy: 0.7481\n","\n","Epoch 00207: val_loss did not improve from 0.37686\n","Epoch 208/1000\n"," - 76s - loss: 0.4134 - accuracy: 0.8048 - val_loss: 0.5096 - val_accuracy: 0.7485\n","\n","Epoch 00208: val_loss did not improve from 0.37686\n","Epoch 209/1000\n"," - 76s - loss: 0.4132 - accuracy: 0.8056 - val_loss: 0.5979 - val_accuracy: 0.7472\n","\n","Epoch 00209: val_loss did not improve from 0.37686\n","Epoch 210/1000\n"," - 77s - loss: 0.4142 - accuracy: 0.8033 - val_loss: 0.5608 - val_accuracy: 0.7493\n","\n","Epoch 00210: val_loss did not improve from 0.37686\n","Epoch 211/1000\n"," - 76s - loss: 0.4127 - accuracy: 0.8063 - val_loss: 0.6619 - val_accuracy: 0.7495\n","\n","Epoch 00211: val_loss did not improve from 0.37686\n","Epoch 212/1000\n"," - 76s - loss: 0.4123 - accuracy: 0.8061 - val_loss: 0.6889 - val_accuracy: 0.7430\n","\n","Epoch 00212: val_loss did not improve from 0.37686\n","Epoch 213/1000\n"," - 76s - loss: 0.4110 - accuracy: 0.8064 - val_loss: 0.4778 - val_accuracy: 0.7511\n","\n","Epoch 00213: val_loss did not improve from 0.37686\n","Epoch 214/1000\n"," - 76s - loss: 0.4113 - accuracy: 0.8061 - val_loss: 0.6099 - val_accuracy: 0.7514\n","\n","Epoch 00214: val_loss did not improve from 0.37686\n","Epoch 215/1000\n"," - 76s - loss: 0.4116 - accuracy: 0.8057 - val_loss: 0.5846 - val_accuracy: 0.7523\n","\n","Epoch 00215: val_loss did not improve from 0.37686\n","Epoch 216/1000\n"," - 76s - loss: 0.4103 - accuracy: 0.8058 - val_loss: 0.5325 - val_accuracy: 0.7501\n","\n","Epoch 00216: val_loss did not improve from 0.37686\n","Epoch 217/1000\n"," - 76s - loss: 0.4100 - accuracy: 0.8075 - val_loss: 0.7670 - val_accuracy: 0.7506\n","\n","Epoch 00217: val_loss did not improve from 0.37686\n","Epoch 218/1000\n"," - 76s - loss: 0.4103 - accuracy: 0.8068 - val_loss: 0.4200 - val_accuracy: 0.7504\n","\n","Epoch 00218: val_loss did not improve from 0.37686\n","Epoch 219/1000\n"," - 76s - loss: 0.4107 - accuracy: 0.8077 - val_loss: 0.5725 - val_accuracy: 0.7520\n","\n","Epoch 00219: val_loss did not improve from 0.37686\n","Epoch 220/1000\n"," - 76s - loss: 0.4092 - accuracy: 0.8079 - val_loss: 0.5799 - val_accuracy: 0.7475\n","\n","Epoch 00220: val_loss did not improve from 0.37686\n","Epoch 221/1000\n"," - 76s - loss: 0.4086 - accuracy: 0.8076 - val_loss: 0.5873 - val_accuracy: 0.7489\n","\n","Epoch 00221: val_loss did not improve from 0.37686\n","Epoch 222/1000\n"," - 76s - loss: 0.4090 - accuracy: 0.8081 - val_loss: 0.5539 - val_accuracy: 0.7517\n","\n","Epoch 00222: val_loss did not improve from 0.37686\n","Epoch 223/1000\n"," - 76s - loss: 0.4091 - accuracy: 0.8080 - val_loss: 0.5387 - val_accuracy: 0.7473\n","\n","Epoch 00223: val_loss did not improve from 0.37686\n","Epoch 224/1000\n"," - 76s - loss: 0.4074 - accuracy: 0.8084 - val_loss: 0.5777 - val_accuracy: 0.7471\n","\n","Epoch 00224: val_loss did not improve from 0.37686\n","Epoch 225/1000\n"," - 76s - loss: 0.4097 - accuracy: 0.8085 - val_loss: 0.5440 - val_accuracy: 0.7505\n","\n","Epoch 00225: val_loss did not improve from 0.37686\n","Epoch 226/1000\n"," - 75s - loss: 0.4073 - accuracy: 0.8103 - val_loss: 0.6478 - val_accuracy: 0.7516\n","\n","Epoch 00226: val_loss did not improve from 0.37686\n","Epoch 227/1000\n"," - 76s - loss: 0.4072 - accuracy: 0.8085 - val_loss: 0.5199 - val_accuracy: 0.7520\n","\n","Epoch 00227: val_loss did not improve from 0.37686\n","Epoch 228/1000\n"," - 76s - loss: 0.4064 - accuracy: 0.8086 - val_loss: 0.5992 - val_accuracy: 0.7507\n","\n","Epoch 00228: val_loss did not improve from 0.37686\n","Epoch 229/1000\n"," - 77s - loss: 0.4084 - accuracy: 0.8089 - val_loss: 0.5373 - val_accuracy: 0.7502\n","\n","Epoch 00229: val_loss did not improve from 0.37686\n","Epoch 230/1000\n"," - 77s - loss: 0.4061 - accuracy: 0.8092 - val_loss: 0.5141 - val_accuracy: 0.7502\n","\n","Epoch 00230: val_loss did not improve from 0.37686\n","Epoch 231/1000\n"," - 77s - loss: 0.4072 - accuracy: 0.8079 - val_loss: 0.5252 - val_accuracy: 0.7511\n","\n","Epoch 00231: val_loss did not improve from 0.37686\n","Epoch 232/1000\n"," - 77s - loss: 0.4082 - accuracy: 0.8079 - val_loss: 0.5808 - val_accuracy: 0.7488\n","\n","Epoch 00232: val_loss did not improve from 0.37686\n","Epoch 233/1000\n"," - 78s - loss: 0.4071 - accuracy: 0.8077 - val_loss: 0.6324 - val_accuracy: 0.7510\n","\n","Epoch 00233: val_loss did not improve from 0.37686\n","Epoch 234/1000\n"," - 77s - loss: 0.4055 - accuracy: 0.8091 - val_loss: 0.6273 - val_accuracy: 0.7495\n","\n","Epoch 00234: val_loss did not improve from 0.37686\n","Epoch 235/1000\n"," - 76s - loss: 0.4065 - accuracy: 0.8096 - val_loss: 0.6578 - val_accuracy: 0.7496\n","\n","Epoch 00235: val_loss did not improve from 0.37686\n","Epoch 236/1000\n"," - 76s - loss: 0.4033 - accuracy: 0.8105 - val_loss: 0.6546 - val_accuracy: 0.7457\n","\n","Epoch 00236: val_loss did not improve from 0.37686\n","Epoch 237/1000\n"," - 76s - loss: 0.4051 - accuracy: 0.8113 - val_loss: 0.4950 - val_accuracy: 0.7474\n","\n","Epoch 00237: val_loss did not improve from 0.37686\n","Epoch 238/1000\n"," - 76s - loss: 0.4056 - accuracy: 0.8094 - val_loss: 0.6797 - val_accuracy: 0.7515\n","\n","Epoch 00238: val_loss did not improve from 0.37686\n","Epoch 239/1000\n"," - 77s - loss: 0.4045 - accuracy: 0.8105 - val_loss: 0.6089 - val_accuracy: 0.7486\n","\n","Epoch 00239: val_loss did not improve from 0.37686\n","Epoch 240/1000\n"," - 76s - loss: 0.4042 - accuracy: 0.8110 - val_loss: 0.5074 - val_accuracy: 0.7513\n","\n","Epoch 00240: val_loss did not improve from 0.37686\n","Epoch 241/1000\n"," - 76s - loss: 0.4026 - accuracy: 0.8121 - val_loss: 0.5578 - val_accuracy: 0.7522\n","\n","Epoch 00241: val_loss did not improve from 0.37686\n","Epoch 242/1000\n"," - 76s - loss: 0.4035 - accuracy: 0.8118 - val_loss: 0.6272 - val_accuracy: 0.7527\n","\n","Epoch 00242: val_loss did not improve from 0.37686\n","Epoch 243/1000\n"," - 76s - loss: 0.4032 - accuracy: 0.8115 - val_loss: 0.4333 - val_accuracy: 0.7532\n","\n","Epoch 00243: val_loss did not improve from 0.37686\n","Epoch 244/1000\n"," - 76s - loss: 0.4041 - accuracy: 0.8118 - val_loss: 0.7269 - val_accuracy: 0.7517\n","\n","Epoch 00244: val_loss did not improve from 0.37686\n","Epoch 245/1000\n"," - 76s - loss: 0.4040 - accuracy: 0.8099 - val_loss: 0.6897 - val_accuracy: 0.7499\n","\n","Epoch 00245: val_loss did not improve from 0.37686\n","Epoch 246/1000\n"," - 76s - loss: 0.4026 - accuracy: 0.8119 - val_loss: 0.5607 - val_accuracy: 0.7496\n","\n","Epoch 00246: val_loss did not improve from 0.37686\n","Epoch 247/1000\n"," - 77s - loss: 0.4022 - accuracy: 0.8121 - val_loss: 0.4789 - val_accuracy: 0.7508\n","\n","Epoch 00247: val_loss did not improve from 0.37686\n","Epoch 248/1000\n"," - 76s - loss: 0.4018 - accuracy: 0.8127 - val_loss: 0.5967 - val_accuracy: 0.7507\n","\n","Epoch 00248: val_loss did not improve from 0.37686\n","Epoch 249/1000\n"," - 76s - loss: 0.4016 - accuracy: 0.8118 - val_loss: 0.5319 - val_accuracy: 0.7533\n","\n","Epoch 00249: val_loss did not improve from 0.37686\n","Epoch 250/1000\n"," - 76s - loss: 0.4002 - accuracy: 0.8139 - val_loss: 0.7684 - val_accuracy: 0.7502\n","\n","Epoch 00250: val_loss did not improve from 0.37686\n","Epoch 251/1000\n"," - 76s - loss: 0.4018 - accuracy: 0.8115 - val_loss: 0.5626 - val_accuracy: 0.7525\n","\n","Epoch 00251: val_loss did not improve from 0.37686\n","Epoch 252/1000\n"," - 77s - loss: 0.4032 - accuracy: 0.8107 - val_loss: 0.6025 - val_accuracy: 0.7516\n","\n","Epoch 00252: val_loss did not improve from 0.37686\n","Epoch 253/1000\n"," - 76s - loss: 0.4016 - accuracy: 0.8116 - val_loss: 0.6321 - val_accuracy: 0.7494\n","\n","Epoch 00253: val_loss did not improve from 0.37686\n","Epoch 254/1000\n"," - 76s - loss: 0.4019 - accuracy: 0.8120 - val_loss: 0.5246 - val_accuracy: 0.7500\n","\n","Epoch 00254: val_loss did not improve from 0.37686\n","Epoch 255/1000\n"," - 76s - loss: 0.4009 - accuracy: 0.8124 - val_loss: 0.5864 - val_accuracy: 0.7455\n","\n","Epoch 00255: val_loss did not improve from 0.37686\n","Epoch 256/1000\n"," - 76s - loss: 0.4017 - accuracy: 0.8127 - val_loss: 0.6448 - val_accuracy: 0.7492\n","\n","Epoch 00256: val_loss did not improve from 0.37686\n","Epoch 257/1000\n"," - 77s - loss: 0.3995 - accuracy: 0.8132 - val_loss: 0.6466 - val_accuracy: 0.7500\n","\n","Epoch 00257: val_loss did not improve from 0.37686\n","Epoch 258/1000\n"," - 76s - loss: 0.4003 - accuracy: 0.8128 - val_loss: 0.6271 - val_accuracy: 0.7503\n","\n","Epoch 00258: val_loss did not improve from 0.37686\n","Epoch 259/1000\n"," - 76s - loss: 0.4001 - accuracy: 0.8146 - val_loss: 0.5441 - val_accuracy: 0.7516\n","\n","Epoch 00259: val_loss did not improve from 0.37686\n","Epoch 260/1000\n"," - 76s - loss: 0.4009 - accuracy: 0.8130 - val_loss: 0.5518 - val_accuracy: 0.7504\n","\n","Epoch 00260: val_loss did not improve from 0.37686\n","Epoch 261/1000\n"," - 76s - loss: 0.3988 - accuracy: 0.8132 - val_loss: 0.5305 - val_accuracy: 0.7490\n","\n","Epoch 00261: val_loss did not improve from 0.37686\n","Epoch 262/1000\n"," - 76s - loss: 0.3987 - accuracy: 0.8140 - val_loss: 0.5152 - val_accuracy: 0.7539\n","\n","Epoch 00262: val_loss did not improve from 0.37686\n","Epoch 263/1000\n"," - 76s - loss: 0.3988 - accuracy: 0.8139 - val_loss: 0.4482 - val_accuracy: 0.7514\n","\n","Epoch 00263: val_loss did not improve from 0.37686\n","Epoch 264/1000\n"," - 76s - loss: 0.3990 - accuracy: 0.8134 - val_loss: 0.6989 - val_accuracy: 0.7491\n","\n","Epoch 00264: val_loss did not improve from 0.37686\n","Epoch 265/1000\n"," - 76s - loss: 0.3963 - accuracy: 0.8154 - val_loss: 0.6447 - val_accuracy: 0.7508\n","\n","Epoch 00265: val_loss did not improve from 0.37686\n","Epoch 266/1000\n"," - 76s - loss: 0.3967 - accuracy: 0.8152 - val_loss: 0.5442 - val_accuracy: 0.7493\n","\n","Epoch 00266: val_loss did not improve from 0.37686\n","Epoch 267/1000\n"," - 76s - loss: 0.3966 - accuracy: 0.8159 - val_loss: 0.6738 - val_accuracy: 0.7493\n","\n","Epoch 00267: val_loss did not improve from 0.37686\n","Epoch 268/1000\n"," - 76s - loss: 0.3962 - accuracy: 0.8152 - val_loss: 0.7073 - val_accuracy: 0.7521\n","\n","Epoch 00268: val_loss did not improve from 0.37686\n","Epoch 269/1000\n"," - 77s - loss: 0.3963 - accuracy: 0.8155 - val_loss: 0.5601 - val_accuracy: 0.7526\n","\n","Epoch 00269: val_loss did not improve from 0.37686\n","Epoch 270/1000\n"," - 77s - loss: 0.3977 - accuracy: 0.8149 - val_loss: 0.4967 - val_accuracy: 0.7530\n","\n","Epoch 00270: val_loss did not improve from 0.37686\n","Epoch 271/1000\n"," - 77s - loss: 0.3958 - accuracy: 0.8154 - val_loss: 0.6173 - val_accuracy: 0.7528\n","\n","Epoch 00271: val_loss did not improve from 0.37686\n","Epoch 272/1000\n"," - 76s - loss: 0.3978 - accuracy: 0.8141 - val_loss: 0.5408 - val_accuracy: 0.7525\n","\n","Epoch 00272: val_loss did not improve from 0.37686\n","Epoch 273/1000\n"," - 76s - loss: 0.3981 - accuracy: 0.8146 - val_loss: 0.7273 - val_accuracy: 0.7487\n","\n","Epoch 00273: val_loss did not improve from 0.37686\n","Epoch 274/1000\n"," - 77s - loss: 0.3967 - accuracy: 0.8155 - val_loss: 0.5758 - val_accuracy: 0.7527\n","\n","Epoch 00274: val_loss did not improve from 0.37686\n","Epoch 275/1000\n"," - 77s - loss: 0.3946 - accuracy: 0.8170 - val_loss: 0.4916 - val_accuracy: 0.7521\n","\n","Epoch 00275: val_loss did not improve from 0.37686\n","Epoch 276/1000\n"," - 76s - loss: 0.3949 - accuracy: 0.8149 - val_loss: 0.4848 - val_accuracy: 0.7514\n","\n","Epoch 00276: val_loss did not improve from 0.37686\n","Epoch 277/1000\n"," - 76s - loss: 0.3963 - accuracy: 0.8153 - val_loss: 0.4889 - val_accuracy: 0.7527\n","\n","Epoch 00277: val_loss did not improve from 0.37686\n","Epoch 278/1000\n"," - 76s - loss: 0.3952 - accuracy: 0.8164 - val_loss: 0.7071 - val_accuracy: 0.7493\n","\n","Epoch 00278: val_loss did not improve from 0.37686\n","Epoch 279/1000\n"," - 76s - loss: 0.3943 - accuracy: 0.8167 - val_loss: 0.6021 - val_accuracy: 0.7494\n","\n","Epoch 00279: val_loss did not improve from 0.37686\n","Epoch 280/1000\n"," - 76s - loss: 0.3944 - accuracy: 0.8151 - val_loss: 0.6473 - val_accuracy: 0.7521\n","\n","Epoch 00280: val_loss did not improve from 0.37686\n","Epoch 281/1000\n"," - 76s - loss: 0.3953 - accuracy: 0.8159 - val_loss: 0.5075 - val_accuracy: 0.7482\n","\n","Epoch 00281: val_loss did not improve from 0.37686\n","Epoch 282/1000\n"," - 76s - loss: 0.3946 - accuracy: 0.8161 - val_loss: 0.5798 - val_accuracy: 0.7523\n","\n","Epoch 00282: val_loss did not improve from 0.37686\n","Epoch 283/1000\n"," - 76s - loss: 0.3923 - accuracy: 0.8172 - val_loss: 0.6272 - val_accuracy: 0.7499\n","\n","Epoch 00283: val_loss did not improve from 0.37686\n","Epoch 284/1000\n"," - 76s - loss: 0.3928 - accuracy: 0.8180 - val_loss: 0.4421 - val_accuracy: 0.7533\n","\n","Epoch 00284: val_loss did not improve from 0.37686\n","Epoch 285/1000\n"," - 76s - loss: 0.3925 - accuracy: 0.8168 - val_loss: 0.5283 - val_accuracy: 0.7532\n","\n","Epoch 00285: val_loss did not improve from 0.37686\n","Epoch 286/1000\n"," - 77s - loss: 0.3931 - accuracy: 0.8165 - val_loss: 0.5904 - val_accuracy: 0.7491\n","\n","Epoch 00286: val_loss did not improve from 0.37686\n","Epoch 287/1000\n"," - 77s - loss: 0.3938 - accuracy: 0.8173 - val_loss: 0.5815 - val_accuracy: 0.7527\n","\n","Epoch 00287: val_loss did not improve from 0.37686\n","Epoch 288/1000\n"," - 77s - loss: 0.3930 - accuracy: 0.8165 - val_loss: 0.5382 - val_accuracy: 0.7527\n","\n","Epoch 00288: val_loss did not improve from 0.37686\n","Epoch 289/1000\n"," - 76s - loss: 0.3922 - accuracy: 0.8172 - val_loss: 0.7257 - val_accuracy: 0.7517\n","\n","Epoch 00289: val_loss did not improve from 0.37686\n","Epoch 290/1000\n"," - 76s - loss: 0.3906 - accuracy: 0.8180 - val_loss: 0.5796 - val_accuracy: 0.7514\n","\n","Epoch 00290: val_loss did not improve from 0.37686\n","Epoch 291/1000\n"," - 76s - loss: 0.3925 - accuracy: 0.8167 - val_loss: 0.6960 - val_accuracy: 0.7522\n","\n","Epoch 00291: val_loss did not improve from 0.37686\n","Epoch 292/1000\n"," - 77s - loss: 0.3932 - accuracy: 0.8162 - val_loss: 0.6374 - val_accuracy: 0.7527\n","\n","Epoch 00292: val_loss did not improve from 0.37686\n","Epoch 293/1000\n"," - 76s - loss: 0.3928 - accuracy: 0.8173 - val_loss: 0.4981 - val_accuracy: 0.7526\n","\n","Epoch 00293: val_loss did not improve from 0.37686\n","Epoch 294/1000\n"," - 76s - loss: 0.3908 - accuracy: 0.8176 - val_loss: 0.5970 - val_accuracy: 0.7519\n","\n","Epoch 00294: val_loss did not improve from 0.37686\n","Epoch 295/1000\n"," - 76s - loss: 0.3901 - accuracy: 0.8195 - val_loss: 0.5556 - val_accuracy: 0.7510\n","\n","Epoch 00295: val_loss did not improve from 0.37686\n","Epoch 296/1000\n"," - 76s - loss: 0.3889 - accuracy: 0.8201 - val_loss: 0.5301 - val_accuracy: 0.7527\n","\n","Epoch 00296: val_loss did not improve from 0.37686\n","Epoch 297/1000\n"," - 76s - loss: 0.3895 - accuracy: 0.8184 - val_loss: 0.6203 - val_accuracy: 0.7513\n","\n","Epoch 00297: val_loss did not improve from 0.37686\n","Epoch 298/1000\n"," - 77s - loss: 0.3897 - accuracy: 0.8189 - val_loss: 0.6459 - val_accuracy: 0.7518\n","\n","Epoch 00298: val_loss did not improve from 0.37686\n","Epoch 299/1000\n"," - 76s - loss: 0.3895 - accuracy: 0.8191 - val_loss: 0.6908 - val_accuracy: 0.7526\n","\n","Epoch 00299: val_loss did not improve from 0.37686\n","Epoch 300/1000\n"," - 76s - loss: 0.3896 - accuracy: 0.8195 - val_loss: 0.5706 - val_accuracy: 0.7505\n","\n","Epoch 00300: val_loss did not improve from 0.37686\n","Epoch 301/1000\n"," - 76s - loss: 0.3893 - accuracy: 0.8193 - val_loss: 0.6011 - val_accuracy: 0.7519\n","\n","Epoch 00301: val_loss did not improve from 0.37686\n","Epoch 302/1000\n"," - 77s - loss: 0.3897 - accuracy: 0.8186 - val_loss: 0.6710 - val_accuracy: 0.7507\n","\n","Epoch 00302: val_loss did not improve from 0.37686\n","Epoch 303/1000\n"," - 76s - loss: 0.3895 - accuracy: 0.8179 - val_loss: 0.7607 - val_accuracy: 0.7492\n","\n","Epoch 00303: val_loss did not improve from 0.37686\n","Epoch 304/1000\n"," - 76s - loss: 0.3879 - accuracy: 0.8203 - val_loss: 0.5406 - val_accuracy: 0.7507\n","\n","Epoch 00304: val_loss did not improve from 0.37686\n","Epoch 305/1000\n"," - 76s - loss: 0.3884 - accuracy: 0.8200 - val_loss: 0.5502 - val_accuracy: 0.7535\n","\n","Epoch 00305: val_loss did not improve from 0.37686\n","Epoch 306/1000\n"," - 77s - loss: 0.3902 - accuracy: 0.8200 - val_loss: 0.7408 - val_accuracy: 0.7543\n","\n","Epoch 00306: val_loss did not improve from 0.37686\n","Epoch 307/1000\n"," - 76s - loss: 0.3871 - accuracy: 0.8202 - val_loss: 0.5022 - val_accuracy: 0.7507\n","\n","Epoch 00307: val_loss did not improve from 0.37686\n","Epoch 308/1000\n"," - 78s - loss: 0.3884 - accuracy: 0.8199 - val_loss: 0.5041 - val_accuracy: 0.7511\n","\n","Epoch 00308: val_loss did not improve from 0.37686\n","Epoch 309/1000\n"," - 76s - loss: 0.3879 - accuracy: 0.8207 - val_loss: 0.5385 - val_accuracy: 0.7524\n","\n","Epoch 00309: val_loss did not improve from 0.37686\n","Epoch 310/1000\n"," - 76s - loss: 0.3868 - accuracy: 0.8205 - val_loss: 0.4832 - val_accuracy: 0.7537\n","\n","Epoch 00310: val_loss did not improve from 0.37686\n","Epoch 311/1000\n"," - 76s - loss: 0.3873 - accuracy: 0.8202 - val_loss: 0.7455 - val_accuracy: 0.7506\n","\n","Epoch 00311: val_loss did not improve from 0.37686\n","Epoch 312/1000\n"," - 77s - loss: 0.3873 - accuracy: 0.8202 - val_loss: 0.4879 - val_accuracy: 0.7531\n","\n","Epoch 00312: val_loss did not improve from 0.37686\n","Epoch 313/1000\n"," - 76s - loss: 0.3861 - accuracy: 0.8202 - val_loss: 0.4404 - val_accuracy: 0.7523\n","\n","Epoch 00313: val_loss did not improve from 0.37686\n","Epoch 314/1000\n"," - 76s - loss: 0.3848 - accuracy: 0.8222 - val_loss: 0.6660 - val_accuracy: 0.7518\n","\n","Epoch 00314: val_loss did not improve from 0.37686\n","Epoch 315/1000\n"," - 76s - loss: 0.3872 - accuracy: 0.8205 - val_loss: 0.5426 - val_accuracy: 0.7552\n","\n","Epoch 00315: val_loss did not improve from 0.37686\n","Epoch 316/1000\n"," - 76s - loss: 0.3854 - accuracy: 0.8219 - val_loss: 0.5132 - val_accuracy: 0.7533\n","\n","Epoch 00316: val_loss did not improve from 0.37686\n","Epoch 317/1000\n"," - 77s - loss: 0.3853 - accuracy: 0.8222 - val_loss: 0.5038 - val_accuracy: 0.7537\n","\n","Epoch 00317: val_loss did not improve from 0.37686\n","Epoch 318/1000\n"," - 77s - loss: 0.3849 - accuracy: 0.8226 - val_loss: 0.6052 - val_accuracy: 0.7500\n","\n","Epoch 00318: val_loss did not improve from 0.37686\n","Epoch 319/1000\n"," - 76s - loss: 0.3844 - accuracy: 0.8233 - val_loss: 0.7917 - val_accuracy: 0.7519\n","\n","Epoch 00319: val_loss did not improve from 0.37686\n","Epoch 320/1000\n"," - 76s - loss: 0.3838 - accuracy: 0.8226 - val_loss: 0.5384 - val_accuracy: 0.7529\n","\n","Epoch 00320: val_loss did not improve from 0.37686\n","Epoch 321/1000\n"," - 76s - loss: 0.3844 - accuracy: 0.8218 - val_loss: 0.6187 - val_accuracy: 0.7532\n","\n","Epoch 00321: val_loss did not improve from 0.37686\n","Epoch 322/1000\n"," - 77s - loss: 0.3849 - accuracy: 0.8212 - val_loss: 0.5516 - val_accuracy: 0.7529\n","\n","Epoch 00322: val_loss did not improve from 0.37686\n","Epoch 323/1000\n"," - 76s - loss: 0.3838 - accuracy: 0.8228 - val_loss: 0.7215 - val_accuracy: 0.7504\n","\n","Epoch 00323: val_loss did not improve from 0.37686\n","Epoch 324/1000\n"," - 77s - loss: 0.3838 - accuracy: 0.8227 - val_loss: 0.5717 - val_accuracy: 0.7529\n","\n","Epoch 00324: val_loss did not improve from 0.37686\n","Epoch 325/1000\n"," - 76s - loss: 0.3844 - accuracy: 0.8220 - val_loss: 0.4949 - val_accuracy: 0.7541\n","\n","Epoch 00325: val_loss did not improve from 0.37686\n","Epoch 326/1000\n"," - 76s - loss: 0.3836 - accuracy: 0.8231 - val_loss: 0.6098 - val_accuracy: 0.7549\n","\n","Epoch 00326: val_loss did not improve from 0.37686\n","Epoch 327/1000\n"," - 76s - loss: 0.3829 - accuracy: 0.8232 - val_loss: 0.4610 - val_accuracy: 0.7518\n","\n","Epoch 00327: val_loss did not improve from 0.37686\n","Epoch 328/1000\n"," - 76s - loss: 0.3842 - accuracy: 0.8232 - val_loss: 0.6715 - val_accuracy: 0.7536\n","\n","Epoch 00328: val_loss did not improve from 0.37686\n","Epoch 329/1000\n"," - 76s - loss: 0.3824 - accuracy: 0.8236 - val_loss: 0.5827 - val_accuracy: 0.7543\n","\n","Epoch 00329: val_loss did not improve from 0.37686\n","Epoch 330/1000\n"," - 76s - loss: 0.3808 - accuracy: 0.8241 - val_loss: 0.6756 - val_accuracy: 0.7522\n","\n","Epoch 00330: val_loss did not improve from 0.37686\n","Epoch 331/1000\n"," - 77s - loss: 0.3817 - accuracy: 0.8234 - val_loss: 0.5926 - val_accuracy: 0.7554\n","\n","Epoch 00331: val_loss did not improve from 0.37686\n","Epoch 332/1000\n"," - 77s - loss: 0.3820 - accuracy: 0.8238 - val_loss: 0.5910 - val_accuracy: 0.7545\n","\n","Epoch 00332: val_loss did not improve from 0.37686\n","Epoch 333/1000\n"," - 76s - loss: 0.3804 - accuracy: 0.8251 - val_loss: 0.7654 - val_accuracy: 0.7534\n","\n","Epoch 00333: val_loss did not improve from 0.37686\n","Epoch 334/1000\n"," - 76s - loss: 0.3824 - accuracy: 0.8232 - val_loss: 0.4947 - val_accuracy: 0.7543\n","\n","Epoch 00334: val_loss did not improve from 0.37686\n","Epoch 335/1000\n"," - 76s - loss: 0.3822 - accuracy: 0.8235 - val_loss: 0.6082 - val_accuracy: 0.7517\n","\n","Epoch 00335: val_loss did not improve from 0.37686\n","Epoch 336/1000\n"," - 77s - loss: 0.3817 - accuracy: 0.8237 - val_loss: 0.6822 - val_accuracy: 0.7554\n","\n","Epoch 00336: val_loss did not improve from 0.37686\n","Epoch 337/1000\n"," - 76s - loss: 0.3807 - accuracy: 0.8242 - val_loss: 0.4694 - val_accuracy: 0.7549\n","\n","Epoch 00337: val_loss did not improve from 0.37686\n","Epoch 338/1000\n"," - 77s - loss: 0.3798 - accuracy: 0.8254 - val_loss: 0.5980 - val_accuracy: 0.7544\n","\n","Epoch 00338: val_loss did not improve from 0.37686\n","Epoch 339/1000\n"," - 77s - loss: 0.3792 - accuracy: 0.8254 - val_loss: 0.5986 - val_accuracy: 0.7513\n","\n","Epoch 00339: val_loss did not improve from 0.37686\n","Epoch 340/1000\n"," - 76s - loss: 0.3811 - accuracy: 0.8237 - val_loss: 0.6977 - val_accuracy: 0.7550\n","\n","Epoch 00340: val_loss did not improve from 0.37686\n","Epoch 341/1000\n"," - 76s - loss: 0.3775 - accuracy: 0.8260 - val_loss: 0.4763 - val_accuracy: 0.7521\n","\n","Epoch 00341: val_loss did not improve from 0.37686\n","Epoch 342/1000\n"," - 77s - loss: 0.3783 - accuracy: 0.8260 - val_loss: 0.7458 - val_accuracy: 0.7547\n","\n","Epoch 00342: val_loss did not improve from 0.37686\n","Epoch 343/1000\n"," - 76s - loss: 0.3779 - accuracy: 0.8267 - val_loss: 0.4798 - val_accuracy: 0.7498\n","\n","Epoch 00343: val_loss did not improve from 0.37686\n","Epoch 344/1000\n"," - 76s - loss: 0.3791 - accuracy: 0.8248 - val_loss: 0.6093 - val_accuracy: 0.7551\n","\n","Epoch 00344: val_loss did not improve from 0.37686\n","Epoch 345/1000\n"," - 76s - loss: 0.3792 - accuracy: 0.8259 - val_loss: 0.4651 - val_accuracy: 0.7540\n","\n","Epoch 00345: val_loss did not improve from 0.37686\n","Epoch 346/1000\n"," - 76s - loss: 0.3798 - accuracy: 0.8250 - val_loss: 0.5649 - val_accuracy: 0.7524\n","\n","Epoch 00346: val_loss did not improve from 0.37686\n","Epoch 347/1000\n"," - 76s - loss: 0.3775 - accuracy: 0.8253 - val_loss: 0.5670 - val_accuracy: 0.7524\n","\n","Epoch 00347: val_loss did not improve from 0.37686\n","Epoch 348/1000\n"," - 76s - loss: 0.3782 - accuracy: 0.8256 - val_loss: 0.5513 - val_accuracy: 0.7546\n","\n","Epoch 00348: val_loss did not improve from 0.37686\n","Epoch 349/1000\n"," - 76s - loss: 0.3788 - accuracy: 0.8248 - val_loss: 0.6173 - val_accuracy: 0.7555\n","\n","Epoch 00349: val_loss did not improve from 0.37686\n","Epoch 350/1000\n"," - 76s - loss: 0.3764 - accuracy: 0.8270 - val_loss: 0.4180 - val_accuracy: 0.7551\n","\n","Epoch 00350: val_loss did not improve from 0.37686\n","Epoch 351/1000\n"," - 77s - loss: 0.3778 - accuracy: 0.8258 - val_loss: 0.6250 - val_accuracy: 0.7550\n","\n","Epoch 00351: val_loss did not improve from 0.37686\n","Epoch 352/1000\n"," - 77s - loss: 0.3760 - accuracy: 0.8270 - val_loss: 0.7543 - val_accuracy: 0.7555\n","\n","Epoch 00352: val_loss did not improve from 0.37686\n","Epoch 353/1000\n"," - 76s - loss: 0.3767 - accuracy: 0.8272 - val_loss: 0.6349 - val_accuracy: 0.7529\n","\n","Epoch 00353: val_loss did not improve from 0.37686\n","Epoch 354/1000\n"," - 76s - loss: 0.3764 - accuracy: 0.8267 - val_loss: 0.7557 - val_accuracy: 0.7530\n","\n","Epoch 00354: val_loss did not improve from 0.37686\n","Epoch 355/1000\n"," - 77s - loss: 0.3767 - accuracy: 0.8269 - val_loss: 0.6675 - val_accuracy: 0.7535\n","\n","Epoch 00355: val_loss did not improve from 0.37686\n","Epoch 356/1000\n"," - 76s - loss: 0.3761 - accuracy: 0.8268 - val_loss: 0.7454 - val_accuracy: 0.7555\n","\n","Epoch 00356: val_loss did not improve from 0.37686\n","Epoch 357/1000\n"," - 77s - loss: 0.3762 - accuracy: 0.8271 - val_loss: 0.4665 - val_accuracy: 0.7485\n","\n","Epoch 00357: val_loss did not improve from 0.37686\n","Epoch 358/1000\n"," - 76s - loss: 0.3742 - accuracy: 0.8287 - val_loss: 0.4097 - val_accuracy: 0.7551\n","\n","Epoch 00358: val_loss did not improve from 0.37686\n","Epoch 359/1000\n"," - 77s - loss: 0.3759 - accuracy: 0.8266 - val_loss: 0.6057 - val_accuracy: 0.7527\n","\n","Epoch 00359: val_loss did not improve from 0.37686\n","Epoch 360/1000\n"," - 76s - loss: 0.3757 - accuracy: 0.8269 - val_loss: 0.4685 - val_accuracy: 0.7531\n","\n","Epoch 00360: val_loss did not improve from 0.37686\n","Epoch 361/1000\n"," - 76s - loss: 0.3756 - accuracy: 0.8270 - val_loss: 0.6602 - val_accuracy: 0.7541\n","\n","Epoch 00361: val_loss did not improve from 0.37686\n","Epoch 362/1000\n"," - 76s - loss: 0.3740 - accuracy: 0.8279 - val_loss: 0.4709 - val_accuracy: 0.7525\n","\n","Epoch 00362: val_loss did not improve from 0.37686\n","Epoch 363/1000\n"," - 77s - loss: 0.3735 - accuracy: 0.8285 - val_loss: 0.5702 - val_accuracy: 0.7550\n","\n","Epoch 00363: val_loss did not improve from 0.37686\n","Epoch 364/1000\n"," - 76s - loss: 0.3752 - accuracy: 0.8277 - val_loss: 0.8177 - val_accuracy: 0.7535\n","\n","Epoch 00364: val_loss did not improve from 0.37686\n","Epoch 365/1000\n"," - 76s - loss: 0.3743 - accuracy: 0.8277 - val_loss: 0.6641 - val_accuracy: 0.7531\n","\n","Epoch 00365: val_loss did not improve from 0.37686\n","Epoch 366/1000\n"," - 76s - loss: 0.3749 - accuracy: 0.8271 - val_loss: 0.5288 - val_accuracy: 0.7570\n","\n","Epoch 00366: val_loss did not improve from 0.37686\n","Epoch 367/1000\n"," - 77s - loss: 0.3750 - accuracy: 0.8276 - val_loss: 0.5189 - val_accuracy: 0.7551\n","\n","Epoch 00367: val_loss did not improve from 0.37686\n","Epoch 368/1000\n"," - 77s - loss: 0.3754 - accuracy: 0.8264 - val_loss: 0.6834 - val_accuracy: 0.7555\n","\n","Epoch 00368: val_loss did not improve from 0.37686\n","Epoch 369/1000\n"," - 76s - loss: 0.3726 - accuracy: 0.8290 - val_loss: 0.5099 - val_accuracy: 0.7543\n","\n","Epoch 00369: val_loss did not improve from 0.37686\n","Epoch 370/1000\n"," - 77s - loss: 0.3735 - accuracy: 0.8286 - val_loss: 0.4356 - val_accuracy: 0.7565\n","\n","Epoch 00370: val_loss did not improve from 0.37686\n","Epoch 371/1000\n"," - 77s - loss: 0.3739 - accuracy: 0.8288 - val_loss: 0.5109 - val_accuracy: 0.7573\n","\n","Epoch 00371: val_loss did not improve from 0.37686\n","Epoch 372/1000\n"," - 76s - loss: 0.3734 - accuracy: 0.8289 - val_loss: 0.5708 - val_accuracy: 0.7543\n","\n","Epoch 00372: val_loss did not improve from 0.37686\n","Epoch 373/1000\n"," - 77s - loss: 0.3721 - accuracy: 0.8299 - val_loss: 0.5413 - val_accuracy: 0.7562\n","\n","Epoch 00373: val_loss did not improve from 0.37686\n","Epoch 374/1000\n"," - 76s - loss: 0.3713 - accuracy: 0.8304 - val_loss: 0.5125 - val_accuracy: 0.7538\n","\n","Epoch 00374: val_loss did not improve from 0.37686\n","Epoch 375/1000\n"," - 76s - loss: 0.3724 - accuracy: 0.8291 - val_loss: 0.6338 - val_accuracy: 0.7548\n","\n","Epoch 00375: val_loss did not improve from 0.37686\n","Epoch 376/1000\n"," - 76s - loss: 0.3733 - accuracy: 0.8286 - val_loss: 0.5816 - val_accuracy: 0.7550\n","\n","Epoch 00376: val_loss did not improve from 0.37686\n","Epoch 377/1000\n"," - 77s - loss: 0.3715 - accuracy: 0.8305 - val_loss: 0.6486 - val_accuracy: 0.7555\n","\n","Epoch 00377: val_loss did not improve from 0.37686\n","Epoch 378/1000\n"," - 76s - loss: 0.3719 - accuracy: 0.8290 - val_loss: 0.7099 - val_accuracy: 0.7521\n","\n","Epoch 00378: val_loss did not improve from 0.37686\n","Epoch 379/1000\n"," - 76s - loss: 0.3720 - accuracy: 0.8289 - val_loss: 0.5827 - val_accuracy: 0.7555\n","\n","Epoch 00379: val_loss did not improve from 0.37686\n","Epoch 380/1000\n"," - 77s - loss: 0.3710 - accuracy: 0.8295 - val_loss: 0.5278 - val_accuracy: 0.7540\n","\n","Epoch 00380: val_loss did not improve from 0.37686\n","Epoch 381/1000\n"," - 76s - loss: 0.3700 - accuracy: 0.8301 - val_loss: 0.7657 - val_accuracy: 0.7550\n","\n","Epoch 00381: val_loss did not improve from 0.37686\n","Epoch 382/1000\n"," - 77s - loss: 0.3701 - accuracy: 0.8316 - val_loss: 0.4819 - val_accuracy: 0.7519\n","\n","Epoch 00382: val_loss did not improve from 0.37686\n","Epoch 383/1000\n"," - 76s - loss: 0.3721 - accuracy: 0.8290 - val_loss: 0.6082 - val_accuracy: 0.7525\n","\n","Epoch 00383: val_loss did not improve from 0.37686\n","Epoch 384/1000\n"," - 76s - loss: 0.3691 - accuracy: 0.8309 - val_loss: 0.6347 - val_accuracy: 0.7557\n","\n","Epoch 00384: val_loss did not improve from 0.37686\n","Epoch 385/1000\n"," - 76s - loss: 0.3700 - accuracy: 0.8301 - val_loss: 0.4972 - val_accuracy: 0.7551\n","\n","Epoch 00385: val_loss did not improve from 0.37686\n","Epoch 386/1000\n"," - 76s - loss: 0.3698 - accuracy: 0.8307 - val_loss: 0.8163 - val_accuracy: 0.7550\n","\n","Epoch 00386: val_loss did not improve from 0.37686\n","Epoch 387/1000\n"," - 77s - loss: 0.3713 - accuracy: 0.8290 - val_loss: 0.5658 - val_accuracy: 0.7568\n","\n","Epoch 00387: val_loss did not improve from 0.37686\n","Epoch 388/1000\n"," - 77s - loss: 0.3698 - accuracy: 0.8317 - val_loss: 0.7288 - val_accuracy: 0.7556\n","\n","Epoch 00388: val_loss did not improve from 0.37686\n","Epoch 389/1000\n"," - 76s - loss: 0.3690 - accuracy: 0.8317 - val_loss: 0.6826 - val_accuracy: 0.7578\n","\n","Epoch 00389: val_loss did not improve from 0.37686\n","Epoch 390/1000\n"," - 76s - loss: 0.3673 - accuracy: 0.8320 - val_loss: 0.6938 - val_accuracy: 0.7555\n","\n","Epoch 00390: val_loss did not improve from 0.37686\n","Epoch 391/1000\n"," - 76s - loss: 0.3678 - accuracy: 0.8318 - val_loss: 0.5836 - val_accuracy: 0.7528\n","\n","Epoch 00391: val_loss did not improve from 0.37686\n","Epoch 392/1000\n"," - 76s - loss: 0.3690 - accuracy: 0.8312 - val_loss: 0.5334 - val_accuracy: 0.7563\n","\n","Epoch 00392: val_loss did not improve from 0.37686\n","Epoch 393/1000\n"," - 77s - loss: 0.3688 - accuracy: 0.8304 - val_loss: 0.5602 - val_accuracy: 0.7555\n","\n","Epoch 00393: val_loss did not improve from 0.37686\n","Epoch 394/1000\n"," - 77s - loss: 0.3673 - accuracy: 0.8318 - val_loss: 0.6748 - val_accuracy: 0.7559\n","\n","Epoch 00394: val_loss did not improve from 0.37686\n","Epoch 395/1000\n"," - 77s - loss: 0.3677 - accuracy: 0.8314 - val_loss: 0.5401 - val_accuracy: 0.7578\n","\n","Epoch 00395: val_loss did not improve from 0.37686\n","Epoch 396/1000\n"," - 77s - loss: 0.3674 - accuracy: 0.8314 - val_loss: 0.5395 - val_accuracy: 0.7536\n","\n","Epoch 00396: val_loss did not improve from 0.37686\n","Epoch 397/1000\n"," - 77s - loss: 0.3659 - accuracy: 0.8331 - val_loss: 0.7340 - val_accuracy: 0.7534\n","\n","Epoch 00397: val_loss did not improve from 0.37686\n","Epoch 398/1000\n"," - 77s - loss: 0.3665 - accuracy: 0.8319 - val_loss: 0.5713 - val_accuracy: 0.7562\n","\n","Epoch 00398: val_loss did not improve from 0.37686\n","Epoch 399/1000\n"," - 76s - loss: 0.3663 - accuracy: 0.8314 - val_loss: 0.4590 - val_accuracy: 0.7522\n","\n","Epoch 00399: val_loss did not improve from 0.37686\n","Epoch 400/1000\n"," - 77s - loss: 0.3667 - accuracy: 0.8317 - val_loss: 0.6173 - val_accuracy: 0.7532\n","\n","Epoch 00400: val_loss did not improve from 0.37686\n","Epoch 401/1000\n"," - 76s - loss: 0.3648 - accuracy: 0.8335 - val_loss: 0.5612 - val_accuracy: 0.7564\n","\n","Epoch 00401: val_loss did not improve from 0.37686\n","Epoch 402/1000\n"," - 76s - loss: 0.3665 - accuracy: 0.8328 - val_loss: 0.6106 - val_accuracy: 0.7563\n","\n","Epoch 00402: val_loss did not improve from 0.37686\n","Epoch 403/1000\n"," - 76s - loss: 0.3660 - accuracy: 0.8335 - val_loss: 0.6441 - val_accuracy: 0.7540\n","\n","Epoch 00403: val_loss did not improve from 0.37686\n","Epoch 404/1000\n"," - 77s - loss: 0.3659 - accuracy: 0.8327 - val_loss: 0.4580 - val_accuracy: 0.7578\n","\n","Epoch 00404: val_loss did not improve from 0.37686\n","Epoch 405/1000\n"," - 77s - loss: 0.3659 - accuracy: 0.8321 - val_loss: 0.6751 - val_accuracy: 0.7571\n","\n","Epoch 00405: val_loss did not improve from 0.37686\n","Epoch 406/1000\n"," - 77s - loss: 0.3680 - accuracy: 0.8327 - val_loss: 0.6493 - val_accuracy: 0.7556\n","\n","Epoch 00406: val_loss did not improve from 0.37686\n","Epoch 407/1000\n"," - 77s - loss: 0.3647 - accuracy: 0.8338 - val_loss: 0.4759 - val_accuracy: 0.7584\n","\n","Epoch 00407: val_loss did not improve from 0.37686\n","Epoch 408/1000\n"," - 77s - loss: 0.3651 - accuracy: 0.8336 - val_loss: 0.4610 - val_accuracy: 0.7542\n","\n","Epoch 00408: val_loss did not improve from 0.37686\n","Epoch 409/1000\n"," - 77s - loss: 0.3642 - accuracy: 0.8336 - val_loss: 0.5625 - val_accuracy: 0.7522\n","\n","Epoch 00409: val_loss did not improve from 0.37686\n","Epoch 410/1000\n"," - 77s - loss: 0.3651 - accuracy: 0.8330 - val_loss: 0.5099 - val_accuracy: 0.7558\n","\n","Epoch 00410: val_loss did not improve from 0.37686\n","Epoch 411/1000\n"," - 77s - loss: 0.3654 - accuracy: 0.8325 - val_loss: 0.6412 - val_accuracy: 0.7577\n","\n","Epoch 00411: val_loss did not improve from 0.37686\n","Epoch 412/1000\n"," - 77s - loss: 0.3632 - accuracy: 0.8339 - val_loss: 0.5589 - val_accuracy: 0.7576\n","\n","Epoch 00412: val_loss did not improve from 0.37686\n","Epoch 413/1000\n"," - 76s - loss: 0.3649 - accuracy: 0.8335 - val_loss: 0.6009 - val_accuracy: 0.7562\n","\n","Epoch 00413: val_loss did not improve from 0.37686\n","Epoch 414/1000\n"," - 76s - loss: 0.3649 - accuracy: 0.8335 - val_loss: 0.7030 - val_accuracy: 0.7574\n","\n","Epoch 00414: val_loss did not improve from 0.37686\n","Epoch 415/1000\n"," - 76s - loss: 0.3619 - accuracy: 0.8349 - val_loss: 0.5362 - val_accuracy: 0.7542\n","\n","Epoch 00415: val_loss did not improve from 0.37686\n","Epoch 416/1000\n"," - 77s - loss: 0.3645 - accuracy: 0.8327 - val_loss: 0.6007 - val_accuracy: 0.7554\n","\n","Epoch 00416: val_loss did not improve from 0.37686\n","Epoch 417/1000\n"," - 76s - loss: 0.3635 - accuracy: 0.8346 - val_loss: 0.5412 - val_accuracy: 0.7568\n","\n","Epoch 00417: val_loss did not improve from 0.37686\n","Epoch 418/1000\n"," - 77s - loss: 0.3622 - accuracy: 0.8352 - val_loss: 0.4925 - val_accuracy: 0.7565\n","\n","Epoch 00418: val_loss did not improve from 0.37686\n","Epoch 419/1000\n"," - 77s - loss: 0.3640 - accuracy: 0.8338 - val_loss: 0.8583 - val_accuracy: 0.7506\n","\n","Epoch 00419: val_loss did not improve from 0.37686\n","Epoch 420/1000\n"," - 77s - loss: 0.3622 - accuracy: 0.8359 - val_loss: 0.6886 - val_accuracy: 0.7572\n","\n","Epoch 00420: val_loss did not improve from 0.37686\n","Epoch 421/1000\n"," - 76s - loss: 0.3620 - accuracy: 0.8358 - val_loss: 0.4967 - val_accuracy: 0.7561\n","\n","Epoch 00421: val_loss did not improve from 0.37686\n","Epoch 422/1000\n"," - 77s - loss: 0.3616 - accuracy: 0.8353 - val_loss: 0.5249 - val_accuracy: 0.7550\n","\n","Epoch 00422: val_loss did not improve from 0.37686\n","Epoch 423/1000\n"," - 77s - loss: 0.3624 - accuracy: 0.8346 - val_loss: 0.5588 - val_accuracy: 0.7567\n","\n","Epoch 00423: val_loss did not improve from 0.37686\n","Epoch 424/1000\n"," - 77s - loss: 0.3612 - accuracy: 0.8364 - val_loss: 0.6935 - val_accuracy: 0.7560\n","\n","Epoch 00424: val_loss did not improve from 0.37686\n","Epoch 425/1000\n"," - 76s - loss: 0.3611 - accuracy: 0.8351 - val_loss: 0.4614 - val_accuracy: 0.7564\n","\n","Epoch 00425: val_loss did not improve from 0.37686\n","Epoch 426/1000\n"," - 77s - loss: 0.3608 - accuracy: 0.8362 - val_loss: 0.6306 - val_accuracy: 0.7561\n","\n","Epoch 00426: val_loss did not improve from 0.37686\n","Epoch 427/1000\n"," - 76s - loss: 0.3615 - accuracy: 0.8358 - val_loss: 0.7040 - val_accuracy: 0.7579\n","\n","Epoch 00427: val_loss did not improve from 0.37686\n","Epoch 428/1000\n"," - 76s - loss: 0.3609 - accuracy: 0.8354 - val_loss: 0.6078 - val_accuracy: 0.7537\n","\n","Epoch 00428: val_loss did not improve from 0.37686\n","Epoch 429/1000\n"," - 76s - loss: 0.3606 - accuracy: 0.8368 - val_loss: 0.7262 - val_accuracy: 0.7584\n","\n","Epoch 00429: val_loss did not improve from 0.37686\n","Epoch 430/1000\n"," - 77s - loss: 0.3610 - accuracy: 0.8363 - val_loss: 0.4546 - val_accuracy: 0.7560\n","\n","Epoch 00430: val_loss did not improve from 0.37686\n","Epoch 431/1000\n"," - 76s - loss: 0.3582 - accuracy: 0.8374 - val_loss: 0.4918 - val_accuracy: 0.7538\n","\n","Epoch 00431: val_loss did not improve from 0.37686\n","Epoch 432/1000\n"," - 77s - loss: 0.3593 - accuracy: 0.8366 - val_loss: 0.6723 - val_accuracy: 0.7559\n","\n","Epoch 00432: val_loss did not improve from 0.37686\n","Epoch 433/1000\n"," - 76s - loss: 0.3617 - accuracy: 0.8349 - val_loss: 0.6610 - val_accuracy: 0.7573\n","\n","Epoch 00433: val_loss did not improve from 0.37686\n","Epoch 434/1000\n"," - 76s - loss: 0.3595 - accuracy: 0.8370 - val_loss: 0.5918 - val_accuracy: 0.7552\n","\n","Epoch 00434: val_loss did not improve from 0.37686\n","Epoch 435/1000\n"," - 76s - loss: 0.3583 - accuracy: 0.8368 - val_loss: 0.6388 - val_accuracy: 0.7564\n","\n","Epoch 00435: val_loss did not improve from 0.37686\n","Epoch 436/1000\n"," - 77s - loss: 0.3591 - accuracy: 0.8366 - val_loss: 0.6542 - val_accuracy: 0.7560\n","\n","Epoch 00436: val_loss did not improve from 0.37686\n","Epoch 437/1000\n"," - 76s - loss: 0.3578 - accuracy: 0.8381 - val_loss: 0.5510 - val_accuracy: 0.7590\n","\n","Epoch 00437: val_loss did not improve from 0.37686\n","Epoch 438/1000\n"," - 77s - loss: 0.3589 - accuracy: 0.8371 - val_loss: 0.5745 - val_accuracy: 0.7574\n","\n","Epoch 00438: val_loss did not improve from 0.37686\n","Epoch 439/1000\n"," - 77s - loss: 0.3571 - accuracy: 0.8378 - val_loss: 0.5703 - val_accuracy: 0.7583\n","\n","Epoch 00439: val_loss did not improve from 0.37686\n","Epoch 440/1000\n"," - 77s - loss: 0.3591 - accuracy: 0.8372 - val_loss: 0.7395 - val_accuracy: 0.7589\n","\n","Epoch 00440: val_loss did not improve from 0.37686\n","Epoch 441/1000\n"," - 77s - loss: 0.3579 - accuracy: 0.8373 - val_loss: 0.6781 - val_accuracy: 0.7563\n","\n","Epoch 00441: val_loss did not improve from 0.37686\n","Epoch 442/1000\n"," - 76s - loss: 0.3575 - accuracy: 0.8380 - val_loss: 0.6240 - val_accuracy: 0.7576\n","\n","Epoch 00442: val_loss did not improve from 0.37686\n","Epoch 443/1000\n"," - 76s - loss: 0.3557 - accuracy: 0.8380 - val_loss: 0.6738 - val_accuracy: 0.7541\n","\n","Epoch 00443: val_loss did not improve from 0.37686\n","Epoch 444/1000\n"," - 77s - loss: 0.3574 - accuracy: 0.8373 - val_loss: 0.6403 - val_accuracy: 0.7572\n","\n","Epoch 00444: val_loss did not improve from 0.37686\n","Epoch 445/1000\n"," - 77s - loss: 0.3582 - accuracy: 0.8373 - val_loss: 0.4443 - val_accuracy: 0.7552\n","\n","Epoch 00445: val_loss did not improve from 0.37686\n","Epoch 446/1000\n"," - 77s - loss: 0.3564 - accuracy: 0.8384 - val_loss: 0.5807 - val_accuracy: 0.7568\n","\n","Epoch 00446: val_loss did not improve from 0.37686\n","Epoch 447/1000\n"," - 77s - loss: 0.3577 - accuracy: 0.8373 - val_loss: 0.5794 - val_accuracy: 0.7567\n","\n","Epoch 00447: val_loss did not improve from 0.37686\n","Epoch 448/1000\n"," - 76s - loss: 0.3565 - accuracy: 0.8384 - val_loss: 0.6358 - val_accuracy: 0.7567\n","\n","Epoch 00448: val_loss did not improve from 0.37686\n","Epoch 449/1000\n"," - 77s - loss: 0.3573 - accuracy: 0.8377 - val_loss: 0.4443 - val_accuracy: 0.7557\n","\n","Epoch 00449: val_loss did not improve from 0.37686\n","Epoch 450/1000\n"," - 76s - loss: 0.3556 - accuracy: 0.8382 - val_loss: 0.7970 - val_accuracy: 0.7590\n","\n","Epoch 00450: val_loss did not improve from 0.37686\n","Epoch 451/1000\n"," - 76s - loss: 0.3545 - accuracy: 0.8386 - val_loss: 0.5288 - val_accuracy: 0.7574\n","\n","Epoch 00451: val_loss did not improve from 0.37686\n","Epoch 452/1000\n"," - 76s - loss: 0.3555 - accuracy: 0.8384 - val_loss: 0.6788 - val_accuracy: 0.7576\n","\n","Epoch 00452: val_loss did not improve from 0.37686\n","Epoch 453/1000\n"," - 76s - loss: 0.3557 - accuracy: 0.8382 - val_loss: 0.5250 - val_accuracy: 0.7566\n","\n","Epoch 00453: val_loss did not improve from 0.37686\n","Epoch 454/1000\n"," - 77s - loss: 0.3536 - accuracy: 0.8411 - val_loss: 0.6066 - val_accuracy: 0.7573\n","\n","Epoch 00454: val_loss did not improve from 0.37686\n","Epoch 455/1000\n"," - 76s - loss: 0.3559 - accuracy: 0.8384 - val_loss: 0.5854 - val_accuracy: 0.7573\n","\n","Epoch 00455: val_loss did not improve from 0.37686\n","Epoch 456/1000\n"," - 77s - loss: 0.3560 - accuracy: 0.8383 - val_loss: 0.5828 - val_accuracy: 0.7597\n","\n","Epoch 00456: val_loss did not improve from 0.37686\n","Epoch 457/1000\n"," - 76s - loss: 0.3549 - accuracy: 0.8391 - val_loss: 0.5322 - val_accuracy: 0.7574\n","\n","Epoch 00457: val_loss did not improve from 0.37686\n","Epoch 458/1000\n"," - 77s - loss: 0.3541 - accuracy: 0.8396 - val_loss: 0.7446 - val_accuracy: 0.7573\n","\n","Epoch 00458: val_loss did not improve from 0.37686\n","Epoch 459/1000\n"," - 76s - loss: 0.3530 - accuracy: 0.8395 - val_loss: 0.5495 - val_accuracy: 0.7576\n","\n","Epoch 00459: val_loss did not improve from 0.37686\n","Epoch 460/1000\n"," - 76s - loss: 0.3550 - accuracy: 0.8389 - val_loss: 0.4531 - val_accuracy: 0.7593\n","\n","Epoch 00460: val_loss did not improve from 0.37686\n","Epoch 461/1000\n"," - 77s - loss: 0.3554 - accuracy: 0.8380 - val_loss: 0.4935 - val_accuracy: 0.7565\n","\n","Epoch 00461: val_loss did not improve from 0.37686\n","Epoch 462/1000\n"," - 77s - loss: 0.3538 - accuracy: 0.8389 - val_loss: 0.5826 - val_accuracy: 0.7584\n","\n","Epoch 00462: val_loss did not improve from 0.37686\n","Epoch 463/1000\n"," - 77s - loss: 0.3527 - accuracy: 0.8400 - val_loss: 0.6758 - val_accuracy: 0.7574\n","\n","Epoch 00463: val_loss did not improve from 0.37686\n","Epoch 464/1000\n"," - 77s - loss: 0.3525 - accuracy: 0.8404 - val_loss: 0.6484 - val_accuracy: 0.7569\n","\n","Epoch 00464: val_loss did not improve from 0.37686\n","Epoch 465/1000\n"," - 77s - loss: 0.3536 - accuracy: 0.8391 - val_loss: 0.6425 - val_accuracy: 0.7593\n","\n","Epoch 00465: val_loss did not improve from 0.37686\n","Epoch 466/1000\n"," - 76s - loss: 0.3538 - accuracy: 0.8404 - val_loss: 0.6066 - val_accuracy: 0.7595\n","\n","Epoch 00466: val_loss did not improve from 0.37686\n","Epoch 467/1000\n"," - 76s - loss: 0.3524 - accuracy: 0.8406 - val_loss: 0.4986 - val_accuracy: 0.7591\n","\n","Epoch 00467: val_loss did not improve from 0.37686\n","Epoch 468/1000\n"," - 76s - loss: 0.3517 - accuracy: 0.8398 - val_loss: 0.5282 - val_accuracy: 0.7587\n","\n","Epoch 00468: val_loss did not improve from 0.37686\n","Epoch 469/1000\n"," - 76s - loss: 0.3528 - accuracy: 0.8403 - val_loss: 0.5821 - val_accuracy: 0.7574\n","\n","Epoch 00469: val_loss did not improve from 0.37686\n","Epoch 470/1000\n"," - 77s - loss: 0.3519 - accuracy: 0.8406 - val_loss: 0.6013 - val_accuracy: 0.7564\n","\n","Epoch 00470: val_loss did not improve from 0.37686\n","Epoch 471/1000\n"," - 76s - loss: 0.3504 - accuracy: 0.8410 - val_loss: 0.7659 - val_accuracy: 0.7596\n","\n","Epoch 00471: val_loss did not improve from 0.37686\n","Epoch 472/1000\n"," - 76s - loss: 0.3527 - accuracy: 0.8398 - val_loss: 0.5785 - val_accuracy: 0.7576\n","\n","Epoch 00472: val_loss did not improve from 0.37686\n","Epoch 473/1000\n"," - 77s - loss: 0.3501 - accuracy: 0.8419 - val_loss: 0.5791 - val_accuracy: 0.7583\n","\n","Epoch 00473: val_loss did not improve from 0.37686\n","Epoch 474/1000\n"," - 76s - loss: 0.3505 - accuracy: 0.8411 - val_loss: 0.4910 - val_accuracy: 0.7585\n","\n","Epoch 00474: val_loss did not improve from 0.37686\n","Epoch 475/1000\n"," - 76s - loss: 0.3509 - accuracy: 0.8405 - val_loss: 0.5040 - val_accuracy: 0.7602\n","\n","Epoch 00475: val_loss did not improve from 0.37686\n","Epoch 476/1000\n"," - 77s - loss: 0.3499 - accuracy: 0.8411 - val_loss: 0.6646 - val_accuracy: 0.7588\n","\n","Epoch 00476: val_loss did not improve from 0.37686\n","Epoch 477/1000\n"," - 76s - loss: 0.3492 - accuracy: 0.8418 - val_loss: 0.6076 - val_accuracy: 0.7582\n","\n","Epoch 00477: val_loss did not improve from 0.37686\n","Epoch 478/1000\n"," - 76s - loss: 0.3507 - accuracy: 0.8419 - val_loss: 0.5075 - val_accuracy: 0.7594\n","\n","Epoch 00478: val_loss did not improve from 0.37686\n","Epoch 479/1000\n"," - 77s - loss: 0.3484 - accuracy: 0.8429 - val_loss: 0.5773 - val_accuracy: 0.7562\n","\n","Epoch 00479: val_loss did not improve from 0.37686\n","Epoch 480/1000\n"," - 77s - loss: 0.3505 - accuracy: 0.8422 - val_loss: 0.5675 - val_accuracy: 0.7593\n","\n","Epoch 00480: val_loss did not improve from 0.37686\n","Epoch 481/1000\n"," - 77s - loss: 0.3487 - accuracy: 0.8410 - val_loss: 0.5563 - val_accuracy: 0.7570\n","\n","Epoch 00481: val_loss did not improve from 0.37686\n","Epoch 482/1000\n"," - 76s - loss: 0.3481 - accuracy: 0.8425 - val_loss: 0.6036 - val_accuracy: 0.7587\n","\n","Epoch 00482: val_loss did not improve from 0.37686\n","Epoch 483/1000\n"," - 76s - loss: 0.3491 - accuracy: 0.8422 - val_loss: 0.5954 - val_accuracy: 0.7589\n","\n","Epoch 00483: val_loss did not improve from 0.37686\n","Epoch 484/1000\n"," - 76s - loss: 0.3483 - accuracy: 0.8428 - val_loss: 0.5969 - val_accuracy: 0.7586\n","\n","Epoch 00484: val_loss did not improve from 0.37686\n","Epoch 485/1000\n"," - 76s - loss: 0.3466 - accuracy: 0.8439 - val_loss: 0.6608 - val_accuracy: 0.7592\n","\n","Epoch 00485: val_loss did not improve from 0.37686\n","Epoch 486/1000\n"," - 77s - loss: 0.3484 - accuracy: 0.8422 - val_loss: 0.4993 - val_accuracy: 0.7580\n","\n","Epoch 00486: val_loss did not improve from 0.37686\n","Epoch 487/1000\n"," - 76s - loss: 0.3495 - accuracy: 0.8414 - val_loss: 0.6467 - val_accuracy: 0.7598\n","\n","Epoch 00487: val_loss did not improve from 0.37686\n","Epoch 488/1000\n"," - 77s - loss: 0.3481 - accuracy: 0.8423 - val_loss: 0.7148 - val_accuracy: 0.7600\n","\n","Epoch 00488: val_loss did not improve from 0.37686\n","Epoch 489/1000\n"," - 76s - loss: 0.3487 - accuracy: 0.8426 - val_loss: 0.6512 - val_accuracy: 0.7568\n","\n","Epoch 00489: val_loss did not improve from 0.37686\n","Epoch 490/1000\n"," - 77s - loss: 0.3484 - accuracy: 0.8416 - val_loss: 0.6188 - val_accuracy: 0.7576\n","\n","Epoch 00490: val_loss did not improve from 0.37686\n","Epoch 491/1000\n"," - 77s - loss: 0.3501 - accuracy: 0.8407 - val_loss: 0.6303 - val_accuracy: 0.7590\n","\n","Epoch 00491: val_loss did not improve from 0.37686\n","Epoch 492/1000\n"," - 77s - loss: 0.3463 - accuracy: 0.8433 - val_loss: 0.7720 - val_accuracy: 0.7608\n","\n","Epoch 00492: val_loss did not improve from 0.37686\n","Epoch 493/1000\n"," - 77s - loss: 0.3483 - accuracy: 0.8411 - val_loss: 0.6717 - val_accuracy: 0.7593\n","\n","Epoch 00493: val_loss did not improve from 0.37686\n","Epoch 494/1000\n"," - 76s - loss: 0.3481 - accuracy: 0.8430 - val_loss: 0.5186 - val_accuracy: 0.7604\n","\n","Epoch 00494: val_loss did not improve from 0.37686\n","Epoch 495/1000\n"," - 76s - loss: 0.3468 - accuracy: 0.8437 - val_loss: 0.5920 - val_accuracy: 0.7569\n","\n","Epoch 00495: val_loss did not improve from 0.37686\n","Epoch 496/1000\n"," - 76s - loss: 0.3453 - accuracy: 0.8447 - val_loss: 0.7411 - val_accuracy: 0.7589\n","\n","Epoch 00496: val_loss did not improve from 0.37686\n","Epoch 497/1000\n"," - 77s - loss: 0.3476 - accuracy: 0.8433 - val_loss: 0.5870 - val_accuracy: 0.7581\n","\n","Epoch 00497: val_loss did not improve from 0.37686\n","Epoch 498/1000\n"," - 76s - loss: 0.3465 - accuracy: 0.8433 - val_loss: 0.6881 - val_accuracy: 0.7582\n","\n","Epoch 00498: val_loss did not improve from 0.37686\n","Epoch 499/1000\n"," - 77s - loss: 0.3450 - accuracy: 0.8451 - val_loss: 0.7742 - val_accuracy: 0.7603\n","\n","Epoch 00499: val_loss did not improve from 0.37686\n","Epoch 500/1000\n"," - 77s - loss: 0.3469 - accuracy: 0.8437 - val_loss: 0.6930 - val_accuracy: 0.7591\n","\n","Epoch 00500: val_loss did not improve from 0.37686\n","Epoch 501/1000\n"," - 77s - loss: 0.3455 - accuracy: 0.8440 - val_loss: 0.4345 - val_accuracy: 0.7596\n","\n","Epoch 00501: val_loss did not improve from 0.37686\n","Epoch 502/1000\n"," - 76s - loss: 0.3444 - accuracy: 0.8451 - val_loss: 0.5981 - val_accuracy: 0.7569\n","\n","Epoch 00502: val_loss did not improve from 0.37686\n","Epoch 503/1000\n"," - 76s - loss: 0.3446 - accuracy: 0.8453 - val_loss: 0.7361 - val_accuracy: 0.7571\n","\n","Epoch 00503: val_loss did not improve from 0.37686\n","Epoch 504/1000\n"," - 76s - loss: 0.3464 - accuracy: 0.8435 - val_loss: 0.6220 - val_accuracy: 0.7594\n","\n","Epoch 00504: val_loss did not improve from 0.37686\n","Epoch 505/1000\n"," - 77s - loss: 0.3449 - accuracy: 0.8443 - val_loss: 0.8789 - val_accuracy: 0.7581\n","\n","Epoch 00505: val_loss did not improve from 0.37686\n","Epoch 506/1000\n"," - 77s - loss: 0.3450 - accuracy: 0.8443 - val_loss: 0.5392 - val_accuracy: 0.7585\n","\n","Epoch 00506: val_loss did not improve from 0.37686\n","Epoch 507/1000\n"," - 77s - loss: 0.3439 - accuracy: 0.8444 - val_loss: 0.6112 - val_accuracy: 0.7598\n","\n","Epoch 00507: val_loss did not improve from 0.37686\n","Epoch 508/1000\n"," - 76s - loss: 0.3426 - accuracy: 0.8466 - val_loss: 0.5198 - val_accuracy: 0.7600\n","\n","Epoch 00508: val_loss did not improve from 0.37686\n","Epoch 509/1000\n"," - 76s - loss: 0.3445 - accuracy: 0.8444 - val_loss: 0.7090 - val_accuracy: 0.7592\n","\n","Epoch 00509: val_loss did not improve from 0.37686\n","Epoch 510/1000\n"," - 77s - loss: 0.3437 - accuracy: 0.8444 - val_loss: 0.8905 - val_accuracy: 0.7564\n","\n","Epoch 00510: val_loss did not improve from 0.37686\n","Epoch 511/1000\n"," - 76s - loss: 0.3422 - accuracy: 0.8459 - val_loss: 0.4456 - val_accuracy: 0.7578\n","\n","Epoch 00511: val_loss did not improve from 0.37686\n","Epoch 512/1000\n"," - 76s - loss: 0.3420 - accuracy: 0.8459 - val_loss: 0.5704 - val_accuracy: 0.7567\n","\n","Epoch 00512: val_loss did not improve from 0.37686\n","Epoch 513/1000\n"," - 76s - loss: 0.3429 - accuracy: 0.8463 - val_loss: 0.7318 - val_accuracy: 0.7569\n","\n","Epoch 00513: val_loss did not improve from 0.37686\n","Epoch 514/1000\n"," - 76s - loss: 0.3424 - accuracy: 0.8463 - val_loss: 0.7250 - val_accuracy: 0.7587\n","\n","Epoch 00514: val_loss did not improve from 0.37686\n","Epoch 515/1000\n"," - 76s - loss: 0.3448 - accuracy: 0.8435 - val_loss: 0.7039 - val_accuracy: 0.7582\n","\n","Epoch 00515: val_loss did not improve from 0.37686\n","Epoch 516/1000\n"," - 76s - loss: 0.3399 - accuracy: 0.8481 - val_loss: 1.0960 - val_accuracy: 0.7578\n","\n","Epoch 00516: val_loss did not improve from 0.37686\n","Epoch 517/1000\n"," - 77s - loss: 0.3400 - accuracy: 0.8478 - val_loss: 0.4549 - val_accuracy: 0.7589\n","\n","Epoch 00517: val_loss did not improve from 0.37686\n","Epoch 518/1000\n"," - 77s - loss: 0.3419 - accuracy: 0.8470 - val_loss: 0.6278 - val_accuracy: 0.7571\n","\n","Epoch 00518: val_loss did not improve from 0.37686\n","Epoch 519/1000\n"," - 76s - loss: 0.3424 - accuracy: 0.8460 - val_loss: 0.6931 - val_accuracy: 0.7588\n","\n","Epoch 00519: val_loss did not improve from 0.37686\n","Epoch 520/1000\n"," - 77s - loss: 0.3402 - accuracy: 0.8467 - val_loss: 0.4924 - val_accuracy: 0.7588\n","\n","Epoch 00520: val_loss did not improve from 0.37686\n","Epoch 521/1000\n"," - 77s - loss: 0.3431 - accuracy: 0.8455 - val_loss: 0.4383 - val_accuracy: 0.7610\n","\n","Epoch 00521: val_loss did not improve from 0.37686\n","Epoch 522/1000\n"," - 76s - loss: 0.3409 - accuracy: 0.8467 - val_loss: 0.6209 - val_accuracy: 0.7606\n","\n","Epoch 00522: val_loss did not improve from 0.37686\n","Epoch 523/1000\n"," - 76s - loss: 0.3409 - accuracy: 0.8466 - val_loss: 0.5968 - val_accuracy: 0.7571\n","\n","Epoch 00523: val_loss did not improve from 0.37686\n","Epoch 524/1000\n"," - 76s - loss: 0.3413 - accuracy: 0.8458 - val_loss: 0.6838 - val_accuracy: 0.7597\n","\n","Epoch 00524: val_loss did not improve from 0.37686\n","Epoch 525/1000\n"," - 76s - loss: 0.3389 - accuracy: 0.8493 - val_loss: 0.6918 - val_accuracy: 0.7595\n","\n","Epoch 00525: val_loss did not improve from 0.37686\n","Epoch 526/1000\n"," - 76s - loss: 0.3401 - accuracy: 0.8475 - val_loss: 0.5739 - val_accuracy: 0.7598\n","\n","Epoch 00526: val_loss did not improve from 0.37686\n","Epoch 527/1000\n"," - 76s - loss: 0.3421 - accuracy: 0.8458 - val_loss: 0.7439 - val_accuracy: 0.7581\n","\n","Epoch 00527: val_loss did not improve from 0.37686\n","Epoch 528/1000\n"," - 76s - loss: 0.3394 - accuracy: 0.8473 - val_loss: 0.6466 - val_accuracy: 0.7593\n","\n","Epoch 00528: val_loss did not improve from 0.37686\n","Epoch 529/1000\n"," - 77s - loss: 0.3405 - accuracy: 0.8465 - val_loss: 0.7441 - val_accuracy: 0.7594\n","\n","Epoch 00529: val_loss did not improve from 0.37686\n","Epoch 530/1000\n"," - 77s - loss: 0.3408 - accuracy: 0.8465 - val_loss: 0.5920 - val_accuracy: 0.7581\n","\n","Epoch 00530: val_loss did not improve from 0.37686\n","Epoch 531/1000\n"," - 77s - loss: 0.3399 - accuracy: 0.8478 - val_loss: 0.5864 - val_accuracy: 0.7593\n","\n","Epoch 00531: val_loss did not improve from 0.37686\n","Epoch 532/1000\n"," - 77s - loss: 0.3411 - accuracy: 0.8468 - val_loss: 0.4651 - val_accuracy: 0.7588\n","\n","Epoch 00532: val_loss did not improve from 0.37686\n","Epoch 533/1000\n"," - 77s - loss: 0.3386 - accuracy: 0.8475 - val_loss: 0.4933 - val_accuracy: 0.7601\n","\n","Epoch 00533: val_loss did not improve from 0.37686\n","Epoch 534/1000\n"," - 77s - loss: 0.3393 - accuracy: 0.8480 - val_loss: 0.6745 - val_accuracy: 0.7598\n","\n","Epoch 00534: val_loss did not improve from 0.37686\n","Epoch 535/1000\n"," - 76s - loss: 0.3373 - accuracy: 0.8485 - val_loss: 0.4972 - val_accuracy: 0.7611\n","\n","Epoch 00535: val_loss did not improve from 0.37686\n","Epoch 536/1000\n"," - 77s - loss: 0.3397 - accuracy: 0.8469 - val_loss: 0.6084 - val_accuracy: 0.7600\n","\n","Epoch 00536: val_loss did not improve from 0.37686\n","Epoch 537/1000\n"," - 77s - loss: 0.3387 - accuracy: 0.8471 - val_loss: 0.6734 - val_accuracy: 0.7590\n","\n","Epoch 00537: val_loss did not improve from 0.37686\n","Epoch 538/1000\n"," - 77s - loss: 0.3377 - accuracy: 0.8493 - val_loss: 0.4304 - val_accuracy: 0.7582\n","\n","Epoch 00538: val_loss did not improve from 0.37686\n","Epoch 539/1000\n"," - 77s - loss: 0.3376 - accuracy: 0.8484 - val_loss: 0.7028 - val_accuracy: 0.7589\n","\n","Epoch 00539: val_loss did not improve from 0.37686\n","Epoch 540/1000\n"," - 77s - loss: 0.3367 - accuracy: 0.8487 - val_loss: 0.5373 - val_accuracy: 0.7600\n","\n","Epoch 00540: val_loss did not improve from 0.37686\n","Epoch 541/1000\n"," - 77s - loss: 0.3392 - accuracy: 0.8483 - val_loss: 0.6051 - val_accuracy: 0.7605\n","\n","Epoch 00541: val_loss did not improve from 0.37686\n","Epoch 542/1000\n"," - 76s - loss: 0.3384 - accuracy: 0.8487 - val_loss: 0.5884 - val_accuracy: 0.7613\n","\n","Epoch 00542: val_loss did not improve from 0.37686\n","Epoch 543/1000\n"," - 76s - loss: 0.3381 - accuracy: 0.8481 - val_loss: 0.5289 - val_accuracy: 0.7593\n","\n","Epoch 00543: val_loss did not improve from 0.37686\n","Epoch 544/1000\n"," - 76s - loss: 0.3382 - accuracy: 0.8485 - val_loss: 0.4838 - val_accuracy: 0.7615\n","\n","Epoch 00544: val_loss did not improve from 0.37686\n","Epoch 545/1000\n"," - 76s - loss: 0.3360 - accuracy: 0.8481 - val_loss: 0.6260 - val_accuracy: 0.7603\n","\n","Epoch 00545: val_loss did not improve from 0.37686\n","Epoch 546/1000\n"," - 76s - loss: 0.3376 - accuracy: 0.8480 - val_loss: 0.6086 - val_accuracy: 0.7614\n","\n","Epoch 00546: val_loss did not improve from 0.37686\n","Epoch 547/1000\n"," - 77s - loss: 0.3359 - accuracy: 0.8493 - val_loss: 0.5297 - val_accuracy: 0.7618\n","\n","Epoch 00547: val_loss did not improve from 0.37686\n","Epoch 548/1000\n"," - 77s - loss: 0.3367 - accuracy: 0.8488 - val_loss: 0.6114 - val_accuracy: 0.7613\n","\n","Epoch 00548: val_loss did not improve from 0.37686\n","Epoch 549/1000\n"," - 76s - loss: 0.3375 - accuracy: 0.8489 - val_loss: 0.5958 - val_accuracy: 0.7609\n","\n","Epoch 00549: val_loss did not improve from 0.37686\n","Epoch 550/1000\n"," - 77s - loss: 0.3360 - accuracy: 0.8494 - val_loss: 0.5616 - val_accuracy: 0.7611\n","\n","Epoch 00550: val_loss did not improve from 0.37686\n","Epoch 551/1000\n"," - 76s - loss: 0.3369 - accuracy: 0.8488 - val_loss: 0.5695 - val_accuracy: 0.7591\n","\n","Epoch 00551: val_loss did not improve from 0.37686\n","Epoch 552/1000\n"," - 77s - loss: 0.3349 - accuracy: 0.8504 - val_loss: 0.3845 - val_accuracy: 0.7610\n","\n","Epoch 00552: val_loss did not improve from 0.37686\n","Epoch 553/1000\n"," - 76s - loss: 0.3364 - accuracy: 0.8493 - val_loss: 0.7013 - val_accuracy: 0.7610\n","\n","Epoch 00553: val_loss did not improve from 0.37686\n","Epoch 554/1000\n"," - 76s - loss: 0.3357 - accuracy: 0.8500 - val_loss: 0.5875 - val_accuracy: 0.7611\n","\n","Epoch 00554: val_loss did not improve from 0.37686\n","Epoch 555/1000\n"," - 77s - loss: 0.3354 - accuracy: 0.8499 - val_loss: 0.5199 - val_accuracy: 0.7601\n","\n","Epoch 00555: val_loss did not improve from 0.37686\n","Epoch 556/1000\n"," - 76s - loss: 0.3349 - accuracy: 0.8499 - val_loss: 0.6657 - val_accuracy: 0.7632\n","\n","Epoch 00556: val_loss did not improve from 0.37686\n","Epoch 557/1000\n"," - 77s - loss: 0.3367 - accuracy: 0.8489 - val_loss: 0.4974 - val_accuracy: 0.7608\n","\n","Epoch 00557: val_loss did not improve from 0.37686\n","Epoch 558/1000\n"," - 77s - loss: 0.3326 - accuracy: 0.8515 - val_loss: 0.5600 - val_accuracy: 0.7594\n","\n","Epoch 00558: val_loss did not improve from 0.37686\n","Epoch 559/1000\n"," - 76s - loss: 0.3354 - accuracy: 0.8493 - val_loss: 0.6360 - val_accuracy: 0.7613\n","\n","Epoch 00559: val_loss did not improve from 0.37686\n","Epoch 560/1000\n"," - 77s - loss: 0.3350 - accuracy: 0.8494 - val_loss: 0.7360 - val_accuracy: 0.7573\n","\n","Epoch 00560: val_loss did not improve from 0.37686\n","Epoch 561/1000\n"," - 76s - loss: 0.3329 - accuracy: 0.8518 - val_loss: 0.4471 - val_accuracy: 0.7615\n","\n","Epoch 00561: val_loss did not improve from 0.37686\n","Epoch 562/1000\n"," - 77s - loss: 0.3337 - accuracy: 0.8512 - val_loss: 0.5776 - val_accuracy: 0.7607\n","\n","Epoch 00562: val_loss did not improve from 0.37686\n","Epoch 563/1000\n"," - 76s - loss: 0.3337 - accuracy: 0.8501 - val_loss: 0.7476 - val_accuracy: 0.7611\n","\n","Epoch 00563: val_loss did not improve from 0.37686\n","Epoch 564/1000\n"," - 76s - loss: 0.3339 - accuracy: 0.8501 - val_loss: 0.4850 - val_accuracy: 0.7606\n","\n","Epoch 00564: val_loss did not improve from 0.37686\n","Epoch 565/1000\n"," - 76s - loss: 0.3334 - accuracy: 0.8515 - val_loss: 0.5544 - val_accuracy: 0.7607\n","\n","Epoch 00565: val_loss did not improve from 0.37686\n","Epoch 566/1000\n"," - 76s - loss: 0.3338 - accuracy: 0.8504 - val_loss: 0.5446 - val_accuracy: 0.7602\n","\n","Epoch 00566: val_loss did not improve from 0.37686\n","Epoch 567/1000\n"," - 76s - loss: 0.3311 - accuracy: 0.8525 - val_loss: 0.6498 - val_accuracy: 0.7622\n","\n","Epoch 00567: val_loss did not improve from 0.37686\n","Epoch 568/1000\n"," - 76s - loss: 0.3318 - accuracy: 0.8522 - val_loss: 0.4761 - val_accuracy: 0.7567\n","\n","Epoch 00568: val_loss did not improve from 0.37686\n","Epoch 569/1000\n"," - 76s - loss: 0.3331 - accuracy: 0.8514 - val_loss: 0.4777 - val_accuracy: 0.7578\n","\n","Epoch 00569: val_loss did not improve from 0.37686\n","Epoch 570/1000\n"," - 76s - loss: 0.3324 - accuracy: 0.8516 - val_loss: 0.7534 - val_accuracy: 0.7616\n","\n","Epoch 00570: val_loss did not improve from 0.37686\n","Epoch 571/1000\n"," - 77s - loss: 0.3312 - accuracy: 0.8518 - val_loss: 0.6872 - val_accuracy: 0.7584\n","\n","Epoch 00571: val_loss did not improve from 0.37686\n","Epoch 572/1000\n"," - 77s - loss: 0.3322 - accuracy: 0.8515 - val_loss: 0.5794 - val_accuracy: 0.7621\n","\n","Epoch 00572: val_loss did not improve from 0.37686\n","Epoch 573/1000\n"," - 76s - loss: 0.3313 - accuracy: 0.8522 - val_loss: 0.4770 - val_accuracy: 0.7593\n","\n","Epoch 00573: val_loss did not improve from 0.37686\n","Epoch 574/1000\n"," - 77s - loss: 0.3315 - accuracy: 0.8519 - val_loss: 0.6185 - val_accuracy: 0.7620\n","\n","Epoch 00574: val_loss did not improve from 0.37686\n","Epoch 575/1000\n"," - 77s - loss: 0.3317 - accuracy: 0.8511 - val_loss: 0.7465 - val_accuracy: 0.7595\n","\n","Epoch 00575: val_loss did not improve from 0.37686\n","Epoch 576/1000\n"," - 76s - loss: 0.3311 - accuracy: 0.8529 - val_loss: 0.7447 - val_accuracy: 0.7610\n","\n","Epoch 00576: val_loss did not improve from 0.37686\n","Epoch 577/1000\n"," - 76s - loss: 0.3302 - accuracy: 0.8525 - val_loss: 0.5699 - val_accuracy: 0.7588\n","\n","Epoch 00577: val_loss did not improve from 0.37686\n","Epoch 578/1000\n"," - 76s - loss: 0.3309 - accuracy: 0.8525 - val_loss: 0.6270 - val_accuracy: 0.7623\n","\n","Epoch 00578: val_loss did not improve from 0.37686\n","Epoch 579/1000\n"," - 76s - loss: 0.3306 - accuracy: 0.8534 - val_loss: 0.5859 - val_accuracy: 0.7609\n","\n","Epoch 00579: val_loss did not improve from 0.37686\n","Epoch 580/1000\n"," - 76s - loss: 0.3304 - accuracy: 0.8527 - val_loss: 0.7356 - val_accuracy: 0.7613\n","\n","Epoch 00580: val_loss did not improve from 0.37686\n","Epoch 581/1000\n"," - 76s - loss: 0.3311 - accuracy: 0.8521 - val_loss: 0.5160 - val_accuracy: 0.7597\n","\n","Epoch 00581: val_loss did not improve from 0.37686\n","Epoch 582/1000\n"," - 76s - loss: 0.3297 - accuracy: 0.8524 - val_loss: 0.6339 - val_accuracy: 0.7601\n","\n","Epoch 00582: val_loss did not improve from 0.37686\n","Epoch 583/1000\n"," - 76s - loss: 0.3292 - accuracy: 0.8534 - val_loss: 0.6522 - val_accuracy: 0.7565\n","\n","Epoch 00583: val_loss did not improve from 0.37686\n","Epoch 584/1000\n"," - 76s - loss: 0.3285 - accuracy: 0.8537 - val_loss: 0.6027 - val_accuracy: 0.7630\n","\n","Epoch 00584: val_loss did not improve from 0.37686\n","Epoch 585/1000\n"," - 76s - loss: 0.3302 - accuracy: 0.8527 - val_loss: 0.5322 - val_accuracy: 0.7623\n","\n","Epoch 00585: val_loss did not improve from 0.37686\n","Epoch 586/1000\n"," - 76s - loss: 0.3283 - accuracy: 0.8530 - val_loss: 0.7397 - val_accuracy: 0.7608\n","\n","Epoch 00586: val_loss did not improve from 0.37686\n","Epoch 587/1000\n"," - 77s - loss: 0.3296 - accuracy: 0.8527 - val_loss: 0.6469 - val_accuracy: 0.7624\n","\n","Epoch 00587: val_loss did not improve from 0.37686\n","Epoch 588/1000\n"," - 76s - loss: 0.3287 - accuracy: 0.8537 - val_loss: 0.7199 - val_accuracy: 0.7622\n","\n","Epoch 00588: val_loss did not improve from 0.37686\n","Epoch 589/1000\n"," - 76s - loss: 0.3280 - accuracy: 0.8535 - val_loss: 0.6780 - val_accuracy: 0.7606\n","\n","Epoch 00589: val_loss did not improve from 0.37686\n","Epoch 590/1000\n"," - 76s - loss: 0.3279 - accuracy: 0.8541 - val_loss: 0.5125 - val_accuracy: 0.7607\n","\n","Epoch 00590: val_loss did not improve from 0.37686\n","Epoch 591/1000\n"," - 76s - loss: 0.3277 - accuracy: 0.8540 - val_loss: 0.6430 - val_accuracy: 0.7582\n","\n","Epoch 00591: val_loss did not improve from 0.37686\n","Epoch 592/1000\n"," - 76s - loss: 0.3293 - accuracy: 0.8530 - val_loss: 0.7382 - val_accuracy: 0.7600\n","\n","Epoch 00592: val_loss did not improve from 0.37686\n","Epoch 593/1000\n"," - 76s - loss: 0.3284 - accuracy: 0.8537 - val_loss: 0.7884 - val_accuracy: 0.7608\n","\n","Epoch 00593: val_loss did not improve from 0.37686\n","Epoch 594/1000\n"," - 77s - loss: 0.3268 - accuracy: 0.8540 - val_loss: 0.5186 - val_accuracy: 0.7608\n","\n","Epoch 00594: val_loss did not improve from 0.37686\n","Epoch 595/1000\n"," - 76s - loss: 0.3277 - accuracy: 0.8539 - val_loss: 0.7445 - val_accuracy: 0.7622\n","\n","Epoch 00595: val_loss did not improve from 0.37686\n","Epoch 596/1000\n"," - 77s - loss: 0.3279 - accuracy: 0.8541 - val_loss: 0.8202 - val_accuracy: 0.7604\n","\n","Epoch 00596: val_loss did not improve from 0.37686\n","Epoch 597/1000\n"," - 76s - loss: 0.3276 - accuracy: 0.8534 - val_loss: 0.5271 - val_accuracy: 0.7610\n","\n","Epoch 00597: val_loss did not improve from 0.37686\n","Epoch 598/1000\n"," - 76s - loss: 0.3250 - accuracy: 0.8553 - val_loss: 0.7261 - val_accuracy: 0.7597\n","\n","Epoch 00598: val_loss did not improve from 0.37686\n","Epoch 599/1000\n"," - 77s - loss: 0.3261 - accuracy: 0.8550 - val_loss: 0.4025 - val_accuracy: 0.7622\n","\n","Epoch 00599: val_loss did not improve from 0.37686\n","Epoch 600/1000\n"," - 76s - loss: 0.3281 - accuracy: 0.8540 - val_loss: 0.6222 - val_accuracy: 0.7610\n","\n","Epoch 00600: val_loss did not improve from 0.37686\n","Epoch 601/1000\n"," - 76s - loss: 0.3261 - accuracy: 0.8545 - val_loss: 0.6166 - val_accuracy: 0.7598\n","\n","Epoch 00601: val_loss did not improve from 0.37686\n","Epoch 602/1000\n"," - 76s - loss: 0.3282 - accuracy: 0.8537 - val_loss: 0.7023 - val_accuracy: 0.7610\n","\n","Epoch 00602: val_loss did not improve from 0.37686\n","Epoch 603/1000\n"," - 76s - loss: 0.3246 - accuracy: 0.8561 - val_loss: 0.5574 - val_accuracy: 0.7608\n","\n","Epoch 00603: val_loss did not improve from 0.37686\n","Epoch 604/1000\n"," - 77s - loss: 0.3263 - accuracy: 0.8550 - val_loss: 0.6868 - val_accuracy: 0.7609\n","\n","Epoch 00604: val_loss did not improve from 0.37686\n","Epoch 605/1000\n"," - 76s - loss: 0.3271 - accuracy: 0.8543 - val_loss: 0.6494 - val_accuracy: 0.7616\n","\n","Epoch 00605: val_loss did not improve from 0.37686\n","Epoch 606/1000\n"," - 76s - loss: 0.3261 - accuracy: 0.8545 - val_loss: 0.7170 - val_accuracy: 0.7631\n","\n","Epoch 00606: val_loss did not improve from 0.37686\n","Epoch 607/1000\n"," - 76s - loss: 0.3255 - accuracy: 0.8555 - val_loss: 0.6071 - val_accuracy: 0.7614\n","\n","Epoch 00607: val_loss did not improve from 0.37686\n","Epoch 608/1000\n"," - 77s - loss: 0.3235 - accuracy: 0.8569 - val_loss: 0.6503 - val_accuracy: 0.7613\n","\n","Epoch 00608: val_loss did not improve from 0.37686\n","Epoch 609/1000\n"," - 76s - loss: 0.3251 - accuracy: 0.8548 - val_loss: 0.6196 - val_accuracy: 0.7618\n","\n","Epoch 00609: val_loss did not improve from 0.37686\n","Epoch 610/1000\n"," - 76s - loss: 0.3249 - accuracy: 0.8556 - val_loss: 0.6334 - val_accuracy: 0.7632\n","\n","Epoch 00610: val_loss did not improve from 0.37686\n","Epoch 611/1000\n"," - 76s - loss: 0.3246 - accuracy: 0.8559 - val_loss: 0.7298 - val_accuracy: 0.7609\n","\n","Epoch 00611: val_loss did not improve from 0.37686\n","Epoch 612/1000\n"," - 77s - loss: 0.3263 - accuracy: 0.8542 - val_loss: 0.6665 - val_accuracy: 0.7619\n","\n","Epoch 00612: val_loss did not improve from 0.37686\n","Epoch 613/1000\n"," - 76s - loss: 0.3257 - accuracy: 0.8554 - val_loss: 0.5351 - val_accuracy: 0.7629\n","\n","Epoch 00613: val_loss did not improve from 0.37686\n","Epoch 614/1000\n"," - 77s - loss: 0.3256 - accuracy: 0.8549 - val_loss: 0.7039 - val_accuracy: 0.7608\n","\n","Epoch 00614: val_loss did not improve from 0.37686\n","Epoch 615/1000\n"," - 76s - loss: 0.3256 - accuracy: 0.8549 - val_loss: 0.7113 - val_accuracy: 0.7627\n","\n","Epoch 00615: val_loss did not improve from 0.37686\n","Epoch 616/1000\n"," - 76s - loss: 0.3236 - accuracy: 0.8559 - val_loss: 0.6885 - val_accuracy: 0.7627\n","\n","Epoch 00616: val_loss did not improve from 0.37686\n","Epoch 617/1000\n"," - 76s - loss: 0.3218 - accuracy: 0.8571 - val_loss: 0.7760 - val_accuracy: 0.7613\n","\n","Epoch 00617: val_loss did not improve from 0.37686\n","Epoch 618/1000\n"," - 76s - loss: 0.3244 - accuracy: 0.8555 - val_loss: 0.5733 - val_accuracy: 0.7628\n","\n","Epoch 00618: val_loss did not improve from 0.37686\n","Epoch 619/1000\n"," - 76s - loss: 0.3245 - accuracy: 0.8547 - val_loss: 0.7852 - val_accuracy: 0.7625\n","\n","Epoch 00619: val_loss did not improve from 0.37686\n","Epoch 620/1000\n"," - 76s - loss: 0.3227 - accuracy: 0.8566 - val_loss: 0.5372 - val_accuracy: 0.7620\n","\n","Epoch 00620: val_loss did not improve from 0.37686\n","Epoch 621/1000\n"," - 76s - loss: 0.3220 - accuracy: 0.8582 - val_loss: 0.5549 - val_accuracy: 0.7630\n","\n","Epoch 00621: val_loss did not improve from 0.37686\n","Epoch 622/1000\n"," - 77s - loss: 0.3232 - accuracy: 0.8564 - val_loss: 0.7887 - val_accuracy: 0.7630\n","\n","Epoch 00622: val_loss did not improve from 0.37686\n","Epoch 623/1000\n"," - 76s - loss: 0.3217 - accuracy: 0.8583 - val_loss: 0.5275 - val_accuracy: 0.7615\n","\n","Epoch 00623: val_loss did not improve from 0.37686\n","Epoch 624/1000\n"," - 77s - loss: 0.3219 - accuracy: 0.8563 - val_loss: 0.7594 - val_accuracy: 0.7627\n","\n","Epoch 00624: val_loss did not improve from 0.37686\n","Epoch 625/1000\n"," - 76s - loss: 0.3217 - accuracy: 0.8571 - val_loss: 0.6359 - val_accuracy: 0.7611\n","\n","Epoch 00625: val_loss did not improve from 0.37686\n","Epoch 626/1000\n"," - 76s - loss: 0.3226 - accuracy: 0.8568 - val_loss: 0.7161 - val_accuracy: 0.7629\n","\n","Epoch 00626: val_loss did not improve from 0.37686\n","Epoch 627/1000\n"," - 76s - loss: 0.3212 - accuracy: 0.8575 - val_loss: 0.6411 - val_accuracy: 0.7615\n","\n","Epoch 00627: val_loss did not improve from 0.37686\n","Epoch 628/1000\n"," - 76s - loss: 0.3229 - accuracy: 0.8564 - val_loss: 0.5850 - val_accuracy: 0.7600\n","\n","Epoch 00628: val_loss did not improve from 0.37686\n","Epoch 629/1000\n"," - 76s - loss: 0.3222 - accuracy: 0.8572 - val_loss: 0.6813 - val_accuracy: 0.7598\n","\n","Epoch 00629: val_loss did not improve from 0.37686\n","Epoch 630/1000\n"," - 76s - loss: 0.3199 - accuracy: 0.8585 - val_loss: 0.5687 - val_accuracy: 0.7632\n","\n","Epoch 00630: val_loss did not improve from 0.37686\n","Epoch 631/1000\n"," - 77s - loss: 0.3226 - accuracy: 0.8563 - val_loss: 0.5050 - val_accuracy: 0.7607\n","\n","Epoch 00631: val_loss did not improve from 0.37686\n","Epoch 632/1000\n"," - 76s - loss: 0.3229 - accuracy: 0.8569 - val_loss: 0.7845 - val_accuracy: 0.7619\n","\n","Epoch 00632: val_loss did not improve from 0.37686\n","Epoch 633/1000\n"," - 76s - loss: 0.3189 - accuracy: 0.8592 - val_loss: 0.5287 - val_accuracy: 0.7626\n","\n","Epoch 00633: val_loss did not improve from 0.37686\n","Epoch 634/1000\n"," - 76s - loss: 0.3193 - accuracy: 0.8594 - val_loss: 0.5598 - val_accuracy: 0.7626\n","\n","Epoch 00634: val_loss did not improve from 0.37686\n","Epoch 635/1000\n"," - 76s - loss: 0.3191 - accuracy: 0.8590 - val_loss: 0.6081 - val_accuracy: 0.7608\n","\n","Epoch 00635: val_loss did not improve from 0.37686\n","Epoch 636/1000\n"," - 76s - loss: 0.3189 - accuracy: 0.8590 - val_loss: 0.4259 - val_accuracy: 0.7617\n","\n","Epoch 00636: val_loss did not improve from 0.37686\n","Epoch 637/1000\n"," - 77s - loss: 0.3199 - accuracy: 0.8583 - val_loss: 0.7741 - val_accuracy: 0.7621\n","\n","Epoch 00637: val_loss did not improve from 0.37686\n","Epoch 638/1000\n"," - 76s - loss: 0.3221 - accuracy: 0.8568 - val_loss: 0.6237 - val_accuracy: 0.7620\n","\n","Epoch 00638: val_loss did not improve from 0.37686\n","Epoch 639/1000\n"," - 76s - loss: 0.3193 - accuracy: 0.8583 - val_loss: 0.7035 - val_accuracy: 0.7623\n","\n","Epoch 00639: val_loss did not improve from 0.37686\n","Epoch 640/1000\n"," - 77s - loss: 0.3194 - accuracy: 0.8588 - val_loss: 0.6607 - val_accuracy: 0.7617\n","\n","Epoch 00640: val_loss did not improve from 0.37686\n","Epoch 641/1000\n"," - 76s - loss: 0.3194 - accuracy: 0.8594 - val_loss: 0.6861 - val_accuracy: 0.7610\n","\n","Epoch 00641: val_loss did not improve from 0.37686\n","Epoch 642/1000\n"," - 76s - loss: 0.3179 - accuracy: 0.8591 - val_loss: 0.6477 - val_accuracy: 0.7614\n","\n","Epoch 00642: val_loss did not improve from 0.37686\n","Epoch 643/1000\n"," - 76s - loss: 0.3206 - accuracy: 0.8582 - val_loss: 0.4995 - val_accuracy: 0.7622\n","\n","Epoch 00643: val_loss did not improve from 0.37686\n","Epoch 644/1000\n"," - 76s - loss: 0.3202 - accuracy: 0.8580 - val_loss: 0.6783 - val_accuracy: 0.7619\n","\n","Epoch 00644: val_loss did not improve from 0.37686\n","Epoch 645/1000\n"," - 77s - loss: 0.3184 - accuracy: 0.8599 - val_loss: 0.7147 - val_accuracy: 0.7625\n","\n","Epoch 00645: val_loss did not improve from 0.37686\n","Epoch 646/1000\n"," - 77s - loss: 0.3179 - accuracy: 0.8596 - val_loss: 0.7820 - val_accuracy: 0.7611\n","\n","Epoch 00646: val_loss did not improve from 0.37686\n","Epoch 647/1000\n"," - 76s - loss: 0.3171 - accuracy: 0.8597 - val_loss: 0.5614 - val_accuracy: 0.7629\n","\n","Epoch 00647: val_loss did not improve from 0.37686\n","Epoch 648/1000\n"," - 76s - loss: 0.3179 - accuracy: 0.8593 - val_loss: 0.4751 - val_accuracy: 0.7629\n","\n","Epoch 00648: val_loss did not improve from 0.37686\n","Epoch 649/1000\n"," - 76s - loss: 0.3183 - accuracy: 0.8587 - val_loss: 0.8388 - val_accuracy: 0.7632\n","\n","Epoch 00649: val_loss did not improve from 0.37686\n","Epoch 650/1000\n"," - 76s - loss: 0.3177 - accuracy: 0.8590 - val_loss: 0.6639 - val_accuracy: 0.7630\n","\n","Epoch 00650: val_loss did not improve from 0.37686\n","Epoch 651/1000\n"," - 77s - loss: 0.3169 - accuracy: 0.8604 - val_loss: 0.6165 - val_accuracy: 0.7644\n","\n","Epoch 00651: val_loss did not improve from 0.37686\n","Epoch 652/1000\n"," - 76s - loss: 0.3174 - accuracy: 0.8603 - val_loss: 0.6503 - val_accuracy: 0.7628\n","\n","Epoch 00652: val_loss did not improve from 0.37686\n","Epoch 653/1000\n"],"name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-8-94e1fe20354b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     38\u001b[0m                     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_flow\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m                     \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_val\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m                     shuffle=False)\n\u001b[0m","\u001b[0;32m/tensorflow-1.15.2/python3.7/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tensorflow-1.15.2/python3.7/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1730\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1731\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1732\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1733\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tensorflow-1.15.2/python3.7/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    218\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m                                             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 220\u001b[0;31m                                             reset_metrics=False)\n\u001b[0m\u001b[1;32m    221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tensorflow-1.15.2/python3.7/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight, reset_metrics)\u001b[0m\n\u001b[1;32m   1512\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1514\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1515\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1516\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tensorflow-1.15.2/python3.7/tensorflow_core/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3474\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3475\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 3476\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   3477\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3478\u001b[0m     output_structure = nest.pack_sequence_as(\n","\u001b[0;32m/tensorflow-1.15.2/python3.7/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1470\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1471\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1472\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1473\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1474\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","metadata":{"id":"pfPMGMvm0rGu","executionInfo":{"status":"ok","timestamp":1618538410286,"user_tz":-540,"elapsed":7878,"user":{"displayName":"7th June","photoUrl":"","userId":"08178289703395036410"}}},"source":["# model_name = \"classifier_45_min_pr_re2.h5\"  # <-- specifying model name\n","# ckpt_path = current_path + 'ckpt/'\n","# model_name = 'classifier_%s_ma7_pr3.h5' % period\n","\n","model = keras.models.load_model(ckpt_path + model_name)"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"zhRu2BIK792m","colab":{"base_uri":"https://localhost:8080/","height":626},"executionInfo":{"status":"error","timestamp":1618538422329,"user_tz":-540,"elapsed":16764,"user":{"displayName":"7th June","photoUrl":"","userId":"08178289703395036410"}},"outputId":"b3c52310-dd57-4f56-c53d-404ad359be32"},"source":["test_result = model.predict(x_test)\n","# test_result = model.predict(test_set)\n","\n","print('test_result.shape :', test_result.shape)\n","print('pr_val.shape :', pr_val.shape)\n","\n","y_score = test_result[:, [1]]\n","print('y_test[:5] :', y_test.reshape(-1,)[:5])\n","# print('np.unique(y_test) :', np.unique(y_test, return_counts=True))\n","print('y_score[:5] :', y_score[:5])\n","# print('np.unique(y_score) :', np.unique(y_score, return_counts=True))\n","\n","print('y_test.shape :', y_test.shape)\n","print('y_score.shape :', y_score.shape)\n","\n","print('len(y_test) :', len(y_test))\n","\n","#     precision recall curve   #\n","precision, recall, threshold = precision_recall_curve(y_test, y_score)\n","precision, recall = precision[:-1], recall[:-1]\n","\n","plt.plot(threshold, precision, label='precision')\n","plt.plot(threshold, recall, label='recall')\n","plt.legend()\n","plt.title('precision recall')\n","plt.show()\n","# print(y_pred)\n","\n","# thresh = 0.6\n","# threshold = [thresh]\n","print('threshold :', threshold)\n","break\n","\n","acc_pr_bythr = []\n","for thresh in threshold:\n","\n","  y_pred = np.where(y_score[:, -1] > thresh, 1, 0)\n","  print('y_pred.shape :', y_pred.shape)\n","  # print('y_pred :', y_pred)\n","\n","  #     compare precision     #\n","\n","  print('thresh :', thresh)\n","  print('precision :', precision_score(y_test, y_pred))\n","  print('recall :', recall_score(y_test, y_pred))\n","  print()\n","\n","  print('np.isnan(np.sum(x_test)) :', np.isnan(np.sum(x_test)))\n","  print('np.isnan(np.sum(y_test)) :', np.isnan(np.sum(y_test)))\n","\n","  # plot_confusion_matrix(best_model, x_test, y_test, normalize=None)\n","  # plt.show()  \n","  print()\n","\n","  #     check win-ratio improvement     #\n","  cmat = confusion_matrix(y_test, y_pred)\n","  # print(cmat)\n","  # print(np.sum(cmat, axis=1))\n","\n","  test_size = len(y_test)\n","  test_pr_list = pr_test\n","  print('origin ac_pr :', np.cumprod(test_pr_list)[-1])\n","\n","  org_wr = np.sum(cmat, axis=1)[-1] / sum(np.sum(cmat, axis=1))\n","  ml_wr = cmat[1][1] / np.sum(cmat, axis=0)[-1]\n","  print('win ratio improvement %.2f --> %.2f' % (org_wr, ml_wr))\n","\n","  # print('pr_test.shape :', pr_test.shape)\n","\n","  # print(y_pred)\n","  # print(test_pr_list)\n","  pred_pr_list = np.where(y_pred == 1, test_pr_list.reshape(-1, ), 1.0)\n","  # print('pred_pr_list.shape :', pred_pr_list.shape)\n","\n","  if np.cumprod(test_pr_list)[-1] < np.cumprod(pred_pr_list)[-1]:\n","    print('accum_pr increased ! : %.3f --> %.3f' % (np.cumprod(test_pr_list)[-1], np.cumprod(pred_pr_list)[-1]))\n","    print('thresh :', thresh)\n","    \n","  # if len(threshold) == 1:\n","    plt.figure(figsize=(10, 5))\n","    plt.subplot(121)\n","    plt.plot(np.cumprod(test_pr_list))\n","    plt.title('%.3f' % (np.cumprod(test_pr_list)[-1]))\n","  # plt.show()\n","\n","    plt.subplot(122)\n","    plt.plot(np.cumprod(pred_pr_list))\n","    plt.title('%.3f' % (np.cumprod(pred_pr_list)[-1]))\n","    plt.show()\n","\n","\n","  acc_pr_bythr.append(np.cumprod(pred_pr_list)[-1])\n","\n","print('acc_pr_bythr :', acc_pr_bythr)\n","\n","plt.figure(figsize=(10, 5))\n","plt.subplot(121)\n","plt.plot(threshold, precision, label='precision')\n","plt.plot(threshold, recall, label='recall')\n","plt.legend()\n","plt.title('precision recall')\n","# plt.show()\n","plt.subplot(122)\n","plt.plot(threshold, acc_pr_bythr)\n","plt.axhline(np.cumprod(test_pr_list)[-1], linestyle='--', color='r')\n","plt.axvline(threshold[np.argmax(acc_pr_bythr)], linestyle='--', color='b')\n","plt.title('best thr : %.4f' % threshold[np.argmax(acc_pr_bythr)])\n","plt.show()"],"execution_count":10,"outputs":[{"output_type":"stream","text":["test_result.shape : (63354, 2)\n","pr_val.shape : (63354, 1)\n","y_test[:5] : [1 0 1 1 0]\n","y_score[:5] : [[0.94643515]\n"," [0.33013365]\n"," [0.28666568]\n"," [0.11195175]\n"," [0.22927664]]\n","y_test.shape : (63354, 1)\n","y_score.shape : (63354, 1)\n","len(y_test) : 63354\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}},{"output_type":"stream","text":["threshold : [3.4450607e-06 4.1998760e-06 6.9573111e-06 ... 9.9999380e-01 9.9999893e-01\n"," 9.9999928e-01]\n"],"name":"stdout"},{"output_type":"error","ename":"SyntaxError","evalue":"ignored","traceback":["\u001b[0;36m  File \u001b[0;32m\"<ipython-input-10-55468d3fb15f>\"\u001b[0;36m, line \u001b[0;32m32\u001b[0m\n\u001b[0;31m    break\u001b[0m\n\u001b[0m         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m 'break' outside loop\n"]}]},{"cell_type":"markdown","metadata":{"id":"-jo3k5MdhFyg"},"source":["#### **clustering output**"]},{"cell_type":"code","metadata":{"id":"njxxm-TJ-RP-"},"source":["# x_train_for_k = test_result.flatten().reshape(-1, 1)\n","x_train_for_k = test_result\n","print(x_train_for_k[:10])\n","# x_train_for_k = test_result[:, [1]]\n","pr_train = pr_test\n","\n","print('x_train_for_k.shape :', x_train_for_k.shape)\n","print('pr_train.shape :', pr_train.shape)\n","\n","K = range(2, 10)\n","s_dist = []\n","sil = []\n","for k in K:\n","  # if cen_data.shape[0] < k:\n","  #   break\n","\n","  km = KMeans(n_clusters=k)\n","  km = km.fit(x_train_for_k)\n","\n","  labels = km.labels_\n","  # print('len(labels) :', len(labels))\n","  # print('labels[:10] :', labels[:10])\n","  sil.append(silhouette_score(x_train_for_k, labels, metric='euclidean'))\n","\n","  # inertia = km.inertia_\n","  # s_dist.append(inertia)\n","\n","best_k = K[np.argmax(np.array(sil))]\n","\n","plt.figure(figsize=(10, 5))\n","plt.plot(K, sil)\n","plt.axvline(best_k, linestyle='--')\n","# plt.plot(K, s_dist)\n","plt.show()\n","\n","\n","\n","\n","\n","#   with best_k, label 별 pr_list 확인\n","km = KMeans(n_clusters=best_k)\n","km = km.fit(x_train_for_k)\n","\n","labels = km.labels_\n","\n","print(km.score(x_train_for_k))\n","print(len(labels), len(pr_train))\n","\n","\n","\n","\n","\n","#   label 별로 profit 을 저장, 승률을 확인한다\n","label_types = np.unique(labels, return_counts=False)\n","\n","label_pr_dict = {}\n","#   init dict   #\n","for label in label_types:\n","  label_pr_dict[label] = []\n","print(label_pr_dict)\n","# break\n","\n","for i, (label, pr) in enumerate(zip(labels, pr_train)):\n","  label_pr_dict[label].append(pr[0])\n","\n","  \n","# for label in label_types:\n","print(label_pr_dict)\n","\n","\n","\n","\n","\n","def win_ratio(list_x):\n","\n","  win_cnt = np.sum(np.array(list_x) > 1)\n","  return win_cnt / len(list_x)\n","\n","\n","def acc_pr(list_x):\n","\n","  return np.cumprod(np.array(list_x))[-1]\n","\n","\n","for key in label_pr_dict:\n","  \n","  print(key, ':', 'win_ratio : %.2f' % (win_ratio(label_pr_dict[key])), 'acc_pr : %.2f' % (acc_pr(label_pr_dict[key])))\n","\n","\n","\n","\n","\n","#     predict test && test 의 라벨에 따른 win_ratio 확인\n","# test_labels = km.predict(x_test)\n","# # print(test_labels)\n","\n","# label_pr_dict = {}\n","# #   init dict   #\n","# for label in label_types:\n","#   label_pr_dict[label] = []\n","# print(label_pr_dict)\n","# # break\n","\n","# for i, (label, pr) in enumerate(zip(test_labels, pr_test)):\n","#   label_pr_dict[label].append(pr[0])\n","\n","# for key in label_pr_dict:\n","\n","#   print(key, ':', 'win_ratio : %.2f' % (win_ratio(label_pr_dict[key])), 'acc_pr : %.2f' % (acc_pr(label_pr_dict[key])))\n","\n"],"execution_count":null,"outputs":[]}]}