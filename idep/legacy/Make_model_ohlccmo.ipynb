{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python [conda env:tensorflow2_p36]","language":"python","name":"conda-env-tensorflow2_p36-py"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.5"},"colab":{"name":"Make_model_ohlccmo.ipynb","provenance":[{"file_id":"1WEMU-VCj-p8mZvMxqBpQAdViCsHXpo20","timestamp":1585494039789},{"file_id":"1z4z_KLPzc6RWsxo3X_dHbpByxUAgjoMJ","timestamp":1583754134002}],"collapsed_sections":[]},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"8s5fopqwFUf9","colab_type":"code","outputId":"426bcb10-7eb2-471e-e07e-85c7d146c0a9","executionInfo":{"status":"ok","timestamp":1585494185520,"user_tz":-540,"elapsed":1015,"user":{"displayName":"J 1","photoUrl":"","userId":"08178289703395036410"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["from google.colab import drive\n","\n","drive.mount('/content/gdrive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"scrolled":false,"id":"2iYLNSeSEp7p","colab_type":"code","outputId":"33c57150-da1d-4474-9b72-a8950e21bc93","executionInfo":{"status":"ok","timestamp":1585495948964,"user_tz":-540,"elapsed":1673860,"user":{"displayName":"J 1","photoUrl":"","userId":"08178289703395036410"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["import os\n","# os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   \n","# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","\n","import pandas as pd\n","import numpy as np\n","from matplotlib import pyplot\n","import scipy.misc \n","from math import sqrt \n","import itertools\n","from IPython.display import display\n","%tensorflow_version 1.x\n","%matplotlib inline\n","\n","# input_data_length = int(input('input_data_length : '))\n","input_data_length = 30\n","model_num = 102\n","num_classes = 3\n","\n","gdrive_path = '/content/gdrive/My Drive/Colab Notebooks/'\n","\n","Made_X = np.load(gdrive_path + 'Made_X/Made_X %s_%s.npy' % (input_data_length, model_num))\n","Made_Y = np.load(gdrive_path + 'Made_X/Made_Y %s_%s.npy' % (input_data_length, model_num)).reshape(-1, 1)\n","\n","\n","#       dataset 분리      #\n","# dataX 구성 : VOLUME,  CMO, OBV, RSI, MACD, MACD_SIGNAL, MACD_OSC, Zero\n","Made_X = Made_X[:, :, [0, 1, 2, 3, 5]]\n","print(Made_X.shape)\n","print(Made_Y.shape)\n","\n","row = Made_X.shape[1]\n","col = Made_X.shape[2]\n","\n","from sklearn.model_selection import train_test_split\n","X_train, X_val, Y_train, Y_val = train_test_split(Made_X, Made_Y, test_size=0.3,\n","                                                   shuffle=False)\n","\n","X_train = X_train.astype('float32').reshape(-1, input_data_length, col, 1)\n","X_val = X_val.astype('float32').reshape(-1, input_data_length, col, 1)\n","print(X_train.shape)\n","print(X_val.shape)\n","\n","from keras.utils import np_utils\n","from keras.preprocessing.image import ImageDataGenerator \n","\n","# Data Class Weight\n","from sklearn.utils import class_weight\n","\n","print(Y_train[:, 0])\n","class_weights = class_weight.compute_class_weight('balanced', \n","                                                  np.unique(Y_train[:, 0]),\n","                                                  Y_train[:, 0])\n","class_weights = dict(enumerate(class_weights))\n","print(class_weights)\n","# quit()\n","\n","Y_train = Y_train.astype('float32')\n","Y_val = Y_val.astype('float32')\n","Y_train = np_utils.to_categorical(Y_train, num_classes)\n","Y_val = np_utils.to_categorical(Y_val, num_classes)\n","print(Y_train.shape)\n","print(Y_val.shape)\n","\n","datagen = ImageDataGenerator( \n","#     rotation_range = 60,\n","#     zoom_range = 0.6,\n","#     shear_range = 0.6,\n","#     horizontal_flip = True,\n","#     width_shift_range=0.6,\n","#     height_shift_range=0.6,\n","    fill_mode = 'nearest'\n","    )\n","\n","testgen = ImageDataGenerator( \n","    )\n","datagen.fit(X_train)\n","batch_size = 128\n","\n","for X_batch, _ in datagen.flow(X_train, Y_train, batch_size=9):\n","    for i in range(0, 9): \n","        pyplot.axis('off') \n","        pyplot.subplot(330 + 1 + i) \n","        pyplot.imshow(X_batch[i].reshape(input_data_length, col), cmap=pyplot.get_cmap('gray'))\n","    pyplot.axis('off') \n","    pyplot.show() \n","    break\n","    \n","    \n","train_flow = datagen.flow(X_train, Y_train, batch_size=batch_size) \n","val_flow = testgen.flow(X_val, Y_val, batch_size=batch_size) \n","\n","\n","from keras.utils import plot_model\n","import keras.backend as K\n","from keras.models import Model, Sequential\n","import keras.layers as layers\n","from keras.optimizers import Adam, SGD\n","from keras.regularizers import l1, l2\n","from sklearn.metrics import confusion_matrix\n","\n","def FER_Model(input_shape=(row, col, 1)):\n","    # first input model\n","    visible = layers.Input(shape=input_shape, name='input')\n","    \n","    net = layers.Conv2D(64, kernel_size=3, padding='same')(visible)\n","    # net = layers.Activation('relu')(net)\n","    net = layers.LeakyReLU()(net)\n","    net = layers.MaxPool2D(pool_size=2)(net)\n","\n","    shortcut_1 = net\n","\n","    net = layers.Conv2D(128, kernel_size=3, padding='same')(net)\n","    # net = layers.Activation('relu')(net)\n","    net = layers.LeakyReLU()(net)\n","    net = layers.MaxPool2D(pool_size=2)(net)\n","\n","    shortcut_2 = net\n","\n","#     net = layers.Conv2D(256, kernel_size=3, padding='same')(net)\n","#     # net = layers.Activation('relu')(net)\n","#     net = layers.LeakyReLU()(net)\n","#     net = layers.MaxPool2D(pool_size=2)(net)\n","\n","#     shortcut_3 = net\n","\n","#     net = layers.Conv2D(128, kernel_size=1, padding='same')(net)\n","#     # net = layers.Activation('relu')(net)\n","#     net = layers.LeakyReLU()(net)\n","#     net = layers.MaxPool2D(pool_size=2)(net)\n","\n","    net = layers.Flatten()(net)\n","    net = layers.Dense(64)(net)\n","    net = layers.LeakyReLU()(net)\n","    net = layers.Dense(num_classes, activation='softmax')(net)\n","\n","    # create model \n","    model = Model(inputs =visible, outputs = net)\n","    # summary layers\n","    print(model.summary())\n","    \n","    return model\n","\n","model = FER_Model()\n","# from keras.models import load_model\n","# model = load_model(gdrive_path + 'model/rapid_ascending %s_%s_ohlc.hdf5' % (input_data_length, model_num))\n","opt = Adam(lr=0.0001, decay=1e-6)\n","model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n","  \n","    \n","from keras.callbacks import ModelCheckpoint, EarlyStopping, TensorBoard\n","filepath = gdrive_path + \"model/rapid_ascending %s_%s_ohlccmo.hdf5\" % (input_data_length, model_num)\n","checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='auto')\n","checkpoint2 = TensorBoard(log_dir='Tensorboard_graph',\n","                          histogram_freq=0,\n","                          write_graph=True,\n","                          write_images=True)\n","checkpoint3 = EarlyStopping(monitor='val_loss', patience=100)\n","callbacks_list = [checkpoint, checkpoint2, checkpoint3]\n","\n","# keras.callbacks.Callback 로 부터 log 를 받아와 history log 를 작성할 수 있다.\n","\n","# we iterate 200 times over the entire training set\n","num_epochs = 500\n","history = model.fit_generator(train_flow, \n","                    steps_per_epoch=len(X_train) / batch_size, \n","                    epochs=num_epochs,  \n","                    verbose=2,  \n","                    callbacks=callbacks_list,\n","                    class_weight=class_weights,\n","                    validation_data=val_flow,  \n","                    validation_steps=len(X_val) / batch_size,\n","                    shuffle=False)\n","\n"],"execution_count":3,"outputs":[{"output_type":"stream","text":["(294090, 30, 5)\n","(294090, 1)\n","(205863, 30, 5, 1)\n","(88227, 30, 5, 1)\n","[2. 0. 0. ... 0. 2. 0.]\n","{0: 0.396117390350624, 1: 4.277850508073063, 2: 4.1367856281649384}\n","(205863, 3)\n","(88227, 3)\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAQUAAADnCAYAAAAXbUOsAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nmYCiQAiZ4P8Bj5vM7W5U4LQAAAAASUVORK5CYII=\n","text/plain":["<Figure size 432x288 with 9 Axes>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["Model: \"model_2\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","input (InputLayer)           (None, 30, 5, 1)          0         \n","_________________________________________________________________\n","conv2d_3 (Conv2D)            (None, 30, 5, 64)         640       \n","_________________________________________________________________\n","leaky_re_lu_4 (LeakyReLU)    (None, 30, 5, 64)         0         \n","_________________________________________________________________\n","max_pooling2d_3 (MaxPooling2 (None, 15, 2, 64)         0         \n","_________________________________________________________________\n","conv2d_4 (Conv2D)            (None, 15, 2, 128)        73856     \n","_________________________________________________________________\n","leaky_re_lu_5 (LeakyReLU)    (None, 15, 2, 128)        0         \n","_________________________________________________________________\n","max_pooling2d_4 (MaxPooling2 (None, 7, 1, 128)         0         \n","_________________________________________________________________\n","flatten_2 (Flatten)          (None, 896)               0         \n","_________________________________________________________________\n","dense_3 (Dense)              (None, 64)                57408     \n","_________________________________________________________________\n","leaky_re_lu_6 (LeakyReLU)    (None, 64)                0         \n","_________________________________________________________________\n","dense_4 (Dense)              (None, 3)                 195       \n","=================================================================\n","Total params: 132,099\n","Trainable params: 132,099\n","Non-trainable params: 0\n","_________________________________________________________________\n","None\n","Epoch 1/500\n"," - 11s - loss: 1.0359 - acc: 0.2133 - val_loss: 1.1246 - val_acc: 0.1207\n","\n","Epoch 00001: val_loss improved from inf to 1.12460, saving model to /content/gdrive/My Drive/Colab Notebooks/model/rapid_ascending 30_102_ohlccmo.hdf5\n","Epoch 2/500\n"," - 11s - loss: 1.0248 - acc: 0.2200 - val_loss: 1.0780 - val_acc: 0.2090\n","\n","Epoch 00002: val_loss improved from 1.12460 to 1.07805, saving model to /content/gdrive/My Drive/Colab Notebooks/model/rapid_ascending 30_102_ohlccmo.hdf5\n","Epoch 3/500\n"," - 11s - loss: 1.0205 - acc: 0.2252 - val_loss: 1.0676 - val_acc: 0.2437\n","\n","Epoch 00003: val_loss improved from 1.07805 to 1.06758, saving model to /content/gdrive/My Drive/Colab Notebooks/model/rapid_ascending 30_102_ohlccmo.hdf5\n","Epoch 4/500\n"," - 11s - loss: 1.0174 - acc: 0.2236 - val_loss: 1.0815 - val_acc: 0.2006\n","\n","Epoch 00004: val_loss did not improve from 1.06758\n","Epoch 5/500\n"," - 11s - loss: 1.0154 - acc: 0.2305 - val_loss: 1.0987 - val_acc: 0.1720\n","\n","Epoch 00005: val_loss did not improve from 1.06758\n","Epoch 6/500\n"," - 11s - loss: 1.0135 - acc: 0.2257 - val_loss: 1.0893 - val_acc: 0.1806\n","\n","Epoch 00006: val_loss did not improve from 1.06758\n","Epoch 7/500\n"," - 11s - loss: 1.0120 - acc: 0.2275 - val_loss: 1.0474 - val_acc: 0.2867\n","\n","Epoch 00007: val_loss improved from 1.06758 to 1.04737, saving model to /content/gdrive/My Drive/Colab Notebooks/model/rapid_ascending 30_102_ohlccmo.hdf5\n","Epoch 8/500\n"," - 11s - loss: 1.0104 - acc: 0.2281 - val_loss: 1.0878 - val_acc: 0.2248\n","\n","Epoch 00008: val_loss did not improve from 1.04737\n","Epoch 9/500\n"," - 11s - loss: 1.0088 - acc: 0.2306 - val_loss: 1.0751 - val_acc: 0.2125\n","\n","Epoch 00009: val_loss did not improve from 1.04737\n","Epoch 10/500\n"," - 11s - loss: 1.0081 - acc: 0.2331 - val_loss: 1.0663 - val_acc: 0.2416\n","\n","Epoch 00010: val_loss did not improve from 1.04737\n","Epoch 11/500\n"," - 11s - loss: 1.0068 - acc: 0.2305 - val_loss: 1.0435 - val_acc: 0.2841\n","\n","Epoch 00011: val_loss improved from 1.04737 to 1.04350, saving model to /content/gdrive/My Drive/Colab Notebooks/model/rapid_ascending 30_102_ohlccmo.hdf5\n","Epoch 12/500\n"," - 11s - loss: 1.0054 - acc: 0.2327 - val_loss: 1.0568 - val_acc: 0.2591\n","\n","Epoch 00012: val_loss did not improve from 1.04350\n","Epoch 13/500\n"," - 11s - loss: 1.0044 - acc: 0.2343 - val_loss: 1.0623 - val_acc: 0.2259\n","\n","Epoch 00013: val_loss did not improve from 1.04350\n","Epoch 14/500\n"," - 11s - loss: 1.0033 - acc: 0.2351 - val_loss: 1.0643 - val_acc: 0.2326\n","\n","Epoch 00014: val_loss did not improve from 1.04350\n","Epoch 15/500\n"," - 11s - loss: 1.0025 - acc: 0.2388 - val_loss: 1.1183 - val_acc: 0.1658\n","\n","Epoch 00015: val_loss did not improve from 1.04350\n","Epoch 16/500\n"," - 11s - loss: 1.0012 - acc: 0.2417 - val_loss: 1.1023 - val_acc: 0.1913\n","\n","Epoch 00016: val_loss did not improve from 1.04350\n","Epoch 17/500\n"," - 11s - loss: 1.0000 - acc: 0.2432 - val_loss: 1.0705 - val_acc: 0.2284\n","\n","Epoch 00017: val_loss did not improve from 1.04350\n","Epoch 18/500\n"," - 11s - loss: 0.9990 - acc: 0.2488 - val_loss: 1.0930 - val_acc: 0.1959\n","\n","Epoch 00018: val_loss did not improve from 1.04350\n","Epoch 19/500\n"," - 11s - loss: 0.9975 - acc: 0.2455 - val_loss: 1.1014 - val_acc: 0.1965\n","\n","Epoch 00019: val_loss did not improve from 1.04350\n","Epoch 20/500\n"," - 11s - loss: 0.9961 - acc: 0.2550 - val_loss: 1.0976 - val_acc: 0.1970\n","\n","Epoch 00020: val_loss did not improve from 1.04350\n","Epoch 21/500\n"," - 11s - loss: 0.9952 - acc: 0.2558 - val_loss: 1.0850 - val_acc: 0.2264\n","\n","Epoch 00021: val_loss did not improve from 1.04350\n","Epoch 22/500\n"," - 11s - loss: 0.9936 - acc: 0.2569 - val_loss: 1.0537 - val_acc: 0.2921\n","\n","Epoch 00022: val_loss did not improve from 1.04350\n","Epoch 23/500\n"," - 11s - loss: 0.9925 - acc: 0.2628 - val_loss: 1.0782 - val_acc: 0.2450\n","\n","Epoch 00023: val_loss did not improve from 1.04350\n","Epoch 24/500\n"," - 11s - loss: 0.9908 - acc: 0.2646 - val_loss: 1.0810 - val_acc: 0.2222\n","\n","Epoch 00024: val_loss did not improve from 1.04350\n","Epoch 25/500\n"," - 11s - loss: 0.9895 - acc: 0.2668 - val_loss: 1.1862 - val_acc: 0.1462\n","\n","Epoch 00025: val_loss did not improve from 1.04350\n","Epoch 26/500\n"," - 11s - loss: 0.9881 - acc: 0.2691 - val_loss: 1.0671 - val_acc: 0.2564\n","\n","Epoch 00026: val_loss did not improve from 1.04350\n","Epoch 27/500\n"," - 11s - loss: 0.9864 - acc: 0.2704 - val_loss: 1.0327 - val_acc: 0.3247\n","\n","Epoch 00027: val_loss improved from 1.04350 to 1.03267, saving model to /content/gdrive/My Drive/Colab Notebooks/model/rapid_ascending 30_102_ohlccmo.hdf5\n","Epoch 28/500\n"," - 11s - loss: 0.9849 - acc: 0.2737 - val_loss: 1.1031 - val_acc: 0.2177\n","\n","Epoch 00028: val_loss did not improve from 1.03267\n","Epoch 29/500\n"," - 11s - loss: 0.9840 - acc: 0.2768 - val_loss: 1.0660 - val_acc: 0.2615\n","\n","Epoch 00029: val_loss did not improve from 1.03267\n","Epoch 30/500\n"," - 11s - loss: 0.9819 - acc: 0.2794 - val_loss: 1.0520 - val_acc: 0.2779\n","\n","Epoch 00030: val_loss did not improve from 1.03267\n","Epoch 31/500\n"," - 11s - loss: 0.9800 - acc: 0.2830 - val_loss: 1.0406 - val_acc: 0.2948\n","\n","Epoch 00031: val_loss did not improve from 1.03267\n","Epoch 32/500\n"," - 11s - loss: 0.9786 - acc: 0.2839 - val_loss: 0.9977 - val_acc: 0.3860\n","\n","Epoch 00032: val_loss improved from 1.03267 to 0.99769, saving model to /content/gdrive/My Drive/Colab Notebooks/model/rapid_ascending 30_102_ohlccmo.hdf5\n","Epoch 33/500\n"," - 11s - loss: 0.9766 - acc: 0.2892 - val_loss: 1.0480 - val_acc: 0.3295\n","\n","Epoch 00033: val_loss did not improve from 0.99769\n","Epoch 34/500\n"," - 11s - loss: 0.9748 - acc: 0.2941 - val_loss: 1.1260 - val_acc: 0.1971\n","\n","Epoch 00034: val_loss did not improve from 0.99769\n","Epoch 35/500\n"," - 11s - loss: 0.9729 - acc: 0.2916 - val_loss: 1.0546 - val_acc: 0.2975\n","\n","Epoch 00035: val_loss did not improve from 0.99769\n","Epoch 36/500\n"," - 11s - loss: 0.9720 - acc: 0.2956 - val_loss: 1.0922 - val_acc: 0.2426\n","\n","Epoch 00036: val_loss did not improve from 0.99769\n","Epoch 37/500\n"," - 11s - loss: 0.9699 - acc: 0.2963 - val_loss: 1.0708 - val_acc: 0.2671\n","\n","Epoch 00037: val_loss did not improve from 0.99769\n","Epoch 38/500\n"," - 11s - loss: 0.9682 - acc: 0.3008 - val_loss: 1.1453 - val_acc: 0.1908\n","\n","Epoch 00038: val_loss did not improve from 0.99769\n","Epoch 39/500\n"," - 11s - loss: 0.9663 - acc: 0.3015 - val_loss: 1.0626 - val_acc: 0.2825\n","\n","Epoch 00039: val_loss did not improve from 0.99769\n","Epoch 40/500\n"," - 11s - loss: 0.9637 - acc: 0.3040 - val_loss: 1.1042 - val_acc: 0.2492\n","\n","Epoch 00040: val_loss did not improve from 0.99769\n","Epoch 41/500\n"," - 11s - loss: 0.9626 - acc: 0.3066 - val_loss: 1.1096 - val_acc: 0.2347\n","\n","Epoch 00041: val_loss did not improve from 0.99769\n","Epoch 42/500\n"," - 11s - loss: 0.9597 - acc: 0.3074 - val_loss: 1.0593 - val_acc: 0.2950\n","\n","Epoch 00042: val_loss did not improve from 0.99769\n","Epoch 43/500\n"," - 11s - loss: 0.9584 - acc: 0.3120 - val_loss: 1.0385 - val_acc: 0.3318\n","\n","Epoch 00043: val_loss did not improve from 0.99769\n","Epoch 44/500\n"," - 11s - loss: 0.9570 - acc: 0.3129 - val_loss: 1.1198 - val_acc: 0.2275\n","\n","Epoch 00044: val_loss did not improve from 0.99769\n","Epoch 45/500\n"," - 11s - loss: 0.9542 - acc: 0.3152 - val_loss: 1.0047 - val_acc: 0.3821\n","\n","Epoch 00045: val_loss did not improve from 0.99769\n","Epoch 46/500\n"," - 11s - loss: 0.9524 - acc: 0.3190 - val_loss: 1.1354 - val_acc: 0.2198\n","\n","Epoch 00046: val_loss did not improve from 0.99769\n","Epoch 47/500\n"," - 11s - loss: 0.9502 - acc: 0.3191 - val_loss: 1.0601 - val_acc: 0.2996\n","\n","Epoch 00047: val_loss did not improve from 0.99769\n","Epoch 48/500\n"," - 11s - loss: 0.9490 - acc: 0.3215 - val_loss: 1.0503 - val_acc: 0.3110\n","\n","Epoch 00048: val_loss did not improve from 0.99769\n","Epoch 49/500\n"," - 11s - loss: 0.9467 - acc: 0.3241 - val_loss: 1.0679 - val_acc: 0.3063\n","\n","Epoch 00049: val_loss did not improve from 0.99769\n","Epoch 50/500\n"," - 11s - loss: 0.9445 - acc: 0.3265 - val_loss: 1.0860 - val_acc: 0.2727\n","\n","Epoch 00050: val_loss did not improve from 0.99769\n","Epoch 51/500\n"," - 11s - loss: 0.9421 - acc: 0.3286 - val_loss: 1.0750 - val_acc: 0.2846\n","\n","Epoch 00051: val_loss did not improve from 0.99769\n","Epoch 52/500\n"," - 11s - loss: 0.9399 - acc: 0.3295 - val_loss: 1.0536 - val_acc: 0.3215\n","\n","Epoch 00052: val_loss did not improve from 0.99769\n","Epoch 53/500\n"," - 11s - loss: 0.9386 - acc: 0.3330 - val_loss: 1.0966 - val_acc: 0.2993\n","\n","Epoch 00053: val_loss did not improve from 0.99769\n","Epoch 54/500\n"," - 11s - loss: 0.9354 - acc: 0.3357 - val_loss: 1.0670 - val_acc: 0.3101\n","\n","Epoch 00054: val_loss did not improve from 0.99769\n","Epoch 55/500\n"," - 11s - loss: 0.9335 - acc: 0.3345 - val_loss: 0.9939 - val_acc: 0.4071\n","\n","Epoch 00055: val_loss improved from 0.99769 to 0.99394, saving model to /content/gdrive/My Drive/Colab Notebooks/model/rapid_ascending 30_102_ohlccmo.hdf5\n","Epoch 56/500\n"," - 11s - loss: 0.9315 - acc: 0.3397 - val_loss: 1.1393 - val_acc: 0.2411\n","\n","Epoch 00056: val_loss did not improve from 0.99394\n","Epoch 57/500\n"," - 11s - loss: 0.9308 - acc: 0.3396 - val_loss: 1.0914 - val_acc: 0.2886\n","\n","Epoch 00057: val_loss did not improve from 0.99394\n","Epoch 58/500\n"," - 11s - loss: 0.9266 - acc: 0.3425 - val_loss: 1.1283 - val_acc: 0.2551\n","\n","Epoch 00058: val_loss did not improve from 0.99394\n","Epoch 59/500\n"," - 11s - loss: 0.9265 - acc: 0.3438 - val_loss: 1.0982 - val_acc: 0.2890\n","\n","Epoch 00059: val_loss did not improve from 0.99394\n","Epoch 60/500\n"," - 11s - loss: 0.9230 - acc: 0.3471 - val_loss: 1.0797 - val_acc: 0.2992\n","\n","Epoch 00060: val_loss did not improve from 0.99394\n","Epoch 61/500\n"," - 11s - loss: 0.9213 - acc: 0.3478 - val_loss: 1.0682 - val_acc: 0.3130\n","\n","Epoch 00061: val_loss did not improve from 0.99394\n","Epoch 62/500\n"," - 11s - loss: 0.9188 - acc: 0.3502 - val_loss: 1.0080 - val_acc: 0.3897\n","\n","Epoch 00062: val_loss did not improve from 0.99394\n","Epoch 63/500\n"," - 11s - loss: 0.9166 - acc: 0.3513 - val_loss: 1.1137 - val_acc: 0.2659\n","\n","Epoch 00063: val_loss did not improve from 0.99394\n","Epoch 64/500\n"," - 11s - loss: 0.9156 - acc: 0.3522 - val_loss: 1.0981 - val_acc: 0.2912\n","\n","Epoch 00064: val_loss did not improve from 0.99394\n","Epoch 65/500\n"," - 11s - loss: 0.9122 - acc: 0.3545 - val_loss: 1.0719 - val_acc: 0.3145\n","\n","Epoch 00065: val_loss did not improve from 0.99394\n","Epoch 66/500\n"," - 11s - loss: 0.9096 - acc: 0.3562 - val_loss: 1.0502 - val_acc: 0.3454\n","\n","Epoch 00066: val_loss did not improve from 0.99394\n","Epoch 67/500\n"," - 11s - loss: 0.9079 - acc: 0.3579 - val_loss: 1.1063 - val_acc: 0.2893\n","\n","Epoch 00067: val_loss did not improve from 0.99394\n","Epoch 68/500\n"," - 11s - loss: 0.9053 - acc: 0.3602 - val_loss: 1.0804 - val_acc: 0.3070\n","\n","Epoch 00068: val_loss did not improve from 0.99394\n","Epoch 69/500\n"," - 11s - loss: 0.9027 - acc: 0.3633 - val_loss: 1.0833 - val_acc: 0.3144\n","\n","Epoch 00069: val_loss did not improve from 0.99394\n","Epoch 70/500\n"," - 11s - loss: 0.9005 - acc: 0.3631 - val_loss: 1.0334 - val_acc: 0.3669\n","\n","Epoch 00070: val_loss did not improve from 0.99394\n","Epoch 71/500\n"," - 11s - loss: 0.8998 - acc: 0.3660 - val_loss: 1.0067 - val_acc: 0.4022\n","\n","Epoch 00071: val_loss did not improve from 0.99394\n","Epoch 72/500\n"," - 11s - loss: 0.8977 - acc: 0.3668 - val_loss: 1.0399 - val_acc: 0.3741\n","\n","Epoch 00072: val_loss did not improve from 0.99394\n","Epoch 73/500\n"," - 11s - loss: 0.8953 - acc: 0.3695 - val_loss: 1.0876 - val_acc: 0.3188\n","\n","Epoch 00073: val_loss did not improve from 0.99394\n","Epoch 74/500\n"," - 11s - loss: 0.8928 - acc: 0.3693 - val_loss: 1.0547 - val_acc: 0.3563\n","\n","Epoch 00074: val_loss did not improve from 0.99394\n","Epoch 75/500\n"," - 11s - loss: 0.8906 - acc: 0.3715 - val_loss: 1.0629 - val_acc: 0.3522\n","\n","Epoch 00075: val_loss did not improve from 0.99394\n","Epoch 76/500\n"," - 11s - loss: 0.8896 - acc: 0.3708 - val_loss: 1.0451 - val_acc: 0.3643\n","\n","Epoch 00076: val_loss did not improve from 0.99394\n","Epoch 77/500\n"," - 11s - loss: 0.8855 - acc: 0.3751 - val_loss: 1.0418 - val_acc: 0.3600\n","\n","Epoch 00077: val_loss did not improve from 0.99394\n","Epoch 78/500\n"," - 11s - loss: 0.8855 - acc: 0.3747 - val_loss: 1.0764 - val_acc: 0.3371\n","\n","Epoch 00078: val_loss did not improve from 0.99394\n","Epoch 79/500\n"," - 11s - loss: 0.8836 - acc: 0.3780 - val_loss: 1.0464 - val_acc: 0.3578\n","\n","Epoch 00079: val_loss did not improve from 0.99394\n","Epoch 80/500\n"," - 11s - loss: 0.8808 - acc: 0.3781 - val_loss: 1.0332 - val_acc: 0.3745\n","\n","Epoch 00080: val_loss did not improve from 0.99394\n","Epoch 81/500\n"," - 11s - loss: 0.8789 - acc: 0.3807 - val_loss: 1.0706 - val_acc: 0.3364\n","\n","Epoch 00081: val_loss did not improve from 0.99394\n","Epoch 82/500\n"," - 11s - loss: 0.8767 - acc: 0.3829 - val_loss: 1.0742 - val_acc: 0.3347\n","\n","Epoch 00082: val_loss did not improve from 0.99394\n","Epoch 83/500\n"," - 11s - loss: 0.8739 - acc: 0.3834 - val_loss: 1.0802 - val_acc: 0.3436\n","\n","Epoch 00083: val_loss did not improve from 0.99394\n","Epoch 84/500\n"," - 11s - loss: 0.8748 - acc: 0.3858 - val_loss: 1.0647 - val_acc: 0.3613\n","\n","Epoch 00084: val_loss did not improve from 0.99394\n","Epoch 85/500\n"," - 11s - loss: 0.8707 - acc: 0.3855 - val_loss: 1.1159 - val_acc: 0.3158\n","\n","Epoch 00085: val_loss did not improve from 0.99394\n","Epoch 86/500\n"," - 11s - loss: 0.8690 - acc: 0.3886 - val_loss: 1.0138 - val_acc: 0.4077\n","\n","Epoch 00086: val_loss did not improve from 0.99394\n","Epoch 87/500\n"," - 11s - loss: 0.8658 - acc: 0.3904 - val_loss: 1.1312 - val_acc: 0.2920\n","\n","Epoch 00087: val_loss did not improve from 0.99394\n","Epoch 88/500\n"," - 11s - loss: 0.8648 - acc: 0.3914 - val_loss: 1.1341 - val_acc: 0.2913\n","\n","Epoch 00088: val_loss did not improve from 0.99394\n","Epoch 89/500\n"," - 11s - loss: 0.8633 - acc: 0.3913 - val_loss: 1.1233 - val_acc: 0.2943\n","\n","Epoch 00089: val_loss did not improve from 0.99394\n","Epoch 90/500\n"," - 11s - loss: 0.8597 - acc: 0.3952 - val_loss: 1.1453 - val_acc: 0.2870\n","\n","Epoch 00090: val_loss did not improve from 0.99394\n","Epoch 91/500\n"," - 11s - loss: 0.8575 - acc: 0.3954 - val_loss: 1.0702 - val_acc: 0.3469\n","\n","Epoch 00091: val_loss did not improve from 0.99394\n","Epoch 92/500\n"," - 11s - loss: 0.8569 - acc: 0.3945 - val_loss: 1.0684 - val_acc: 0.3563\n","\n","Epoch 00092: val_loss did not improve from 0.99394\n","Epoch 93/500\n"," - 11s - loss: 0.8547 - acc: 0.3972 - val_loss: 1.1494 - val_acc: 0.2973\n","\n","Epoch 00093: val_loss did not improve from 0.99394\n","Epoch 94/500\n"," - 11s - loss: 0.8527 - acc: 0.3990 - val_loss: 1.0524 - val_acc: 0.3735\n","\n","Epoch 00094: val_loss did not improve from 0.99394\n","Epoch 95/500\n"," - 11s - loss: 0.8512 - acc: 0.4003 - val_loss: 1.1116 - val_acc: 0.3210\n","\n","Epoch 00095: val_loss did not improve from 0.99394\n","Epoch 96/500\n"," - 11s - loss: 0.8484 - acc: 0.4023 - val_loss: 1.0392 - val_acc: 0.3958\n","\n","Epoch 00096: val_loss did not improve from 0.99394\n","Epoch 97/500\n"," - 11s - loss: 0.8463 - acc: 0.4033 - val_loss: 1.0313 - val_acc: 0.3946\n","\n","Epoch 00097: val_loss did not improve from 0.99394\n","Epoch 98/500\n"," - 11s - loss: 0.8439 - acc: 0.4053 - val_loss: 1.0965 - val_acc: 0.3318\n","\n","Epoch 00098: val_loss did not improve from 0.99394\n","Epoch 99/500\n"," - 11s - loss: 0.8434 - acc: 0.4043 - val_loss: 1.0348 - val_acc: 0.4094\n","\n","Epoch 00099: val_loss did not improve from 0.99394\n","Epoch 100/500\n"," - 11s - loss: 0.8416 - acc: 0.4069 - val_loss: 1.1322 - val_acc: 0.3223\n","\n","Epoch 00100: val_loss did not improve from 0.99394\n","Epoch 101/500\n"," - 11s - loss: 0.8390 - acc: 0.4089 - val_loss: 1.0661 - val_acc: 0.3754\n","\n","Epoch 00101: val_loss did not improve from 0.99394\n","Epoch 102/500\n"," - 11s - loss: 0.8377 - acc: 0.4098 - val_loss: 1.0697 - val_acc: 0.3596\n","\n","Epoch 00102: val_loss did not improve from 0.99394\n","Epoch 103/500\n"," - 11s - loss: 0.8349 - acc: 0.4121 - val_loss: 1.1399 - val_acc: 0.3125\n","\n","Epoch 00103: val_loss did not improve from 0.99394\n","Epoch 104/500\n"," - 11s - loss: 0.8340 - acc: 0.4124 - val_loss: 1.0148 - val_acc: 0.4205\n","\n","Epoch 00104: val_loss did not improve from 0.99394\n","Epoch 105/500\n"," - 11s - loss: 0.8314 - acc: 0.4145 - val_loss: 1.1375 - val_acc: 0.3176\n","\n","Epoch 00105: val_loss did not improve from 0.99394\n","Epoch 106/500\n"," - 11s - loss: 0.8304 - acc: 0.4146 - val_loss: 1.0805 - val_acc: 0.3613\n","\n","Epoch 00106: val_loss did not improve from 0.99394\n","Epoch 107/500\n"," - 11s - loss: 0.8296 - acc: 0.4158 - val_loss: 1.1087 - val_acc: 0.3393\n","\n","Epoch 00107: val_loss did not improve from 0.99394\n","Epoch 108/500\n"," - 11s - loss: 0.8260 - acc: 0.4180 - val_loss: 1.0861 - val_acc: 0.3615\n","\n","Epoch 00108: val_loss did not improve from 0.99394\n","Epoch 109/500\n"," - 11s - loss: 0.8262 - acc: 0.4185 - val_loss: 1.1047 - val_acc: 0.3508\n","\n","Epoch 00109: val_loss did not improve from 0.99394\n","Epoch 110/500\n"," - 11s - loss: 0.8227 - acc: 0.4187 - val_loss: 1.0842 - val_acc: 0.3671\n","\n","Epoch 00110: val_loss did not improve from 0.99394\n","Epoch 111/500\n"," - 11s - loss: 0.8198 - acc: 0.4210 - val_loss: 1.0542 - val_acc: 0.3998\n","\n","Epoch 00111: val_loss did not improve from 0.99394\n","Epoch 112/500\n"," - 11s - loss: 0.8189 - acc: 0.4222 - val_loss: 1.1096 - val_acc: 0.3409\n","\n","Epoch 00112: val_loss did not improve from 0.99394\n","Epoch 113/500\n"," - 11s - loss: 0.8175 - acc: 0.4247 - val_loss: 1.1479 - val_acc: 0.3278\n","\n","Epoch 00113: val_loss did not improve from 0.99394\n","Epoch 114/500\n"," - 11s - loss: 0.8162 - acc: 0.4245 - val_loss: 1.0386 - val_acc: 0.4086\n","\n","Epoch 00114: val_loss did not improve from 0.99394\n","Epoch 115/500\n"," - 11s - loss: 0.8119 - acc: 0.4268 - val_loss: 1.1024 - val_acc: 0.3448\n","\n","Epoch 00115: val_loss did not improve from 0.99394\n","Epoch 116/500\n"," - 11s - loss: 0.8117 - acc: 0.4270 - val_loss: 1.0955 - val_acc: 0.3692\n","\n","Epoch 00116: val_loss did not improve from 0.99394\n","Epoch 117/500\n"," - 11s - loss: 0.8110 - acc: 0.4276 - val_loss: 1.0578 - val_acc: 0.4020\n","\n","Epoch 00117: val_loss did not improve from 0.99394\n","Epoch 118/500\n"," - 10s - loss: 0.8103 - acc: 0.4292 - val_loss: 1.0633 - val_acc: 0.3928\n","\n","Epoch 00118: val_loss did not improve from 0.99394\n","Epoch 119/500\n"," - 10s - loss: 0.8056 - acc: 0.4320 - val_loss: 1.0841 - val_acc: 0.3715\n","\n","Epoch 00119: val_loss did not improve from 0.99394\n","Epoch 120/500\n"," - 11s - loss: 0.8066 - acc: 0.4309 - val_loss: 1.0825 - val_acc: 0.3739\n","\n","Epoch 00120: val_loss did not improve from 0.99394\n","Epoch 121/500\n"," - 11s - loss: 0.8037 - acc: 0.4323 - val_loss: 1.0837 - val_acc: 0.3794\n","\n","Epoch 00121: val_loss did not improve from 0.99394\n","Epoch 122/500\n"," - 11s - loss: 0.8009 - acc: 0.4351 - val_loss: 1.0729 - val_acc: 0.4042\n","\n","Epoch 00122: val_loss did not improve from 0.99394\n","Epoch 123/500\n"," - 11s - loss: 0.8003 - acc: 0.4356 - val_loss: 1.0710 - val_acc: 0.3909\n","\n","Epoch 00123: val_loss did not improve from 0.99394\n","Epoch 124/500\n"," - 11s - loss: 0.7975 - acc: 0.4374 - val_loss: 1.0954 - val_acc: 0.3686\n","\n","Epoch 00124: val_loss did not improve from 0.99394\n","Epoch 125/500\n"," - 11s - loss: 0.7961 - acc: 0.4379 - val_loss: 1.0913 - val_acc: 0.3728\n","\n","Epoch 00125: val_loss did not improve from 0.99394\n","Epoch 126/500\n"," - 11s - loss: 0.7951 - acc: 0.4396 - val_loss: 1.0385 - val_acc: 0.4172\n","\n","Epoch 00126: val_loss did not improve from 0.99394\n","Epoch 127/500\n"," - 11s - loss: 0.7922 - acc: 0.4403 - val_loss: 1.0716 - val_acc: 0.4035\n","\n","Epoch 00127: val_loss did not improve from 0.99394\n","Epoch 128/500\n"," - 11s - loss: 0.7923 - acc: 0.4410 - val_loss: 1.0977 - val_acc: 0.3747\n","\n","Epoch 00128: val_loss did not improve from 0.99394\n","Epoch 129/500\n"," - 11s - loss: 0.7934 - acc: 0.4419 - val_loss: 1.1004 - val_acc: 0.3659\n","\n","Epoch 00129: val_loss did not improve from 0.99394\n","Epoch 130/500\n"," - 10s - loss: 0.7889 - acc: 0.4439 - val_loss: 1.1547 - val_acc: 0.3396\n","\n","Epoch 00130: val_loss did not improve from 0.99394\n","Epoch 131/500\n"," - 11s - loss: 0.7873 - acc: 0.4445 - val_loss: 1.0517 - val_acc: 0.4098\n","\n","Epoch 00131: val_loss did not improve from 0.99394\n","Epoch 132/500\n"," - 11s - loss: 0.7840 - acc: 0.4461 - val_loss: 1.0701 - val_acc: 0.4019\n","\n","Epoch 00132: val_loss did not improve from 0.99394\n","Epoch 133/500\n"," - 10s - loss: 0.7844 - acc: 0.4458 - val_loss: 1.0054 - val_acc: 0.4613\n","\n","Epoch 00133: val_loss did not improve from 0.99394\n","Epoch 134/500\n"," - 11s - loss: 0.7818 - acc: 0.4488 - val_loss: 1.1062 - val_acc: 0.3741\n","\n","Epoch 00134: val_loss did not improve from 0.99394\n","Epoch 135/500\n"," - 11s - loss: 0.7800 - acc: 0.4503 - val_loss: 1.1376 - val_acc: 0.3459\n","\n","Epoch 00135: val_loss did not improve from 0.99394\n","Epoch 136/500\n"," - 11s - loss: 0.7790 - acc: 0.4500 - val_loss: 1.0456 - val_acc: 0.4294\n","\n","Epoch 00136: val_loss did not improve from 0.99394\n","Epoch 137/500\n"," - 11s - loss: 0.7788 - acc: 0.4511 - val_loss: 1.1033 - val_acc: 0.3873\n","\n","Epoch 00137: val_loss did not improve from 0.99394\n","Epoch 138/500\n"," - 11s - loss: 0.7772 - acc: 0.4518 - val_loss: 1.1122 - val_acc: 0.3739\n","\n","Epoch 00138: val_loss did not improve from 0.99394\n","Epoch 139/500\n"," - 10s - loss: 0.7757 - acc: 0.4531 - val_loss: 1.0684 - val_acc: 0.4027\n","\n","Epoch 00139: val_loss did not improve from 0.99394\n","Epoch 140/500\n"," - 11s - loss: 0.7734 - acc: 0.4536 - val_loss: 1.0766 - val_acc: 0.4059\n","\n","Epoch 00140: val_loss did not improve from 0.99394\n","Epoch 141/500\n"," - 11s - loss: 0.7711 - acc: 0.4548 - val_loss: 1.0812 - val_acc: 0.4035\n","\n","Epoch 00141: val_loss did not improve from 0.99394\n","Epoch 142/500\n"," - 10s - loss: 0.7701 - acc: 0.4570 - val_loss: 1.1134 - val_acc: 0.3718\n","\n","Epoch 00142: val_loss did not improve from 0.99394\n","Epoch 143/500\n"," - 10s - loss: 0.7698 - acc: 0.4574 - val_loss: 1.0816 - val_acc: 0.4053\n","\n","Epoch 00143: val_loss did not improve from 0.99394\n","Epoch 144/500\n"," - 11s - loss: 0.7667 - acc: 0.4589 - val_loss: 1.1463 - val_acc: 0.3525\n","\n","Epoch 00144: val_loss did not improve from 0.99394\n","Epoch 145/500\n"," - 11s - loss: 0.7649 - acc: 0.4610 - val_loss: 1.1042 - val_acc: 0.3830\n","\n","Epoch 00145: val_loss did not improve from 0.99394\n","Epoch 146/500\n"," - 10s - loss: 0.7645 - acc: 0.4599 - val_loss: 1.1309 - val_acc: 0.3664\n","\n","Epoch 00146: val_loss did not improve from 0.99394\n","Epoch 147/500\n"," - 10s - loss: 0.7621 - acc: 0.4608 - val_loss: 1.1347 - val_acc: 0.3657\n","\n","Epoch 00147: val_loss did not improve from 0.99394\n","Epoch 148/500\n"," - 11s - loss: 0.7612 - acc: 0.4614 - val_loss: 1.1214 - val_acc: 0.3762\n","\n","Epoch 00148: val_loss did not improve from 0.99394\n","Epoch 149/500\n"," - 10s - loss: 0.7588 - acc: 0.4616 - val_loss: 1.2332 - val_acc: 0.3144\n","\n","Epoch 00149: val_loss did not improve from 0.99394\n","Epoch 150/500\n"," - 11s - loss: 0.7580 - acc: 0.4637 - val_loss: 1.2208 - val_acc: 0.3171\n","\n","Epoch 00150: val_loss did not improve from 0.99394\n","Epoch 151/500\n"," - 11s - loss: 0.7568 - acc: 0.4655 - val_loss: 1.0755 - val_acc: 0.4260\n","\n","Epoch 00151: val_loss did not improve from 0.99394\n","Epoch 152/500\n"," - 10s - loss: 0.7563 - acc: 0.4658 - val_loss: 1.1076 - val_acc: 0.3937\n","\n","Epoch 00152: val_loss did not improve from 0.99394\n","Epoch 153/500\n"," - 11s - loss: 0.7549 - acc: 0.4655 - val_loss: 1.0020 - val_acc: 0.4736\n","\n","Epoch 00153: val_loss did not improve from 0.99394\n","Epoch 154/500\n"," - 10s - loss: 0.7520 - acc: 0.4687 - val_loss: 1.0989 - val_acc: 0.4070\n","\n","Epoch 00154: val_loss did not improve from 0.99394\n","Epoch 155/500\n"," - 10s - loss: 0.7511 - acc: 0.4686 - val_loss: 1.1850 - val_acc: 0.3417\n","\n","Epoch 00155: val_loss did not improve from 0.99394\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"a01LFE7QEp70","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}