{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"ARIMA_analysis_dynamic_tp.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"machine_shape":"hm","mount_file_id":"12_D2aTKIWjQFdxAWdkoJFqDhWHQDPTAm","authorship_tag":"ABX9TyNBbowZ2oi7OQiqXVoohtM6"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"YzB9a98qMAdM"},"source":["import os, sys\n","\n","current_path = '/content/drive/My Drive/Colab Notebooks/Project_Stock/'\n","\n","os.chdir(current_path)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NvqYPbmocynh"},"source":["# pickle.format_version\n","# print(pd.show_versions())\n","!pip install pandas==0.25.3\n","# !pip install mpl_finance"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NNoQXskPN-Ge"},"source":["### **ARIMA**"]},{"cell_type":"code","metadata":{"id":"NvdpArctN_6l"},"source":["from statsmodels.tsa.arima_model import ARIMA\n","from datetime import datetime\n","\n","\n","def arima_test(close, use_rows=None):\n","\n","  size = int(len(close) * 0.66)\n","  train, test = close[0:size].values, close[size:len(close)]\n","  test_shift = test.shift(1).values\n","  test = test.values\n","  # break\n","\n","  history = list(train)\n","  predictions = list()\n","  err_ranges = list()\n","  for t in range(len(test)):\n","    \n","      if use_rows is not None:\n","        history = history[-use_rows:]\n","        \n","      model = ARIMA(history, order=(0, 2, 1))\n","      model_fit = model.fit(trend='c', disp=0)\n","      output = model_fit.forecast()\n","      # print(output)\n","      # break\n","      yhat = output[0]\n","      predictions.append(yhat)\n","      err_ranges.append(output[1])\n","      obs = test[t]\n","      # print('obs :', obs)\n","      history.append(obs)\n","      # break\n","      print('\\r %.2f%%' % (t / len(test) * 100), end='')\n","\n","  print(len(test), len(predictions))\n","\n","  return predictions, err_ranges\n","\n","\n","# print(high)\n","\n","\n","def get_back_result(ohlcv, predictions, err_ranges, tp=0.04, sl=None, leverage=1, show_detail=False, show_plot=False, return_pr=False, cumsum=False, \n","                    close_ver=False, reverse_short=False):\n","\n","  \n","  high, low, test = np.split(ohlcv.values[-len(predictions):, [1, 2, 3]], 3, axis=1)\n","\n","  if close_ver:\n","    predictions = ohlcv['close'].shift(1).values[-len(test):]\n","\n","  fee = 0.0006\n","  long_profits = []\n","  short_profits = []\n","  liquidations = []\n","  win_cnt = 0\n","  for i in range(len(test)):\n","\n","    long_ep = (predictions[i] - err_ranges[i]) * (1 / (tp + 1))\n","    if sl is not None:\n","      long_sl = long_ep * (1 / (sl + 1))\n","\n","    # assert long_ep < long_exit, 'long_exit < long_ep !, %s, %s' % (long_exit, long_ep)\n","    \n","    short_ep = (predictions[i] + err_ranges[i]) * (1 + tp)\n","    # short_ep = (predictions[i] + err_ranges[i]) * (1 / (1 - tp))\n","    if sl is not None:\n","      short_sl = short_ep * (1 / (1 - sl))\n","\n","    # print((low[i]))\n","\n","    #    long 우선   # <-- long & short 둘다 체결된 상황에서는 long 체결을 우선으로 한다.\n","    if low[i] < long_ep:\n","      \n","      liquidation = low[i] / long_ep - fee\n","      l_liquidation = 1 + (liquidation - 1) * leverage\n","      liquidations.append(l_liquidation)\n","\n","      if max(l_liquidation, 0) == 0:\n","        l_profit = 0\n","        # print('low[i], long_ep, l_liquidation :', low[i], long_ep, l_liquidation)\n","      else:\n","\n","        if sl is not None:\n","          if low[i] < long_sl:\n","            profit = long_sl / long_ep - fee\n","          else:\n","            profit = test[i] / long_ep - fee\n","\n","        else:\n","          profit = test[i] / long_ep - fee\n","\n","        l_profit = 1 + (profit - 1) * leverage\n","        l_profit = max(l_profit, 0)\n","        \n","        if profit >= 1:\n","          win_cnt += 1\n","\n","      long_profits.append(l_profit)\n","      short_profits.append(1.0)\n","\n","      if show_detail:\n","        print(test[i], predictions[i], long_ep)\n","\n","    # if high[i] > short_ep > low[i]: # 지정 대기가 아니라, 해당 price 가 지나면, long 한다.\n","\n","    #   if not reverse_short:\n","    #     liquidation = short_ep / high[i]  - fee\n","    #   else:\n","    #     liquidation = low[i] / short_ep  - fee\n","    #   l_liquidation = 1 + (liquidation - 1) * leverage\n","\n","    #   if max(l_liquidation, 0) == 0:\n","    #     l_profit = 0\n","    #   else:\n","\n","    #     if sl is not None:\n","    #       if high[i] > short_sl:\n","\n","    #         if not reverse_short:\n","    #           profit = short_ep / short_sl - fee\n","    #         else:\n","    #           profit = short_sl / short_ep - fee\n","\n","    #       else:\n","    #         if not reverse_short:\n","    #           profit = short_ep / test[i] - fee\n","    #         else:\n","    #           profit = test[i] / short_ep - fee\n","\n","    #     else:\n","\n","    #       if not reverse_short:\n","    #         profit = short_ep / test[i] - fee\n","    #       else:\n","    #         profit = test[i] / short_ep - fee\n","\n","    #     l_profit = 1 + (profit - 1) * leverage\n","    #     l_profit = max(l_profit, 0)\n","\n","    #     if profit >= 1:\n","    #       win_cnt += 1\n","\n","    #   short_profits.append(l_profit)\n","    #   long_profits.append(1.0)\n","\n","    #   if show_detail:\n","    #     print(test[i], predictions[i], short_ep)\n","    \n","    else:\n","      long_profits.append(1.0)\n","      short_profits.append(1.0)\n","      liquidations.append(1.0)\n","\n","\n","  long_win_ratio = sum(np.array(long_profits) > 1.0) / sum(np.array(long_profits) != 1.0)\n","  short_win_ratio = sum(np.array(short_profits) > 1.0) / sum(np.array(short_profits) != 1.0)\n","  long_frequency = sum(np.array(long_profits) != 1.0) / len(test)\n","  short_frequency = sum(np.array(short_profits) != 1.0) / len(test)\n","  if not cumsum:\n","    long_accum_profit = np.array(long_profits).cumprod()\n","    short_accum_profit = np.array(short_profits).cumprod()\n","  else:\n","    long_accum_profit = (np.array(long_profits) - 1.0).cumsum()\n","    short_accum_profit = (np.array(short_profits) - 1.0).cumsum()\n","\n","  # print(win_ratio)\n","\n","  if show_plot:\n","\n","    plt.figure(figsize=(10, 5))\n","    plt.suptitle('tp=%.4f, lvrg=%d' % (tp, leverage))\n","\n","    plt.subplot(151)\n","    plt.plot(liquidations)\n","    plt.title('liquidations')\n","\n","    plt.subplot(152)\n","    plt.plot(long_profits)\n","    plt.title('Win Ratio : %.2f %%\\nrequency : %.2f %%' % (long_win_ratio * 100, long_frequency * 100), color='black')\n","    # plt.show()\n","\n","    # print()\n","    plt.subplot(153)\n","    plt.plot(long_accum_profit)\n","    plt.title('Accum_profit : %.2f' % long_accum_profit[-1], color='black')\n","\n","    plt.subplot(154)\n","    plt.plot(short_profits)\n","    plt.title('Win Ratio : %.2f %%\\nrequency : %.2f %%' % (short_win_ratio * 100, short_frequency * 100), color='black')\n","    # plt.show()\n","\n","    # print()\n","    plt.subplot(155)\n","    plt.plot(short_accum_profit)\n","    plt.title('Accum_profit : %.2f' % short_accum_profit[-1], color='black')\n","    plt.show()\n","\n","  return [long_win_ratio, short_win_ratio], [long_frequency, short_frequency], [long_accum_profit[-1], short_accum_profit[-1]], [long_profits, short_profits]\n","\n","\n","# get_back_result(tp=0.04, leverage=1, show_plot=True)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aDkU3tMiM2lO"},"source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","interval = '30m'\n","date_path = './candlestick_concated/%s/' % interval\n","file_list = os.listdir(date_path)\n","\n","print((file_list))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fhCZVnLGaEOE"},"source":["### **Add coin**"]},{"cell_type":"code","metadata":{"id":"Z0RlZXHH2Diw"},"source":["import pickle\n","\n","with open('./arima_result/arima_profit_result_%s.pickle' % interval, 'rb') as f:\n","  load_candi_pr_dict = pickle.load(f)\n","\n","result_dict = dict()\n","\n","for file in file_list:\n","\n","  # file = '2021-02-07 ETH.xlsx'\n","  print(file)\n","  if not file.endswith('xlsx'):\n","    continue\n","\n","  #     add new data    #\n","  elif file not in list(load_candi_pr_dict.keys()):\n","\n","    ohlcv = pd.read_excel(date_path + file, index_col=0)\n","    print(len(ohlcv))\n","    # print(ohlcv.head())\n","\n","    temp_dict =  dict()\n","    temp_dict['ohlcv'] = ohlcv\n","    # print(temp_dict)\n","    load_candi_pr_dict[file] = temp_dict\n","\n","    # break\n","\n","    close = ohlcv.iloc[:, [3]]\n","    predictions, err_ranges = arima_test(close, 3000)\n","\n","    temp_dict['predictions'] = predictions\n","    temp_dict['err_ranges'] = err_ranges\n","    # result_dict[file] = temp_dict\n","    load_candi_pr_dict[file] = temp_dict\n","\n","  # break\n","\n","#         save dict       #\n","# with open('./arima_result/arima_test_result_%s.pickle' % interval, 'wb') as f:\n","  # pickle.dump(load_candi_pr_dict, f)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sU4Vt6404iH3"},"source":["### **Load & Individually Plotting & Add Backtest_Result**"]},{"cell_type":"code","metadata":{"id":"vUCKueyUNFE0"},"source":["import pickle\n","import time\n","import datetime\n","\n","with open('./arima_result/arima_test_result_%s.pickle' % interval, 'rb') as f:\n","  load_dict = pickle.load(f)\n","\n","tp_list = np.arange(-0.03, 0.05, 0.001)\n","leverage_list = np.arange(1, 6, 1)\n","safety_threshold = 0.\n","long_index = 0\n","leverage = 3\n","\n","def get_tp(ohlcv, predictions, err_ranges):\n","\n","    wr_list, fr_list, ap_list = list(), list(), list()\n","    max_ap_list = list()  \n","\n","    progress_cnt = 0\n","    # selected_ap_list = list()\n","    start_time = time.time()\n","\n","    for tp in tp_list:\n","\n","      progress_cnt += 1\n","\n","      if progress_cnt != 1:\n","\n","        if progress_cnt == 2:\n","          a_loop_time = time.time() - start_time\n","\n","        r_time = a_loop_time * (len(tp_list) - progress_cnt)\n","        # print(r_time)\n","        print('\\r remaining time : %s' % (datetime.timedelta(seconds=r_time)), end='')\n","      # print('\\r %.2f%%' % (progress_cnt / len(tp_list) * 100), end='')\n","\n","      #     leverage analysis     #\n","      temp_ap_list = list()\n","      for leverage in leverage_list:\n","\n","        try:\n","          result = get_back_result(ohlcv, predictions, err_ranges, tp=tp, leverage=leverage, reverse_short=False)\n","\n","          # print(np.min(result[-1]))\n","          temp_ap_list.append(result[2])\n","          # print('leverage, result[2] :', leverage, result[2])\n","\n","          if round(leverage) == 1:\n","\n","            wr_list.append(result[0])\n","            fr_list.append(result[1])\n","            ap_list.append(result[2])\n","\n","            break # <-- 어차피 acc 와 max acc 를 결정하는 tp 는 큰 차이가 없다고 생각해, \n","            #  소모 시간을 줄이기 위해 break\n","\n","          # if leverage > 1:\n","          #   break\n","\n","        except Exception as e:\n","          print(e)\n","          break\n","\n","      # print('np.max(temp_ap_list, axis=0) :', np.max(temp_ap_list, axis=0))\n","      max_ap_list.append(np.max(temp_ap_list, axis=0))\n","\n","    # print(wr_list)\n","    # break\n","\n","    #     stack dict    #\n","    temp_dict = load_dict[key]\n","    temp_dict['tp_list'] = tp_list[:len(ap_list)]\n","    wr_list = np.array(wr_list).reshape(-1, 2)\n","    fr_list = np.array(fr_list).reshape(-1, 2)\n","    ap_list = np.array(ap_list).reshape(-1, 2)\n","    max_ap_list = np.array(max_ap_list).reshape(-1, 2)\n","\n","    temp_dict['wr_list'] = wr_list\n","    temp_dict['fr_list'] = fr_list\n","    temp_dict['ap_list'] = ap_list\n","    temp_dict['max_ap_list'] = max_ap_list\n","\n","\n","    #     individual plot   #\n","    plt.figure(figsize=(15, 5))\n","    plt.suptitle('get best tp & figure')\n","\n","    plt.subplot(151)\n","    plt.plot(tp_list[:len(ap_list)], wr_list)\n","    plt.title('win ratio')\n","\n","    plt.subplot(152)\n","    plt.plot(tp_list[:len(ap_list)], fr_list)\n","    plt.title('frequency')\n","\n","    plt.subplot(153)\n","    plt.plot(tp_list[:len(ap_list)], ap_list)\n","    argmax_l, argmax_s = np.argmax(ap_list, axis=0)   # --> best tp in basic_acc\n","    plt.axvline(tp_list[:len(ap_list)][argmax_l], linestyle='--')\n","    plt.axvline(tp_list[:len(ap_list)][argmax_s], linestyle='--')\n","    plt.title('acc profit, max at l:%.4f, s:%.4f' % (tp_list[:len(ap_list)][argmax_l], tp_list[:len(ap_list)][argmax_s]))  \n","\n","    return_tp = tp_list[:len(ap_list)][argmax_l]\n","\n","    plt.subplot(154)\n","    plt.plot(tp_list[:len(max_ap_list)], max_ap_list[:, [long_index]]) \n","    argmax_l, argmax_s = np.argmax(max_ap_list, axis=0)  # --> best tp in basic_acc\n","    plt.axvline(tp_list[:len(max_ap_list)][argmax_l], linestyle='--')\n","    plt.axvline(tp_list[:len(max_ap_list)][argmax_s], linestyle='--')\n","    plt.title('max acc profit by leverage\\n max at l:%.4f, s:%.4f' % (tp_list[:len(max_ap_list)][argmax_l], tp_list[:len(max_ap_list)][argmax_s])) \n","\n","    # plt.show()\n","    l_selected_tp = tp_list[:len(max_ap_list)][argmax_l]\n","    s_selected_tp = tp_list[:len(max_ap_list)][argmax_s]\n","    #     leverage analysis     #\n","    l_selected_ap_list = list()\n","    s_selected_ap_list = list()\n","    for leverage in leverage_list:\n","\n","      try:\n","        l_result = get_back_result(ohlcv, predictions, err_ranges, tp=l_selected_tp, leverage=leverage, reverse_short=False)\n","        s_result = get_back_result(ohlcv, predictions, err_ranges, tp=s_selected_tp, leverage=leverage, reverse_short=False)\n","        \n","        if min(s_result[-1][long_index]) < safety_threshold:\n","          continue\n","\n","        l_selected_ap_list.append(l_result[2])\n","        s_selected_ap_list.append(s_result[2])\n","\n","      except Exception as e:\n","        print(e)\n","        break\n","    \n","    #     stack dict    #\n","    temp_dict['leverage_list'] = leverage_list[:len(s_selected_ap_list)]\n","    l_selected_ap_list = np.array(l_selected_ap_list).reshape(-1, 2)[:, :1]\n","    s_selected_ap_list = np.array(s_selected_ap_list).reshape(-1, 2)[:, 1:]\n","    temp_dict['leverage_ap_list'] = np.hstack((l_selected_ap_list, s_selected_ap_list))\n","    \n","    plt.subplot(155)\n","    plt.plot(leverage_list[:len(l_selected_ap_list)], l_selected_ap_list)\n","    plt.plot(leverage_list[:len(s_selected_ap_list)], s_selected_ap_list)\n","    l_argmax = np.argmax(l_selected_ap_list)\n","    s_argmax = np.argmax(s_selected_ap_list)\n","    plt.title('acc profit by leverage\\n max at l:tp=%.3f lvrg=%.0f\\n s:tp=%.3f lvrg=%.0f' \\\n","              % (l_selected_tp, leverage_list[:len(l_selected_ap_list)][l_argmax], s_selected_tp, leverage_list[:len(s_selected_ap_list)][s_argmax]))  \n","    plt.axvline(leverage_list[:len(l_selected_ap_list)][l_argmax], linestyle='--')\n","    plt.axvline(leverage_list[:len(s_selected_ap_list)][s_argmax], linestyle='--')\n","    plt.xlim(0, 13)\n","\n","    # plt.show()\n","    plt.close()\n","\n","    return return_tp\n","\n","# tp_list = [selected_tp]\n","\n","for file in file_list:\n","\n","  # file = '2021-02-07 ETH.xlsx'\n","  print(file)\n","  if not file.endswith('xlsx'):\n","    continue\n","\n","  key = file\n","\n","  ohlcv = pd.read_excel(date_path + file, index_col=0)\n","  print(len(ohlcv)) \n","\n","\n","# for key in load_dict.keys():\n","\n","  # print(key)\n","\n","  # if len(list(load_dict[key].keys())) > 3:\n","  #   print(list(load_dict[key].keys()))\n","  #   continue\n","\n","  #       tp analysis     #\n","  # ohlcv = load_dict[key]['ohlcv']\n","  predictions = load_dict[key]['predictions']\n","  err_ranges = load_dict[key]['err_ranges']\n","  # predictions = ohlcv['close'].shift(1).values\n","  # err_ranges = np.zeros_like(predictions)\n","\n","  global_tp = get_tp(ohlcv, predictions, err_ranges)\n","\n","  #   slice usable ohlcv  #\n","  ohlcv = ohlcv.iloc[-len(predictions):]\n","\n","  split_size = 3\n","  a_size = int(len(ohlcv) / 10)\n","\n","  global_acc_pr, local_acc_pr = [], []\n","\n","  #     split data    #\n","  for i in range(split_size):\n","\n","    if i != split_size - 1:\n","\n","        if i !=0 :\n","          prev_ohlcv = temp_ohlcv\n","          prev_predictions = temp_predictions\n","          prev_err_ranges = temp_err_ranges\n","\n","        temp_ohlcv = ohlcv.iloc[a_size * i : a_size * (i + 1)]\n","        temp_predictions = predictions[a_size * i : a_size * (i + 1)]\n","        temp_err_ranges = err_ranges[a_size * i : a_size * (i + 1)]\n","\n","    else:\n","\n","      if i !=0 :\n","          prev_ohlcv = temp_ohlcv\n","          prev_predictions = temp_predictions\n","          prev_err_ranges = temp_err_ranges\n","\n","      temp_ohlcv = ohlcv.iloc[a_size * i : ]\n","      temp_predictions = predictions[a_size * i : ]\n","      temp_err_ranges = err_ranges[a_size * i : ]\n","\n","    if i != 0:      \n","    \n","      #     check inner data --> prev && curr   #\n","      print('prev_predictions[:5] :', prev_predictions[:5])\n","      print('temp_predictions[:5] :', temp_predictions[:5])\n","\n","      #   get prev_best_tp    #\n","      prev_best_tp = get_tp(prev_ohlcv, prev_predictions, prev_err_ranges)\n","\n","      result = get_back_result(temp_ohlcv, temp_predictions, temp_err_ranges, tp=prev_best_tp, leverage=leverage, show_plot=False)\n","      local_acc_pr.append(result[2][0])\n","      result = get_back_result(temp_ohlcv, temp_predictions, temp_err_ranges, tp=global_tp, leverage=leverage, show_plot=False)\n","      global_acc_pr.append(result[2][0])\n","\n","  xticks = ['local', 'global']\n","  yticks = [np.cumprod(local_acc_pr)[-1], np.cumprod(global_acc_pr)[-1]]\n","  # print('yticks :', yticks)\n","  plt.bar(xticks, yticks)\n","  # plt.bar())\n","  plt.show()\n","  # print()\n","  # break   # temporary adujst on one pair\n","\n","  \n","\n","\n","#         save dict 2       #\n","# with open('./arima_result/arima_profit_ls_only_long_result_%s.pickle' % interval, 'wb') as f:\n","#   pickle.dump(load_dict, f)\n","  "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JbDmxQ7y2Our"},"source":["### **Keep Connection**"]},{"cell_type":"code","metadata":{"id":"3brDVbCf2RFd"},"source":["while 1: 1 "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lBTgw0XbdWEg"},"source":["### **Sum Plot**"]},{"cell_type":"code","metadata":{"id":"Y1pCQ6oRV1cv"},"source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import pickle\n","# print(type(result_dict))\n","\n","with open('./arima_result/arima_profit_ls_only_long_result_%s.pickle' % interval, 'rb') as f:\n","# with open('./arima_result/arima_profit_ls_only_long_expanded_result_%s.pickle' % interval, 'rb') as f:\n","  profit_result_dict = pickle.load(f)\n","\n","keys = profit_result_dict.keys()\n","print(keys)\n","# keys = ['2021-02-07 ETH.xlsx']\n","# break\n","\n","\n","for i in range(2):\n","\n","  plt.figure(figsize=(30, 5))\n","\n","  plt.subplot(151)\n","  plt.title('win ratio')  \n","  for key in keys:  \n","    # print(profit_result_dict[key]['wr_list'].shape)\n","    plt.plot(profit_result_dict[key]['tp_list'], profit_result_dict[key]['wr_list'][:, [i]])\n","\n","\n","  plt.subplot(152)\n","  plt.title('frequency')  \n","  for key in keys:  \n","    plt.plot(profit_result_dict[key]['tp_list'], profit_result_dict[key]['fr_list'][:, [i]])\n","\n","    \n","  plt.subplot(153)\n","  plt.title('acc profit')  \n","  for key in keys:  \n","    plt.plot(profit_result_dict[key]['tp_list'], profit_result_dict[key]['ap_list'][:, [i]])\n","\n","  plt.subplot(155)\n","  plt.title('max acc profit by leverage')  \n","  for key in keys:  \n","    plt.plot(profit_result_dict[key]['tp_list'], profit_result_dict[key]['max_ap_list'][:, [i]], label=key)\n","    \n","  plt.subplot(154)\n","  plt.title('acc profit by leverage')  \n","  for key in keys:  \n","    plt.plot(profit_result_dict[key]['leverage_list'], profit_result_dict[key]['leverage_ap_list'][:, [i]], label=key)\n","    # print(result_dict[key]['leverage_ap_list'])\n","  plt.legend()\n","\n","  plt.show()\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nYW6fsjGdamG"},"source":["### **Extract Candidate pairs**"]},{"cell_type":"code","metadata":{"id":"S0JmoerZz9vu"},"source":["\n","pd.set_option('display.width', 1000)\n","pd.set_option('display.max_rows', 2500)\n","pd.set_option('display.max_columns', 2500)\n","\n","pairs = list(profit_result_dict.keys())\n","result_df = pd.DataFrame(index=pairs)\n","profit = list()\n","max_lv_profit = list()\n","pr_fr_std = list()\n","max_pr_fr_std = list()\n","\n","long_index = 0\n","\n","for key in keys:  \n","  # print(np.max(profit_result_dict[key]['ap_list']))\n","  profit.append(np.max(profit_result_dict[key]['ap_list'][:, [long_index]]))\n","  # print(np.max(profit_result_dict[key]['leverage_ap_list']))\n","  max_lv_profit.append(np.max(profit_result_dict[key]['leverage_ap_list'][:, [long_index]]))\n","\n","  ohlcv = profit_result_dict[key]['ohlcv']\n","  predictions = profit_result_dict[key]['predictions']\n","  err_ranges = profit_result_dict[key]['err_ranges']\n","  \n","  #             편의를 위해 long position 분석만을 진행한다.           #\n","  argmax = np.argmax(profit_result_dict[key]['ap_list'][:, [long_index]])\n","  peak_tp = profit_result_dict[key]['tp_list'][argmax]\n","\n","  result = get_back_result(ohlcv, predictions, err_ranges, tp=peak_tp, leverage=4)\n","  profit_list = result[3][long_index]\n","  pr_list_fr = list(map(lambda x : 0 if x == 1.0 else 1, profit_list))\n","  profited_index = np.argwhere(np.array(pr_list_fr) == 1)\n","  # print(profited_index)\n","  term_list = list()\n","  for i in range(len(profited_index) - 1):\n","    term_list.append(profited_index[i + 1] - profited_index[i])\n","\n","  profit_result_dict[key]['pr_list'] = profit_list\n","  profit_result_dict[key]['pr_fr_std'] = np.std(term_list)\n","  pr_fr_std.append(np.std(term_list))\n","\n","  argmax = np.argmax(profit_result_dict[key]['max_ap_list'][:, [long_index]])\n","  max_peak_tp = profit_result_dict[key]['tp_list'][argmax]\n","\n","  result = get_back_result(ohlcv, predictions, err_ranges, tp=max_peak_tp, leverage=4)\n","  profit_list = result[3][long_index]\n","  pr_list_fr = list(map(lambda x : 0 if x == 1.0 else 1, profit_list))\n","  profited_index = np.argwhere(np.array(pr_list_fr) == 1)\n","  # print(profited_index)\n","  term_list = list()\n","  for i in range(len(profited_index) - 1):\n","    term_list.append(profited_index[i + 1] - profited_index[i])\n","\n","  profit_result_dict[key]['max_pr_list'] = profit_list\n","  profit_result_dict[key]['max_pr_fr_std'] = np.std(term_list)\n","  max_pr_fr_std.append(np.std(term_list))\n","\n","\n","# result_df['index'] = pairs\n","result_df['profit']= profit\n","result_df['max_lv_profit']= max_lv_profit\n","result_df['pr_fr_std'] = pr_fr_std\n","result_df['max_pr_fr_std'] = max_pr_fr_std\n","# print(result_df)\n","\n","#     sort by values    #\n","print(result_df.sort_values(by='profit', ascending=False))\n","print(result_df.sort_values(by=['max_lv_profit'], ascending=False))\n","print(result_df.sort_values(by=['pr_fr_std'], ascending=True))\n","print(result_df.sort_values(by=['max_pr_fr_std'], ascending=True))\n","\n","#     extract candidates    #\n","candis = result_df.sort_values(by=['max_lv_profit'], ascending=False).index\n","print(candis)\n","\n","\n","\n","#         save dict 2       #\n","# with open('./arima_result/arima_candi_profit_ls_only_long_result_%s.pickle' % interval, 'wb') as f:\n","#   pickle.dump(profit_result_dict, f)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IKke4koCbzqi"},"source":["### **Candi Result Analysis**"]},{"cell_type":"code","metadata":{"id":"4dmtTA9raAwD"},"source":["long_index = 0\n","safety_threshold = 0.6\n","\n","\n","\n","for i in range(len(candis)):\n","\n","  keys = [candis[i]]\n","\n","  plt.figure(figsize=(35, 7))\n","  plt.suptitle('%s %s' % (interval, keys))\n","\n","  plt.subplot(1,10,1)\n","  plt.title('win ratio')  \n","  for key in keys:  \n","    plt.plot(profit_result_dict[key]['tp_list'], profit_result_dict[key]['wr_list'][:, [long_index]])\n","\n","\n","  plt.subplot(1,10,2)\n","  plt.title('frequency')  \n","  for key in keys:  \n","    plt.plot(profit_result_dict[key]['tp_list'], profit_result_dict[key]['fr_list'][:, [long_index]])\n","\n","    \n","  plt.subplot(1,10,3)\n","  for key in keys:  \n","    plt.plot(profit_result_dict[key]['tp_list'], profit_result_dict[key]['ap_list'][:, [long_index]])\n","    argmax = np.argmax(profit_result_dict[key]['ap_list'][:, [long_index]])\n","    peak_tp = profit_result_dict[key]['tp_list'][argmax]\n","    plt.axvline(peak_tp, linestyle='--')\n","    plt.title('acc profit, max at %.4f' % (peak_tp))  \n","\n","  plt.subplot(1,10,4)\n","  plt.title('max acc profit by leverage')  \n","  for key in keys:  \n","    plt.plot(profit_result_dict[key]['tp_list'], profit_result_dict[key]['max_ap_list'][:, [long_index]], label=key)\n","    argmax = np.argmax(profit_result_dict[key]['max_ap_list'][:, [long_index]])\n","    max_peak_tp = profit_result_dict[key]['tp_list'][argmax]\n","    plt.axvline(max_peak_tp, linestyle='--')\n","    plt.title('max acc profit, max at %.4f' % (max_peak_tp))  \n","\n","\n","  for key in keys:  \n","    # print(profit_result_dict[key]['leverage_ap_list'])\n","\n","    for tp in [peak_tp, max_peak_tp]:\n","\n","      if tp == peak_tp:\n","        plt.subplot(1,10,5)\n","      else:\n","        plt.subplot(1,10,6)\n","\n","      #     leverage analysis     #\n","      ohlcv = profit_result_dict[key]['ohlcv']\n","      predictions = profit_result_dict[key]['predictions']\n","      err_ranges = profit_result_dict[key]['err_ranges']\n","      leverage_list = profit_result_dict[key]['leverage_list']\n","      temp_ap_list = list()\n","      temp_pr_list = list()\n","\n","      for leverage in leverage_list:\n","\n","        try:\n","          result = get_back_result(ohlcv, predictions, err_ranges, tp=tp, leverage=leverage, reverse_short=False)\n","\n","          # if min(result[-1][long_index]) < safety_threshold:\n","          #   continue\n","\n","          temp_ap_list.append(result[2][long_index])\n","          temp_pr_list.append(result[3][long_index])\n","\n","          # if round(leverage) == 1:\n","          #   temp_pr_list = result[3]\n","\n","        except Exception as e:\n","          print(e)\n","          break\n","\n","      \n","      # profit_result_dict[key]['pr_list'] = temp_pr_list\n","\n","      plt.plot(profit_result_dict[key]['leverage_list'][:len(temp_ap_list)],temp_ap_list, label=key)\n","      argmax = np.argmax(temp_ap_list)\n","      plt.title('acc profit by leverage\\n max at tp=%.4f lvrg=%.0f' % (tp, profit_result_dict[key]['leverage_list'][:len(temp_ap_list)][argmax]))  \n","      plt.axvline(profit_result_dict[key]['leverage_list'][:len(temp_ap_list)][argmax], linestyle='--')\n","\n","      if tp == peak_tp:\n","        plt.subplot(1,10,7)\n","      else:\n","        plt.subplot(1,10,9)\n","\n","      plt.plot(temp_pr_list[argmax])\n","    # plt.xlim(0, 13)\n","\n","    \n","  # plt.subplot(1,10,7)\n","  for key in keys:  \n","    pr_list = profit_result_dict[key]['pr_list']\n","    # plt.plot(pr_list)\n","\n","    pr_fr_std = profit_result_dict[key]['pr_fr_std']\n","    pr_list_fr = list(map(lambda x : 0 if x == 1.0 else 1, pr_list))\n","\n","    plt.subplot(1,10,8)\n","    plt.title('std : %s' % pr_fr_std)\n","    plt.plot(pr_list_fr)\n","\n","  # plt.subplot(1,10,9)\n","  for key in keys:  \n","    pr_list = profit_result_dict[key]['max_pr_list']\n","    # plt.plot(pr_list)\n","\n","    pr_fr_std = profit_result_dict[key]['max_pr_fr_std']\n","    pr_list_fr = list(map(lambda x : 0 if x == 1.0 else 1, pr_list))\n","\n","    plt.subplot(1,10,10)\n","    plt.title('std : %s' % pr_fr_std)\n","    plt.plot(pr_list_fr)\n","\n","\n","  \n","  \n","\n","  plt.show()\n","\n","  "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4kzJjVTbF3S_"},"source":["#### **RM - Maximum Loss**"]},{"cell_type":"code","metadata":{"id":"fCRzI_CNjrUH"},"source":["leverage_list2 = np.arange(1, 6, 1)\n","\n","for i in range(len(candis)):\n","\n","  keys = [candis[i]]\n","  \n","  if 'eth'.upper() not in candis[i]:\n","    continue\n","\n","  # plt.figure(figsize=(35, 10))\n","  # plt.suptitle('%s %s' % (interval, keys))\n","\n","  # plt.subplot(1,10,3)\n","  for key in keys:  \n","    # plt.plot(profit_result_dict[key]['tp_list'], profit_result_dict[key]['ap_list'])\n","    argmax = np.argmax(profit_result_dict[key]['ap_list'][:, [long_index]])\n","    peak_tp = profit_result_dict[key]['tp_list'][argmax]\n","    # plt.axvline(peak_tp, linestyle='--')\n","    # plt.title('acc profit, max at %.4f' % (peak_tp))  \n","\n","  # plt.subplot(1,10,4)\n","  # plt.title('max acc profit by leverage')  \n","  for key in keys:  \n","    # plt.plot(profit_result_dict[key]['tp_list'], profit_result_dict[key]['max_ap_list'], label=key)\n","    argmax = np.argmax(profit_result_dict[key]['max_ap_list'][:, [long_index]])\n","    max_peak_tp = profit_result_dict[key]['tp_list'][argmax]\n","    # plt.axvline(max_peak_tp, linestyle='--')\n","    # plt.title('max acc profit, max at %.4f' % (max_peak_tp))  \n","\n","\n","  for key in keys:  \n","    # print(profit_result_dict[key]['leverage_ap_list'])\n","\n","    for tp in [peak_tp, max_peak_tp]:\n","\n","      # if tp == peak_tp:\n","      #   plt.subplot(1,10,5)\n","      # else:\n","      #   plt.subplot(1,10,6)\n","\n","      #     leverage analysis     #\n","      ohlcv = profit_result_dict[key]['ohlcv']\n","      predictions = profit_result_dict[key]['predictions']\n","      err_ranges = profit_result_dict[key]['err_ranges']\n","      # leverage_list = profit_result_dict[key]['leverage_list']\n","      # temp_ap_list = list()\n","      # temp_pr_list = list()\n","\n","      for leverage in leverage_list2:\n","\n","        try:\n","          print('-------------- %s --------------' % key)\n","          result = get_back_result(ohlcv, predictions, err_ranges, tp=tp, leverage=leverage, show_plot=True, reverse_short=False, show_detail=False)\n","          # temp_ap_list.append(result[2])\n","          # temp_pr_list.append(result[3])\n","\n","          # if round(leverage) == 1:\n","          #   temp_pr_list = result[3]\n","\n","        except Exception as e:\n","          print(e)\n","          break\n","        \n","  # break\n","\n","  "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"d7zUmBoJjP83"},"source":["### **Optimization : Classify**"]},{"cell_type":"code","metadata":{"id":"4XiZSRpAIhE7"},"source":["\n","      \n","# from sklearn.cluster import KMeans\n","# from sklearn.metrics import silhouette_score\n","# from sklearn.model_selection import train_test_split\n","# from sklearn.metrics import precision_score, recall_score, precision_recall_curve\n","# from sklearn.metrics import confusion_matrix\n","# from sklearn.metrics import plot_confusion_matrix\n","\n","\n","# from sklearn.utils.testing import all_estimators\n","# from sklearn.metrics import accuracy_score, classification_report\n","\n","# from functools import partial\n","# from sklearn.utils import class_weight\n","\n","# np.random.seed(1)\n","# tf.random.set_seed(1)\n","\n","# # coin = 'dot'\n","# leverage = 3\n","\n","\n","# for i in range(len(candis)):\n","\n","#   keys = [candis[i]]\n","  \n","#   # if 'algo'.upper() not in candis[i]:\n","#   #   continue\n","#   if '2021-02-11 DOTUSDT.xlsx' not in candis[i]:\n","#     continue\n","\n","#   # plt.figure(figsize=(35, 10))\n","#   # plt.suptitle('%s %s' % (interval, keys))\n","\n","\n","#   #         get tp parameter        #\n","\n","#   # plt.subplot(1,10,3)\n","#   # for key in keys:  \n","#   #   # plt.plot(profit_result_dict[key]['tp_list'], profit_result_dict[key]['ap_list'])\n","#   #   argmax = np.argmax(profit_result_dict[key]['ap_list'][:, [long_index]])\n","#   #   peak_tp = profit_result_dict[key]['tp_list'][argmax]\n","#   #   # plt.axvline(peak_tp, linestyle='--')\n","#   #   # plt.title('acc profit, max at %.4f' % (peak_tp))  \n","\n","#   # plt.subplot(1,10,4)\n","#   # plt.title('max acc profit by leverage')  \n","#   for key in keys:  \n","#     # plt.plot(profit_result_dict[key]['tp_list'], profit_result_dict[key]['max_ap_list'], label=key)\n","#     argmax = np.argmax(profit_result_dict[key]['max_ap_list'][:, [long_index]])\n","#     max_peak_tp = profit_result_dict[key]['tp_list'][argmax]\n","#     # plt.axvline(max_peak_tp, linestyle='--')\n","#     # plt.title('max acc profit, max at %.4f' % (max_peak_tp))  \n","\n","\n","#   for key in keys:  \n","#     # print(profit_result_dict[key]['leverage_ap_list'])\n","\n","#     for tp in [max_peak_tp]:\n","\n","#       # if tp == peak_tp:\n","#       #   plt.subplot(1,10,5)\n","#       # else:\n","#       #   plt.subplot(1,10,6)\n","\n","#       #     leverage analysis     #\n","#       ohlcv = profit_result_dict[key]['ohlcv']\n","#       predictions = profit_result_dict[key]['predictions']\n","#       err_ranges = profit_result_dict[key]['err_ranges']\n","\n","#       # predictions = ohlcv['close'].shift(1).values\n","#       # err_ranges = np.zeros_like(predictions)\n","\n","#       leverage_list = profit_result_dict[key]['leverage_list']\n","#       # temp_ap_list = list()\n","#       # temp_pr_list = list()\n","\n","#       try:\n","#         print('-------------- %s --------------' % key)\n","#         result = get_back_result(ohlcv, predictions, err_ranges, tp=tp, leverage=leverage, show_plot=True, reverse_short=False, show_detail=False)\n","#         # temp_ap_list.append(result[2])\n","#         # temp_pr_list.append(result[3])\n","\n","#         # if round(leverage) == 1:\n","#         #   temp_pr_list = result[3]\n","#         pr_list = result[3][long_index]\n","\n","#       except Exception as e:\n","#         print(e)\n","#         break    \n","#   # break\n","\n","#       from sklearn.preprocessing import MinMaxScaler, MaxAbsScaler\n","\n","#       pd.set_option('display.max_rows', 500)\n","#       pd.set_option('display.max_columns', 500)\n","#       pd.set_option('display.width', 1000)\n","\n","#       #         clustering zone           #\n","\n","#       #       set data features : ohlc, v, ep\n","#       ohlc = ohlcv.iloc[-len(predictions):, :4]\n","#       vol = ohlcv.iloc[-len(predictions):, [4]]\n","#       long_ep = (np.array(predictions) - np.array(err_ranges)) * (1 / (tp + 1))\n","#       long_ep = long_ep.reshape(-1, 1)\n","\n","#       ohlcv['u_wick'] = ohlcv['high'] / np.maximum(ohlcv['close'] , ohlcv['open'])\n","#       ohlcv['d_wick'] = np.minimum(ohlcv['close'] , ohlcv['open']) / ohlcv['low']\n","#       ohlcv['body'] = ohlcv['close'] / ohlcv['open']\n","\n","#       candle = ohlcv.iloc[-len(predictions):, -3:]\n","\n","\n","#       print('len(ohlc) :', len(ohlc))\n","#       print('long_ep.shape :', long_ep.shape)\n","#       print('len(pr_list) :', len(pr_list))\n","\n","\n","#       #       set params    #\n","#       period = 5\n","#       data_x, data_pr, data_updown = [], [], []\n","\n","#       for i in range(period, len(predictions)):\n","\n","#         #   pr_list != 1 인 데이터만 사용한다\n","#         # if 1:\n","#         if pr_list[i] != 1:\n","          \n","#           #   prediction 을 제외한 이전 데이터를 사용해야한다\n","#           temp_ohlc = ohlc.iloc[i - period : i].values\n","#           temp_long_ep = long_ep[i - period : i]\n","#           temp_vol = vol.iloc[i - period : i].values\n","#           temp_candle = candle.iloc[i - period : i].values\n","\n","#           # print(temp_ohlc.shape)\n","#           # print(temp_long_ep.shape)\n","#           # print(temp_vol.shape)\n","#           # print(temp_candle.shape)\n","#           # break\n","\n","#           #   stacking  \n","#           # temp_data = np.hstack((temp_ohlc, temp_long_ep, temp_vol, temp_candle))\n","#           temp_data = np.hstack((temp_ohlc, temp_long_ep, temp_vol))\n","#           # temp_data = np.hstack((temp_ohlc, temp_long_ep))\n","#           # temp_data = temp_vol\n","\n","#           #   scaler 설정\n","\n","#           #   ohlc & ep -> max_abs\n","#           # max_abs = MaxAbsScaler()\n","#           # temp_data[:, :5] = max_abs.fit_transform(temp_data[:, :5])\n","\n","\n","#           min_max = MinMaxScaler()\n","#           temp_data[:, :5] = min_max.fit_transform(temp_data[:, :5])\n","\n","\n","#           #   vol -> min_max\n","#           min_max = MinMaxScaler()\n","#           temp_data[:, [5]] = min_max.fit_transform(temp_data[:, [5]])\n","\n","\n","#           #   candle -> max_abs    \n","#           # max_abs = MaxAbsScaler()\n","#           # temp_data[:, -3:] = max_abs.fit_transform(temp_data[:, -3:])\n","\n","#           # min_max = MinMaxScaler()\n","#           # temp_data[:, -3:] = min_max.fit_transform(temp_data[:, -3:])\n","\n","#           if np.isnan(np.sum(temp_data)):\n","#             continue\n","\n","#           data_x.append(temp_data)\n","#           data_pr.append(pr_list[i])\n","#           data_updown.append(ohlc['close'].iloc[i] / ohlc['open'].iloc[i])\n","\n","\n","#       print('np.array(data_x).shape :', np.array(data_x).shape)\n","#       # print(data_x[0])\n","\n","\n","#       #       Reshape data for image deep - learning     #\n","#       _, row, col = np.array(data_x).shape\n","\n","#       input_x = np.array(data_x).reshape(-1, row, col, 1).astype(np.float32)\n","\n","#       #     1c to 3c    #\n","#       input_x = input_x * np.ones(3, dtype=np.float32)[None, None, None, :]\n","\n","#       input_pr = np.array(data_pr).reshape(-1, 1).astype(np.float32)\n","#       input_ud = np.array(data_updown).reshape(-1, 1).astype(np.float32)\n","#       print('input_x.shape :', input_x.shape)\n","#       print('input_x.dtype :', input_x.dtype)\n","#       print('input_pr.shape :', input_pr.shape)\n","#       print('input_ud.shape :', input_ud.shape)\n","\n","#       # break\n","\n","\n","#       #       check input     #\n","#       for i in range(len(input_x)):\n","#         plt.imshow(input_x[i])\n","#         plt.axis('off')\n","#         plt.show()\n","#         break\n","\n","\n","#       #         train / test split      #\n","#       x_train, x_test_, pr_train, pr_test_, ud_train, ud_test_ = train_test_split(input_x, input_pr, input_ud, test_size=0.4, shuffle=True, random_state=124)\n","#       x_test, x_val, pr_test, pr_val, ud_test, ud_val = train_test_split(x_test_, pr_test_, ud_test_, test_size=0.5, shuffle=True, random_state=124)\n","\n","\n","#       #         pr label   #\n","#       # y_train = np.where(pr_train > 1, 1, 0).astype(np.float32)\n","#       # y_test = np.where(pr_test > 1, 1, 0).astype(np.float32)\n","#       # y_val = np.where(pr_val > 1, 1, 0).astype(np.float32)\n","\n","#       #         up label      #\n","#       y_train = np.where(ud_train > 1, 1, 0).astype(np.float32)\n","#       y_test = np.where(ud_test > 1, 1, 0).astype(np.float32)\n","#       y_val = np.where(ud_val > 1, 1, 0).astype(np.float32)\n","\n","#       print('pr_train[:5] :', pr_train[:5])\n","#       print('ud_train[:5] :', ud_train[:5])\n","#       print('y_train[:5] :', y_train[:5])\n","#       print('y_train.dtype :', y_train.dtype)\n","\n","#       print('x_train.shape :', x_train.shape)\n","#       print('x_test.shape :', x_test.shape)\n","#       print('x_val.shape :', x_val.shape)\n","#       print('y_train.shape :', y_train.shape)\n","#       print('y_test.shape :', y_test.shape)\n","#       print('y_val.shape :', y_val.shape)\n","\n","#       print('np.unique(y_train, return_counts=True :', np.unique(y_train, return_counts=True))\n","#       print('np.unique(y_val, return_counts=True :', np.unique(y_val, return_counts=True))\n","\n","#       label = y_train.reshape(-1, )\n","#       class_weights = class_weight.compute_class_weight('balanced', \n","#                                                           classes=np.unique(label),\n","#                                                           y=label)\n","#       class_weights = dict(enumerate(class_weights))\n","#       print('class_weights :', class_weights)\n","\n","#       print('np.isnan(np.sum(x_train)) :', np.isnan(np.sum(x_train)))\n","#       print('np.isnan(np.sum(y_train)) :', np.isnan(np.sum(y_train)))\n","\n","\n","#       train_set_ = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n","#       test_set_ = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n","#       val_set_ = tf.data.Dataset.from_tensor_slices((x_val, y_val))\n","\n","#       resize_shape = [row * 2, col * 2]\n","#       # resize_shape = [224, 224]\n","\n","#       def preprocess(image, label, flip=False):\n","#         if flip:\n","#             flipped_image = tf.image.random_flip_left_right(image)\n","#             resized_image = tf.image.resize(flipped_image, resize_shape)\n","#         else:\n","#             resized_image = tf.image.resize(image, resize_shape)\n","#         final_image = keras.applications.xception.preprocess_input(resized_image * 255)\n","#         return  final_image, label\n","\n","#       # Batch Size\n","#       batch_size = 32\n","      \n","#       train_set = train_set_.shuffle(1000).repeat(2)\n","#       train_set = train_set.map(partial(preprocess, flip=False)).batch(batch_size).prefetch(1)\n","#       val_set = val_set_.map(preprocess).batch(batch_size).prefetch(1)\n","#       test_set = test_set_.map(preprocess).batch(batch_size).prefetch(1)\n","\n","#       # print('test_set[0] :', test_set[0])\n","#       # print('len(train_set) :', len(train_set))\n","      \n","#       for set in train_set:\n","#         print(set[0].numpy()[0].shape)\n","#         plt.imshow(set[0].numpy()[0, :, :, 0])\n","#         break\n","\n","#       break"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HweGhVHHrV_U"},"source":["#### **Data stacking**"]},{"cell_type":"code","metadata":{"id":"SvZuk1rPrUMe"},"source":["from sklearn.preprocessing import MinMaxScaler, MaxAbsScaler\n","\n","\n","leverage = 2\n","prev_x = None\n","for i in range(len(candis)):\n","\n","  keys = [candis[i]]\n","  \n","  # if 'algo'.upper() not in candis[i]:\n","  #   continue\n","  if '2021-03-02 DOTUSDT.xlsx' in candis[i]:\n","    # print('')\n","    continue\n","\n","  # plt.figure(figsize=(35, 10))\n","  # plt.suptitle('%s %s' % (interval, keys))\n","\n","\n","  #         get tp parameter        #\n","\n","  # plt.subplot(1,10,3)\n","  # for key in keys:  \n","  #   # plt.plot(profit_result_dict[key]['tp_list'], profit_result_dict[key]['ap_list'])\n","  #   argmax = np.argmax(profit_result_dict[key]['ap_list'][:, [long_index]])\n","  #   peak_tp = profit_result_dict[key]['tp_list'][argmax]\n","  #   # plt.axvline(peak_tp, linestyle='--')\n","  #   # plt.title('acc profit, max at %.4f' % (peak_tp))  \n","\n","  # plt.subplot(1,10,4)\n","  # plt.title('max acc profit by leverage')  \n","  for key in keys:  \n","    # plt.plot(profit_result_dict[key]['tp_list'], profit_result_dict[key]['max_ap_list'], label=key)\n","    argmax = np.argmax(profit_result_dict[key]['max_ap_list'][:, [long_index]])\n","    max_peak_tp = profit_result_dict[key]['tp_list'][argmax]\n","    # plt.axvline(max_peak_tp, linestyle='--')\n","    # plt.title('max acc profit, max at %.4f' % (max_peak_tp))  \n","\n","\n","  for key in keys:  \n","    # print(profit_result_dict[key]['leverage_ap_list'])\n","\n","    for tp in [max_peak_tp]:\n","\n","      # if tp == peak_tp:\n","      #   plt.subplot(1,10,5)\n","      # else:\n","      #   plt.subplot(1,10,6)\n","\n","      #     leverage analysis     #\n","      ohlcv = profit_result_dict[key]['ohlcv']\n","      predictions = profit_result_dict[key]['predictions']\n","      err_ranges = profit_result_dict[key]['err_ranges']\n","\n","      # predictions = ohlcv['close'].shift(1).values\n","      # err_ranges = np.zeros_like(predictions)\n","\n","      leverage_list = profit_result_dict[key]['leverage_list']\n","      # temp_ap_list = list()\n","      # temp_pr_list = list()\n","\n","      try:\n","        print('-------------- %s --------------' % key)\n","        result = get_back_result(ohlcv, predictions, err_ranges, tp=tp, leverage=leverage, show_plot=True, reverse_short=False, show_detail=False)\n","        # temp_ap_list.append(result[2])\n","        # temp_pr_list.append(result[3])\n","\n","        # if round(leverage) == 1:\n","        #   temp_pr_list = result[3]\n","        pr_list = result[3][long_index]\n","\n","      except Exception as e:\n","        print(e)\n","        break    \n","  # break\n","\n","\n","      pd.set_option('display.max_rows', 500)\n","      pd.set_option('display.max_columns', 500)\n","      pd.set_option('display.width', 1000)\n","\n","      #         clustering zone           #\n","\n","      #       set data features : ohlc, v, ep\n","      ohlc = ohlcv.iloc[-len(predictions):, :4]\n","      vol = ohlcv.iloc[-len(predictions):, [4]]\n","      long_ep = (np.array(predictions) - np.array(err_ranges)) * (1 / (tp + 1))\n","      long_ep = long_ep.reshape(-1, 1)\n","\n","      ohlcv['u_wick'] = ohlcv['high'] / np.maximum(ohlcv['close'] , ohlcv['open'])\n","      ohlcv['d_wick'] = np.minimum(ohlcv['close'] , ohlcv['open']) / ohlcv['low']\n","      ohlcv['body'] = ohlcv['close'] / ohlcv['open']\n","\n","      candle = ohlcv.iloc[-len(predictions):, -3:]\n","\n","\n","      print('len(ohlc) :', len(ohlc))\n","      print('long_ep.shape :', long_ep.shape)\n","      print('len(pr_list) :', len(pr_list))\n","\n","\n","      #       set params    #\n","      period = 5\n","      data_x, data_pr, data_updown = [], [], []\n","      key_i = i\n","\n","      for i in range(period, len(predictions)):\n","\n","        #   pr_list != 1 인 데이터만 사용한다\n","        # if 1:\n","        if pr_list[i] != 1:\n","          \n","          #   prediction 을 제외한 이전 데이터를 사용해야한다\n","          temp_ohlc = ohlc.iloc[i - period : i].values\n","          temp_long_ep = long_ep[i - period : i]\n","          temp_vol = vol.iloc[i - period : i].values\n","          temp_candle = candle.iloc[i - period : i].values\n","\n","          # print(temp_ohlc.shape)\n","          # print(temp_long_ep.shape)\n","          # print(temp_vol.shape)\n","          # print(temp_candle.shape)\n","          # break\n","\n","          #   stacking  \n","          # temp_data = np.hstack((temp_ohlc, temp_long_ep, temp_vol, temp_candle))\n","          temp_data = np.hstack((temp_ohlc, temp_long_ep, temp_vol))\n","          # temp_data = np.hstack((temp_ohlc, temp_long_ep))\n","          # temp_data = temp_vol\n","\n","          #   scaler 설정\n","\n","          #   ohlc & ep -> max_abs\n","          # max_abs = MaxAbsScaler()\n","          # temp_data[:, :5] = max_abs.fit_transform(temp_data[:, :5])\n","\n","\n","          min_max = MinMaxScaler()\n","          temp_data[:, :5] = min_max.fit_transform(temp_data[:, :5])\n","\n","\n","          #   vol -> min_max\n","          min_max = MinMaxScaler()\n","          temp_data[:, [5]] = min_max.fit_transform(temp_data[:, [5]])\n","\n","\n","          #   candle -> max_abs    \n","          # max_abs = MaxAbsScaler()\n","          # temp_data[:, -3:] = max_abs.fit_transform(temp_data[:, -3:])\n","\n","          # min_max = MinMaxScaler()\n","          # temp_data[:, -3:] = min_max.fit_transform(temp_data[:, -3:])\n","\n","          if np.isnan(np.sum(temp_data)):\n","            continue\n","\n","          data_x.append(temp_data)\n","          data_pr.append(pr_list[i])\n","          data_updown.append(ohlc['close'].iloc[i] / ohlc['open'].iloc[i])\n","\n","\n","      print('np.array(data_x).shape :', np.array(data_x).shape)\n","      # print(data_x[0])\n","\n","\n","      #       Reshape data for image deep - learning     #\n","      _, row, col = np.array(data_x).shape\n","\n","      input_x = np.array(data_x).reshape(-1, row, col, 1).astype(np.float32)\n","\n","      #     1c to 3c    #\n","      input_x = input_x * np.ones(3, dtype=np.float32)[None, None, None, :]\n","\n","      input_pr = np.array(data_pr).reshape(-1, 1).astype(np.float32)\n","      input_ud = np.array(data_updown).reshape(-1, 1).astype(np.float32)\n","      print('input_x.shape :', input_x.shape)\n","      print('input_x.dtype :', input_x.dtype)\n","      print('input_pr.shape :', input_pr.shape)\n","      print('input_ud.shape :', input_ud.shape)\n","\n","      #     do stacking   #\n","      if prev_x is None:\n","        prev_x = input_x\n","        prev_pr = input_pr\n","        prev_ud = input_ud\n","      else:\n","        total_x = np.vstack((prev_x, input_x))\n","        total_pr = np.vstack((prev_pr, input_pr))\n","        total_ud = np.vstack((prev_ud, input_ud))\n","\n","        prev_x = total_x\n","        prev_pr = total_pr\n","        prev_ud = total_ud\n","\n","        print('total_x.shape :', total_x.shape)\n","        print('total_pr.shape :', total_pr.shape)\n","        print('total_ud.shape :', total_ud.shape)\n","        "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FPLR7hWat4sV"},"source":["import keras\n","import tensorflow as tf\n","\n","from sklearn.cluster import KMeans\n","from sklearn.metrics import silhouette_score\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import precision_score, recall_score, precision_recall_curve\n","from sklearn.metrics import confusion_matrix\n","from sklearn.metrics import plot_confusion_matrix\n","\n","\n","from sklearn.utils.testing import all_estimators\n","from sklearn.metrics import accuracy_score, classification_report\n","\n","from functools import partial\n","from sklearn.utils import class_weight\n","\n","seed = 4\n","random_state = 2\n","np.random.seed(seed)\n","tf.random.set_seed(seed)\n","\n","#         train / test split      #\n","x_train, x_test_, pr_train, pr_test_, ud_train, ud_test_ = train_test_split(total_x, total_pr, total_ud, test_size=0.4, shuffle=True, random_state=random_state)\n","x_test, x_val, pr_test, pr_val, ud_test, ud_val = train_test_split(x_test_, pr_test_, ud_test_, test_size=0.5, shuffle=True, random_state=random_state)\n","\n","\n","#         pr label   #\n","# y_train = np.where(pr_train > 1, 1, 0).astype(np.float32)\n","# y_test = np.where(pr_test > 1, 1, 0).astype(np.float32)\n","# y_val = np.where(pr_val > 1, 1, 0).astype(np.float32)\n","\n","#         up label      #\n","y_train = np.where(ud_train > 1, 1, 0).astype(np.float32)\n","y_test = np.where(ud_test > 1, 1, 0).astype(np.float32)\n","y_val = np.where(ud_val > 1, 1, 0).astype(np.float32)\n","\n","print('pr_train[:5] :', pr_train[:5])\n","print('ud_train[:5] :', ud_train[:5])\n","print('y_train[:5] :', y_train[:5])\n","print('y_train.dtype :', y_train.dtype)\n","\n","print('x_train.shape :', x_train.shape)\n","print('x_test.shape :', x_test.shape)\n","print('x_val.shape :', x_val.shape)\n","print('y_train.shape :', y_train.shape)\n","print('y_test.shape :', y_test.shape)\n","print('y_val.shape :', y_val.shape)\n","\n","def class_ratio(in_list):\n","\n","  return in_list / in_list[1]\n","\n","print('np.unique(y_train, return_counts=True :', np.unique(y_train, return_counts=True), class_ratio(np.unique(y_train, return_counts=True)[1]))\n","print('np.unique(y_val, return_counts=True :', np.unique(y_val, return_counts=True), class_ratio(np.unique(y_val, return_counts=True)[1]))\n","print('np.unique(y_test, return_counts=True :', np.unique(y_test, return_counts=True), class_ratio(np.unique(y_test, return_counts=True)[1]))\n","\n","label = y_train.reshape(-1, )\n","class_weights = class_weight.compute_class_weight('balanced', \n","                                                    classes=np.unique(label),\n","                                                    y=label)\n","class_weights = dict(enumerate(class_weights))\n","print('class_weights :', class_weights)\n","\n","sample_weight = np.ones(shape=(len(y_train),))\n","sample_weight[(y_train == 1).reshape(-1,)] = 1.5\n","print('sample_weight[:20] :', sample_weight[:20])\n","\n","\n","print('np.isnan(np.sum(x_train)) :', np.isnan(np.sum(x_train)))\n","print('np.isnan(np.sum(y_train)) :', np.isnan(np.sum(y_train)))\n","\n","\n","train_set_ = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n","test_set_ = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n","val_set_ = tf.data.Dataset.from_tensor_slices((x_val, y_val))\n","\n","resize_shape = [row * 2, col * 2]\n","# resize_shape = [224, 224]\n","\n","def preprocess(image, label, flip=False):\n","  if flip:\n","      flipped_image = tf.image.random_flip_left_right(image)\n","      resized_image = tf.image.resize(flipped_image, resize_shape)\n","  else:\n","      resized_image = tf.image.resize(image, resize_shape)\n","  final_image = keras.applications.xception.preprocess_input(resized_image * 255)\n","  return  final_image, label\n","\n","# Batch Size\n","batch_size = 32\n","\n","train_set = train_set_.shuffle(1000).repeat(2)\n","train_set = train_set.map(partial(preprocess, flip=False)).batch(batch_size).prefetch(1)\n","val_set = val_set_.map(preprocess).batch(batch_size).prefetch(1)\n","test_set = test_set_.map(preprocess).batch(batch_size).prefetch(1)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2DSqm09gtzxW"},"source":["#### **DL**"]},{"cell_type":"code","metadata":{"id":"xeiB3qAwtzbF"},"source":["base_model = keras.applications.xception.Xception(weights=\"imagenet\",\n","                                                  include_top=False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iYs4eU4zvCg-"},"source":["# print(base_model.summary())\n","# take the average of each feature map for dimensionality reduction \n","avg = keras.layers.GlobalAveragePooling2D()(base_model.output)\n","# softmax class probability layer with the chosen 30 categories\n","output = keras.layers.Dense(2, activation=\"softmax\")(avg)\n","# output = keras.layers.Dense(2, activation=\"softmax\")(base_model.output)\n","# combining the base model and the customized top layers "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TdHlub1Ivukz"},"source":["# for layer in base_model.layers:\n","#     # print(layer.name)\n","#     layer.trainable = False\n","\n","# # training the top layers\n","# optimizer = keras.optimizers.SGD(lr=0.2, momentum=0.9, decay=0.01)\n","# model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer,\n","#               metrics=[\"accuracy\"])\n","# history = model.fit(train_set, epochs=1000, validation_data=val_set)\n","# score = model.evaluate(test_set)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DPQ_11vW5dlq"},"source":["keras.backend.clear_session()\n","\n","import gc\n","\n","try:\n","  del model\n","  gc.collect()\n","except:\n","  pass\n","\n","# unfreeze the weights of the pre trained layers\n","for layer in base_model.layers:\n","    layer.trainable = True\n","# training the whole layers\n","optimizer = keras.optimizers.SGD(learning_rate=0.01, momentum=0.9,\n","                                 nesterov=True, decay=0.001)\n","# optimizer = keras.optimizers.Adam(learning_rate=0.001,\n","#                                  decay=0.001)\n","model = keras.models.Model(inputs=base_model.input, outputs=output)\n","model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer,\n","              metrics=[\"accuracy\"])\n","# model.compile(loss=\"categorical_crossentropy\", optimizer=optimizer,\n","#               metrics=[\"accuracy\"])\n","# save best performance on the validation set\n","\n","ckpt_path = current_path + 'ckpt/'\n","board_path = current_path + 'graph/'\n","\n","model_name = 'classifier_%s_min_re.h5' % period\n","# model_name = 'classifier_%s_min.h5' % period\n","\n","checkpoint_cb = keras.callbacks.ModelCheckpoint(ckpt_path + model_name, \n","                                                save_best_only=True, \n","                                                monitor='val_loss',\n","                                                mode='auto')\n","# interrupt the training when there is no progress on the validation set for 10 epocs\n","early_stopping_cb = keras.callbacks.EarlyStopping(patience=50, restore_best_weights=True)\n","#view the learning curves during training using tensorboard\n","# tensorboard_cb = keras.callbacks.TensorBoard(board_path)\n","history = model.fit(train_set, epochs=1000, \n","                    class_weight=class_weights,\n","                    # sample_weight=sample_weight,\n","                    validation_data=val_set, \n","                    shuffle=False,\n","                    callbacks=[checkpoint_cb, early_stopping_cb])\n","                    # callbacks=[checkpoint_cb, early_stopping_cb, tensorboard_cb])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RR8XMJM66k24"},"source":["# model_name = \"classifier_5_min.h5\"  # <-- specifying model name\n","model = keras.models.load_model(ckpt_path + model_name)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"z9deK1gW8YnZ"},"source":["# model.evaluate(val_set)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zhRu2BIK792m"},"source":["test_result = model.predict(test_set)\n","\n","print('test_result.shape :', test_result.shape)\n","print('pr_val.shape :', pr_val.shape)\n","\n","\n","y_score = test_result[:, [1]]\n","print('y_test[:5] :', y_test.reshape(-1,)[:5])\n","print('np.unique(y_test) :', np.unique(y_test, return_counts=True))\n","print('y_score[:5] :', y_score[:5])\n","print('np.unique(y_score) :', np.unique(y_score, return_counts=True))\n","\n","print('y_test.shape :', y_test.shape)\n","print('y_score.shape :', y_score.shape)\n","\n","#     precision recall curve   #\n","precision, recall, threshold = precision_recall_curve(y_test, y_score)\n","precision, recall = precision[:-1], recall[:-1]\n","\n","plt.plot(threshold, precision, label='precision')\n","plt.plot(threshold, recall, label='recall')\n","plt.legend()\n","plt.title('precision recall')\n","plt.show()\n","# print(y_pred)\n","\n","# thresh = 0.19\n","# threshold = [thresh]\n","print('threshold :', threshold)\n","\n","acc_pr_bythr = []\n","for thresh in threshold:\n","\n","  y_pred = np.where(y_score[:, -1] > thresh, 1, 0)\n","  print('y_pred.shape :', y_pred.shape)\n","  # print('y_pred :', y_pred)\n","\n","  #     compare precision     #\n","\n","  print('precision :', precision_score(y_test, y_pred))\n","  print('recall :', recall_score(y_test, y_pred))\n","  print()\n","\n","  print('np.isnan(np.sum(x_test)) :', np.isnan(np.sum(x_test)))\n","  print('np.isnan(np.sum(y_test)) :', np.isnan(np.sum(y_test)))\n","\n","  # plot_confusion_matrix(best_model, x_test, y_test, normalize=None)\n","  # plt.show()  \n","  print()\n","\n","  #     check win-ratio improvement     #\n","  cmat = confusion_matrix(y_test, y_pred)\n","  # print(cmat)\n","  # print(np.sum(cmat, axis=1))\n","\n","  test_size = len(y_test)\n","  test_pr_list = pr_test\n","  print('origin ac_pr :', np.cumprod(test_pr_list)[-1])\n","\n","  org_wr = np.sum(cmat, axis=1)[-1] / sum(np.sum(cmat, axis=1))\n","  ml_wr = cmat[1][1] / np.sum(cmat, axis=0)[-1]\n","  print('win ratio improvement %.2f --> %.2f' % (org_wr, ml_wr))\n","\n","  # print('pr_test.shape :', pr_test.shape)\n","\n","  # print(y_pred)\n","  # print(test_pr_list)\n","  pred_pr_list = np.where(y_pred == 1, test_pr_list.reshape(-1, ), 1.0)\n","  # print('pred_pr_list.shape :', pred_pr_list.shape)\n","\n","  if np.cumprod(test_pr_list)[-1] < np.cumprod(pred_pr_list)[-1]:\n","    print('accum_pr increased ! : %.3f --> %.3f' % (np.cumprod(test_pr_list)[-1], np.cumprod(pred_pr_list)[-1]))\n","    print('thresh :', thresh)\n","    \n","  # if len(threshold) == 1:\n","    plt.figure(figsize=(10, 5))\n","    plt.subplot(121)\n","    plt.plot(np.cumprod(test_pr_list))\n","    plt.title('%.3f' % (np.cumprod(test_pr_list)[-1]))\n","  # plt.show()\n","\n","    plt.subplot(122)\n","    plt.plot(np.cumprod(pred_pr_list))\n","    plt.title('%.3f' % (np.cumprod(pred_pr_list)[-1]))\n","    plt.show()\n","\n","\n","  acc_pr_bythr.append(np.cumprod(pred_pr_list)[-1])\n","\n","print('acc_pr_bythr :', acc_pr_bythr)\n","\n","plt.figure(figsize=(10, 5))\n","plt.subplot(121)\n","plt.plot(threshold, precision, label='precision')\n","plt.plot(threshold, recall, label='recall')\n","plt.legend()\n","plt.title('precision recall')\n","# plt.show()\n","plt.subplot(122)\n","plt.plot(threshold, acc_pr_bythr)\n","plt.axhline(np.cumprod(test_pr_list)[-1], linestyle='--', color='r')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-jo3k5MdhFyg"},"source":["#### **clustering output**"]},{"cell_type":"code","metadata":{"id":"njxxm-TJ-RP-"},"source":["# x_train = test_result.flatten().reshape(-1, 1)\n","x_train = test_result\n","print(x_train[:10])\n","# x_train = test_result[:, [1]]\n","pr_train = pr_test\n","\n","print('x_train.shape :', x_train.shape)\n","print('pr_train.shape :', pr_train.shape)\n","\n","K = range(2, 10)\n","s_dist = []\n","sil = []\n","for k in K:\n","  # if cen_data.shape[0] < k:\n","  #   break\n","\n","  km = KMeans(n_clusters=k)\n","  km = km.fit(x_train)\n","\n","  labels = km.labels_\n","  # print('len(labels) :', len(labels))\n","  # print('labels[:10] :', labels[:10])\n","  sil.append(silhouette_score(x_train, labels, metric='euclidean'))\n","\n","  # inertia = km.inertia_\n","  # s_dist.append(inertia)\n","\n","best_k = K[np.argmax(np.array(sil))]\n","\n","plt.figure(figsize=(10, 5))\n","plt.plot(K, sil)\n","plt.axvline(best_k, linestyle='--')\n","# plt.plot(K, s_dist)\n","plt.show()\n","\n","\n","\n","\n","\n","#   with best_k, label 별 pr_list 확인\n","km = KMeans(n_clusters=best_k)\n","km = km.fit(x_train)\n","\n","labels = km.labels_\n","\n","print(km.score(x_train))\n","print(len(labels), len(pr_train))\n","\n","\n","\n","\n","\n","#   label 별로 profit 을 저장, 승률을 확인한다\n","label_types = np.unique(labels, return_counts=False)\n","\n","label_pr_dict = {}\n","#   init dict   #\n","for label in label_types:\n","  label_pr_dict[label] = []\n","print(label_pr_dict)\n","# break\n","\n","for i, (label, pr) in enumerate(zip(labels, pr_train)):\n","  label_pr_dict[label].append(pr[0])\n","\n","  \n","# for label in label_types:\n","print(label_pr_dict)\n","\n","\n","\n","\n","\n","def win_ratio(list_x):\n","\n","  win_cnt = np.sum(np.array(list_x) > 1)\n","  return win_cnt / len(list_x)\n","\n","\n","def acc_pr(list_x):\n","\n","  return np.cumprod(np.array(list_x))[-1]\n","\n","\n","for key in label_pr_dict:\n","  \n","  print(key, ':', 'win_ratio : %.2f' % (win_ratio(label_pr_dict[key])), 'acc_pr : %.2f' % (acc_pr(label_pr_dict[key])))\n","\n","\n","\n","\n","\n","#     predict test && test 의 라벨에 따른 win_ratio 확인\n","# test_labels = km.predict(x_test)\n","# # print(test_labels)\n","\n","# label_pr_dict = {}\n","# #   init dict   #\n","# for label in label_types:\n","#   label_pr_dict[label] = []\n","# print(label_pr_dict)\n","# # break\n","\n","# for i, (label, pr) in enumerate(zip(test_labels, pr_test)):\n","#   label_pr_dict[label].append(pr[0])\n","\n","# for key in label_pr_dict:\n","\n","#   print(key, ':', 'win_ratio : %.2f' % (win_ratio(label_pr_dict[key])), 'acc_pr : %.2f' % (acc_pr(label_pr_dict[key])))\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RPLLdsldiVnS"},"source":[""],"execution_count":null,"outputs":[]}]}