{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python [conda env:tensorflow2_p36]","language":"python","name":"conda-env-tensorflow2_p36-py"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.5"},"colab":{"name":"Copy of Make_model_ohlcmacd.ipynb","provenance":[{"file_id":"1lDuIGJrFpZ6AR6soyMEp8QBm51qLYWIp","timestamp":1586871379369},{"file_id":"1WEMU-VCj-p8mZvMxqBpQAdViCsHXpo20","timestamp":1585569709354},{"file_id":"1z4z_KLPzc6RWsxo3X_dHbpByxUAgjoMJ","timestamp":1583754134002}],"collapsed_sections":[]},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"8s5fopqwFUf9","colab_type":"code","outputId":"4eeb7fe7-2eef-4b27-c47f-df8bbe2ea733","executionInfo":{"status":"ok","timestamp":1586997653328,"user_tz":-540,"elapsed":25489,"user":{"displayName":"JJANGJAE","photoUrl":"","userId":"08178289703395036410"}},"colab":{"base_uri":"https://localhost:8080/","height":125}},"source":["from google.colab import drive\n","\n","drive.mount('/content/gdrive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"scrolled":false,"id":"2iYLNSeSEp7p","colab_type":"code","outputId":"06522b08-4254-41c7-ea3d-5a3d5dabb8c9","executionInfo":{"status":"error","timestamp":1587000254816,"user_tz":-540,"elapsed":2584023,"user":{"displayName":"JJANGJAE","photoUrl":"","userId":"08178289703395036410"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["import os\n","# os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   \n","# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","\n","import pandas as pd\n","import numpy as np\n","from matplotlib import pyplot\n","import scipy.misc \n","from math import sqrt \n","import itertools\n","from IPython.display import display\n","%tensorflow_version 1.x\n","%matplotlib inline\n","\n","# input_data_length = int(input('input_data_length : '))\n","input_data_length = 54\n","model_num = 116\n","num_classes = 3\n","\n","gdrive_path = '/content/gdrive/My Drive/Colab Notebooks/'\n","\n","Made_X = np.load(gdrive_path + 'Made_X/Made_X %s_%s.npy' % (input_data_length, model_num))\n","Made_Y = np.load(gdrive_path + 'Made_X/Made_Y %s_%s.npy' % (input_data_length, model_num)).reshape(-1, 1)\n","\n","\n","#       dataset 분리      #\n","# dataX 구성 : VOLUME, CMO, OBV, RSI, MACD, MACD_SIGNAL, MACD_OSC, zero\n","Made_X = Made_X[:, :, [0, 1, 2, 3, -2]]\n","print(Made_X.shape)\n","print(Made_Y.shape)\n","\n","row = Made_X.shape[1]\n","col = Made_X.shape[2]\n","\n","from sklearn.model_selection import train_test_split\n","X_train, X_val, Y_train, Y_val = train_test_split(Made_X, Made_Y, test_size=0.3,\n","                                                   shuffle=False)\n","# X_test, X_val, Y_test, Y_val = train_test_split(X_test, Y_test, test_size=0.5,\n","#                                                    shuffle=False)\n","\n","X_train = X_train.astype('float32').reshape(-1, input_data_length, col, 1)\n","# X_test = X_test.astype('float32').reshape(-1, input_data_length, col, 1)\n","X_val = X_val.astype('float32').reshape(-1, input_data_length, col, 1)\n","print(X_train.shape)\n","# print(X_test.shape)\n","print(X_val.shape)\n","\n","from keras.utils import np_utils\n","from keras.preprocessing.image import ImageDataGenerator \n","\n","# Data Class Weight\n","from sklearn.utils import class_weight\n","\n","print(Y_train[:, 0])\n","class_weights = class_weight.compute_class_weight('balanced', \n","                                                  np.unique(Y_train[:, 0]),\n","                                                  Y_train[:, 0])\n","class_weights = dict(enumerate(class_weights))\n","print(class_weights)\n","# quit()\n","\n","Y_train = Y_train.astype('float32')\n","# Y_test = Y_test.astype('float32')\n","Y_val = Y_val.astype('float32')\n","Y_train = np_utils.to_categorical(Y_train, num_classes)\n","# Y_test = np_utils.to_categorical(Y_test, num_classes)\n","Y_val = np_utils.to_categorical(Y_val, num_classes)\n","print(Y_train.shape)\n","# print(Y_test.shape)\n","print(Y_val.shape)\n","\n","datagen = ImageDataGenerator( \n","#     rotation_range = 60,\n","#     zoom_range = 0.6,\n","#     shear_range = 0.6,\n","#     horizontal_flip = True,\n","#     width_shift_range=0.6,\n","#     height_shift_range=0.6,\n","    fill_mode = 'nearest'\n","    )\n","\n","testgen = ImageDataGenerator( \n","    )\n","datagen.fit(X_train)\n","batch_size = 128\n","\n","for X_batch, _ in datagen.flow(X_train, Y_train, batch_size=9):\n","    for i in range(0, 9): \n","        pyplot.axis('off') \n","        pyplot.subplot(330 + 1 + i) \n","        pyplot.imshow(X_batch[i].reshape(input_data_length, col))\n","    pyplot.axis('off') \n","    pyplot.show() \n","    break\n","    \n","    \n","train_flow = datagen.flow(X_train, Y_train, batch_size=batch_size) \n","val_flow = testgen.flow(X_val, Y_val, batch_size=batch_size) \n","\n","\n","from keras.utils import plot_model\n","import keras.backend as K\n","from keras.models import Model, Sequential\n","import keras.layers as layers\n","from keras.optimizers import Adam, SGD\n","from keras.regularizers import l1, l2\n","from sklearn.metrics import confusion_matrix\n","\n","def FER_Model(input_shape=(row, col, 1)):\n","    # first input model\n","    visible = layers.Input(shape=input_shape, name='input')\n","    \n","    net = layers.Conv2D(64, kernel_size=3, padding='same')(visible)\n","    # net = layers.Activation('relu')(net)\n","    net = layers.LeakyReLU()(net)\n","    net = layers.MaxPool2D(pool_size=2)(net)\n","\n","    shortcut_1 = net\n","\n","    net = layers.Conv2D(128, kernel_size=3, padding='same')(net)\n","    # net = layers.Activation('relu')(net)\n","    net = layers.LeakyReLU()(net)\n","    net = layers.MaxPool2D(pool_size=2)(net)\n","\n","    shortcut_2 = net\n","\n","    net = layers.Conv2D(256, kernel_size=3, padding='same')(net)\n","    # net = layers.Activation('relu')(net)\n","    net = layers.LeakyReLU()(net)\n","    # net = layers.MaxPool2D(pool_size=2)(net)\n","\n","    shortcut_3 = net\n","\n","#     net = layers.Conv2D(128, kernel_size=1, padding='same')(net)\n","#     # net = layers.Activation('relu')(net)\n","#     net = layers.LeakyReLU()(net)\n","#     net = layers.MaxPool2D(pool_size=2)(net)\n","\n","    net = layers.Flatten()(net)\n","    net = layers.Dense(64)(net)\n","    net = layers.LeakyReLU()(net)\n","    net = layers.Dense(num_classes, activation='softmax')(net)\n","\n","    # create model \n","    model = Model(inputs =visible, outputs = net)\n","    # summary layers\n","    print(model.summary())\n","    \n","    return model\n","\n","model = FER_Model()\n","# from keras.models import load_model\n","# model = load_model(gdrive_path + 'model/rapid_ascending %s_%s_ohlcmacd.hdf5' % (input_data_length, model_num))\n","opt = Adam(lr=0.0001, decay=1e-6)\n","model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n","  \n","    \n","from keras.callbacks import ModelCheckpoint, EarlyStopping, TensorBoard\n","filepath = gdrive_path + \"model/rapid_ascending %s_%s_ohlcmacd.hdf5\" % (input_data_length, model_num)\n","checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='auto')\n","checkpoint2 = TensorBoard(log_dir='Tensorboard_graph',\n","                          histogram_freq=0,\n","                          write_graph=True,\n","                          write_images=True)\n","checkpoint3 = EarlyStopping(monitor='val_loss', patience=100)\n","callbacks_list = [checkpoint, checkpoint2, checkpoint3]\n","\n","# keras.callbacks.Callback 로 부터 log 를 받아와 history log 를 작성할 수 있다.\n","\n","# we iterate 200 times over the entire training set\n","num_epochs = 500\n","history = model.fit_generator(train_flow, \n","                    steps_per_epoch=len(X_train) / batch_size, \n","                    epochs=num_epochs,  \n","                    verbose=2,  \n","                    callbacks=callbacks_list,\n","                    class_weight=class_weights,\n","                    validation_data=val_flow,  \n","                    validation_steps=len(X_val) / batch_size,\n","                    shuffle=False)\n","\n"],"execution_count":2,"outputs":[{"output_type":"stream","text":["TensorFlow 1.x selected.\n","(297341, 54, 5)\n","(297341, 1)\n","(208138, 54, 5, 1)\n","(89203, 54, 5, 1)\n"],"name":"stdout"},{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"},{"output_type":"stream","text":["[0. 0. 0. ... 0. 0. 0.]\n","{0: 0.4318394954147475, 1: 2.970768747680626, 2: 2.8759464986458854}\n","(208138, 3)\n","(89203, 3)\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAQAAAADnCAYAAADxRIjoAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAgAElEQVR4nO2dz48kWX7QP9/3IiIjMyurqrurp6enp2fsnbF3PdjYlrCx8GGFhQRGCHzigAR/AQKJCxzggOQTEncD4oLEaSUulpBhAWF2hYyFV/6xi9ee3dme2Znu6a7+UVVZmRkRL957HCKruqpmOqo3u7Q1+er7kVJdWRNTetKL933f31+JMaIoytXEXPYCFEW5PFQAKMoVRgWAolxhVAAoyhVGBYCiXGGyvv/4t7/xD+N7mw8YmJYnbsx//eArvP13/4Svh6/Jj2uBysXzN373H58K/bz/4DW+9Pf+UPc1AT795I04DZHBcif/4/4v8j///i/xX771Lz93b3s1gF+98T0+WlynDhlD6y5+tcqlUPvs1CdGAWMve1nKBdDEyJvZgNt2xOQl9rRXAyil5Z3xLhu2AmBnc3Yxq1Quldz4U99FIpL3vgrKmhCAP20CW6bGITxqJtCT63PurrtgCaZTFHwUENUS1x0fTyt+ERCrGkAqzGJOHsNLPXuuAHjixuTG44Jl73DE5isvT7ls6jbDyPNbIXoBFQBJ4GOnuRu6/R1at7oG4BF+ZvwAS+RBs9X9UjRwsO7k1pMbj5GIDwaxEbG6rykwjRn33A43swMAHlRbnV3wAnoFwDefvktmAl/eePj8xnhJ1UL54jKw7fHP1gSsDZCpDyAF/qS+A8AP3Q18FBY+R8KLz2zvrn/1+vsAGOn+wE/uPCGoqrj2fP+bb3P9Lz3iWrng+492KP7vhpoAieCi/YyPB/9iAdCr9+XSkkuLJfCsHfPpdKImQAKYrxxiJTJ3BVsbCw7faRF17ibBLAyoYn78aYLt9QGce5o/qm8wDSUuWmqnamIK3NqaEoEQhTJrkVGr0Z1ECNHwyG0yDwXzUPCsHsGqJgB0UYCBeW4zqg9g/WmDoWkzMtvlA0QvYFSzS4GpL7lfbXO9mBGi8HQ24o12/sLnewWAixk38hmlcUx9iTFRTYAE2F+UiETqdmn3t7qnqfCDxQ7TdkAbDbXPWNR5rw/gXA3gyPs/sRWbw+riVqpcGrn1ZLZ7KWIUpjaqCZAQk6wGwFjHoGihbV/47LkC4B9c+z0mEvmwHfKN8l18kV/cSpVLo/Vnbn0VAMlhJJJZT1zVB/A3N77DjrXkWN7NK37p2of8n/zmhS9U+fFSuwxjIiKREAw4Pfyp8Ea5x6+O/5yxqfmkvcb3DnZ6Izy9AuCWzTAYrAgjcnbyKWS3L3zRyo+XtrXIiVRgggqAVHDB8u3qLgPjOPQljbcMV00FzsVilpFCI5CL14SRBAjedCXASyEgTnpjxcr68NbgCS52x3rDVtwYzmnDigJgGhqumSFWDHV0zEOBaMro2uNbQ3RdDQCAqY0KgET4leEHp75/snmNPzSvv/D53tP8u4vb/NrwUzbMgI/bmj+a3tV4cQLEWbftR0feOHqTRZT14bvN6cP+0eI6rKoB3LCH/PbsLW5kh/g4YeoGF7NK5XKxEbzASdNfNYAk+ObBTwOQGU8TMv740W3eDA9f+HyvALjndvi5wceMpGU3jMhMAC0bTQNzoh+AAZ0QlQbZiW5PhWkZFW51DSAXz3fqN9i0FVXIqdpcb4oUMLHz/J/UAHpeEmV9CPFzIjref/Z3S3oFwCwMmIeCaRh2fxwNFyVBa5Z9wLqvEtAaj4Tp0+7OqQWwXejv6GHxeDQTcN0Rd/r2Ny2q2SXEyXZvwOrVgDfsIR6DXfYUGmWOmvLVV6hcOtFEEBBNAkoKI5FcunZvbVj6616lK/DTdoOR6YoLDlyptmIiiD9z8HVfk8ASeKd8RC6efT86nfH5OfQKgE/bLUI0TIPe+smjPoAkeG90n18oP6IUz64f883xOyxWrQUIy95iz//VlNEUiPb0HkaDagCJ8MBts+9HGAm4aNlvhhTxxQN9egXAln3eScRFrQFIBYnS+QCgiwZEzQNIhS274Gk7ZmQ7s92H/rydXgEwXtr+AFXMKUxLLZoNuO5EiadzANQPmAzXs0Nyadm2c5poT7WA/zxeurKnFMdGXlOjAmDtOWkCCESL1gIkgiXwXvkJBZ6DUHKjnPGs53kt7buCSHNaLTSNqgCpcDM7wEWLw2IlMMkrnvX08XzpxP5ZGLDXjC5kkcrlIu2JAx9BPOrcvaKc3xbcb5zyBSjrTxiELgFoKQfUBEiH79ZvsG3nWAJNtDxYbAHTFz5/rgYwNjW5eErjGGXNRa5VuSykywSMsvwYvf1TYWxqfBT88mhn8uJCIHgJDaAUB0AT7edXGinrh/C8HDgKGuFNB0MgYI7TOj5TF3CGcwXAUS1AwNB49RkmgY3PIwExEo1KgFSoYnHq+3mX9rknugo5ubQYAoVtcVoNuPZI4eFEU9CYqQmQCkeFey9LrwDYbTfxCJaIR5i3hXqLEyDWFmnM8cHPZkYzARPFSFy9GtBFyzwUxz0Bap9RxH6ngrIGeOnqAY6sAKuHPyVK0/nt/Ev47M41AQbGYZdvSm708CvKF5nSOO5kz8ilZRqGXR/PVasBgePDD506ETVxfP2Rrh/AUfhPAloNmAilOEam7sx2U1OYtreV/4/u1teEkfVHIlHQIqAE8Qh7foSVgIvZspR/xaagSqIEOREGFKJ2ek+GPT/mabtxHP8/dANg/sLnewXAPBSYEyaA85YsuotZqXK5aC/AJPnK4D6ftltsmgqAMnPUqzYFHZmGneyAQjyzMCC3nqhz5Nefz50MpKZdCvy/6g4AT9kA6EL3vLiWp1cAHB1+6LyLm3nFvvYDSIOTk4E0ETAZjiYDH3FeG79eAVCI5/Vsn1xaZmHQ2RU6HHT9aeVUNaD2A0iHs6372mB6fb29AuC6PcRIYCwODBjpjykq64EsBcBRnohpIWoYMAn8meMeoyA9PoDe6/xOdsCeH1Et1Qp7TmWRsh4cDQOR2H20yPPq0j8XwG+waSoaLCEaxrZRDSABPpP6a1AnYEK4kGGl28+AID0+gF4NoBTHXujagBkJzHzR97iiKJfM3A9w0XbTvEOO87Y3y7NXAHgM2+Z5EsEkqy5upcqlEbN46hO0GCgpAvL8E6VXuzu3GhDgVKqYURNg3Tk7F1AHhKZD+BHzu3+kVOCgOaNpcObCV99uOhjisf3vo+kK+HpMgB+xGlDDgCkQh74z/pYhgHamgj0V8hNNQI14rOl37r5ET0AhF6+3f0LcvfuEUd5QWE/jLX+e3UKspgNeRc4VAHezA3Ii06MUQ9UA1p6N4nlueGE9We6RTAtDk6XHCdh7rf9Ets91A9vGcNO03BnsEa1qAutOcbazk0RQDSAZXLTHUYBXmg68ZYR8OVesxLBhK9UAEqAJ3WFvT74cWuORBEeRO3/SZF81ESg/MVTQiHQth1UArD0+dN7hbOkgisEgqtldSc41/FwM2GVs0WNUACSASCST0EV1TPddTYB0OAoDvgy9AiDEuBwzEJffjSYCJUBpW24MZgxsy8LnZLkHnQ6UBGcPv7zKaDB3dPBfcVHKF4/ffON3uJ1t8N8Xlt//5C1koHUeVxHRiTCKcnVRz4+iXGFUACjKFUYFgKJcYVQAKMoVRgWAolxhVAAoyhVGBYCiXGFUACjKFUYFgKJcYVQAKMoVRgWAolxhVAAoyhWmtxrwN7/9t+LEVhgJuGj57Qd/keyvfcTXw9e0JniNuffx7bgXMu7awNMQ+Kcf/Qbz3wj8zu6/0X1dcw7u340bpgTg4/aQf/Xor/Ldf/Qe/+2b//xz97ZXAIxsjZGAJRDoushoQ5D15167wVO/wa5fMAsDKp8D9bn/n/LFZ8OUfNQect1kTIzl9eKAP1t1LkAuvusxdvLMi1oN6869ZoeAwUdDFfNufJSOB08CHwNVFFwM7IXAh9V1xK/YE9DSqf4nZwKIdgRae6ZhSCmOiVkwNnWn2em+JsGzsKCKlmkMzI9G+4UVZwN6DC5k5LZrI90GgzaOWn8sgZGpec0echDdZS9HuUA+aAvmYcA0tABs2LpXA+gVAIe+ZN8PjwcOVm3G4AIXq1wOpXH80N1g01TMYsHMFQzVB5AE7zevs9tOmJgKj+HPprcQ51/4fK8AeNBsAVCHDUI8f8iAsh782ugD/tP0Z8mlZSKhaw+uPoAkmJgFk2Jx/P2t8TPuteMXPv9S86BGpnn1lSlfGEoR7uZP2TY1e2FAGwz5ZS9KuRA+bbdx0XLDHuIxfDzfRlz7wuf7w4DLg58bjwtq/adCjvALg/tMjFDFlnHeEHQwSBIcTQY6CENCFAwRv6oT0Ejs/qD2BU+KD1tLFQfAAo+QSaDR8G4S3MwOTn3fLBY88S/W7/rnAkTbzY8zdLFiJQk+8VtUoWBkHJZIZjyNzgZMgkJOO/xyCauHAeuQMTCd/RCiEKNoIlACbJs5UyITaQkCk7xmIaPLXpZyAXxQv3bq+8NqAv7FUYDe07zw+amb3wfVAlLgnruJR5jGjF0/ZNbqVKCUie2LnYC9AmA7X+Ci5Z3yET81fHjunDFlPfgLxX3u2H1CFKqYUxivw0EToQr5sSPQRcthMwC/ogkQohCicK/aAZbZolE9ginwP2Y/w7uDTwkYPpltMQqaCJQCpXH4ZeKekcDAtr0mwLlOwBANh17z/1LiXnuDB80W17NDABYuZxSrS16VchEcHX5/ooIvrioA2mXs/6gYSJPF0uDbizd53GzwA3sTgFldcL1HTVTWnFUFQKBLJFDS4ruHr3PgSmqfERDqKofw4pdEWW/6JoD3JwItD//AdBVjRiCqGrD27DVDMglUPiNEg5jYe0so60tAsD1n9qWC+nXIqYNmi6fE02rE1JVM3YBmnhN78sWV9eHt4jF1yJmYiompqNtsdR/A0Da8VhxgiUx912dMG4KsP4/nY8ZFgyE+r/FQDSAJ/nRxh518Si6eaejObF/krlcA7OSH3M2fkotnz4/YHi40XpwAIrHrArT8WaTfTlTWh7cHjynFYSRQGsdroynTnuzdXgFwO39GLp7SNJTL3nGZNgVdewbWM7Ato6yh8RmYqCGeRNi28+OfC/GUtmXa83yvANi2c8JZN4EWjaw9A9uyXSy4XsyYtmXnBFSSoImWKuTHRUHztt931ysApmHInewZubRYIreGU56pCbD2iES28wUD090OxmhT0FSYhwHfq25xKz/AIzyYbbIZH7/w+V4BcNMeHNsTE7NgklfsWS0cWXfe3njKzBfUIcNHIcvVAZgKH9Y7/ORgl4ldMPVDBrZ/b3sFwKbppgIVBBCwErUcOBEKcxT2U40uJW7l++z7EfMwoIoZtbcMehy8vaf56PAbiRSE7qXR1lFrTxOyU7MelHSYhwEeoYrd3T7K+9u+92oABYGBeKxEHKZrDqIawNqz8DmT7HT1n2h0JwmOsnaPGGaOPhHQKwAG4hmbLg7g8JTGIaoBrD1HZd5gaIPFez38qVCKw0V7nOdhzunh0SsAxiYwEUMuhip6BtJqIlACFMZTh4zceJpg8a3uaSp4DB5DjsNjaLztHejbKwC2jMUiDCTHYJjYSvMAEmB3sYE1gTujfSZ53YUBlSQ49CVfLu9TiuOJ3+h+uWomYI7lMDo8ERdDVw+gJkAS+GB4Uo8I0RCCaCpwQrxfv/7Sz/bPBoyOa6bEIBxSdzMC1Vm09gyzzi10YzBn1hZE9QEkxb1qh2tZlxL8ZD7mek8xUO91niO46GnxWIQtuyCqBrD2LNqcNhraaDhsB8T5S02IU9aAh27zuJMXLEs8Vs0DyMXg8Ljo8UedgVQDWHtqn1HaTguo2wxxomHARJi1A8ZZTS4vl93ZK/o9kXnwlGJwJ3oCKOtNGwwhCi5YmmARnfeQDAPj2MmnWCIhCLnt7/V4TjGQ57HP2VneFtodOA0WLuP9Jze5MZ4zrQtspU7AVLhbPj3+ecNWbJcL4qphQOjaC8/jUathtf9TYGc058tbDxmYlj989iZTQDS/IwneyJ/hYoZdTvTdLCr2Vw0D1hHezhy5GPaD7/6o2oprz7Vyztd/8BXevtHdFr6IvX3jlPVht91kbOplPUDBoRv0tvE7pyswzGOE6PERNqxOj0mB+4dbGBOo2pzaWySiPQETIUTD1A8xErocj3MG+p5rArgTpqEVHR6RArMmp8g8tbe03hJKtf9TwSNMTIWVQLOsCOxr9nKuAGiiwUrER+FpO76whSqXR4yCj0LtMnwwRIk67yERQjRdBy+J+GXJd1+It382IMI05oxpmYaCTxbbvUkFynrgg2F2WDIoHd4b8gOrQ18T4nG7efyzkUjsqd85JxMwHjeOmJiG2+X+BS1RuUwOD0uMiRgTsTYQsqiCPTFy8S+VDNSrAdz3IwD2won4v74oa89o3Dlzi6zFeUulQ5+S4e3idAPQnfKQ3Z4Qb38/AHE0GCwRFy0LXyBqK649IhHvDU273P5M9zQVpqFkFgbHt/9eM0Jk/sLnz0kFFrZNQ05kHi0D44jaPjoJ3tzeJ7eeg7rk0GxofkciTP0Qjxyb7m3oT97rFQCleCYSKcWQR89GpnkAKZCZwI1yxsAuOwPbiGRqB6TAvh+esv1dsPTtbK8AuGUDFsNAMox4Nmx1UetULpFR4RhadzweTEqvvR4T4ZkbsZl1eQBVyDlsCq71PH9OS7CuGYgVwzw0nWRRVXHtGWQtf+fGt3jNTvm9xTt8K7uLDIeXvSzlAvAYcvFYCcxjQe2y1ROBcrE88jO2jE4DSol//c7XOIgDjAR+Zfh9/vTd23w82Dz/f1S+8DQh4weLHYwE2mA7R2+P475XADzyM2YhAg0Adcg1DJgAe6G77eexC+8OTAsDFfIp4ILl7eETBsbxzI2xJqyuATTLw24AFyNVUEdRKjTRHtd2hGiIubYFS4E3y2eMTHdhb2ULdjZmvc/37no4868WA6VBEy3/ef/neW90nxCF33/0Fju05/+Pyheeke1KgaEb7Zcbv3o1oI/d4fdLTSAXr4lACfBH1Vt84/6XyO94QhR2P97mZti97GUpF0CIhnkojkOBVZsz6nleYz9XkP/w/l8mLPsC1iEHQX07iZCLZ7eZ4KLFRctBNegt9OrVACZGmIZILrLUBIy+KAnwV+78gDcG+1gJfLi4gRm1xExbgqXCu8NHxz+/Ppn2mgC9GsB+iHzQbrHrhSde+Li+pgIgAf7o8R12mwm38z1uDQ6IQXTkW0Ic+QCqkHNQl68SBTD4aKjjidvBqyNw3fnq7e9xu9ijNI6JrRhPKlANIAmODr9HsBIY583qw0F3Q+c+OIiDTpq4UjWABLiVH1CKowo5RgJbw4qYacv3FLhuZ0xDSSnddGBrwupRgCrkFEtvosPSqg8gCVy0PG4njExDiIbMhN6uMcr6YCVQimNkatxL9ATs3fXSOEpxFHjG0jDJaghqAqw7U1/y/flNnrZj9tsh+wud+JQKu+3kOAToosUHg/QI914N4OnRfPElIYpqAImwW21QmJZnzYi9RxNu8eKmEcp68dQ/b97bpQK/WACIjoRSlKuLGn6KcoVRAaAoVxgVAIpyhVEBoChXGBUAinKFUQGgKFcYFQCKcoVRAaAoVxgVAIpyhVEBoChXGBUAinKFUQGgKFeY3mrA3/qzr8axqSnF4aLlfx18md//rV/kW//un+h8sDXmX/zJb8Rfn/wxb2QLvttc498++Cp//I2f4nv/TPd13bn38e14NL3DA//+2S/zv3++4Ovha5+7t70CYGzqbhiI6ZqD7DVD0Fdk7Zn7gj9vbuF5xDQMyUxARz6kwehE+6+cbk4AvHjqU68AcMtegEf/Dq1TAZAAPzf6IaVxVDHHRYshglaFJ8FZOR6iWb0nYBMzLAEfDV7dBcmwbefMwwAfu0myQ+uQoJI9BZoz/T18NKv3BITn48BcsNQhI+p7svZMw5A9PyIXj0eWAuCyV6VcBJ/RAM5R2XsFgKVrMFgaRyU546x+1fUpXwB22wmP3YR56GzDx82YZRs5JUFk1bkAY1PzerbPyNTMw4ANWxPVElh7jppGTkzFPAzYq4cqAFJm1clAP5E/5kv5AT+bR94rptwZ7KkTMAG+M7tzPBjksdvg4XSCqBMwXVZtC/5GtmAihkDAAFt2Tuz5Y8p68O7wETezA67bQ356+Ck/99p9jQIkjKwaBShFeBoCExNpYqQ6TjFQ1pmd7GAZHuoiAteLuWoAiXD2RjfnSPZeATALkU/9iCZWBAzP2nHf48qaUIhnFgaUxj3/pUYBkuDshEcr/XMBegXAD5eDQXbDiCrkTH2pPoAEaKLFSiCXFh8NIYpqAIngznz3r5IIFKJhYiosEWsCI9NcxBqVS+at/Cm5tADsLQfAqg8gYXo0gF4n4MRUXDcNt6zjDTvneja78LUpP34+aa/xpWzOLw8i7+WPNcU7IfKzH9Ou7gS8bhoCUEWYR8u+H2omYAL82vBDHOCiZz/kfDi/ftlLUi6IsyaAC1lvGLBXAOyGwfGs8SrmPHMjtRUT4L4vcNFS5hUDEXaKmZoAiRKQ1WsBdv2EPT9mYhZUMefQD/RFSQCPHGcDjk3g9cG+CvZE8Cf2MbCs5F1VA9jz487zD/goNCFTAZAAd22NFWEkFh9bde4mRACaaLBLif5KUYCpL9n3I1zM8AgLnyNBJcC6U0XY9xk7trMYB+as5aisM8WJ0s7SuNVNgIdui/dnr3G73Afgw4NrGC0aWXvm0fJ9d5NcHuIR7lU7WgyUCLmc/e5XNwH+YO8tni5GPFxMcN7y6Mkm1/VFWXs+9Rt82OzwleIhAA+qLRUAiZADpRhyMbgYyMUjq+YBPFmMyK3HEHHBEL3oi5IAT/wGf33jO7yZQSme7Xxx2UtSLohSDGZp8xuRzrxbNRU4N4GtoqKwLdYEdgctEgYXu2Llx06IBhcNj73jU7+Bi0adu4lwdPj9ckMtr1ALMMobrg3mDK2jtI5Pyi11AibCH1Rv83q+xywMeNaMdF8TIsR4QhCY1X0ApXVkEhjbGkOkyLzGixNgz4/4QX2Tie1U/3lbqHM3EVwMXWHnsjmoi3b1MOAoc+y7kqHt4sQhos0jE+DbszfZc0OeNj8LwLNqiEYC0+Gkwt9FAVY0AZpgaXzGo3oCQOstpVcVYN3JjGdgWnLjOWwLDhYl41b3NQXcmbbgLtreLl69AqANhso/fyREUQ0gAWbtgIfVhNvDA6xEYhTVABLh7Da+0mCQyufUJwRA21oVAAnwpB5RWkeIQu0zYgSjml0SnN1G/ypzARZtzsI97wPonFVvcQLM24LrgznQVYuJoPkdieDOHHgXslfQANqMeZNjTSBEwdcWo7bi2lNYTxsNRiKFaRGJuq+J4M807OjKgVcUAIdVl/QTohCjQNRMwBQorWOS1WxmC+qQU2QtoiZAMhwVA52n/sM5AmBYOGqX0fqu16iYqD6ABChtF97NlsH/EIwKgIRozo7vWjkK4Ds10YelBgCIqoprTxss72w8ZmQanrVdU1AVAGlQnLmhc/GrmwA+dGMFJAoRiEGQoCrAuvMToyds2JrcdJ2Br4/nRHTmQwqcTfl5JQEQAe8Nxixvh6A3RQpcy2dMTMW2nVNlOVvFHfZ1W5PgbD8AI4G48lyAYGjqnCxfev4ag4T2lRepXC4j07Bt5/xU8ZBZLMiManWp8JnJQMTeov9eAeCcxT8Z4CctMULxxCJOwwDrzjvFI+41OwCU0hV8aTlwGnxmMtCrhAHdwyHihbifYwKYVhCvt8W6czfb45uHP83NbArAg/kmVhO8kuDs+N7zfAC9HYHEC2EYiOO2+9dGpFUBsO58ffYzvDV4QmkaPm23OKi0yUsqnNQAPMtioJV9AMPlYQ8CEgk5n002VtaO/XbE4zjhsZt0056iqHM3IRzdzX58Va/qAwA62zDI8c8aBlx/nrUjHlUTJnlFEzJql2mNRyIcyfEjT90rVQMeO4YkdmnAEVAfwNrzqJqw1wwxEmmjwTWZ+nYS4Wwx0HlOwF4fQPdEhCJAEYgGUA1g7Xkw36S0jsEyESioCZAMPsqpTzibFnyGXg1Ahh4kkpctwRuizY97jSnry6SoqXzOvos0IevuDHXuJsHnpQL3TfTuNwFMJDYGF7vgggVVFROgbjOGmaMwniZknRNQNbskuNDJQLF5rj4cX/yqAaw9bTTs1cPnvxAN76bC2TyAV0oFxkZMFjC26xsXbVQBkAA+GMrMUWaOZlnqrb6dNLAi+Bixy0NvCfS1Bej1EJgsIAbEBJDY2RIaLlp7ysyRmUAmAbOM8Gh0Jy18jN2H/jBgfyaggeCF4G1XChxRDSABXht2KcBh6R2KEdUAEqGKEQfHH2B1J2DwAkGIy3LgKKgASIBcAqV1GIldCFDQRKBEOOsD6EyAVX0AywzA6JfTRvWmSIKFz3lWj5jkNW00xIDuayLkZw57FwV48fP9UYAIcpgRy+7lyKdGNYAEuD/bArqOsccNX1st806Bs5OBPGb1KIDMsqXnn04biBBVAKw9A9uSW08mnvboetB9TYJchIkpMBjq6LCE1UeDAZ36cNQRLIsaBUiAgGAkkpmAOdpPNQGSoBRLhsVKJ9hL4+jLBu43AbLl7X8USDTQGYzKutMGc7p/lGoASTCQnEVsyD/THOzzOVcDkMZA1r0cxom+KAkwzhoWbc68LTpBoKZdMgQCP2wDN23n0/m4ud77fK8AMJVBnBCX/iHTAF6dRevO48WYeZNzYzzvnIDO6L4mwgeui/7vLiN3r1QLYJpubHSwggDGQdSMsbWnbjMOZyXuaOLT3EKj88GvIv0CwEH5SHAb3ffRwwhOX5R152BWEoKhrrvtFy/EVtu9X0V6BUDxrEv/ldB9TBtVA0gA78+4hQWimgBJYOW0L8ecM8yzVwBkFfji+feQiUYBEkDoCryOJj61gvoAEiE/M+DhlRqCtGX375FQ8QO9KVJATMDaiEgkLDs+676mgT1z2F+tFgCIy3DisWah4aL1Jwq+FYwNx0JS2pUAAAA4SURBVN91X9PgcweD9CAa/1WUq8v5XYEVRUkWFQCKcoVRAaAoVxgVAIpyhVEBoChXGBUAinKF+f/KJpXsTCVLBgAAAABJRU5ErkJggg==\n","text/plain":["<Figure size 432x288 with 9 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}},{"output_type":"stream","text":["WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n","Instructions for updating:\n","If using Keras pass *_constraint arguments to layers.\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4070: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n","\n","Model: \"model_1\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","input (InputLayer)           (None, 54, 5, 1)          0         \n","_________________________________________________________________\n","conv2d_1 (Conv2D)            (None, 54, 5, 64)         640       \n","_________________________________________________________________\n","leaky_re_lu_1 (LeakyReLU)    (None, 54, 5, 64)         0         \n","_________________________________________________________________\n","max_pooling2d_1 (MaxPooling2 (None, 27, 2, 64)         0         \n","_________________________________________________________________\n","conv2d_2 (Conv2D)            (None, 27, 2, 128)        73856     \n","_________________________________________________________________\n","leaky_re_lu_2 (LeakyReLU)    (None, 27, 2, 128)        0         \n","_________________________________________________________________\n","max_pooling2d_2 (MaxPooling2 (None, 13, 1, 128)        0         \n","_________________________________________________________________\n","conv2d_3 (Conv2D)            (None, 13, 1, 256)        295168    \n","_________________________________________________________________\n","leaky_re_lu_3 (LeakyReLU)    (None, 13, 1, 256)        0         \n","_________________________________________________________________\n","flatten_1 (Flatten)          (None, 3328)              0         \n","_________________________________________________________________\n","dense_1 (Dense)              (None, 64)                213056    \n","_________________________________________________________________\n","leaky_re_lu_4 (LeakyReLU)    (None, 64)                0         \n","_________________________________________________________________\n","dense_2 (Dense)              (None, 3)                 195       \n","=================================================================\n","Total params: 582,915\n","Trainable params: 582,915\n","Non-trainable params: 0\n","_________________________________________________________________\n","None\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/callbacks/tensorboard_v1.py:200: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/callbacks/tensorboard_v1.py:203: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n","\n","Epoch 1/500\n"," - 22s - loss: 1.0662 - accuracy: 0.2295 - val_loss: 1.0657 - val_accuracy: 0.2643\n","\n","Epoch 00001: val_loss improved from inf to 1.06574, saving model to /content/gdrive/My Drive/Colab Notebooks/model/rapid_ascending 54_116_ohlcmacd.hdf5\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/callbacks/tensorboard_v1.py:343: The name tf.Summary is deprecated. Please use tf.compat.v1.Summary instead.\n","\n","Epoch 2/500\n"," - 15s - loss: 1.0570 - accuracy: 0.2222 - val_loss: 1.1148 - val_accuracy: 0.2091\n","\n","Epoch 00002: val_loss did not improve from 1.06574\n","Epoch 3/500\n"," - 15s - loss: 1.0536 - accuracy: 0.2259 - val_loss: 1.0946 - val_accuracy: 0.2037\n","\n","Epoch 00003: val_loss did not improve from 1.06574\n","Epoch 4/500\n"," - 15s - loss: 1.0517 - accuracy: 0.2270 - val_loss: 1.0784 - val_accuracy: 0.2347\n","\n","Epoch 00004: val_loss did not improve from 1.06574\n","Epoch 5/500\n"," - 15s - loss: 1.0505 - accuracy: 0.2358 - val_loss: 1.1524 - val_accuracy: 0.1815\n","\n","Epoch 00005: val_loss did not improve from 1.06574\n","Epoch 6/500\n"," - 15s - loss: 1.0489 - accuracy: 0.2371 - val_loss: 1.0696 - val_accuracy: 0.2474\n","\n","Epoch 00006: val_loss did not improve from 1.06574\n","Epoch 7/500\n"," - 15s - loss: 1.0468 - accuracy: 0.2452 - val_loss: 1.1257 - val_accuracy: 0.1997\n","\n","Epoch 00007: val_loss did not improve from 1.06574\n","Epoch 8/500\n"," - 15s - loss: 1.0461 - accuracy: 0.2458 - val_loss: 1.0725 - val_accuracy: 0.2220\n","\n","Epoch 00008: val_loss did not improve from 1.06574\n","Epoch 9/500\n"," - 15s - loss: 1.0443 - accuracy: 0.2486 - val_loss: 1.1242 - val_accuracy: 0.2368\n","\n","Epoch 00009: val_loss did not improve from 1.06574\n","Epoch 10/500\n"," - 15s - loss: 1.0426 - accuracy: 0.2523 - val_loss: 1.0783 - val_accuracy: 0.2543\n","\n","Epoch 00010: val_loss did not improve from 1.06574\n","Epoch 11/500\n"," - 15s - loss: 1.0406 - accuracy: 0.2574 - val_loss: 1.0839 - val_accuracy: 0.2180\n","\n","Epoch 00011: val_loss did not improve from 1.06574\n","Epoch 12/500\n"," - 15s - loss: 1.0392 - accuracy: 0.2599 - val_loss: 1.0631 - val_accuracy: 0.2473\n","\n","Epoch 00012: val_loss improved from 1.06574 to 1.06305, saving model to /content/gdrive/My Drive/Colab Notebooks/model/rapid_ascending 54_116_ohlcmacd.hdf5\n","Epoch 13/500\n"," - 15s - loss: 1.0375 - accuracy: 0.2618 - val_loss: 1.0037 - val_accuracy: 0.2668\n","\n","Epoch 00013: val_loss improved from 1.06305 to 1.00366, saving model to /content/gdrive/My Drive/Colab Notebooks/model/rapid_ascending 54_116_ohlcmacd.hdf5\n","Epoch 14/500\n"," - 15s - loss: 1.0363 - accuracy: 0.2637 - val_loss: 1.0570 - val_accuracy: 0.2391\n","\n","Epoch 00014: val_loss did not improve from 1.00366\n","Epoch 15/500\n"," - 15s - loss: 1.0335 - accuracy: 0.2653 - val_loss: 1.0347 - val_accuracy: 0.2632\n","\n","Epoch 00015: val_loss did not improve from 1.00366\n","Epoch 16/500\n"," - 15s - loss: 1.0319 - accuracy: 0.2692 - val_loss: 1.0729 - val_accuracy: 0.2520\n","\n","Epoch 00016: val_loss did not improve from 1.00366\n","Epoch 17/500\n"," - 15s - loss: 1.0302 - accuracy: 0.2707 - val_loss: 1.0629 - val_accuracy: 0.2565\n","\n","Epoch 00017: val_loss did not improve from 1.00366\n","Epoch 18/500\n"," - 15s - loss: 1.0279 - accuracy: 0.2716 - val_loss: 1.0793 - val_accuracy: 0.2353\n","\n","Epoch 00018: val_loss did not improve from 1.00366\n","Epoch 19/500\n"," - 15s - loss: 1.0263 - accuracy: 0.2752 - val_loss: 1.0847 - val_accuracy: 0.2567\n","\n","Epoch 00019: val_loss did not improve from 1.00366\n","Epoch 20/500\n"," - 15s - loss: 1.0244 - accuracy: 0.2755 - val_loss: 1.0344 - val_accuracy: 0.2688\n","\n","Epoch 00020: val_loss did not improve from 1.00366\n","Epoch 21/500\n"," - 15s - loss: 1.0217 - accuracy: 0.2766 - val_loss: 1.0475 - val_accuracy: 0.2983\n","\n","Epoch 00021: val_loss did not improve from 1.00366\n","Epoch 22/500\n"," - 15s - loss: 1.0202 - accuracy: 0.2802 - val_loss: 1.0322 - val_accuracy: 0.2871\n","\n","Epoch 00022: val_loss did not improve from 1.00366\n","Epoch 23/500\n"," - 15s - loss: 1.0176 - accuracy: 0.2861 - val_loss: 1.0756 - val_accuracy: 0.3189\n","\n","Epoch 00023: val_loss did not improve from 1.00366\n","Epoch 24/500\n"," - 15s - loss: 1.0160 - accuracy: 0.2829 - val_loss: 1.0448 - val_accuracy: 0.2829\n","\n","Epoch 00024: val_loss did not improve from 1.00366\n","Epoch 25/500\n"," - 15s - loss: 1.0137 - accuracy: 0.2880 - val_loss: 1.1286 - val_accuracy: 0.2258\n","\n","Epoch 00025: val_loss did not improve from 1.00366\n","Epoch 26/500\n"," - 15s - loss: 1.0112 - accuracy: 0.2900 - val_loss: 1.0797 - val_accuracy: 0.2606\n","\n","Epoch 00026: val_loss did not improve from 1.00366\n","Epoch 27/500\n"," - 15s - loss: 1.0089 - accuracy: 0.2916 - val_loss: 1.0473 - val_accuracy: 0.2976\n","\n","Epoch 00027: val_loss did not improve from 1.00366\n","Epoch 28/500\n"," - 15s - loss: 1.0060 - accuracy: 0.2954 - val_loss: 1.0156 - val_accuracy: 0.2872\n","\n","Epoch 00028: val_loss did not improve from 1.00366\n","Epoch 29/500\n"," - 15s - loss: 1.0038 - accuracy: 0.2967 - val_loss: 1.0720 - val_accuracy: 0.3044\n","\n","Epoch 00029: val_loss did not improve from 1.00366\n","Epoch 30/500\n"," - 15s - loss: 1.0014 - accuracy: 0.3002 - val_loss: 1.0591 - val_accuracy: 0.2888\n","\n","Epoch 00030: val_loss did not improve from 1.00366\n","Epoch 31/500\n"," - 15s - loss: 0.9986 - accuracy: 0.3026 - val_loss: 1.0652 - val_accuracy: 0.3310\n","\n","Epoch 00031: val_loss did not improve from 1.00366\n","Epoch 32/500\n"," - 15s - loss: 0.9964 - accuracy: 0.3057 - val_loss: 1.0191 - val_accuracy: 0.3109\n","\n","Epoch 00032: val_loss did not improve from 1.00366\n","Epoch 33/500\n"," - 15s - loss: 0.9938 - accuracy: 0.3085 - val_loss: 1.1165 - val_accuracy: 0.3159\n","\n","Epoch 00033: val_loss did not improve from 1.00366\n","Epoch 34/500\n"," - 15s - loss: 0.9912 - accuracy: 0.3104 - val_loss: 1.0926 - val_accuracy: 0.2853\n","\n","Epoch 00034: val_loss did not improve from 1.00366\n","Epoch 35/500\n"," - 15s - loss: 0.9891 - accuracy: 0.3140 - val_loss: 1.0704 - val_accuracy: 0.2643\n","\n","Epoch 00035: val_loss did not improve from 1.00366\n","Epoch 36/500\n"," - 15s - loss: 0.9865 - accuracy: 0.3138 - val_loss: 1.0120 - val_accuracy: 0.3332\n","\n","Epoch 00036: val_loss did not improve from 1.00366\n","Epoch 37/500\n"," - 15s - loss: 0.9836 - accuracy: 0.3167 - val_loss: 1.1203 - val_accuracy: 0.2609\n","\n","Epoch 00037: val_loss did not improve from 1.00366\n","Epoch 38/500\n"," - 15s - loss: 0.9810 - accuracy: 0.3203 - val_loss: 1.0406 - val_accuracy: 0.2774\n","\n","Epoch 00038: val_loss did not improve from 1.00366\n","Epoch 39/500\n"," - 15s - loss: 0.9777 - accuracy: 0.3225 - val_loss: 1.0439 - val_accuracy: 0.3097\n","\n","Epoch 00039: val_loss did not improve from 1.00366\n","Epoch 40/500\n"," - 15s - loss: 0.9754 - accuracy: 0.3243 - val_loss: 1.1297 - val_accuracy: 0.2846\n","\n","Epoch 00040: val_loss did not improve from 1.00366\n","Epoch 41/500\n"," - 15s - loss: 0.9731 - accuracy: 0.3268 - val_loss: 1.1287 - val_accuracy: 0.2820\n","\n","Epoch 00041: val_loss did not improve from 1.00366\n","Epoch 42/500\n"," - 15s - loss: 0.9698 - accuracy: 0.3298 - val_loss: 1.1188 - val_accuracy: 0.2917\n","\n","Epoch 00042: val_loss did not improve from 1.00366\n","Epoch 43/500\n"," - 15s - loss: 0.9667 - accuracy: 0.3330 - val_loss: 0.9993 - val_accuracy: 0.3403\n","\n","Epoch 00043: val_loss improved from 1.00366 to 0.99930, saving model to /content/gdrive/My Drive/Colab Notebooks/model/rapid_ascending 54_116_ohlcmacd.hdf5\n","Epoch 44/500\n"," - 15s - loss: 0.9643 - accuracy: 0.3359 - val_loss: 1.0795 - val_accuracy: 0.2977\n","\n","Epoch 00044: val_loss did not improve from 0.99930\n","Epoch 45/500\n"," - 15s - loss: 0.9619 - accuracy: 0.3368 - val_loss: 1.0547 - val_accuracy: 0.3241\n","\n","Epoch 00045: val_loss did not improve from 0.99930\n","Epoch 46/500\n"," - 15s - loss: 0.9588 - accuracy: 0.3407 - val_loss: 1.1870 - val_accuracy: 0.2726\n","\n","Epoch 00046: val_loss did not improve from 0.99930\n","Epoch 47/500\n"," - 15s - loss: 0.9559 - accuracy: 0.3425 - val_loss: 1.0836 - val_accuracy: 0.3421\n","\n","Epoch 00047: val_loss did not improve from 0.99930\n","Epoch 48/500\n"," - 15s - loss: 0.9526 - accuracy: 0.3462 - val_loss: 1.0725 - val_accuracy: 0.3200\n","\n","Epoch 00048: val_loss did not improve from 0.99930\n","Epoch 49/500\n"," - 15s - loss: 0.9498 - accuracy: 0.3475 - val_loss: 1.0929 - val_accuracy: 0.2691\n","\n","Epoch 00049: val_loss did not improve from 0.99930\n","Epoch 50/500\n"," - 15s - loss: 0.9477 - accuracy: 0.3483 - val_loss: 1.1027 - val_accuracy: 0.3143\n","\n","Epoch 00050: val_loss did not improve from 0.99930\n","Epoch 51/500\n"," - 15s - loss: 0.9449 - accuracy: 0.3516 - val_loss: 1.0466 - val_accuracy: 0.3270\n","\n","Epoch 00051: val_loss did not improve from 0.99930\n","Epoch 52/500\n"," - 15s - loss: 0.9419 - accuracy: 0.3523 - val_loss: 1.0890 - val_accuracy: 0.3131\n","\n","Epoch 00052: val_loss did not improve from 0.99930\n","Epoch 53/500\n"," - 15s - loss: 0.9387 - accuracy: 0.3558 - val_loss: 1.1617 - val_accuracy: 0.2745\n","\n","Epoch 00053: val_loss did not improve from 0.99930\n","Epoch 54/500\n"," - 15s - loss: 0.9362 - accuracy: 0.3580 - val_loss: 1.1091 - val_accuracy: 0.3394\n","\n","Epoch 00054: val_loss did not improve from 0.99930\n","Epoch 55/500\n"," - 15s - loss: 0.9337 - accuracy: 0.3622 - val_loss: 1.0731 - val_accuracy: 0.3091\n","\n","Epoch 00055: val_loss did not improve from 0.99930\n","Epoch 56/500\n"," - 15s - loss: 0.9313 - accuracy: 0.3615 - val_loss: 1.0619 - val_accuracy: 0.3390\n","\n","Epoch 00056: val_loss did not improve from 0.99930\n","Epoch 57/500\n"," - 15s - loss: 0.9275 - accuracy: 0.3653 - val_loss: 1.0972 - val_accuracy: 0.3494\n","\n","Epoch 00057: val_loss did not improve from 0.99930\n","Epoch 58/500\n"," - 15s - loss: 0.9245 - accuracy: 0.3682 - val_loss: 1.1070 - val_accuracy: 0.3557\n","\n","Epoch 00058: val_loss did not improve from 0.99930\n","Epoch 59/500\n"," - 15s - loss: 0.9236 - accuracy: 0.3696 - val_loss: 1.1456 - val_accuracy: 0.3370\n","\n","Epoch 00059: val_loss did not improve from 0.99930\n","Epoch 60/500\n"," - 15s - loss: 0.9207 - accuracy: 0.3718 - val_loss: 1.0424 - val_accuracy: 0.3199\n","\n","Epoch 00060: val_loss did not improve from 0.99930\n","Epoch 61/500\n"," - 15s - loss: 0.9168 - accuracy: 0.3744 - val_loss: 1.1076 - val_accuracy: 0.3075\n","\n","Epoch 00061: val_loss did not improve from 0.99930\n","Epoch 62/500\n"," - 15s - loss: 0.9148 - accuracy: 0.3762 - val_loss: 1.2262 - val_accuracy: 0.3075\n","\n","Epoch 00062: val_loss did not improve from 0.99930\n","Epoch 63/500\n"," - 15s - loss: 0.9118 - accuracy: 0.3790 - val_loss: 1.1060 - val_accuracy: 0.3422\n","\n","Epoch 00063: val_loss did not improve from 0.99930\n","Epoch 64/500\n"," - 15s - loss: 0.9088 - accuracy: 0.3804 - val_loss: 1.0456 - val_accuracy: 0.3513\n","\n","Epoch 00064: val_loss did not improve from 0.99930\n","Epoch 65/500\n"," - 15s - loss: 0.9064 - accuracy: 0.3816 - val_loss: 1.0613 - val_accuracy: 0.3595\n","\n","Epoch 00065: val_loss did not improve from 0.99930\n","Epoch 66/500\n"," - 15s - loss: 0.9043 - accuracy: 0.3854 - val_loss: 1.1025 - val_accuracy: 0.3159\n","\n","Epoch 00066: val_loss did not improve from 0.99930\n","Epoch 67/500\n"," - 15s - loss: 0.9022 - accuracy: 0.3869 - val_loss: 1.1367 - val_accuracy: 0.3114\n","\n","Epoch 00067: val_loss did not improve from 0.99930\n","Epoch 68/500\n"," - 15s - loss: 0.8991 - accuracy: 0.3880 - val_loss: 1.0605 - val_accuracy: 0.3102\n","\n","Epoch 00068: val_loss did not improve from 0.99930\n","Epoch 69/500\n"," - 15s - loss: 0.8967 - accuracy: 0.3907 - val_loss: 1.2348 - val_accuracy: 0.2986\n","\n","Epoch 00069: val_loss did not improve from 0.99930\n","Epoch 70/500\n"," - 15s - loss: 0.8936 - accuracy: 0.3939 - val_loss: 1.0779 - val_accuracy: 0.2853\n","\n","Epoch 00070: val_loss did not improve from 0.99930\n","Epoch 71/500\n"," - 15s - loss: 0.8911 - accuracy: 0.3942 - val_loss: 1.0151 - val_accuracy: 0.3823\n","\n","Epoch 00071: val_loss did not improve from 0.99930\n","Epoch 72/500\n"," - 15s - loss: 0.8885 - accuracy: 0.3972 - val_loss: 1.1122 - val_accuracy: 0.3470\n","\n","Epoch 00072: val_loss did not improve from 0.99930\n","Epoch 73/500\n"," - 15s - loss: 0.8857 - accuracy: 0.3990 - val_loss: 1.0507 - val_accuracy: 0.3433\n","\n","Epoch 00073: val_loss did not improve from 0.99930\n","Epoch 74/500\n"," - 15s - loss: 0.8839 - accuracy: 0.4006 - val_loss: 1.1035 - val_accuracy: 0.3535\n","\n","Epoch 00074: val_loss did not improve from 0.99930\n","Epoch 75/500\n"," - 15s - loss: 0.8800 - accuracy: 0.4018 - val_loss: 1.0918 - val_accuracy: 0.3392\n","\n","Epoch 00075: val_loss did not improve from 0.99930\n","Epoch 76/500\n"," - 15s - loss: 0.8787 - accuracy: 0.4049 - val_loss: 1.1685 - val_accuracy: 0.3150\n","\n","Epoch 00076: val_loss did not improve from 0.99930\n","Epoch 77/500\n"," - 15s - loss: 0.8751 - accuracy: 0.4059 - val_loss: 1.0908 - val_accuracy: 0.3416\n","\n","Epoch 00077: val_loss did not improve from 0.99930\n","Epoch 78/500\n"," - 15s - loss: 0.8724 - accuracy: 0.4085 - val_loss: 1.0416 - val_accuracy: 0.3591\n","\n","Epoch 00078: val_loss did not improve from 0.99930\n","Epoch 79/500\n"," - 15s - loss: 0.8711 - accuracy: 0.4110 - val_loss: 1.0704 - val_accuracy: 0.3725\n","\n","Epoch 00079: val_loss did not improve from 0.99930\n","Epoch 80/500\n"," - 15s - loss: 0.8672 - accuracy: 0.4134 - val_loss: 1.0814 - val_accuracy: 0.3274\n","\n","Epoch 00080: val_loss did not improve from 0.99930\n","Epoch 81/500\n"," - 15s - loss: 0.8644 - accuracy: 0.4140 - val_loss: 1.1236 - val_accuracy: 0.3424\n","\n","Epoch 00081: val_loss did not improve from 0.99930\n","Epoch 82/500\n"," - 15s - loss: 0.8635 - accuracy: 0.4153 - val_loss: 1.1145 - val_accuracy: 0.3353\n","\n","Epoch 00082: val_loss did not improve from 0.99930\n","Epoch 83/500\n"," - 15s - loss: 0.8600 - accuracy: 0.4179 - val_loss: 1.0924 - val_accuracy: 0.3569\n","\n","Epoch 00083: val_loss did not improve from 0.99930\n","Epoch 84/500\n"," - 15s - loss: 0.8574 - accuracy: 0.4197 - val_loss: 1.0196 - val_accuracy: 0.3214\n","\n","Epoch 00084: val_loss did not improve from 0.99930\n","Epoch 85/500\n"," - 15s - loss: 0.8547 - accuracy: 0.4231 - val_loss: 1.1128 - val_accuracy: 0.3357\n","\n","Epoch 00085: val_loss did not improve from 0.99930\n","Epoch 86/500\n"," - 15s - loss: 0.8527 - accuracy: 0.4236 - val_loss: 1.0775 - val_accuracy: 0.3242\n","\n","Epoch 00086: val_loss did not improve from 0.99930\n","Epoch 87/500\n"," - 15s - loss: 0.8507 - accuracy: 0.4242 - val_loss: 1.1193 - val_accuracy: 0.3486\n","\n","Epoch 00087: val_loss did not improve from 0.99930\n","Epoch 88/500\n"," - 15s - loss: 0.8473 - accuracy: 0.4274 - val_loss: 1.0884 - val_accuracy: 0.3412\n","\n","Epoch 00088: val_loss did not improve from 0.99930\n","Epoch 89/500\n"," - 15s - loss: 0.8458 - accuracy: 0.4288 - val_loss: 1.1396 - val_accuracy: 0.3396\n","\n","Epoch 00089: val_loss did not improve from 0.99930\n","Epoch 90/500\n"," - 15s - loss: 0.8433 - accuracy: 0.4299 - val_loss: 1.1204 - val_accuracy: 0.3163\n","\n","Epoch 00090: val_loss did not improve from 0.99930\n","Epoch 91/500\n"," - 15s - loss: 0.8400 - accuracy: 0.4336 - val_loss: 1.1245 - val_accuracy: 0.3451\n","\n","Epoch 00091: val_loss did not improve from 0.99930\n","Epoch 92/500\n"," - 15s - loss: 0.8380 - accuracy: 0.4341 - val_loss: 1.0353 - val_accuracy: 0.3539\n","\n","Epoch 00092: val_loss did not improve from 0.99930\n","Epoch 93/500\n"," - 15s - loss: 0.8351 - accuracy: 0.4367 - val_loss: 0.9977 - val_accuracy: 0.3898\n","\n","Epoch 00093: val_loss improved from 0.99930 to 0.99774, saving model to /content/gdrive/My Drive/Colab Notebooks/model/rapid_ascending 54_116_ohlcmacd.hdf5\n","Epoch 94/500\n"," - 15s - loss: 0.8327 - accuracy: 0.4385 - val_loss: 1.2013 - val_accuracy: 0.3243\n","\n","Epoch 00094: val_loss did not improve from 0.99774\n","Epoch 95/500\n"," - 15s - loss: 0.8299 - accuracy: 0.4405 - val_loss: 1.1700 - val_accuracy: 0.3444\n","\n","Epoch 00095: val_loss did not improve from 0.99774\n","Epoch 96/500\n"," - 15s - loss: 0.8286 - accuracy: 0.4418 - val_loss: 1.2970 - val_accuracy: 0.3298\n","\n","Epoch 00096: val_loss did not improve from 0.99774\n","Epoch 97/500\n"," - 15s - loss: 0.8263 - accuracy: 0.4435 - val_loss: 1.0978 - val_accuracy: 0.3891\n","\n","Epoch 00097: val_loss did not improve from 0.99774\n","Epoch 98/500\n"," - 15s - loss: 0.8227 - accuracy: 0.4470 - val_loss: 1.1864 - val_accuracy: 0.3382\n","\n","Epoch 00098: val_loss did not improve from 0.99774\n","Epoch 99/500\n"," - 15s - loss: 0.8222 - accuracy: 0.4467 - val_loss: 1.2930 - val_accuracy: 0.3282\n","\n","Epoch 00099: val_loss did not improve from 0.99774\n","Epoch 100/500\n"," - 15s - loss: 0.8193 - accuracy: 0.4490 - val_loss: 1.2540 - val_accuracy: 0.3352\n","\n","Epoch 00100: val_loss did not improve from 0.99774\n","Epoch 101/500\n"," - 15s - loss: 0.8164 - accuracy: 0.4498 - val_loss: 1.1614 - val_accuracy: 0.4032\n","\n","Epoch 00101: val_loss did not improve from 0.99774\n","Epoch 102/500\n"," - 15s - loss: 0.8143 - accuracy: 0.4526 - val_loss: 1.2186 - val_accuracy: 0.3606\n","\n","Epoch 00102: val_loss did not improve from 0.99774\n","Epoch 103/500\n"," - 15s - loss: 0.8121 - accuracy: 0.4538 - val_loss: 1.1471 - val_accuracy: 0.3375\n","\n","Epoch 00103: val_loss did not improve from 0.99774\n","Epoch 104/500\n"," - 15s - loss: 0.8099 - accuracy: 0.4553 - val_loss: 1.1714 - val_accuracy: 0.3518\n","\n","Epoch 00104: val_loss did not improve from 0.99774\n","Epoch 105/500\n"," - 15s - loss: 0.8066 - accuracy: 0.4588 - val_loss: 1.1398 - val_accuracy: 0.3780\n","\n","Epoch 00105: val_loss did not improve from 0.99774\n","Epoch 106/500\n"," - 15s - loss: 0.8055 - accuracy: 0.4595 - val_loss: 1.1111 - val_accuracy: 0.3722\n","\n","Epoch 00106: val_loss did not improve from 0.99774\n","Epoch 107/500\n"," - 15s - loss: 0.8028 - accuracy: 0.4610 - val_loss: 1.3908 - val_accuracy: 0.3829\n","\n","Epoch 00107: val_loss did not improve from 0.99774\n","Epoch 108/500\n"," - 15s - loss: 0.7994 - accuracy: 0.4628 - val_loss: 1.1575 - val_accuracy: 0.3844\n","\n","Epoch 00108: val_loss did not improve from 0.99774\n","Epoch 109/500\n"," - 15s - loss: 0.7976 - accuracy: 0.4640 - val_loss: 1.0795 - val_accuracy: 0.3684\n","\n","Epoch 00109: val_loss did not improve from 0.99774\n","Epoch 110/500\n"," - 15s - loss: 0.7960 - accuracy: 0.4661 - val_loss: 1.1615 - val_accuracy: 0.3612\n","\n","Epoch 00110: val_loss did not improve from 0.99774\n","Epoch 111/500\n"," - 15s - loss: 0.7942 - accuracy: 0.4673 - val_loss: 1.3876 - val_accuracy: 0.3434\n","\n","Epoch 00111: val_loss did not improve from 0.99774\n","Epoch 112/500\n"," - 15s - loss: 0.7912 - accuracy: 0.4701 - val_loss: 1.1446 - val_accuracy: 0.3684\n","\n","Epoch 00112: val_loss did not improve from 0.99774\n","Epoch 113/500\n"," - 15s - loss: 0.7892 - accuracy: 0.4702 - val_loss: 1.3001 - val_accuracy: 0.3534\n","\n","Epoch 00113: val_loss did not improve from 0.99774\n","Epoch 114/500\n"," - 15s - loss: 0.7867 - accuracy: 0.4718 - val_loss: 1.0332 - val_accuracy: 0.4005\n","\n","Epoch 00114: val_loss did not improve from 0.99774\n","Epoch 115/500\n"," - 15s - loss: 0.7838 - accuracy: 0.4752 - val_loss: 1.0313 - val_accuracy: 0.4048\n","\n","Epoch 00115: val_loss did not improve from 0.99774\n","Epoch 116/500\n"," - 15s - loss: 0.7815 - accuracy: 0.4768 - val_loss: 1.1561 - val_accuracy: 0.3686\n","\n","Epoch 00116: val_loss did not improve from 0.99774\n","Epoch 117/500\n"," - 15s - loss: 0.7813 - accuracy: 0.4761 - val_loss: 0.9699 - val_accuracy: 0.4202\n","\n","Epoch 00117: val_loss improved from 0.99774 to 0.96989, saving model to /content/gdrive/My Drive/Colab Notebooks/model/rapid_ascending 54_116_ohlcmacd.hdf5\n","Epoch 118/500\n"," - 15s - loss: 0.7766 - accuracy: 0.4792 - val_loss: 1.1751 - val_accuracy: 0.4030\n","\n","Epoch 00118: val_loss did not improve from 0.96989\n","Epoch 119/500\n"," - 15s - loss: 0.7749 - accuracy: 0.4814 - val_loss: 1.0544 - val_accuracy: 0.4592\n","\n","Epoch 00119: val_loss did not improve from 0.96989\n","Epoch 120/500\n"," - 15s - loss: 0.7741 - accuracy: 0.4830 - val_loss: 1.0716 - val_accuracy: 0.4195\n","\n","Epoch 00120: val_loss did not improve from 0.96989\n","Epoch 121/500\n"," - 15s - loss: 0.7708 - accuracy: 0.4845 - val_loss: 1.1979 - val_accuracy: 0.3804\n","\n","Epoch 00121: val_loss did not improve from 0.96989\n","Epoch 122/500\n"," - 15s - loss: 0.7701 - accuracy: 0.4855 - val_loss: 1.0919 - val_accuracy: 0.3479\n","\n","Epoch 00122: val_loss did not improve from 0.96989\n","Epoch 123/500\n"," - 15s - loss: 0.7682 - accuracy: 0.4854 - val_loss: 1.3111 - val_accuracy: 0.3780\n","\n","Epoch 00123: val_loss did not improve from 0.96989\n","Epoch 124/500\n"," - 15s - loss: 0.7657 - accuracy: 0.4877 - val_loss: 1.1721 - val_accuracy: 0.3862\n","\n","Epoch 00124: val_loss did not improve from 0.96989\n","Epoch 125/500\n"," - 15s - loss: 0.7630 - accuracy: 0.4904 - val_loss: 1.1458 - val_accuracy: 0.3972\n","\n","Epoch 00125: val_loss did not improve from 0.96989\n","Epoch 126/500\n"," - 15s - loss: 0.7630 - accuracy: 0.4899 - val_loss: 1.2147 - val_accuracy: 0.3632\n","\n","Epoch 00126: val_loss did not improve from 0.96989\n","Epoch 127/500\n"," - 15s - loss: 0.7587 - accuracy: 0.4925 - val_loss: 1.3479 - val_accuracy: 0.3721\n","\n","Epoch 00127: val_loss did not improve from 0.96989\n","Epoch 128/500\n"," - 15s - loss: 0.7579 - accuracy: 0.4938 - val_loss: 1.0862 - val_accuracy: 0.3950\n","\n","Epoch 00128: val_loss did not improve from 0.96989\n","Epoch 129/500\n"," - 15s - loss: 0.7560 - accuracy: 0.4946 - val_loss: 1.3396 - val_accuracy: 0.4087\n","\n","Epoch 00129: val_loss did not improve from 0.96989\n","Epoch 130/500\n"," - 15s - loss: 0.7530 - accuracy: 0.4964 - val_loss: 1.2332 - val_accuracy: 0.4594\n","\n","Epoch 00130: val_loss did not improve from 0.96989\n","Epoch 131/500\n"," - 15s - loss: 0.7504 - accuracy: 0.4979 - val_loss: 1.0096 - val_accuracy: 0.3956\n","\n","Epoch 00131: val_loss did not improve from 0.96989\n","Epoch 132/500\n"," - 15s - loss: 0.7478 - accuracy: 0.5002 - val_loss: 1.0860 - val_accuracy: 0.4177\n","\n","Epoch 00132: val_loss did not improve from 0.96989\n","Epoch 133/500\n"," - 15s - loss: 0.7474 - accuracy: 0.5013 - val_loss: 1.0923 - val_accuracy: 0.3615\n","\n","Epoch 00133: val_loss did not improve from 0.96989\n","Epoch 134/500\n"," - 15s - loss: 0.7439 - accuracy: 0.5022 - val_loss: 1.3571 - val_accuracy: 0.3898\n","\n","Epoch 00134: val_loss did not improve from 0.96989\n","Epoch 135/500\n"," - 15s - loss: 0.7422 - accuracy: 0.5035 - val_loss: 1.0986 - val_accuracy: 0.4026\n","\n","Epoch 00135: val_loss did not improve from 0.96989\n","Epoch 136/500\n"," - 15s - loss: 0.7396 - accuracy: 0.5055 - val_loss: 1.2138 - val_accuracy: 0.3993\n","\n","Epoch 00136: val_loss did not improve from 0.96989\n","Epoch 137/500\n"," - 15s - loss: 0.7378 - accuracy: 0.5085 - val_loss: 1.2210 - val_accuracy: 0.4139\n","\n","Epoch 00137: val_loss did not improve from 0.96989\n","Epoch 138/500\n"," - 15s - loss: 0.7370 - accuracy: 0.5087 - val_loss: 1.0296 - val_accuracy: 0.4159\n","\n","Epoch 00138: val_loss did not improve from 0.96989\n","Epoch 139/500\n"," - 15s - loss: 0.7353 - accuracy: 0.5098 - val_loss: 1.2949 - val_accuracy: 0.4042\n","\n","Epoch 00139: val_loss did not improve from 0.96989\n","Epoch 140/500\n"," - 15s - loss: 0.7326 - accuracy: 0.5113 - val_loss: 1.2802 - val_accuracy: 0.4244\n","\n","Epoch 00140: val_loss did not improve from 0.96989\n","Epoch 141/500\n"," - 15s - loss: 0.7318 - accuracy: 0.5119 - val_loss: 1.0620 - val_accuracy: 0.3854\n","\n","Epoch 00141: val_loss did not improve from 0.96989\n","Epoch 142/500\n"," - 15s - loss: 0.7283 - accuracy: 0.5135 - val_loss: 1.2132 - val_accuracy: 0.3882\n","\n","Epoch 00142: val_loss did not improve from 0.96989\n","Epoch 143/500\n"," - 15s - loss: 0.7265 - accuracy: 0.5154 - val_loss: 1.4593 - val_accuracy: 0.3526\n","\n","Epoch 00143: val_loss did not improve from 0.96989\n","Epoch 144/500\n"," - 15s - loss: 0.7264 - accuracy: 0.5151 - val_loss: 1.3926 - val_accuracy: 0.3959\n","\n","Epoch 00144: val_loss did not improve from 0.96989\n","Epoch 145/500\n"," - 15s - loss: 0.7233 - accuracy: 0.5170 - val_loss: 1.0401 - val_accuracy: 0.4096\n","\n","Epoch 00145: val_loss did not improve from 0.96989\n","Epoch 146/500\n"," - 15s - loss: 0.7226 - accuracy: 0.5177 - val_loss: 1.3721 - val_accuracy: 0.3740\n","\n","Epoch 00146: val_loss did not improve from 0.96989\n","Epoch 147/500\n"," - 15s - loss: 0.7185 - accuracy: 0.5216 - val_loss: 1.3921 - val_accuracy: 0.3648\n","\n","Epoch 00147: val_loss did not improve from 0.96989\n","Epoch 148/500\n"," - 15s - loss: 0.7169 - accuracy: 0.5222 - val_loss: 1.1805 - val_accuracy: 0.4021\n","\n","Epoch 00148: val_loss did not improve from 0.96989\n","Epoch 149/500\n"," - 15s - loss: 0.7152 - accuracy: 0.5245 - val_loss: 1.2732 - val_accuracy: 0.3744\n","\n","Epoch 00149: val_loss did not improve from 0.96989\n","Epoch 150/500\n"," - 15s - loss: 0.7122 - accuracy: 0.5243 - val_loss: 1.4385 - val_accuracy: 0.4150\n","\n","Epoch 00150: val_loss did not improve from 0.96989\n","Epoch 151/500\n"," - 15s - loss: 0.7117 - accuracy: 0.5263 - val_loss: 1.4474 - val_accuracy: 0.3885\n","\n","Epoch 00151: val_loss did not improve from 0.96989\n","Epoch 152/500\n"," - 15s - loss: 0.7103 - accuracy: 0.5279 - val_loss: 1.2530 - val_accuracy: 0.3839\n","\n","Epoch 00152: val_loss did not improve from 0.96989\n","Epoch 153/500\n"," - 15s - loss: 0.7084 - accuracy: 0.5289 - val_loss: 1.1207 - val_accuracy: 0.4377\n","\n","Epoch 00153: val_loss did not improve from 0.96989\n","Epoch 154/500\n"," - 15s - loss: 0.7062 - accuracy: 0.5290 - val_loss: 1.1017 - val_accuracy: 0.4186\n","\n","Epoch 00154: val_loss did not improve from 0.96989\n","Epoch 155/500\n"," - 15s - loss: 0.7042 - accuracy: 0.5323 - val_loss: 1.3259 - val_accuracy: 0.4122\n","\n","Epoch 00155: val_loss did not improve from 0.96989\n","Epoch 156/500\n"," - 15s - loss: 0.7022 - accuracy: 0.5318 - val_loss: 1.3051 - val_accuracy: 0.4142\n","\n","Epoch 00156: val_loss did not improve from 0.96989\n","Epoch 157/500\n"," - 15s - loss: 0.7017 - accuracy: 0.5334 - val_loss: 1.0765 - val_accuracy: 0.4016\n","\n","Epoch 00157: val_loss did not improve from 0.96989\n","Epoch 158/500\n"," - 15s - loss: 0.7002 - accuracy: 0.5330 - val_loss: 1.0834 - val_accuracy: 0.3968\n","\n","Epoch 00158: val_loss did not improve from 0.96989\n","Epoch 159/500\n"," - 15s - loss: 0.6970 - accuracy: 0.5358 - val_loss: 1.1621 - val_accuracy: 0.3919\n","\n","Epoch 00159: val_loss did not improve from 0.96989\n","Epoch 160/500\n"," - 15s - loss: 0.6952 - accuracy: 0.5384 - val_loss: 1.1571 - val_accuracy: 0.4323\n","\n","Epoch 00160: val_loss did not improve from 0.96989\n","Epoch 161/500\n"," - 15s - loss: 0.6932 - accuracy: 0.5375 - val_loss: 1.2371 - val_accuracy: 0.4008\n","\n","Epoch 00161: val_loss did not improve from 0.96989\n","Epoch 162/500\n"," - 15s - loss: 0.6928 - accuracy: 0.5402 - val_loss: 1.3842 - val_accuracy: 0.3986\n","\n","Epoch 00162: val_loss did not improve from 0.96989\n","Epoch 163/500\n"," - 15s - loss: 0.6898 - accuracy: 0.5413 - val_loss: 1.2788 - val_accuracy: 0.4206\n","\n","Epoch 00163: val_loss did not improve from 0.96989\n","Epoch 164/500\n"," - 15s - loss: 0.6878 - accuracy: 0.5431 - val_loss: 1.2970 - val_accuracy: 0.4075\n","\n","Epoch 00164: val_loss did not improve from 0.96989\n","Epoch 165/500\n"," - 15s - loss: 0.6872 - accuracy: 0.5421 - val_loss: 1.4168 - val_accuracy: 0.4261\n","\n","Epoch 00165: val_loss did not improve from 0.96989\n","Epoch 166/500\n"," - 15s - loss: 0.6854 - accuracy: 0.5439 - val_loss: 1.3722 - val_accuracy: 0.4009\n","\n","Epoch 00166: val_loss did not improve from 0.96989\n","Epoch 167/500\n"," - 15s - loss: 0.6846 - accuracy: 0.5446 - val_loss: 1.2417 - val_accuracy: 0.3782\n","\n","Epoch 00167: val_loss did not improve from 0.96989\n","Epoch 168/500\n"," - 15s - loss: 0.6813 - accuracy: 0.5466 - val_loss: 1.2714 - val_accuracy: 0.4367\n","\n","Epoch 00168: val_loss did not improve from 0.96989\n","Epoch 169/500\n"," - 15s - loss: 0.6797 - accuracy: 0.5478 - val_loss: 1.1288 - val_accuracy: 0.3963\n","\n","Epoch 00169: val_loss did not improve from 0.96989\n","Epoch 170/500\n"],"name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-ce6f9a0c32a9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    178\u001b[0m                     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_flow\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m                     \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m                     shuffle=False)\n\u001b[0m\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1730\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1731\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1732\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1733\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    218\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m                                             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 220\u001b[0;31m                                             reset_metrics=False)\n\u001b[0m\u001b[1;32m    221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight, reset_metrics)\u001b[0m\n\u001b[1;32m   1512\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1514\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1515\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1516\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tensorflow-1.15.2/python3.6/tensorflow_core/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3474\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3475\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 3476\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   3477\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3478\u001b[0m     output_structure = nest.pack_sequence_as(\n","\u001b[0;32m/tensorflow-1.15.2/python3.6/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1470\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1471\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1472\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1473\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1474\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","metadata":{"id":"YjZSsbf6brL_","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}