{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python [conda env:tensorflow2_p36]","language":"python","name":"conda-env-tensorflow2_p36-py"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.5"},"colab":{"name":"close_updown_rnn_sxp_tvnon_shuffle.ipynb","provenance":[{"file_id":"1z4z_KLPzc6RWsxo3X_dHbpByxUAgjoMJ","timestamp":1583754134002}],"collapsed_sections":["6XibtKgphXyQ"],"toc_visible":true,"machine_shape":"hm"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AK9FjWwLOyay","executionInfo":{"status":"ok","timestamp":1621942415124,"user_tz":-540,"elapsed":30067,"user":{"displayName":"7th June","photoUrl":"","userId":"08178289703395036410"}},"outputId":"c03bd4b7-2955-45c5-cd1e-8318c9b3422a"},"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","import os, sys\n","\n","current_path = '/content/drive/My Drive/Colab Notebooks/300/'\n","\n","os.chdir(current_path)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"8uqYv5StTazo"},"source":["### **Requirements**"]},{"cell_type":"code","metadata":{"id":"9qGt60DKTZmf"},"source":["# !pip install statsmodels==0.12.2\n","\n","# import statsmodels\n","# statsmodels.__version__"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"y7bVjhlwPI_-"},"source":["### **ARIMA**"]},{"cell_type":"code","metadata":{"id":"NvdpArctN_6l","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1621942420711,"user_tz":-540,"elapsed":5593,"user":{"displayName":"7th June","photoUrl":"","userId":"08178289703395036410"}},"outputId":"77ddcef9-ef75-49c2-a900-802451f3ada3"},"source":["from statsmodels.tsa.arima_model import ARIMA\n","# from statsmodels.tsa.arima.model import ARIMA\n","\n","from datetime import datetime\n","\n","\n","def arima_test(target, use_rows=None):\n","\n","  size = int(len(target) * 0.66)\n","  train, test = target[:size].values, target[size:]\n","  test_shift = test.shift(1).values\n","  test = test.values\n","  # break\n","\n","  history = list(train)\n","  predictions = list()\n","  err_ranges = list()\n","  for t in range(len(test)):\n","    \n","      if use_rows is not None:\n","        history = history[-use_rows:]\n","        \n","      model = ARIMA(history, order=(0, 2, 4))\n","      model_fit = model.fit()\n","      output = model_fit.forecast()\n","      # print(output)\n","      # break\n","\n","      predictions.append(output[0])\n","      err_ranges.append(output[1])\n","      obs = test[t]\n","      # print('obs :', obs)\n","      history.append(obs)\n","      # break\n","      print('\\r %.2f%%' % (t / len(test) * 100), end='')\n","\n","  print(len(test), len(predictions))\n","\n","  return predictions, err_ranges\n","\n","\n","# print(high)\n","\n","\n","def get_back_result(ohlcv, predictions, err_ranges, tp=0.04, sl=None, leverage=1, show_detail=False, show_plot=False, return_pr=False, cumsum=False, \n","                    close_ver=False, reverse_short=False):\n","\n","  \n","  high, low, test = np.split(ohlcv.values[-len(predictions):, [1, 2, 3]], 3, axis=1)\n","\n","  if close_ver:\n","    predictions = ohlcv['close'].shift(1).values[-len(test):]\n","\n","  fee = 0.0006\n","  long_profits = []\n","  short_profits = []\n","  liquidations = []\n","  win_cnt = 0\n","  for i in range(len(test)):\n","\n","    long_ep = predictions[i]\n","    if sl is not None:\n","      long_sl = long_ep * (1 / (sl + 1))\n","\n","    # assert long_ep < long_exit, 'long_exit < long_ep !, %s, %s' % (long_exit, long_ep)\n","    \n","    short_ep = (predictions[i] + err_ranges[i]) * (1 + tp)\n","    # short_ep = (predictions[i] + err_ranges[i]) * (1 / (1 - tp))\n","    if sl is not None:\n","      short_sl = short_ep * (1 / (1 - sl))\n","\n","    # print((low[i]))\n","\n","    #    long 우선   # <-- long & short 둘다 체결된 상황에서는 long 체결을 우선으로 한다.\n","    if low[i] < long_ep:\n","      \n","      liquidation = low[i] / long_ep - fee\n","      l_liquidation = 1 + (liquidation - 1) * leverage\n","      liquidations.append(l_liquidation)\n","\n","      if max(l_liquidation, 0) == 0:\n","        l_profit = 0\n","        # print('low[i], long_ep, l_liquidation :', low[i], long_ep, l_liquidation)\n","      else:\n","\n","        if sl is not None:\n","          if low[i] < long_sl:\n","            profit = long_sl / long_ep - fee\n","          else:\n","            profit = test[i] / long_ep - fee\n","\n","        else:\n","          profit = test[i] / long_ep - fee\n","\n","        l_profit = 1 + (profit - 1) * leverage\n","        l_profit = max(l_profit, 0)\n","        \n","        if profit >= 1:\n","          win_cnt += 1\n","\n","      long_profits.append(l_profit)\n","      short_profits.append(1.0)\n","\n","      if show_detail:\n","        print(test[i], predictions[i], long_ep)\n","\n","    # if high[i] > short_ep > low[i]: # 지정 대기가 아니라, 해당 price 가 지나면, long 한다.\n","\n","    #   if not reverse_short:\n","    #     liquidation = short_ep / high[i]  - fee\n","    #   else:\n","    #     liquidation = low[i] / short_ep  - fee\n","    #   l_liquidation = 1 + (liquidation - 1) * leverage\n","\n","    #   if max(l_liquidation, 0) == 0:\n","    #     l_profit = 0\n","    #   else:\n","\n","    #     if sl is not None:\n","    #       if high[i] > short_sl:\n","\n","    #         if not reverse_short:\n","    #           profit = short_ep / short_sl - fee\n","    #         else:\n","    #           profit = short_sl / short_ep - fee\n","\n","    #       else:\n","    #         if not reverse_short:\n","    #           profit = short_ep / test[i] - fee\n","    #         else:\n","    #           profit = test[i] / short_ep - fee\n","\n","    #     else:\n","\n","    #       if not reverse_short:\n","    #         profit = short_ep / test[i] - fee\n","    #       else:\n","    #         profit = test[i] / short_ep - fee\n","\n","    #     l_profit = 1 + (profit - 1) * leverage\n","    #     l_profit = max(l_profit, 0)\n","\n","    #     if profit >= 1:\n","    #       win_cnt += 1\n","\n","    #   short_profits.append(l_profit)\n","    #   long_profits.append(1.0)\n","\n","    #   if show_detail:\n","    #     print(test[i], predictions[i], short_ep)\n","    \n","    else:\n","      long_profits.append(1.0)\n","      short_profits.append(1.0)\n","      liquidations.append(1.0)\n","\n","\n","  long_win_ratio = sum(np.array(long_profits) > 1.0) / sum(np.array(long_profits) != 1.0)\n","  short_win_ratio = sum(np.array(short_profits) > 1.0) / sum(np.array(short_profits) != 1.0)\n","  long_frequency = sum(np.array(long_profits) != 1.0) / len(test)\n","  short_frequency = sum(np.array(short_profits) != 1.0) / len(test)\n","  if not cumsum:\n","    long_accum_profit = np.array(long_profits).cumprod()\n","    short_accum_profit = np.array(short_profits).cumprod()\n","  else:\n","    long_accum_profit = (np.array(long_profits) - 1.0).cumsum()\n","    short_accum_profit = (np.array(short_profits) - 1.0).cumsum()\n","\n","  # print(win_ratio)\n","\n","  if show_plot:\n","\n","    plt.figure(figsize=(10, 5))\n","    plt.suptitle('tp=%.4f, lvrg=%d' % (tp, leverage))\n","\n","    plt.subplot(151)\n","    plt.plot(liquidations)\n","    plt.title('liquidations')\n","\n","    plt.subplot(152)\n","    plt.plot(long_profits)\n","    plt.title('Win Ratio : %.2f %%\\nrequency : %.2f %%' % (long_win_ratio * 100, long_frequency * 100), color='black')\n","    # plt.show()\n","\n","    # print()\n","    plt.subplot(153)\n","    plt.plot(long_accum_profit)\n","    plt.title('Accum_profit : %.2f' % long_accum_profit[-1], color='black')\n","\n","    plt.subplot(154)\n","    plt.plot(short_profits)\n","    plt.title('Win Ratio : %.2f %%\\nrequency : %.2f %%' % (short_win_ratio * 100, short_frequency * 100), color='black')\n","    # plt.show()\n","\n","    # print()\n","    plt.subplot(155)\n","    plt.plot(short_accum_profit)\n","    plt.title('Accum_profit : %.2f' % short_accum_profit[-1], color='black')\n","    plt.show()\n","\n","  return [long_win_ratio, short_win_ratio], [long_frequency, short_frequency], [long_accum_profit[-1], short_accum_profit[-1]], [long_profits, short_profits]\n","\n","\n","# get_back_result(tp=0.04, leverage=1, show_plot=True)\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n","  import pandas.util.testing as tm\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"aDkU3tMiM2lO","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1621942422118,"user_tz":-540,"elapsed":1410,"user":{"displayName":"7th June","photoUrl":"","userId":"08178289703395036410"}},"outputId":"777c805d-3989-430b-8690-57a4bb9e689a"},"source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","interval = '30m'\n","date_path = './candlestick_concated/%s/2021-04-27/' % interval\n","file_list = os.listdir(date_path)\n","\n","print((file_list))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["['2021-04-27 BTCUSDT.xlsx', '2021-04-27 ETHUSDT.xlsx', '2021-04-27 BCHUSDT.xlsx', '2021-04-27 XRPUSDT.xlsx', '2021-04-27 EOSUSDT.xlsx', '2021-04-27 LTCUSDT.xlsx', '2021-04-27 ETCUSDT.xlsx', '2021-04-27 LINKUSDT.xlsx', '2021-04-27 XLMUSDT.xlsx', '2021-04-27 ADAUSDT.xlsx', '2021-04-27 XMRUSDT.xlsx', '2021-04-27 SXPUSDT.xlsx', '2021-04-27 KAVAUSDT.xlsx', '2021-04-27 BANDUSDT.xlsx', '2021-04-27 DASHUSDT.xlsx', '2021-04-27 ZECUSDT.xlsx', '2021-04-27 XTZUSDT.xlsx', '2021-04-27 BNBUSDT.xlsx', '2021-04-27 ATOMUSDT.xlsx', '2021-04-27 ONTUSDT.xlsx', '2021-04-27 IOTAUSDT.xlsx', '2021-04-27 BATUSDT.xlsx', '2021-04-27 NEOUSDT.xlsx', '2021-04-27 QTUMUSDT.xlsx', '2021-04-27 WAVESUSDT.xlsx', '2021-04-27 MKRUSDT.xlsx', '2021-04-27 SNXUSDT.xlsx', '2021-04-27 DOTUSDT.xlsx', '2021-04-27 THETAUSDT.xlsx', '2021-04-27 ALGOUSDT.xlsx', '2021-04-27 KNCUSDT.xlsx', '2021-04-27 ZRXUSDT.xlsx', '2021-04-27 COMPUSDT.xlsx', '2021-04-27 OMGUSDT.xlsx']\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"GmmgsEUMqUjN"},"source":["### **Model**"]},{"cell_type":"code","metadata":{"id":"mcDUjgQzqUSr","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1621942426401,"user_tz":-540,"elapsed":4287,"user":{"displayName":"7th June","photoUrl":"","userId":"08178289703395036410"}},"outputId":"214548a1-4338-4ade-ec0d-fb6a0ca38b7a"},"source":["import os\n","# os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   \n","# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","\n","%tensorflow_version 1.x\n","\n","import keras\n","import tensorflow as tf\n","\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import scipy.misc \n","from math import sqrt \n","import itertools\n","from IPython.display import display\n","\n","%matplotlib inline\n","\n","from keras.utils import plot_model\n","import keras.backend as K\n","from keras.models import Model, Sequential\n","import keras.layers as layers\n","from keras.optimizers import Adam, SGD\n","from keras.regularizers import l1, l2\n","\n","from sklearn.cluster import KMeans\n","from sklearn.metrics import silhouette_score\n","from sklearn.metrics import precision_score, recall_score, precision_recall_curve\n","from sklearn.metrics import confusion_matrix\n","from sklearn.metrics import plot_confusion_matrix\n","\n","\n","gdrive_path = current_path\n","\n","num_classes = 2\n","\n","def FER_Model(input_shape):\n","    # first input model\n","    visible = layers.Input(shape=input_shape, name='input')\n","    \n","    # net = layers.LSTM(32, return_sequences=False)(visible)\n","    net = layers.LSTM(10, return_sequences=False)(visible)\n","\n","    # net = layers.Dense(32)(visible)\n","    # net = layers.Conv2D(32, kernel_size=3, padding='same', kernel_initializer='he_normal')(visible)\n","    # net = layers.Conv2D(256, kernel_size=3, padding='same', kernel_initializer='he_normal')(visible)\n","    # net = layers.BatchNormalization()(net)\n","    net = layers.LeakyReLU()(net)\n","\n","    net = layers.Dense(64)(net)\n","    # net = layers.Conv2D(64, kernel_size=3, padding='same', kernel_initializer='he_normal')(net)\n","    # net = layers.Conv2D(128, kernel_size=3, padding='same', kernel_initializer='he_normal')(net)\n","    # net = layers.BatchNormalization()(net)\n","    # net = layers.Activation('relu')(net)\n","    net = layers.LeakyReLU()(net)\n","    net = layers.BatchNormalization()(net)\n","\n","    # net = layers.MaxPool2D(pool_size=2)(net)\n","    # net = layers.AveragePooling2D(padding='same')(net)\n","\n","    shortcut_1 = net\n","\n","    net = layers.Dense(128)(net)\n","    # net = layers.Conv2D(64, kernel_size=3, padding='same', kernel_initializer='he_normal')(net)\n","    # net = layers.Conv2D(128, kernel_size=3, padding='same', kernel_initializer='he_normal')(net)\n","    net = layers.LeakyReLU()(net)\n","\n","    net = layers.Dense(256)(net)\n","    # net = layers.Conv2D(256, kernel_size=3, padding='same', kernel_initializer='he_normal')(net)\n","    # net = layers.Activation('relu')(net)\n","    net = layers.LeakyReLU()(net)\n","    net = layers.BatchNormalization()(net)\n","\n","\n","    # net = layers.MaxPool2D(pool_size=2)(net)\n","\n","    shortcut_2 = net\n","\n","#     net = layers.Conv2D(256, kernel_size=3, padding='same')(net)\n","#     # net = layers.Activation('relu')(net)\n","#     net = layers.LeakyReLU()(net)\n","#     net = layers.MaxPool2D(pool_size=2)(net)\n","\n","#     shortcut_3 = net\n","\n","#     net = layers.Conv2D(128, kernel_size=1, padding='same')(net)\n","#     # net = layers.Activation('relu')(net)\n","#     net = layers.LeakyReLU()(net)\n","#     net = layers.MaxPool2D(pool_size=2)(net)\n","\n","    # net = layers.Flatten()(net)\n","    net = layers.Dense(128)(net)\n","    net = layers.LeakyReLU()(net)\n","\n","    net = layers.Dense(64)(net)\n","    net = layers.LeakyReLU()(net)\n","\n","    net = layers.Dropout(0.3)(net)\n","\n","    net = layers.Dense(num_classes, activation='softmax')(net)\n","\n","    # create model \n","    model = Model(inputs=visible, outputs=net)\n","    # summary layers\n","    print(model.summary())\n","    \n","    return model"],"execution_count":null,"outputs":[{"output_type":"stream","text":["TensorFlow 1.x selected.\n"],"name":"stdout"},{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"OgZyYJPg3RJa"},"source":["def resize_npy(x):\n","\n","  temp_x = []\n","\n","  for d_i, data in enumerate(x):\n","    # resized_data = cv2.resize(data, (row * 2, col * 2)) --> input image 홰손된다\n","    # resized_data = data.repeat(2, axis=0).repeat(2, axis=1)\n","    data = data.repeat(2, axis=0).repeat(2, axis=1)\n","    # resized_data = data.repeat(1, axis=0).repeat(1, axis=1)\n","    # cmapped = plt.cm.Set1(resized_data)[:, :, :3]  # Drop Alpha Channel\n","    \n","    if d_i == 0:\n","      plt.imshow(data)\n","      plt.show()\n","      # plt.imshow(resized_data)\n","      # plt.show()\n","    # print('resized_data.shape :', resized_data.shape)\n","    # break\n","    temp_x.append(data)\n","\n","  return temp_x"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZZtZP5tqenWh"},"source":["def min_max_scale(npy_x):\n","\n","  return (npy_x - np.min(npy_x)) / (np.max(npy_x) - np.min(npy_x))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"D5F8709GI5Tc"},"source":["### save npy"]},{"cell_type":"code","metadata":{"id":"SvZuk1rPrUMe"},"source":["from sklearn.preprocessing import MinMaxScaler, MaxAbsScaler\n","import pickle\n","from sklearn.model_selection import train_test_split\n","from datetime import datetime\n","\n","from funcs_indicator import *\n","\n","pd.set_option('display.max_rows', 500)\n","pd.set_option('display.max_columns', 500)\n","pd.set_option('display.width', 1000)\n","\n","\n","start_stamp = 0\n","# start_stamp = datetime.timestamp(pd.to_datetime('2021-02-12'))\n","print(\"start_stamp :\", start_stamp)\n","# break\n","\n","np.random.shuffle(file_list)\n","candis = file_list\n","\n","long_index = 0\n","leverage = 5\n","prev_x = None\n","total_x = None\n","\n","seed = 1\n","random_state = 201\n","np.random.seed(seed)\n","\n","for i in range(len(candis)):\n","\n","  keys = [candis[i]]\n","  \n","  # if 'algo'.upper() not in candis[i]:\n","  #   continue\n","\n","\n","  # if '02-11' not in candis[i]:  # <-- 04-08 includes all timestamp range\n","  #   continue  \n","\n","  # if 'eth'.upper() not in candis[i]:\n","  #   continue\n","\n","  # if 'neo'.upper() not in candis[i]:\n","  #   continue\n","\n","  # plt.figure(figsize=(35, 10))\n","  # plt.suptitle('%s %s' % (interval, keys))\n","\n","\n","  #         get tp parameter        #\n","\n","  # plt.subplot(1,10,3)\n","  # for key in keys:  \n","  #   # plt.plot(profit_result_dict[key]['tp_list'], profit_result_dict[key]['ap_list'])\n","  #   argmax = np.argmax(profit_result_dict[key]['ap_list'][:, [long_index]])\n","  #   peak_tp = profit_result_dict[key]['tp_list'][argmax]\n","  #   # plt.axvline(peak_tp, linestyle='--')\n","  #   # plt.title('acc profit, max at %.4f' % (peak_tp))  \n","\n","  # plt.subplot(1,10,4)\n","  # plt.title('max acc profit by leverage')  \n","  # for key in keys:  \n","  #   # plt.plot(profit_result_dict[key]['tp_list'], profit_result_dict[key]['max_ap_list'], label=key)\n","  #   argmax = np.argmax(profit_result_dict[key]['max_ap_list'][:, [long_index]])\n","  #   max_peak_tp = profit_result_dict[key]['tp_list'][argmax]\n","  #   # plt.axvline(max_peak_tp, linestyle='--')\n","  #   # plt.title('max acc profit, max at %.4f' % (max_peak_tp))  \n","\n","\n","  for key in keys:  \n","\n","    # print(profit_result_dict[key]['leverage_ap_list'])\n","\n","    # for tp in [max_peak_tp]:\n","\n","      # if tp == peak_tp:\n","      #   plt.subplot(1,10,5)\n","      # else:\n","      #   plt.subplot(1,10,6)\n","\n","      #     leverage analysis     #\n","      # ohlcv = load_dict[key]['ohlcv']\n","\n","    if 'sxp'.upper() not in key:\n","      continue\n","    \n","    ohlcv = pd.read_excel(date_path + key, index_col=0)\n","    print('len(ohlcv) :', len(ohlcv))\n","\n","\n","    #       select timestamp range      #\n","    # time_index = ohlcv.index\n","    # total_stamp = list(map(lambda x: datetime.timestamp(x), time_index)) \n","\n","    # rm_index_amt = np.sum(np.array(total_stamp) < start_stamp)\n","\n","    # ohlcv = ohlcv.iloc[rm_index_amt:]\n","    # print(ohlcv.head())\n","\n","    # ohlcv = ohlcv.iloc[:-2603]  # exclude back_range\n","    # ohlcv = ohlcv.iloc[:-int(len(ohlcv) * 0.3)]  # exclude back_range\n","    # predictions = load_dict[key]['predictions']\n","    # err_ranges = load_dict[key]['err_ranges']\n","    print(\"ohlcv.index[0] :\", ohlcv.index[0])\n","    print(\"ohlcv.index[-1] :\", ohlcv.index[-1])\n","\n","    predictions = ohlcv['close'].shift(1).values\n","    err_ranges = np.zeros_like(predictions)\n","\n","    # leverage_list = profit_result_dict[key]['leverage_list']\n","    # temp_ap_list = list()\n","    # temp_pr_list = list()\n","\n","    try:\n","      print('-------------- %s --------------' % key)\n","      result = get_back_result(ohlcv, predictions, err_ranges, tp=0, leverage=leverage, show_plot=True, reverse_short=False, show_detail=False)\n","      # temp_ap_list.append(result[2])\n","      # temp_pr_list.append(result[3])\n","\n","      # if round(leverage) == 1:\n","      #   temp_pr_list = result[3]\n","      pr_list = result[3][long_index]\n","\n","    except Exception as e:\n","      print(e)\n","      break    \n","\n","\n","    \n","  # break\n","    #         clustering zone           #\n","\n","    #       set data features : ohlc, v, ep\n","    time_index = ohlcv.index[-len(predictions):]\n","\n","    sliced_ohlcv = ohlcv[-len(predictions):]\n","\n","    #       scale with price    #\n","    ohlc = ohlcv.iloc[-len(predictions):, :4]      \n","    long_ep = np.array(predictions)\n","    long_ep = long_ep.reshape(-1, 1)\n","    ha_ohlc = heikinashi(sliced_ohlcv).iloc[:, :4]\n","    sar = lucid_sar(sliced_ohlcv)\n","    ema1, ema2, ema3 = ema_ribbon(sliced_ohlcv)\n","    senkou1, senkou2 = ichimoku(sliced_ohlcv)\n","\n","    #     min max scale   #\n","    vol = sliced_ohlcv.iloc[-len(predictions):, [4]]\n","\n","    #     scale with baseline   #\n","    #     방법 1. 전체 기간 min_max scaling   #\n","    cbo, ema_cbo = cct_bbo(sliced_ohlcv, 21, 13) \n","\n","    _, _, bbw = bb_width(sliced_ohlcv, 20, 2) \n","    \n","    fish = fisher(sliced_ohlcv, 60)\n","    trix = trix_hist(sliced_ohlcv, 14, 1, 5) \n","    rsi_ = rsi(sliced_ohlcv)\n","    macd_hist = macd(sliced_ohlcv)\n","\n","\n","\n","\n","    # ohlcv['u_wick'] = ohlcv['high'] / np.maximum(ohlcv['close'] , ohlcv['open'])\n","    # ohlcv['d_wick'] = np.minimum(ohlcv['close'] , ohlcv['open']) / ohlcv['low']\n","    # ohlcv['body'] = ohlcv['close'] / ohlcv['open']\n","    # candle = ohlcv.iloc[-len(predictions):, -3:]\n","\n","\n","    print('len(ohlc) :', len(ohlc))\n","    print('long_ep.shape :', long_ep.shape)\n","    print('len(ha_ohlc) :', len(ha_ohlc))\n","    print('len(sar) :', len(sar))\n","    print('len(ema1) :', len(ema1))\n","    print('len(senkou1) :', len(senkou1))\n","    print('len(cbo) :', len(cbo))\n","    print('len(ema_cbo) :', len(ema_cbo))\n","    print('len(bbw) :', len(bbw))\n","    print('len(fish) :', len(fish))\n","    print('len(trix) :', len(trix))\n","    print('len(rsi_) :', len(rsi_))\n","    print('len(macd_hist) :', len(macd_hist))\n","\n","\n","    # break\n","\n","\n","    #       set params    #\n","    period = 45\n","    key_i = i\n","\n","    # plt.plot(cbo)\n","    # plt.plot(bbw)\n","    plt.plot(fish, color='b')\n","    # plt.plot(trix)\n","    # plt.plot(rsi_)\n","    # plt.plot(macd_hist)\n","    plt.show()\n","\n","    #      global scaling   #\n","    min_max = MinMaxScaler()\n","    cbo = min_max.fit_transform(cbo.values.reshape(-1, 1))\n","    ema_cbo = min_max.fit_transform(ema_cbo.values.reshape(-1, 1))\n","    bbw = min_max.fit_transform(bbw.values.reshape(-1, 1))\n","    fish = min_max.fit_transform(fish.values.reshape(-1, 1))\n","    trix = min_max.fit_transform(trix.values.reshape(-1, 1))\n","    rsi_ = min_max.fit_transform(rsi_.values.reshape(-1, 1))\n","    macd_hist = min_max.fit_transform(macd_hist.values.reshape(-1, 1))\n","\n","    # plt.plot(cbo)\n","    # plt.plot(bbw)\n","    plt.plot(fish, color='r')\n","    # plt.plot(trix)\n","    # plt.plot(rsi_)\n","    # plt.plot(macd_hist)\n","    plt.show()\n","    # break\n","\n","    plotting = True\n","\n","    for trial_number in range(1):\n","\n","      data_x, data_pr, data_updown = [], [], []\n","      data_index = []\n","\n","      for i in range(period, len(predictions)):\n","\n","        #   pr_list != 1 인 데이터만 사용한다\n","        # if 1:\n","        if pr_list[i] != 1:\n","\n","          min_max = MinMaxScaler()\n","          \n","          #   prediction 을 제외한 이전 데이터를 사용해야한다\n","          temp_ohlc = ohlc.iloc[i - period : i].values\n","          temp_long_ep = long_ep[i - period : i]          \n","          temp_ha_ohlc = ha_ohlc.iloc[i - period : i].values\n","          temp_sar = sar.iloc[i - period : i].values.reshape(-1, 1)\n","          temp_ema1 = ema1.iloc[i - period : i].values.reshape(-1, 1)\n","          temp_ema2 = ema2.iloc[i - period : i].values.reshape(-1, 1)\n","          temp_ema3 = ema3.iloc[i - period : i].values.reshape(-1, 1)\n","          temp_senkou1 = senkou1.iloc[i - period : i].values.reshape(-1, 1)\n","          temp_senkou2 = senkou2.iloc[i - period : i].values.reshape(-1, 1)      \n","\n","          temp_close = min_max_scale(temp_ohlc[:, [3]])    \n","\n","          price_data = np.hstack((temp_ohlc, temp_long_ep, temp_ha_ohlc, temp_sar, temp_ema1, temp_ema2, temp_ema3, temp_senkou1, temp_senkou2))\n","\n","          if np.isnan(np.sum(price_data)):\n","            continue\n","\n","          # print(\"price_data[:10] :\", price_data[:10])\n","          # print(\"temp_ohlc.shape :\", temp_ohlc.shape)\n","          # print(\"temp_long_ep.shape :\", temp_long_ep.shape)\n","          # print(\"temp_ha_ohlc.shape :\", temp_ha_ohlc.shape)\n","          # print(\"price_data.shape :\", price_data.shape)\n","\n","          if plotting:\n","            plt.plot(price_data)\n","            plt.show()\n","\n","          ind_temp_ohlc = min_max_scale(temp_ohlc)\n","          temp_price_data = min_max_scale(price_data)\n","          # temp_price_data = (price_data - np.min(price_data)) / (np.max(price_data) - np.min(price_data))\n","\n","          temp_ohlc = temp_price_data[:, :4]\n","          temp_long_ep = temp_price_data[:, [4]]\n","          temp_ha_ohlc = temp_price_data[:, 5:9]\n","          temp_sar, temp_ema1, temp_ema2, temp_ema3, temp_senkou1, temp_senkou2 = np.split(temp_price_data[:, 9:], 6, axis=1)\n","\n","          if plotting:\n","\n","            plt.plot(ind_temp_ohlc)\n","            plt.show()\n","            plt.plot(temp_price_data)\n","            plt.show()\n","\n","            plotting = False\n","          # break\n","\n","          #   vol -> min_max\n","          temp_vol = min_max.fit_transform(vol.iloc[i - period : i].values.reshape(-1, 1))\n","          \n","          # temp_candle = candle.iloc[i - period : i].values\n","\n","          temp_cbo = cbo[i - period : i]\n","          temp_ema_cbo = ema_cbo[i - period : i]\n","          temp_bbw = bbw[i - period : i]\n","          temp_fish = fish[i - period : i]\n","          temp_trix = trix[i - period : i]\n","          temp_rsi_ = rsi_[i - period : i]\n","          temp_macd_hist = macd_hist[i - period : i]\n","\n","          # print(temp_ohlc.shape)\n","          # print(temp_long_ep.shape)\n","          # print(temp_vol.shape)\n","          # print(temp_candle.shape)\n","          # break\n","\n","          trial_list = [temp_ha_ohlc, temp_sar, temp_ema1, temp_ema2, temp_ema3, temp_senkou1, temp_senkou2,\n","                        temp_cbo, temp_ema_cbo, temp_bbw, temp_fish, temp_trix, temp_rsi_, temp_macd_hist]\n","\n","          # trial_list = [temp_ohlc[:, [0], temp_ohlc[:, [1], temp_ohlc[:, [2], temp_ohlc[:, [3]], temp_vol, temp_ema_cbo]\n","\n","          #                   feature selection                   #  \n","          # temp_data = np.hstack((temp_ohlc, temp_long_ep, temp_vol, temp_candle))\n","          # temp_data = trial_list[trial_number]\n","          # temp_data = np.hstack((temp_ohlc[:, [3]], temp_ohlc[:, [1]], temp_bbw))\n","          temp_data = temp_ohlc[:, [3]]\n","          # temp_data = ind_temp_ohlc[:, [3]]\n","          # temp_data = temp_close\n","          # temp_data = np.hstack((temp_ohlc, temp_vol))\n","\n","          #     only close    #\n","          # temp_data = temp_ohlc[:, [-1]]\n","\n","          # temp_data = temp_ohlc\n","\n","          #     only volume    #\n","          # temp_data = temp_vol\n","\n","          # temp_data = np.hstack((temp_ohlc, temp_long_ep))\n","          # temp_data = temp_vol\n","\n","          #   scaler 설정\n","\n","          #   ohlc & ep -> max_abs\n","          # max_abs = MaxAbsScaler()\n","          # temp_data[:, :-1] = max_abs.fit_transform(temp_data[:, :-1])\n","\n","          \n","          # min_max = MinMaxScaler()\n","          # temp_data = min_max.fit_transform(temp_data)\n","\n","          #   candle -> max_abs    \n","          # max_abs = MaxAbsScaler()\n","          # temp_data[:, -3:] = max_abs.fit_transform(temp_data[:, -3:])\n","\n","          # min_max = MinMaxScaler()\n","          # temp_data[:, -3:] = min_max.fit_transform(temp_data[:, -3:])\n","\n","          if np.isnan(np.sum(temp_data)):\n","            continue\n","\n","          data_x.append(temp_data)\n","          data_pr.append(pr_list[i])\n","          data_index.append(time_index[i])\n","          data_updown.append(ohlc['close'].iloc[i] / ohlc['open'].iloc[i])\n","\n","\n","      print('np.array(data_x).shape :', np.array(data_x).shape)\n","      # print(data_x[0])\n","\n","\n","      #       Reshape data for image deep - learning     #\n","      _, row, col = np.array(data_x).shape\n","\n","      # input_x = np.array(data_x).reshape(-1, row, col, 1).astype(np.float32)\n","      input_x = np.array(data_x).reshape(-1, row, col).astype(np.float32)\n","\n","      #     1c to 3c    #\n","      # input_x = input_x * np.ones(3, dtype=np.float32)[None, None, None, :]\n","      # input_x = np.array(resize_npy(input_x))\n","\n","\n","      input_pr = np.array(data_pr).reshape(-1, 1).astype(np.float32)\n","      input_ud = np.array(data_updown).reshape(-1, 1).astype(np.float32)\n","      input_index = np.array(data_index).reshape(-1, 1)\n","      print('input_x.shape :', input_x.shape)\n","      print('input_x.dtype :', input_x.dtype)\n","      print('input_pr.shape :', input_pr.shape)\n","      print('input_ud.shape :', input_ud.shape)\n","      print('input_index.shape :', input_index.shape)\n","\n","\n","      # x_train_, x_test, pr_train_, pr_test, ud_train_, ud_test = train_test_split(input_x, input_pr, input_ud, test_size=0.4, shuffle=False, random_state=random_state)\n","      # x_train, x_val, pr_train, pr_val, ud_train, ud_val = train_test_split(x_train_, pr_train_, ud_train_, test_size=0.25, shuffle=False, random_state=random_state)\n","\n","      #     do stacking   #\n","      # if prev_x is None:\n","      prev_x = input_x\n","      prev_pr = input_pr\n","      prev_ud = input_ud\n","      prev_index = input_index\n","\n","      total_x = input_x\n","      total_pr = input_pr\n","      total_ud = input_ud\n","      total_index = input_index\n","\n","      # else:\n","      #   total_x = np.vstack((prev_x, input_x))\n","      #   total_pr = np.vstack((prev_pr, input_pr))\n","      #   total_ud = np.vstack((prev_ud, input_ud)) \n","      #   total_index = np.vstack((prev_index, input_index)) \n","\n","      #   prev_x = total_x\n","      #   prev_pr = total_pr\n","      #   prev_ud = total_ud\n","      #   prev_index = total_index\n","\n","      print('total_x.shape :', total_x.shape)\n","      print('total_pr.shape :', total_pr.shape)\n","      print('total_ud.shape :', total_ud.shape)\n","      print('prev_index.shape :', prev_index.shape)\n","\n","      \n","      # _, row, col, _ = input_x.shape\n","      _, row, col = input_x.shape\n","\n","      #       split new test      #\n","\n","      seed = 1\n","      random_state = 201\n","      np.random.seed(seed)\n","      from sklearn.model_selection import train_test_split\n","\n","\n","      #         get unique timestamp      #\n","      print(np.unique(total_index, return_counts=True))\n","      uniq_stamp = np.unique(total_index)\n","\n","      stamp_train_, stamp_test = train_test_split(uniq_stamp, test_size=0.2, shuffle=False, random_state=random_state)\n","      stamp_train, stamp_val = train_test_split(stamp_train_, test_size=0.25, shuffle=False, random_state=random_state)\n","\n","      print(\"stamp_train.shape :\", stamp_train.shape)\n","      print(\"stamp_val.shape :\", stamp_val.shape)\n","      print(\"stamp_test.shape :\", stamp_test.shape)\n","      # break\n","\n","\n","      #         split data by stamp     #\n","      x_train, x_val, x_test = [], [], []\n","      pr_train, pr_val, pr_test = [], [], []\n","      index_train, index_val, index_test = [], [], []\n","\n","\n","      from tqdm.notebook import tqdm\n","\n","      np.random.shuffle(total_index)\n","\n","      for i in tqdm(range(len(total_index))):\n","\n","        if total_index[i] in stamp_train:\n","          x_train.append(total_x[i])\n","          pr_train.append(total_pr[i])\n","          index_train.append(total_index[i])\n","\n","        elif total_index[i] in stamp_val:\n","          x_val.append(total_x[i])\n","          pr_val.append(total_pr[i])\n","          index_val.append(total_index[i])\n","        \n","        elif total_index[i] in stamp_test:\n","          x_test.append(total_x[i])\n","          pr_test.append(total_pr[i])\n","          index_test.append(total_index[i])\n","\n","\n","      x_train = np.array(x_train)\n","      x_val = np.array(x_val)\n","      x_test = np.array(x_test)\n","\n","      pr_train = np.array(pr_train)\n","      pr_val = np.array(pr_val)\n","      pr_test = np.array(pr_test)\n","\n","      index_train = np.array(index_train)\n","      index_val = np.array(index_val)\n","      index_test = np.array(index_test)\n","        \n","      print(\"x_train.shape :\", x_train.shape) # x_train.shape : (3807, 90, 12, 3)\n","      print(\"x_val.shape :\", x_val.shape) # x_train.shape : (3807, 90, 12, 3)\n","      print(\"x_test.shape :\", x_test.shape) # x_train.shape : (3807, 90, 12, 3)\n","\n","      symbol_name = key.split(' ')[1].split('.')[0]\n","\n","      x_save_path = current_path + 'npy/' + '%s_rnn_close_updown_x_train_robust_trial_re_%s_timesplit.npy' % (period, symbol_name)\n","      np.save(x_save_path, x_train)\n","      np.save(x_save_path.replace('x_train', 'x_val'), x_val)\n","      np.save(x_save_path.replace('x_train', 'x_test'), x_test)\n","      # np.save(x_save_path.replace('x_train', 'new_input_x'), new_input_x)\n","      print('x series saved !')\n","\n","      pr_save_path = current_path + 'npy/' + '%s_rnn_close_updown_pr_train_robust_trial_re_%s_timesplit.npy' % (period, symbol_name)\n","      np.save(pr_save_path, pr_train)\n","      np.save(pr_save_path.replace('pr_train', 'pr_val'), pr_val)\n","      np.save(pr_save_path.replace('pr_train', 'pr_test'), pr_test)\n","      # np.save(pr_save_path.replace('pr_train', 'new_input_pr'), new_input_pr)\n","      print('pr series saved !')\n","      \n","\n","  # break # --> use only one pair dataset\n","\n","  #         chunks 로 나누지 않아도, generator 에서 batch_size 만큼만 load 할 것   #\n","  try:\n","    if len(total_x) > 300000:\n","      break\n","  except:\n","    pass\n","\n","  \n","        "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1pOkWbn6JQFN"},"source":["### load npy"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wIqpv89dJNZz","executionInfo":{"status":"ok","timestamp":1621942535092,"user_tz":-540,"elapsed":4258,"user":{"displayName":"7th June","photoUrl":"","userId":"08178289703395036410"}},"outputId":"bad0125d-4fcb-4323-8aff-6338b6de46ba"},"source":["from keras.utils import np_utils\n","\n","# from keras.preprocessing.image import ImageDataGenerator \n","from sklearn.utils import class_weight\n","\n","\n","symbol_name = \"SXPUSDT\"\n","period = 45\n","\n","x_save_path = current_path + 'npy/' + '%s_rnn_close_updown_x_train_%s_tssplit.npy' % (period, symbol_name)\n","\n","\n","x_train = np.load(x_save_path)\n","x_val = np.load(x_save_path.replace('x_train', 'x_val'))\n","x_test = np.load(x_save_path.replace('x_train', 'x_test'))\n","print('x series loaded !')\n","\n","pr_save_path = current_path + 'npy/' + '%s_rnn_close_updown_pr_train_%s_tssplit.npy' % (period, symbol_name)\n","\n","\n","pr_train = np.load(pr_save_path)\n","pr_val = np.load(pr_save_path.replace('pr_train', 'pr_val'))\n","pr_test = np.load(pr_save_path.replace('pr_train', 'pr_test'))\n","print('y series loaded !')\n","\n","# total_x = np.vstack((x_train, x_val, x_test))\n","# total_pr = np.vstack((pr_train, pr_val, pr_test))\n","\n","# print(\"total_x.shape :\", total_x.shape)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["x series loaded !\n","y series loaded !\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"j7huC8gvUTNf","executionInfo":{"status":"ok","timestamp":1621942694910,"user_tz":-540,"elapsed":308,"user":{"displayName":"7th June","photoUrl":"","userId":"08178289703395036410"}},"outputId":"b58d4266-0061-4d0c-bff2-053d627dacf3"},"source":["\n","def class_ratio(in_list):\n","\n","  return in_list / in_list[1]\n","  \n","y_train = np.where(pr_train > 1, 1, 0)\n","y_test = np.where(pr_test > 1, 1, 0)\n","y_val = np.where(pr_val > 1, 1, 0)\n","\n","print('x_train.shape :', x_train.shape)\n","print('x_test.shape :', x_test.shape)\n","print('x_val.shape :', x_val.shape)\n","print('y_train.shape :', y_train.shape)\n","print('y_test.shape :', y_test.shape)\n","print('y_val.shape :', y_val.shape)\n","\n","\n","print('np.unique(y_train, return_counts=True :', np.unique(y_train, return_counts=True), class_ratio(np.unique(y_train, return_counts=True)[1]))\n","print('np.unique(y_val, return_counts=True :', np.unique(y_val, return_counts=True), class_ratio(np.unique(y_val, return_counts=True)[1]))\n","print('np.unique(y_test, return_counts=True :', np.unique(y_test, return_counts=True), class_ratio(np.unique(y_test, return_counts=True)[1]))\n","\n","label = y_train.reshape(-1, )\n","class_weights = class_weight.compute_class_weight('balanced', \n","                                                    classes=np.unique(label),\n","                                                    y=label)\n","class_weights = dict(enumerate(class_weights))\n","print('class_weights :', class_weights)\n","\n","# sample_weight = np.ones(shape=(len(y_train),))\n","# sample_weight[(y_train == 1).reshape(-1,)] = 1.5\n","# print('sample_weight[:20] :', sample_weight[:20])\n","\n","\n","print('np.isnan(np.sum(x_train)) :', np.isnan(np.sum(x_train)))\n","print('np.isnan(np.sum(x_val)) :', np.isnan(np.sum(x_val)))\n","print('np.isnan(np.sum(x_test)) :', np.isnan(np.sum(x_test)))\n","\n","print('np.isnan(np.sum(y_train)) :', np.isnan(np.sum(y_train)))\n","print('np.isnan(np.sum(y_val)) :', np.isnan(np.sum(y_val)))\n","print('np.isnan(np.sum(y_test)) :', np.isnan(np.sum(y_test)))\n","\n","y_train_ohe = np_utils.to_categorical(y_train, num_classes)\n","y_val_ohe = np_utils.to_categorical(y_val, num_classes)\n","y_test_ohe = np_utils.to_categorical(y_test, num_classes)\n","print('y_train_ohe.shape :', y_train_ohe.shape)\n","print('y_val_ohe.shape :', y_val_ohe.shape)\n","print('y_test_ohe.shape :', y_test_ohe.shape)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["x_train.shape : (7806, 45, 1)\n","x_test.shape : (2603, 45, 1)\n","x_val.shape : (2602, 45, 1)\n","y_train.shape : (7806, 1)\n","y_test.shape : (2603, 1)\n","y_val.shape : (2602, 1)\n","np.unique(y_train, return_counts=True : (array([0, 1]), array([4170, 3636])) [1.14686469 1.        ]\n","np.unique(y_val, return_counts=True : (array([0, 1]), array([1392, 1210])) [1.15041322 1.        ]\n","np.unique(y_test, return_counts=True : (array([0, 1]), array([1390, 1213])) [1.14591921 1.        ]\n","class_weights : {0: 0.9359712230215828, 1: 1.0734323432343233}\n","np.isnan(np.sum(x_train)) : False\n","np.isnan(np.sum(x_val)) : False\n","np.isnan(np.sum(x_test)) : False\n","np.isnan(np.sum(y_train)) : False\n","np.isnan(np.sum(y_val)) : False\n","np.isnan(np.sum(y_test)) : False\n","y_train_ohe.shape : (7806, 2)\n","y_val_ohe.shape : (2602, 2)\n","y_test_ohe.shape : (2603, 2)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"-4Ta-VQ7JX6A"},"source":["### train"]},{"cell_type":"code","metadata":{"id":"USEDmvAzJYxM","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1621942914207,"user_tz":-540,"elapsed":210163,"user":{"displayName":"7th June","photoUrl":"","userId":"08178289703395036410"}},"outputId":"e510c7f1-14d3-480e-a360-637ea7527c9d"},"source":["\n","from keras.callbacks import ModelCheckpoint, EarlyStopping, TensorBoard\n","\n","ckpt_path = current_path + 'ckpt/'\n","board_path = current_path + 'graph/'\n","\n","batch_size = 512\n","\n","model = FER_Model(input_shape=x_train.shape[1:])\n","opt = Adam(lr=0.00001, decay=0.000005)\n","# opt = Adam(lr=0.001, decay=0.0005)\n","model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n","\n","model_name = 'classifier_%s_lstm_close_updown_%s_tvnon_shuffle.h5' % (period, symbol_name)\n","\n","\n","\n","checkpoint = ModelCheckpoint(ckpt_path + model_name, monitor='val_loss', verbose=1, save_best_only=True, mode='auto')\n","checkpoint2 = TensorBoard(log_dir=board_path,\n","                          histogram_freq=0,\n","                          write_graph=True,\n","                          write_images=True)\n","checkpoint3 = EarlyStopping(monitor='val_loss', patience=250)\n","callbacks_list = [checkpoint, checkpoint2, checkpoint3]\n","# callbacks_list = [checkpoint, checkpoint2]\n","\n","# keras.callbacks.Callback 로 부터 log 를 받아와 history log 를 작성할 수 있다.\n","\n","# we iterate 200 times over the entire training set\n","num_epochs = 1000                    \n","\n","history = model.fit(x_train, y_train_ohe,\n","                    steps_per_epoch=int(len(x_train) / batch_size), \n","                    epochs=num_epochs,  \n","                    verbose=2,  \n","                    callbacks=callbacks_list,\n","                    class_weight=class_weights,\n","                    validation_data=(x_val, y_val_ohe),  \n","                    validation_steps=int(len(x_val) / batch_size),\n","                    shuffle=False)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Model: \"model_2\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","input (InputLayer)           (None, 45, 1)             0         \n","_________________________________________________________________\n","lstm_2 (LSTM)                (None, 10)                480       \n","_________________________________________________________________\n","leaky_re_lu_7 (LeakyReLU)    (None, 10)                0         \n","_________________________________________________________________\n","dense_7 (Dense)              (None, 64)                704       \n","_________________________________________________________________\n","leaky_re_lu_8 (LeakyReLU)    (None, 64)                0         \n","_________________________________________________________________\n","batch_normalization_3 (Batch (None, 64)                256       \n","_________________________________________________________________\n","dense_8 (Dense)              (None, 128)               8320      \n","_________________________________________________________________\n","leaky_re_lu_9 (LeakyReLU)    (None, 128)               0         \n","_________________________________________________________________\n","dense_9 (Dense)              (None, 256)               33024     \n","_________________________________________________________________\n","leaky_re_lu_10 (LeakyReLU)   (None, 256)               0         \n","_________________________________________________________________\n","batch_normalization_4 (Batch (None, 256)               1024      \n","_________________________________________________________________\n","dense_10 (Dense)             (None, 128)               32896     \n","_________________________________________________________________\n","leaky_re_lu_11 (LeakyReLU)   (None, 128)               0         \n","_________________________________________________________________\n","dense_11 (Dense)             (None, 64)                8256      \n","_________________________________________________________________\n","leaky_re_lu_12 (LeakyReLU)   (None, 64)                0         \n","_________________________________________________________________\n","dropout_2 (Dropout)          (None, 64)                0         \n","_________________________________________________________________\n","dense_12 (Dense)             (None, 2)                 130       \n","=================================================================\n","Total params: 85,090\n","Trainable params: 84,450\n","Non-trainable params: 640\n","_________________________________________________________________\n","None\n","WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n","\n","Train on 7806 samples, validate on 2602 samples\n","WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/keras/callbacks/tensorboard_v1.py:200: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.\n","\n","WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/keras/callbacks/tensorboard_v1.py:203: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n","\n","Epoch 1/1000\n"," - 3s - loss: 0.8715 - accuracy: 0.5219 - val_loss: 0.1386 - val_accuracy: 2.4635\n","\n","Epoch 00001: val_loss improved from inf to 0.13863, saving model to /content/drive/My Drive/Colab Notebooks/300/ckpt/classifier_45_lstm_close_updown_SXPUSDT_tvnon_shuffle.h5\n","WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/keras/callbacks/tensorboard_v1.py:343: The name tf.Summary is deprecated. Please use tf.compat.v1.Summary instead.\n","\n","Epoch 2/1000\n"," - 1s - loss: 0.8050 - accuracy: 0.5215 - val_loss: 0.1387 - val_accuracy: 2.3251\n","\n","Epoch 00002: val_loss did not improve from 0.13863\n","Epoch 3/1000\n"," - 1s - loss: 0.7737 - accuracy: 0.5183 - val_loss: 0.1387 - val_accuracy: 2.3251\n","\n","Epoch 00003: val_loss did not improve from 0.13863\n","Epoch 4/1000\n"," - 1s - loss: 0.7611 - accuracy: 0.5134 - val_loss: 0.1387 - val_accuracy: 2.3444\n","\n","Epoch 00004: val_loss did not improve from 0.13863\n","Epoch 5/1000\n"," - 1s - loss: 0.7515 - accuracy: 0.5147 - val_loss: 0.1387 - val_accuracy: 2.3482\n","\n","Epoch 00005: val_loss did not improve from 0.13863\n","Epoch 6/1000\n"," - 1s - loss: 0.7462 - accuracy: 0.5131 - val_loss: 0.1387 - val_accuracy: 2.3463\n","\n","Epoch 00006: val_loss did not improve from 0.13863\n","Epoch 7/1000\n"," - 1s - loss: 0.7441 - accuracy: 0.5093 - val_loss: 0.1387 - val_accuracy: 2.3463\n","\n","Epoch 00007: val_loss did not improve from 0.13863\n","Epoch 8/1000\n"," - 1s - loss: 0.7376 - accuracy: 0.5125 - val_loss: 0.1387 - val_accuracy: 2.3540\n","\n","Epoch 00008: val_loss did not improve from 0.13863\n","Epoch 9/1000\n"," - 1s - loss: 0.7348 - accuracy: 0.5151 - val_loss: 0.1387 - val_accuracy: 2.3559\n","\n","Epoch 00009: val_loss did not improve from 0.13863\n","Epoch 10/1000\n"," - 1s - loss: 0.7327 - accuracy: 0.5116 - val_loss: 0.1387 - val_accuracy: 2.3828\n","\n","Epoch 00010: val_loss did not improve from 0.13863\n","Epoch 11/1000\n"," - 1s - loss: 0.7276 - accuracy: 0.5140 - val_loss: 0.1387 - val_accuracy: 2.4654\n","\n","Epoch 00011: val_loss did not improve from 0.13863\n","Epoch 12/1000\n"," - 1s - loss: 0.7242 - accuracy: 0.5147 - val_loss: 0.1387 - val_accuracy: 2.4635\n","\n","Epoch 00012: val_loss did not improve from 0.13863\n","Epoch 13/1000\n"," - 1s - loss: 0.7226 - accuracy: 0.5162 - val_loss: 0.1387 - val_accuracy: 2.4808\n","\n","Epoch 00013: val_loss did not improve from 0.13863\n","Epoch 14/1000\n"," - 1s - loss: 0.7189 - accuracy: 0.5151 - val_loss: 0.1387 - val_accuracy: 2.4769\n","\n","Epoch 00014: val_loss did not improve from 0.13863\n","Epoch 15/1000\n"," - 1s - loss: 0.7170 - accuracy: 0.5180 - val_loss: 0.1387 - val_accuracy: 2.4808\n","\n","Epoch 00015: val_loss did not improve from 0.13863\n","Epoch 16/1000\n"," - 1s - loss: 0.7161 - accuracy: 0.5152 - val_loss: 0.1387 - val_accuracy: 2.4769\n","\n","Epoch 00016: val_loss did not improve from 0.13863\n","Epoch 17/1000\n"," - 1s - loss: 0.7141 - accuracy: 0.5163 - val_loss: 0.1387 - val_accuracy: 2.4904\n","\n","Epoch 00017: val_loss did not improve from 0.13863\n","Epoch 18/1000\n"," - 1s - loss: 0.7116 - accuracy: 0.5166 - val_loss: 0.1386 - val_accuracy: 2.4827\n","\n","Epoch 00018: val_loss did not improve from 0.13863\n","Epoch 19/1000\n"," - 1s - loss: 0.7093 - accuracy: 0.5180 - val_loss: 0.1386 - val_accuracy: 2.4827\n","\n","Epoch 00019: val_loss did not improve from 0.13863\n","Epoch 20/1000\n"," - 1s - loss: 0.7093 - accuracy: 0.5187 - val_loss: 0.1387 - val_accuracy: 2.4769\n","\n","Epoch 00020: val_loss did not improve from 0.13863\n","Epoch 21/1000\n"," - 1s - loss: 0.7064 - accuracy: 0.5201 - val_loss: 0.1387 - val_accuracy: 2.4712\n","\n","Epoch 00021: val_loss did not improve from 0.13863\n","Epoch 22/1000\n"," - 1s - loss: 0.7053 - accuracy: 0.5198 - val_loss: 0.1387 - val_accuracy: 2.4865\n","\n","Epoch 00022: val_loss did not improve from 0.13863\n","Epoch 23/1000\n"," - 1s - loss: 0.7040 - accuracy: 0.5196 - val_loss: 0.1387 - val_accuracy: 2.4827\n","\n","Epoch 00023: val_loss did not improve from 0.13863\n","Epoch 24/1000\n"," - 1s - loss: 0.7029 - accuracy: 0.5206 - val_loss: 0.1387 - val_accuracy: 2.4827\n","\n","Epoch 00024: val_loss did not improve from 0.13863\n","Epoch 25/1000\n"," - 1s - loss: 0.7014 - accuracy: 0.5223 - val_loss: 0.1388 - val_accuracy: 2.4789\n","\n","Epoch 00025: val_loss did not improve from 0.13863\n","Epoch 26/1000\n"," - 1s - loss: 0.7018 - accuracy: 0.5198 - val_loss: 0.1388 - val_accuracy: 2.4789\n","\n","Epoch 00026: val_loss did not improve from 0.13863\n","Epoch 27/1000\n"," - 1s - loss: 0.7003 - accuracy: 0.5223 - val_loss: 0.1389 - val_accuracy: 2.4731\n","\n","Epoch 00027: val_loss did not improve from 0.13863\n","Epoch 28/1000\n"," - 1s - loss: 0.6980 - accuracy: 0.5257 - val_loss: 0.1389 - val_accuracy: 2.4616\n","\n","Epoch 00028: val_loss did not improve from 0.13863\n","Epoch 29/1000\n"," - 1s - loss: 0.6981 - accuracy: 0.5255 - val_loss: 0.1390 - val_accuracy: 2.4654\n","\n","Epoch 00029: val_loss did not improve from 0.13863\n","Epoch 30/1000\n"," - 1s - loss: 0.6980 - accuracy: 0.5250 - val_loss: 0.1390 - val_accuracy: 2.4654\n","\n","Epoch 00030: val_loss did not improve from 0.13863\n","Epoch 31/1000\n"," - 1s - loss: 0.6973 - accuracy: 0.5244 - val_loss: 0.1391 - val_accuracy: 2.4731\n","\n","Epoch 00031: val_loss did not improve from 0.13863\n","Epoch 32/1000\n"," - 1s - loss: 0.6957 - accuracy: 0.5271 - val_loss: 0.1391 - val_accuracy: 2.4616\n","\n","Epoch 00032: val_loss did not improve from 0.13863\n","Epoch 33/1000\n"," - 1s - loss: 0.6959 - accuracy: 0.5282 - val_loss: 0.1392 - val_accuracy: 2.4654\n","\n","Epoch 00033: val_loss did not improve from 0.13863\n","Epoch 34/1000\n"," - 1s - loss: 0.6951 - accuracy: 0.5284 - val_loss: 0.1392 - val_accuracy: 2.4539\n","\n","Epoch 00034: val_loss did not improve from 0.13863\n","Epoch 35/1000\n"," - 1s - loss: 0.6948 - accuracy: 0.5291 - val_loss: 0.1393 - val_accuracy: 2.4539\n","\n","Epoch 00035: val_loss did not improve from 0.13863\n","Epoch 36/1000\n"," - 1s - loss: 0.6946 - accuracy: 0.5289 - val_loss: 0.1393 - val_accuracy: 2.4520\n","\n","Epoch 00036: val_loss did not improve from 0.13863\n","Epoch 37/1000\n"," - 1s - loss: 0.6947 - accuracy: 0.5275 - val_loss: 0.1392 - val_accuracy: 2.4673\n","\n","Epoch 00037: val_loss did not improve from 0.13863\n","Epoch 38/1000\n"," - 1s - loss: 0.6935 - accuracy: 0.5309 - val_loss: 0.1392 - val_accuracy: 2.4865\n","\n","Epoch 00038: val_loss did not improve from 0.13863\n","Epoch 39/1000\n"," - 1s - loss: 0.6931 - accuracy: 0.5325 - val_loss: 0.1391 - val_accuracy: 2.5211\n","\n","Epoch 00039: val_loss did not improve from 0.13863\n","Epoch 40/1000\n"," - 1s - loss: 0.6929 - accuracy: 0.5319 - val_loss: 0.1390 - val_accuracy: 2.5058\n","\n","Epoch 00040: val_loss did not improve from 0.13863\n","Epoch 41/1000\n"," - 1s - loss: 0.6925 - accuracy: 0.5298 - val_loss: 0.1390 - val_accuracy: 2.5192\n","\n","Epoch 00041: val_loss did not improve from 0.13863\n","Epoch 42/1000\n"," - 1s - loss: 0.6925 - accuracy: 0.5312 - val_loss: 0.1388 - val_accuracy: 2.5096\n","\n","Epoch 00042: val_loss did not improve from 0.13863\n","Epoch 43/1000\n"," - 1s - loss: 0.6922 - accuracy: 0.5340 - val_loss: 0.1387 - val_accuracy: 2.5576\n","\n","Epoch 00043: val_loss did not improve from 0.13863\n","Epoch 44/1000\n"," - 1s - loss: 0.6919 - accuracy: 0.5324 - val_loss: 0.1387 - val_accuracy: 2.5961\n","\n","Epoch 00044: val_loss did not improve from 0.13863\n","Epoch 45/1000\n"," - 1s - loss: 0.6921 - accuracy: 0.5312 - val_loss: 0.1386 - val_accuracy: 2.6038\n","\n","Epoch 00045: val_loss improved from 0.13863 to 0.13860, saving model to /content/drive/My Drive/Colab Notebooks/300/ckpt/classifier_45_lstm_close_updown_SXPUSDT_tvnon_shuffle.h5\n","Epoch 46/1000\n"," - 1s - loss: 0.6909 - accuracy: 0.5330 - val_loss: 0.1385 - val_accuracy: 2.6018\n","\n","Epoch 00046: val_loss improved from 0.13860 to 0.13852, saving model to /content/drive/My Drive/Colab Notebooks/300/ckpt/classifier_45_lstm_close_updown_SXPUSDT_tvnon_shuffle.h5\n","Epoch 47/1000\n"," - 1s - loss: 0.6910 - accuracy: 0.5336 - val_loss: 0.1385 - val_accuracy: 2.5922\n","\n","Epoch 00047: val_loss improved from 0.13852 to 0.13849, saving model to /content/drive/My Drive/Colab Notebooks/300/ckpt/classifier_45_lstm_close_updown_SXPUSDT_tvnon_shuffle.h5\n","Epoch 48/1000\n"," - 1s - loss: 0.6900 - accuracy: 0.5357 - val_loss: 0.1385 - val_accuracy: 2.5807\n","\n","Epoch 00048: val_loss did not improve from 0.13849\n","Epoch 49/1000\n"," - 1s - loss: 0.6905 - accuracy: 0.5372 - val_loss: 0.1385 - val_accuracy: 2.5884\n","\n","Epoch 00049: val_loss improved from 0.13849 to 0.13847, saving model to /content/drive/My Drive/Colab Notebooks/300/ckpt/classifier_45_lstm_close_updown_SXPUSDT_tvnon_shuffle.h5\n","Epoch 50/1000\n"," - 1s - loss: 0.6899 - accuracy: 0.5377 - val_loss: 0.1384 - val_accuracy: 2.6384\n","\n","Epoch 00050: val_loss improved from 0.13847 to 0.13844, saving model to /content/drive/My Drive/Colab Notebooks/300/ckpt/classifier_45_lstm_close_updown_SXPUSDT_tvnon_shuffle.h5\n","Epoch 51/1000\n"," - 1s - loss: 0.6895 - accuracy: 0.5380 - val_loss: 0.1384 - val_accuracy: 2.6441\n","\n","Epoch 00051: val_loss did not improve from 0.13844\n","Epoch 52/1000\n"," - 1s - loss: 0.6898 - accuracy: 0.5368 - val_loss: 0.1384 - val_accuracy: 2.6480\n","\n","Epoch 00052: val_loss did not improve from 0.13844\n","Epoch 53/1000\n"," - 1s - loss: 0.6896 - accuracy: 0.5370 - val_loss: 0.1384 - val_accuracy: 2.6653\n","\n","Epoch 00053: val_loss did not improve from 0.13844\n","Epoch 54/1000\n"," - 1s - loss: 0.6893 - accuracy: 0.5375 - val_loss: 0.1384 - val_accuracy: 2.6806\n","\n","Epoch 00054: val_loss improved from 0.13844 to 0.13843, saving model to /content/drive/My Drive/Colab Notebooks/300/ckpt/classifier_45_lstm_close_updown_SXPUSDT_tvnon_shuffle.h5\n","Epoch 55/1000\n"," - 1s - loss: 0.6895 - accuracy: 0.5371 - val_loss: 0.1384 - val_accuracy: 2.6576\n","\n","Epoch 00055: val_loss did not improve from 0.13843\n","Epoch 56/1000\n"," - 1s - loss: 0.6883 - accuracy: 0.5394 - val_loss: 0.1385 - val_accuracy: 2.6518\n","\n","Epoch 00056: val_loss did not improve from 0.13843\n","Epoch 57/1000\n"," - 1s - loss: 0.6878 - accuracy: 0.5395 - val_loss: 0.1385 - val_accuracy: 2.6307\n","\n","Epoch 00057: val_loss did not improve from 0.13843\n","Epoch 58/1000\n"," - 1s - loss: 0.6890 - accuracy: 0.5379 - val_loss: 0.1385 - val_accuracy: 2.6326\n","\n","Epoch 00058: val_loss did not improve from 0.13843\n","Epoch 59/1000\n"," - 1s - loss: 0.6885 - accuracy: 0.5374 - val_loss: 0.1386 - val_accuracy: 2.6326\n","\n","Epoch 00059: val_loss did not improve from 0.13843\n","Epoch 60/1000\n"," - 1s - loss: 0.6879 - accuracy: 0.5406 - val_loss: 0.1386 - val_accuracy: 2.6403\n","\n","Epoch 00060: val_loss did not improve from 0.13843\n","Epoch 61/1000\n"," - 1s - loss: 0.6878 - accuracy: 0.5414 - val_loss: 0.1386 - val_accuracy: 2.6287\n","\n","Epoch 00061: val_loss did not improve from 0.13843\n","Epoch 62/1000\n"," - 1s - loss: 0.6880 - accuracy: 0.5393 - val_loss: 0.1386 - val_accuracy: 2.6307\n","\n","Epoch 00062: val_loss did not improve from 0.13843\n","Epoch 63/1000\n"," - 1s - loss: 0.6876 - accuracy: 0.5417 - val_loss: 0.1386 - val_accuracy: 2.6480\n","\n","Epoch 00063: val_loss did not improve from 0.13843\n","Epoch 64/1000\n"," - 1s - loss: 0.6878 - accuracy: 0.5412 - val_loss: 0.1387 - val_accuracy: 2.6211\n","\n","Epoch 00064: val_loss did not improve from 0.13843\n","Epoch 65/1000\n"," - 1s - loss: 0.6869 - accuracy: 0.5432 - val_loss: 0.1387 - val_accuracy: 2.6057\n","\n","Epoch 00065: val_loss did not improve from 0.13843\n","Epoch 66/1000\n"," - 1s - loss: 0.6875 - accuracy: 0.5406 - val_loss: 0.1387 - val_accuracy: 2.5961\n","\n","Epoch 00066: val_loss did not improve from 0.13843\n","Epoch 67/1000\n"," - 1s - loss: 0.6868 - accuracy: 0.5432 - val_loss: 0.1388 - val_accuracy: 2.6018\n","\n","Epoch 00067: val_loss did not improve from 0.13843\n","Epoch 68/1000\n"," - 1s - loss: 0.6866 - accuracy: 0.5457 - val_loss: 0.1388 - val_accuracy: 2.5980\n","\n","Epoch 00068: val_loss did not improve from 0.13843\n","Epoch 69/1000\n"," - 1s - loss: 0.6873 - accuracy: 0.5431 - val_loss: 0.1388 - val_accuracy: 2.6268\n","\n","Epoch 00069: val_loss did not improve from 0.13843\n","Epoch 70/1000\n"," - 1s - loss: 0.6868 - accuracy: 0.5418 - val_loss: 0.1388 - val_accuracy: 2.6115\n","\n","Epoch 00070: val_loss did not improve from 0.13843\n","Epoch 71/1000\n"," - 1s - loss: 0.6867 - accuracy: 0.5436 - val_loss: 0.1388 - val_accuracy: 2.6018\n","\n","Epoch 00071: val_loss did not improve from 0.13843\n","Epoch 72/1000\n"," - 1s - loss: 0.6864 - accuracy: 0.5449 - val_loss: 0.1388 - val_accuracy: 2.6268\n","\n","Epoch 00072: val_loss did not improve from 0.13843\n","Epoch 73/1000\n"," - 1s - loss: 0.6869 - accuracy: 0.5425 - val_loss: 0.1389 - val_accuracy: 2.6326\n","\n","Epoch 00073: val_loss did not improve from 0.13843\n","Epoch 74/1000\n"," - 1s - loss: 0.6860 - accuracy: 0.5445 - val_loss: 0.1389 - val_accuracy: 2.6191\n","\n","Epoch 00074: val_loss did not improve from 0.13843\n","Epoch 75/1000\n"," - 1s - loss: 0.6859 - accuracy: 0.5475 - val_loss: 0.1390 - val_accuracy: 2.6038\n","\n","Epoch 00075: val_loss did not improve from 0.13843\n","Epoch 76/1000\n"," - 1s - loss: 0.6859 - accuracy: 0.5454 - val_loss: 0.1390 - val_accuracy: 2.5961\n","\n","Epoch 00076: val_loss did not improve from 0.13843\n","Epoch 77/1000\n"," - 1s - loss: 0.6857 - accuracy: 0.5466 - val_loss: 0.1389 - val_accuracy: 2.6057\n","\n","Epoch 00077: val_loss did not improve from 0.13843\n","Epoch 78/1000\n"," - 1s - loss: 0.6860 - accuracy: 0.5476 - val_loss: 0.1389 - val_accuracy: 2.6115\n","\n","Epoch 00078: val_loss did not improve from 0.13843\n","Epoch 79/1000\n"," - 1s - loss: 0.6859 - accuracy: 0.5457 - val_loss: 0.1389 - val_accuracy: 2.6134\n","\n","Epoch 00079: val_loss did not improve from 0.13843\n","Epoch 80/1000\n"," - 1s - loss: 0.6861 - accuracy: 0.5444 - val_loss: 0.1389 - val_accuracy: 2.6057\n","\n","Epoch 00080: val_loss did not improve from 0.13843\n","Epoch 81/1000\n"," - 1s - loss: 0.6858 - accuracy: 0.5455 - val_loss: 0.1389 - val_accuracy: 2.6268\n","\n","Epoch 00081: val_loss did not improve from 0.13843\n","Epoch 82/1000\n"," - 1s - loss: 0.6854 - accuracy: 0.5465 - val_loss: 0.1389 - val_accuracy: 2.6153\n","\n","Epoch 00082: val_loss did not improve from 0.13843\n","Epoch 83/1000\n"," - 1s - loss: 0.6855 - accuracy: 0.5459 - val_loss: 0.1389 - val_accuracy: 2.6230\n","\n","Epoch 00083: val_loss did not improve from 0.13843\n","Epoch 84/1000\n"," - 1s - loss: 0.6851 - accuracy: 0.5472 - val_loss: 0.1389 - val_accuracy: 2.6403\n","\n","Epoch 00084: val_loss did not improve from 0.13843\n","Epoch 85/1000\n"," - 1s - loss: 0.6853 - accuracy: 0.5456 - val_loss: 0.1389 - val_accuracy: 2.6518\n","\n","Epoch 00085: val_loss did not improve from 0.13843\n","Epoch 86/1000\n"," - 1s - loss: 0.6855 - accuracy: 0.5452 - val_loss: 0.1388 - val_accuracy: 2.6672\n","\n","Epoch 00086: val_loss did not improve from 0.13843\n","Epoch 87/1000\n"," - 1s - loss: 0.6851 - accuracy: 0.5471 - val_loss: 0.1388 - val_accuracy: 2.6806\n","\n","Epoch 00087: val_loss did not improve from 0.13843\n","Epoch 88/1000\n"," - 1s - loss: 0.6848 - accuracy: 0.5488 - val_loss: 0.1389 - val_accuracy: 2.6653\n","\n","Epoch 00088: val_loss did not improve from 0.13843\n","Epoch 89/1000\n"," - 1s - loss: 0.6848 - accuracy: 0.5481 - val_loss: 0.1389 - val_accuracy: 2.6595\n","\n","Epoch 00089: val_loss did not improve from 0.13843\n","Epoch 90/1000\n"," - 1s - loss: 0.6846 - accuracy: 0.5502 - val_loss: 0.1389 - val_accuracy: 2.6326\n","\n","Epoch 00090: val_loss did not improve from 0.13843\n","Epoch 91/1000\n"," - 1s - loss: 0.6850 - accuracy: 0.5456 - val_loss: 0.1389 - val_accuracy: 2.6441\n","\n","Epoch 00091: val_loss did not improve from 0.13843\n","Epoch 92/1000\n"," - 1s - loss: 0.6847 - accuracy: 0.5486 - val_loss: 0.1389 - val_accuracy: 2.6364\n","\n","Epoch 00092: val_loss did not improve from 0.13843\n","Epoch 93/1000\n"," - 1s - loss: 0.6850 - accuracy: 0.5469 - val_loss: 0.1389 - val_accuracy: 2.6403\n","\n","Epoch 00093: val_loss did not improve from 0.13843\n","Epoch 94/1000\n"," - 1s - loss: 0.6848 - accuracy: 0.5487 - val_loss: 0.1389 - val_accuracy: 2.6364\n","\n","Epoch 00094: val_loss did not improve from 0.13843\n","Epoch 95/1000\n"," - 1s - loss: 0.6844 - accuracy: 0.5487 - val_loss: 0.1389 - val_accuracy: 2.6537\n","\n","Epoch 00095: val_loss did not improve from 0.13843\n","Epoch 96/1000\n"," - 1s - loss: 0.6845 - accuracy: 0.5507 - val_loss: 0.1389 - val_accuracy: 2.6595\n","\n","Epoch 00096: val_loss did not improve from 0.13843\n","Epoch 97/1000\n"," - 1s - loss: 0.6845 - accuracy: 0.5491 - val_loss: 0.1390 - val_accuracy: 2.6287\n","\n","Epoch 00097: val_loss did not improve from 0.13843\n","Epoch 98/1000\n"," - 1s - loss: 0.6844 - accuracy: 0.5483 - val_loss: 0.1390 - val_accuracy: 2.6287\n","\n","Epoch 00098: val_loss did not improve from 0.13843\n","Epoch 99/1000\n"," - 1s - loss: 0.6839 - accuracy: 0.5511 - val_loss: 0.1390 - val_accuracy: 2.6441\n","\n","Epoch 00099: val_loss did not improve from 0.13843\n","Epoch 100/1000\n"," - 1s - loss: 0.6844 - accuracy: 0.5498 - val_loss: 0.1390 - val_accuracy: 2.6134\n","\n","Epoch 00100: val_loss did not improve from 0.13843\n","Epoch 101/1000\n"," - 1s - loss: 0.6834 - accuracy: 0.5523 - val_loss: 0.1391 - val_accuracy: 2.6115\n","\n","Epoch 00101: val_loss did not improve from 0.13843\n","Epoch 102/1000\n"," - 1s - loss: 0.6842 - accuracy: 0.5495 - val_loss: 0.1390 - val_accuracy: 2.6134\n","\n","Epoch 00102: val_loss did not improve from 0.13843\n","Epoch 103/1000\n"," - 1s - loss: 0.6839 - accuracy: 0.5511 - val_loss: 0.1390 - val_accuracy: 2.6499\n","\n","Epoch 00103: val_loss did not improve from 0.13843\n","Epoch 104/1000\n"," - 1s - loss: 0.6842 - accuracy: 0.5480 - val_loss: 0.1390 - val_accuracy: 2.6307\n","\n","Epoch 00104: val_loss did not improve from 0.13843\n","Epoch 105/1000\n"," - 1s - loss: 0.6841 - accuracy: 0.5520 - val_loss: 0.1390 - val_accuracy: 2.6422\n","\n","Epoch 00105: val_loss did not improve from 0.13843\n","Epoch 106/1000\n"," - 1s - loss: 0.6839 - accuracy: 0.5517 - val_loss: 0.1390 - val_accuracy: 2.6384\n","\n","Epoch 00106: val_loss did not improve from 0.13843\n","Epoch 107/1000\n"," - 1s - loss: 0.6841 - accuracy: 0.5518 - val_loss: 0.1390 - val_accuracy: 2.6614\n","\n","Epoch 00107: val_loss did not improve from 0.13843\n","Epoch 108/1000\n"," - 1s - loss: 0.6841 - accuracy: 0.5504 - val_loss: 0.1390 - val_accuracy: 2.6691\n","\n","Epoch 00108: val_loss did not improve from 0.13843\n","Epoch 109/1000\n"," - 1s - loss: 0.6836 - accuracy: 0.5521 - val_loss: 0.1390 - val_accuracy: 2.6364\n","\n","Epoch 00109: val_loss did not improve from 0.13843\n","Epoch 110/1000\n"," - 1s - loss: 0.6838 - accuracy: 0.5512 - val_loss: 0.1389 - val_accuracy: 2.6729\n","\n","Epoch 00110: val_loss did not improve from 0.13843\n","Epoch 111/1000\n"," - 1s - loss: 0.6833 - accuracy: 0.5527 - val_loss: 0.1389 - val_accuracy: 2.6729\n","\n","Epoch 00111: val_loss did not improve from 0.13843\n","Epoch 112/1000\n"," - 1s - loss: 0.6836 - accuracy: 0.5534 - val_loss: 0.1390 - val_accuracy: 2.6480\n","\n","Epoch 00112: val_loss did not improve from 0.13843\n","Epoch 113/1000\n"," - 1s - loss: 0.6834 - accuracy: 0.5539 - val_loss: 0.1389 - val_accuracy: 2.6787\n","\n","Epoch 00113: val_loss did not improve from 0.13843\n","Epoch 114/1000\n"," - 1s - loss: 0.6832 - accuracy: 0.5532 - val_loss: 0.1389 - val_accuracy: 2.6691\n","\n","Epoch 00114: val_loss did not improve from 0.13843\n","Epoch 115/1000\n"," - 1s - loss: 0.6834 - accuracy: 0.5527 - val_loss: 0.1389 - val_accuracy: 2.6614\n","\n","Epoch 00115: val_loss did not improve from 0.13843\n","Epoch 116/1000\n"," - 1s - loss: 0.6835 - accuracy: 0.5524 - val_loss: 0.1390 - val_accuracy: 2.6633\n","\n","Epoch 00116: val_loss did not improve from 0.13843\n","Epoch 117/1000\n"," - 1s - loss: 0.6834 - accuracy: 0.5530 - val_loss: 0.1390 - val_accuracy: 2.6480\n","\n","Epoch 00117: val_loss did not improve from 0.13843\n","Epoch 118/1000\n"," - 1s - loss: 0.6837 - accuracy: 0.5519 - val_loss: 0.1390 - val_accuracy: 2.6518\n","\n","Epoch 00118: val_loss did not improve from 0.13843\n","Epoch 119/1000\n"," - 1s - loss: 0.6833 - accuracy: 0.5526 - val_loss: 0.1391 - val_accuracy: 2.6307\n","\n","Epoch 00119: val_loss did not improve from 0.13843\n","Epoch 120/1000\n"," - 1s - loss: 0.6830 - accuracy: 0.5542 - val_loss: 0.1391 - val_accuracy: 2.6403\n","\n","Epoch 00120: val_loss did not improve from 0.13843\n","Epoch 121/1000\n"," - 1s - loss: 0.6833 - accuracy: 0.5525 - val_loss: 0.1391 - val_accuracy: 2.6230\n","\n","Epoch 00121: val_loss did not improve from 0.13843\n","Epoch 122/1000\n"," - 1s - loss: 0.6832 - accuracy: 0.5543 - val_loss: 0.1391 - val_accuracy: 2.6287\n","\n","Epoch 00122: val_loss did not improve from 0.13843\n","Epoch 123/1000\n"," - 1s - loss: 0.6832 - accuracy: 0.5545 - val_loss: 0.1390 - val_accuracy: 2.6556\n","\n","Epoch 00123: val_loss did not improve from 0.13843\n","Epoch 124/1000\n"," - 1s - loss: 0.6831 - accuracy: 0.5540 - val_loss: 0.1390 - val_accuracy: 2.6537\n","\n","Epoch 00124: val_loss did not improve from 0.13843\n","Epoch 125/1000\n"," - 1s - loss: 0.6830 - accuracy: 0.5532 - val_loss: 0.1390 - val_accuracy: 2.6768\n","\n","Epoch 00125: val_loss did not improve from 0.13843\n","Epoch 126/1000\n"," - 1s - loss: 0.6829 - accuracy: 0.5538 - val_loss: 0.1390 - val_accuracy: 2.6710\n","\n","Epoch 00126: val_loss did not improve from 0.13843\n","Epoch 127/1000\n"," - 1s - loss: 0.6830 - accuracy: 0.5540 - val_loss: 0.1390 - val_accuracy: 2.6441\n","\n","Epoch 00127: val_loss did not improve from 0.13843\n","Epoch 128/1000\n"," - 1s - loss: 0.6828 - accuracy: 0.5549 - val_loss: 0.1390 - val_accuracy: 2.6480\n","\n","Epoch 00128: val_loss did not improve from 0.13843\n","Epoch 129/1000\n"," - 1s - loss: 0.6827 - accuracy: 0.5538 - val_loss: 0.1390 - val_accuracy: 2.6480\n","\n","Epoch 00129: val_loss did not improve from 0.13843\n","Epoch 130/1000\n"," - 1s - loss: 0.6830 - accuracy: 0.5540 - val_loss: 0.1390 - val_accuracy: 2.6518\n","\n","Epoch 00130: val_loss did not improve from 0.13843\n","Epoch 131/1000\n"," - 1s - loss: 0.6823 - accuracy: 0.5551 - val_loss: 0.1390 - val_accuracy: 2.6480\n","\n","Epoch 00131: val_loss did not improve from 0.13843\n","Epoch 132/1000\n"," - 1s - loss: 0.6827 - accuracy: 0.5555 - val_loss: 0.1390 - val_accuracy: 2.6403\n","\n","Epoch 00132: val_loss did not improve from 0.13843\n","Epoch 133/1000\n"," - 1s - loss: 0.6823 - accuracy: 0.5554 - val_loss: 0.1390 - val_accuracy: 2.6326\n","\n","Epoch 00133: val_loss did not improve from 0.13843\n","Epoch 134/1000\n"," - 1s - loss: 0.6827 - accuracy: 0.5549 - val_loss: 0.1390 - val_accuracy: 2.6576\n","\n","Epoch 00134: val_loss did not improve from 0.13843\n","Epoch 135/1000\n"," - 1s - loss: 0.6824 - accuracy: 0.5549 - val_loss: 0.1390 - val_accuracy: 2.6326\n","\n","Epoch 00135: val_loss did not improve from 0.13843\n","Epoch 136/1000\n"," - 1s - loss: 0.6827 - accuracy: 0.5533 - val_loss: 0.1391 - val_accuracy: 2.6384\n","\n","Epoch 00136: val_loss did not improve from 0.13843\n","Epoch 137/1000\n"," - 1s - loss: 0.6826 - accuracy: 0.5555 - val_loss: 0.1391 - val_accuracy: 2.6556\n","\n","Epoch 00137: val_loss did not improve from 0.13843\n","Epoch 138/1000\n"," - 1s - loss: 0.6824 - accuracy: 0.5556 - val_loss: 0.1391 - val_accuracy: 2.6326\n","\n","Epoch 00138: val_loss did not improve from 0.13843\n","Epoch 139/1000\n"," - 1s - loss: 0.6821 - accuracy: 0.5552 - val_loss: 0.1391 - val_accuracy: 2.6403\n","\n","Epoch 00139: val_loss did not improve from 0.13843\n","Epoch 140/1000\n"," - 1s - loss: 0.6823 - accuracy: 0.5558 - val_loss: 0.1391 - val_accuracy: 2.6384\n","\n","Epoch 00140: val_loss did not improve from 0.13843\n","Epoch 141/1000\n"," - 1s - loss: 0.6827 - accuracy: 0.5544 - val_loss: 0.1392 - val_accuracy: 2.6537\n","\n","Epoch 00141: val_loss did not improve from 0.13843\n","Epoch 142/1000\n"," - 1s - loss: 0.6822 - accuracy: 0.5544 - val_loss: 0.1391 - val_accuracy: 2.6441\n","\n","Epoch 00142: val_loss did not improve from 0.13843\n","Epoch 143/1000\n"," - 1s - loss: 0.6822 - accuracy: 0.5548 - val_loss: 0.1392 - val_accuracy: 2.6384\n","\n","Epoch 00143: val_loss did not improve from 0.13843\n","Epoch 144/1000\n"," - 1s - loss: 0.6821 - accuracy: 0.5564 - val_loss: 0.1391 - val_accuracy: 2.6480\n","\n","Epoch 00144: val_loss did not improve from 0.13843\n","Epoch 145/1000\n"," - 1s - loss: 0.6821 - accuracy: 0.5583 - val_loss: 0.1391 - val_accuracy: 2.6403\n","\n","Epoch 00145: val_loss did not improve from 0.13843\n","Epoch 146/1000\n"," - 1s - loss: 0.6820 - accuracy: 0.5578 - val_loss: 0.1391 - val_accuracy: 2.6422\n","\n","Epoch 00146: val_loss did not improve from 0.13843\n","Epoch 147/1000\n"," - 1s - loss: 0.6820 - accuracy: 0.5580 - val_loss: 0.1392 - val_accuracy: 2.6326\n","\n","Epoch 00147: val_loss did not improve from 0.13843\n","Epoch 148/1000\n"," - 1s - loss: 0.6818 - accuracy: 0.5579 - val_loss: 0.1392 - val_accuracy: 2.6499\n","\n","Epoch 00148: val_loss did not improve from 0.13843\n","Epoch 149/1000\n"," - 1s - loss: 0.6821 - accuracy: 0.5565 - val_loss: 0.1391 - val_accuracy: 2.6441\n","\n","Epoch 00149: val_loss did not improve from 0.13843\n","Epoch 150/1000\n"," - 1s - loss: 0.6819 - accuracy: 0.5574 - val_loss: 0.1391 - val_accuracy: 2.6326\n","\n","Epoch 00150: val_loss did not improve from 0.13843\n","Epoch 151/1000\n"," - 1s - loss: 0.6820 - accuracy: 0.5550 - val_loss: 0.1391 - val_accuracy: 2.6576\n","\n","Epoch 00151: val_loss did not improve from 0.13843\n","Epoch 152/1000\n"," - 1s - loss: 0.6816 - accuracy: 0.5581 - val_loss: 0.1391 - val_accuracy: 2.6326\n","\n","Epoch 00152: val_loss did not improve from 0.13843\n","Epoch 153/1000\n"," - 1s - loss: 0.6817 - accuracy: 0.5580 - val_loss: 0.1391 - val_accuracy: 2.6480\n","\n","Epoch 00153: val_loss did not improve from 0.13843\n","Epoch 154/1000\n"," - 1s - loss: 0.6814 - accuracy: 0.5577 - val_loss: 0.1391 - val_accuracy: 2.6556\n","\n","Epoch 00154: val_loss did not improve from 0.13843\n","Epoch 155/1000\n"," - 1s - loss: 0.6818 - accuracy: 0.5570 - val_loss: 0.1391 - val_accuracy: 2.6422\n","\n","Epoch 00155: val_loss did not improve from 0.13843\n","Epoch 156/1000\n"," - 1s - loss: 0.6815 - accuracy: 0.5584 - val_loss: 0.1392 - val_accuracy: 2.6422\n","\n","Epoch 00156: val_loss did not improve from 0.13843\n","Epoch 157/1000\n"," - 1s - loss: 0.6818 - accuracy: 0.5557 - val_loss: 0.1391 - val_accuracy: 2.6364\n","\n","Epoch 00157: val_loss did not improve from 0.13843\n","Epoch 158/1000\n"," - 1s - loss: 0.6816 - accuracy: 0.5569 - val_loss: 0.1392 - val_accuracy: 2.6556\n","\n","Epoch 00158: val_loss did not improve from 0.13843\n","Epoch 159/1000\n"," - 1s - loss: 0.6814 - accuracy: 0.5581 - val_loss: 0.1392 - val_accuracy: 2.6614\n","\n","Epoch 00159: val_loss did not improve from 0.13843\n","Epoch 160/1000\n"," - 1s - loss: 0.6816 - accuracy: 0.5580 - val_loss: 0.1392 - val_accuracy: 2.6441\n","\n","Epoch 00160: val_loss did not improve from 0.13843\n","Epoch 161/1000\n"," - 1s - loss: 0.6816 - accuracy: 0.5573 - val_loss: 0.1392 - val_accuracy: 2.6441\n","\n","Epoch 00161: val_loss did not improve from 0.13843\n","Epoch 162/1000\n"," - 1s - loss: 0.6816 - accuracy: 0.5596 - val_loss: 0.1392 - val_accuracy: 2.6480\n","\n","Epoch 00162: val_loss did not improve from 0.13843\n","Epoch 163/1000\n"," - 1s - loss: 0.6815 - accuracy: 0.5576 - val_loss: 0.1392 - val_accuracy: 2.6153\n","\n","Epoch 00163: val_loss did not improve from 0.13843\n","Epoch 164/1000\n"," - 1s - loss: 0.6811 - accuracy: 0.5588 - val_loss: 0.1392 - val_accuracy: 2.6384\n","\n","Epoch 00164: val_loss did not improve from 0.13843\n","Epoch 165/1000\n"," - 1s - loss: 0.6814 - accuracy: 0.5595 - val_loss: 0.1392 - val_accuracy: 2.6441\n","\n","Epoch 00165: val_loss did not improve from 0.13843\n","Epoch 166/1000\n"," - 1s - loss: 0.6814 - accuracy: 0.5591 - val_loss: 0.1392 - val_accuracy: 2.6345\n","\n","Epoch 00166: val_loss did not improve from 0.13843\n","Epoch 167/1000\n"," - 1s - loss: 0.6812 - accuracy: 0.5574 - val_loss: 0.1392 - val_accuracy: 2.6384\n","\n","Epoch 00167: val_loss did not improve from 0.13843\n","Epoch 168/1000\n"," - 1s - loss: 0.6813 - accuracy: 0.5580 - val_loss: 0.1392 - val_accuracy: 2.6211\n","\n","Epoch 00168: val_loss did not improve from 0.13843\n","Epoch 169/1000\n"," - 1s - loss: 0.6814 - accuracy: 0.5591 - val_loss: 0.1392 - val_accuracy: 2.6403\n","\n","Epoch 00169: val_loss did not improve from 0.13843\n","Epoch 170/1000\n"," - 1s - loss: 0.6815 - accuracy: 0.5590 - val_loss: 0.1392 - val_accuracy: 2.6230\n","\n","Epoch 00170: val_loss did not improve from 0.13843\n","Epoch 171/1000\n"," - 1s - loss: 0.6812 - accuracy: 0.5593 - val_loss: 0.1392 - val_accuracy: 2.6441\n","\n","Epoch 00171: val_loss did not improve from 0.13843\n","Epoch 172/1000\n"," - 1s - loss: 0.6810 - accuracy: 0.5607 - val_loss: 0.1392 - val_accuracy: 2.6326\n","\n","Epoch 00172: val_loss did not improve from 0.13843\n","Epoch 173/1000\n"," - 1s - loss: 0.6808 - accuracy: 0.5608 - val_loss: 0.1392 - val_accuracy: 2.6441\n","\n","Epoch 00173: val_loss did not improve from 0.13843\n","Epoch 174/1000\n"," - 1s - loss: 0.6811 - accuracy: 0.5594 - val_loss: 0.1392 - val_accuracy: 2.6211\n","\n","Epoch 00174: val_loss did not improve from 0.13843\n","Epoch 175/1000\n"," - 1s - loss: 0.6811 - accuracy: 0.5606 - val_loss: 0.1392 - val_accuracy: 2.6403\n","\n","Epoch 00175: val_loss did not improve from 0.13843\n","Epoch 176/1000\n"," - 1s - loss: 0.6809 - accuracy: 0.5599 - val_loss: 0.1392 - val_accuracy: 2.6326\n","\n","Epoch 00176: val_loss did not improve from 0.13843\n","Epoch 177/1000\n"," - 1s - loss: 0.6811 - accuracy: 0.5611 - val_loss: 0.1392 - val_accuracy: 2.6326\n","\n","Epoch 00177: val_loss did not improve from 0.13843\n","Epoch 178/1000\n"," - 1s - loss: 0.6807 - accuracy: 0.5610 - val_loss: 0.1392 - val_accuracy: 2.6364\n","\n","Epoch 00178: val_loss did not improve from 0.13843\n","Epoch 179/1000\n"," - 1s - loss: 0.6814 - accuracy: 0.5596 - val_loss: 0.1392 - val_accuracy: 2.6326\n","\n","Epoch 00179: val_loss did not improve from 0.13843\n","Epoch 180/1000\n"," - 1s - loss: 0.6808 - accuracy: 0.5593 - val_loss: 0.1393 - val_accuracy: 2.6287\n","\n","Epoch 00180: val_loss did not improve from 0.13843\n","Epoch 181/1000\n"," - 1s - loss: 0.6808 - accuracy: 0.5601 - val_loss: 0.1392 - val_accuracy: 2.6287\n","\n","Epoch 00181: val_loss did not improve from 0.13843\n","Epoch 182/1000\n"," - 1s - loss: 0.6808 - accuracy: 0.5594 - val_loss: 0.1392 - val_accuracy: 2.6287\n","\n","Epoch 00182: val_loss did not improve from 0.13843\n","Epoch 183/1000\n"," - 1s - loss: 0.6807 - accuracy: 0.5601 - val_loss: 0.1392 - val_accuracy: 2.6364\n","\n","Epoch 00183: val_loss did not improve from 0.13843\n","Epoch 184/1000\n"," - 1s - loss: 0.6809 - accuracy: 0.5613 - val_loss: 0.1392 - val_accuracy: 2.6364\n","\n","Epoch 00184: val_loss did not improve from 0.13843\n","Epoch 185/1000\n"," - 1s - loss: 0.6803 - accuracy: 0.5610 - val_loss: 0.1393 - val_accuracy: 2.6172\n","\n","Epoch 00185: val_loss did not improve from 0.13843\n","Epoch 186/1000\n"," - 1s - loss: 0.6803 - accuracy: 0.5620 - val_loss: 0.1392 - val_accuracy: 2.6287\n","\n","Epoch 00186: val_loss did not improve from 0.13843\n","Epoch 187/1000\n"," - 1s - loss: 0.6806 - accuracy: 0.5612 - val_loss: 0.1393 - val_accuracy: 2.5980\n","\n","Epoch 00187: val_loss did not improve from 0.13843\n","Epoch 188/1000\n"," - 1s - loss: 0.6805 - accuracy: 0.5603 - val_loss: 0.1393 - val_accuracy: 2.6153\n","\n","Epoch 00188: val_loss did not improve from 0.13843\n","Epoch 189/1000\n"," - 1s - loss: 0.6806 - accuracy: 0.5625 - val_loss: 0.1393 - val_accuracy: 2.6076\n","\n","Epoch 00189: val_loss did not improve from 0.13843\n","Epoch 190/1000\n"," - 1s - loss: 0.6805 - accuracy: 0.5628 - val_loss: 0.1394 - val_accuracy: 2.5942\n","\n","Epoch 00190: val_loss did not improve from 0.13843\n","Epoch 191/1000\n"," - 1s - loss: 0.6807 - accuracy: 0.5620 - val_loss: 0.1394 - val_accuracy: 2.5980\n","\n","Epoch 00191: val_loss did not improve from 0.13843\n","Epoch 192/1000\n"," - 1s - loss: 0.6807 - accuracy: 0.5619 - val_loss: 0.1394 - val_accuracy: 2.6018\n","\n","Epoch 00192: val_loss did not improve from 0.13843\n","Epoch 193/1000\n"," - 1s - loss: 0.6804 - accuracy: 0.5635 - val_loss: 0.1394 - val_accuracy: 2.6057\n","\n","Epoch 00193: val_loss did not improve from 0.13843\n","Epoch 194/1000\n"," - 1s - loss: 0.6803 - accuracy: 0.5615 - val_loss: 0.1394 - val_accuracy: 2.5922\n","\n","Epoch 00194: val_loss did not improve from 0.13843\n","Epoch 195/1000\n"," - 1s - loss: 0.6804 - accuracy: 0.5616 - val_loss: 0.1393 - val_accuracy: 2.6153\n","\n","Epoch 00195: val_loss did not improve from 0.13843\n","Epoch 196/1000\n"," - 1s - loss: 0.6806 - accuracy: 0.5626 - val_loss: 0.1394 - val_accuracy: 2.5999\n","\n","Epoch 00196: val_loss did not improve from 0.13843\n","Epoch 197/1000\n"," - 1s - loss: 0.6800 - accuracy: 0.5636 - val_loss: 0.1394 - val_accuracy: 2.5942\n","\n","Epoch 00197: val_loss did not improve from 0.13843\n","Epoch 198/1000\n"," - 1s - loss: 0.6803 - accuracy: 0.5626 - val_loss: 0.1393 - val_accuracy: 2.6095\n","\n","Epoch 00198: val_loss did not improve from 0.13843\n","Epoch 199/1000\n"," - 1s - loss: 0.6805 - accuracy: 0.5635 - val_loss: 0.1393 - val_accuracy: 2.6364\n","\n","Epoch 00199: val_loss did not improve from 0.13843\n","Epoch 200/1000\n"," - 1s - loss: 0.6800 - accuracy: 0.5643 - val_loss: 0.1393 - val_accuracy: 2.6211\n","\n","Epoch 00200: val_loss did not improve from 0.13843\n","Epoch 201/1000\n"," - 1s - loss: 0.6801 - accuracy: 0.5637 - val_loss: 0.1393 - val_accuracy: 2.6153\n","\n","Epoch 00201: val_loss did not improve from 0.13843\n","Epoch 202/1000\n"," - 1s - loss: 0.6803 - accuracy: 0.5623 - val_loss: 0.1393 - val_accuracy: 2.6172\n","\n","Epoch 00202: val_loss did not improve from 0.13843\n","Epoch 203/1000\n"," - 1s - loss: 0.6803 - accuracy: 0.5622 - val_loss: 0.1394 - val_accuracy: 2.6018\n","\n","Epoch 00203: val_loss did not improve from 0.13843\n","Epoch 204/1000\n"," - 1s - loss: 0.6799 - accuracy: 0.5640 - val_loss: 0.1394 - val_accuracy: 2.6038\n","\n","Epoch 00204: val_loss did not improve from 0.13843\n","Epoch 205/1000\n"," - 1s - loss: 0.6799 - accuracy: 0.5629 - val_loss: 0.1394 - val_accuracy: 2.5865\n","\n","Epoch 00205: val_loss did not improve from 0.13843\n","Epoch 206/1000\n"," - 1s - loss: 0.6801 - accuracy: 0.5632 - val_loss: 0.1394 - val_accuracy: 2.5788\n","\n","Epoch 00206: val_loss did not improve from 0.13843\n","Epoch 207/1000\n"," - 1s - loss: 0.6801 - accuracy: 0.5652 - val_loss: 0.1394 - val_accuracy: 2.6095\n","\n","Epoch 00207: val_loss did not improve from 0.13843\n","Epoch 208/1000\n"," - 1s - loss: 0.6799 - accuracy: 0.5646 - val_loss: 0.1395 - val_accuracy: 2.5769\n","\n","Epoch 00208: val_loss did not improve from 0.13843\n","Epoch 209/1000\n"," - 1s - loss: 0.6798 - accuracy: 0.5638 - val_loss: 0.1394 - val_accuracy: 2.5961\n","\n","Epoch 00209: val_loss did not improve from 0.13843\n","Epoch 210/1000\n"," - 1s - loss: 0.6797 - accuracy: 0.5642 - val_loss: 0.1394 - val_accuracy: 2.6076\n","\n","Epoch 00210: val_loss did not improve from 0.13843\n","Epoch 211/1000\n"," - 1s - loss: 0.6796 - accuracy: 0.5634 - val_loss: 0.1394 - val_accuracy: 2.5980\n","\n","Epoch 00211: val_loss did not improve from 0.13843\n","Epoch 212/1000\n"," - 1s - loss: 0.6798 - accuracy: 0.5642 - val_loss: 0.1394 - val_accuracy: 2.6057\n","\n","Epoch 00212: val_loss did not improve from 0.13843\n","Epoch 213/1000\n"," - 1s - loss: 0.6798 - accuracy: 0.5634 - val_loss: 0.1395 - val_accuracy: 2.5884\n","\n","Epoch 00213: val_loss did not improve from 0.13843\n","Epoch 214/1000\n"," - 1s - loss: 0.6796 - accuracy: 0.5656 - val_loss: 0.1394 - val_accuracy: 2.5980\n","\n","Epoch 00214: val_loss did not improve from 0.13843\n","Epoch 215/1000\n"," - 1s - loss: 0.6799 - accuracy: 0.5644 - val_loss: 0.1395 - val_accuracy: 2.6076\n","\n","Epoch 00215: val_loss did not improve from 0.13843\n","Epoch 216/1000\n"," - 1s - loss: 0.6797 - accuracy: 0.5640 - val_loss: 0.1393 - val_accuracy: 2.6211\n","\n","Epoch 00216: val_loss did not improve from 0.13843\n","Epoch 217/1000\n"," - 1s - loss: 0.6798 - accuracy: 0.5634 - val_loss: 0.1394 - val_accuracy: 2.6038\n","\n","Epoch 00217: val_loss did not improve from 0.13843\n","Epoch 218/1000\n"," - 1s - loss: 0.6795 - accuracy: 0.5652 - val_loss: 0.1394 - val_accuracy: 2.6076\n","\n","Epoch 00218: val_loss did not improve from 0.13843\n","Epoch 219/1000\n"," - 1s - loss: 0.6798 - accuracy: 0.5651 - val_loss: 0.1394 - val_accuracy: 2.6211\n","\n","Epoch 00219: val_loss did not improve from 0.13843\n","Epoch 220/1000\n"," - 1s - loss: 0.6799 - accuracy: 0.5631 - val_loss: 0.1393 - val_accuracy: 2.5999\n","\n","Epoch 00220: val_loss did not improve from 0.13843\n","Epoch 221/1000\n"," - 1s - loss: 0.6794 - accuracy: 0.5660 - val_loss: 0.1393 - val_accuracy: 2.6153\n","\n","Epoch 00221: val_loss did not improve from 0.13843\n","Epoch 222/1000\n"," - 1s - loss: 0.6792 - accuracy: 0.5665 - val_loss: 0.1393 - val_accuracy: 2.6038\n","\n","Epoch 00222: val_loss did not improve from 0.13843\n","Epoch 223/1000\n"," - 1s - loss: 0.6793 - accuracy: 0.5657 - val_loss: 0.1394 - val_accuracy: 2.6211\n","\n","Epoch 00223: val_loss did not improve from 0.13843\n","Epoch 224/1000\n"," - 1s - loss: 0.6795 - accuracy: 0.5656 - val_loss: 0.1394 - val_accuracy: 2.6095\n","\n","Epoch 00224: val_loss did not improve from 0.13843\n","Epoch 225/1000\n"," - 1s - loss: 0.6794 - accuracy: 0.5666 - val_loss: 0.1394 - val_accuracy: 2.6134\n","\n","Epoch 00225: val_loss did not improve from 0.13843\n","Epoch 226/1000\n"," - 1s - loss: 0.6793 - accuracy: 0.5660 - val_loss: 0.1394 - val_accuracy: 2.6018\n","\n","Epoch 00226: val_loss did not improve from 0.13843\n","Epoch 227/1000\n"," - 1s - loss: 0.6794 - accuracy: 0.5657 - val_loss: 0.1394 - val_accuracy: 2.5980\n","\n","Epoch 00227: val_loss did not improve from 0.13843\n","Epoch 228/1000\n"," - 1s - loss: 0.6793 - accuracy: 0.5659 - val_loss: 0.1394 - val_accuracy: 2.5884\n","\n","Epoch 00228: val_loss did not improve from 0.13843\n","Epoch 229/1000\n"," - 1s - loss: 0.6792 - accuracy: 0.5662 - val_loss: 0.1395 - val_accuracy: 2.5980\n","\n","Epoch 00229: val_loss did not improve from 0.13843\n","Epoch 230/1000\n"," - 1s - loss: 0.6793 - accuracy: 0.5655 - val_loss: 0.1395 - val_accuracy: 2.6153\n","\n","Epoch 00230: val_loss did not improve from 0.13843\n","Epoch 231/1000\n"," - 1s - loss: 0.6794 - accuracy: 0.5666 - val_loss: 0.1394 - val_accuracy: 2.5999\n","\n","Epoch 00231: val_loss did not improve from 0.13843\n","Epoch 232/1000\n"," - 1s - loss: 0.6788 - accuracy: 0.5674 - val_loss: 0.1394 - val_accuracy: 2.6038\n","\n","Epoch 00232: val_loss did not improve from 0.13843\n","Epoch 233/1000\n"," - 1s - loss: 0.6792 - accuracy: 0.5654 - val_loss: 0.1394 - val_accuracy: 2.6134\n","\n","Epoch 00233: val_loss did not improve from 0.13843\n","Epoch 234/1000\n"," - 1s - loss: 0.6791 - accuracy: 0.5666 - val_loss: 0.1394 - val_accuracy: 2.6115\n","\n","Epoch 00234: val_loss did not improve from 0.13843\n","Epoch 235/1000\n"," - 1s - loss: 0.6790 - accuracy: 0.5677 - val_loss: 0.1395 - val_accuracy: 2.5961\n","\n","Epoch 00235: val_loss did not improve from 0.13843\n","Epoch 236/1000\n"," - 1s - loss: 0.6791 - accuracy: 0.5668 - val_loss: 0.1395 - val_accuracy: 2.5942\n","\n","Epoch 00236: val_loss did not improve from 0.13843\n","Epoch 237/1000\n"," - 1s - loss: 0.6792 - accuracy: 0.5661 - val_loss: 0.1395 - val_accuracy: 2.5980\n","\n","Epoch 00237: val_loss did not improve from 0.13843\n","Epoch 238/1000\n"," - 1s - loss: 0.6791 - accuracy: 0.5687 - val_loss: 0.1395 - val_accuracy: 2.5961\n","\n","Epoch 00238: val_loss did not improve from 0.13843\n","Epoch 239/1000\n"," - 1s - loss: 0.6791 - accuracy: 0.5661 - val_loss: 0.1395 - val_accuracy: 2.6172\n","\n","Epoch 00239: val_loss did not improve from 0.13843\n","Epoch 240/1000\n"," - 1s - loss: 0.6788 - accuracy: 0.5679 - val_loss: 0.1394 - val_accuracy: 2.5980\n","\n","Epoch 00240: val_loss did not improve from 0.13843\n","Epoch 241/1000\n"," - 1s - loss: 0.6788 - accuracy: 0.5680 - val_loss: 0.1394 - val_accuracy: 2.6172\n","\n","Epoch 00241: val_loss did not improve from 0.13843\n","Epoch 242/1000\n"," - 1s - loss: 0.6785 - accuracy: 0.5686 - val_loss: 0.1394 - val_accuracy: 2.6095\n","\n","Epoch 00242: val_loss did not improve from 0.13843\n","Epoch 243/1000\n"," - 1s - loss: 0.6787 - accuracy: 0.5690 - val_loss: 0.1396 - val_accuracy: 2.5942\n","\n","Epoch 00243: val_loss did not improve from 0.13843\n","Epoch 244/1000\n"," - 1s - loss: 0.6787 - accuracy: 0.5670 - val_loss: 0.1395 - val_accuracy: 2.5922\n","\n","Epoch 00244: val_loss did not improve from 0.13843\n","Epoch 245/1000\n"," - 1s - loss: 0.6785 - accuracy: 0.5685 - val_loss: 0.1395 - val_accuracy: 2.5980\n","\n","Epoch 00245: val_loss did not improve from 0.13843\n","Epoch 246/1000\n"," - 1s - loss: 0.6787 - accuracy: 0.5682 - val_loss: 0.1395 - val_accuracy: 2.5884\n","\n","Epoch 00246: val_loss did not improve from 0.13843\n","Epoch 247/1000\n"," - 1s - loss: 0.6788 - accuracy: 0.5688 - val_loss: 0.1395 - val_accuracy: 2.6018\n","\n","Epoch 00247: val_loss did not improve from 0.13843\n","Epoch 248/1000\n"," - 1s - loss: 0.6786 - accuracy: 0.5676 - val_loss: 0.1395 - val_accuracy: 2.5961\n","\n","Epoch 00248: val_loss did not improve from 0.13843\n","Epoch 249/1000\n"," - 1s - loss: 0.6785 - accuracy: 0.5696 - val_loss: 0.1395 - val_accuracy: 2.5884\n","\n","Epoch 00249: val_loss did not improve from 0.13843\n","Epoch 250/1000\n"," - 1s - loss: 0.6787 - accuracy: 0.5687 - val_loss: 0.1395 - val_accuracy: 2.5922\n","\n","Epoch 00250: val_loss did not improve from 0.13843\n","Epoch 251/1000\n"," - 1s - loss: 0.6786 - accuracy: 0.5691 - val_loss: 0.1395 - val_accuracy: 2.5922\n","\n","Epoch 00251: val_loss did not improve from 0.13843\n","Epoch 252/1000\n"," - 1s - loss: 0.6785 - accuracy: 0.5683 - val_loss: 0.1395 - val_accuracy: 2.6211\n","\n","Epoch 00252: val_loss did not improve from 0.13843\n","Epoch 253/1000\n"," - 1s - loss: 0.6784 - accuracy: 0.5689 - val_loss: 0.1395 - val_accuracy: 2.6115\n","\n","Epoch 00253: val_loss did not improve from 0.13843\n","Epoch 254/1000\n"," - 1s - loss: 0.6785 - accuracy: 0.5690 - val_loss: 0.1395 - val_accuracy: 2.6191\n","\n","Epoch 00254: val_loss did not improve from 0.13843\n","Epoch 255/1000\n"," - 1s - loss: 0.6786 - accuracy: 0.5680 - val_loss: 0.1395 - val_accuracy: 2.5884\n","\n","Epoch 00255: val_loss did not improve from 0.13843\n","Epoch 256/1000\n"," - 1s - loss: 0.6786 - accuracy: 0.5688 - val_loss: 0.1396 - val_accuracy: 2.5711\n","\n","Epoch 00256: val_loss did not improve from 0.13843\n","Epoch 257/1000\n"," - 1s - loss: 0.6788 - accuracy: 0.5670 - val_loss: 0.1396 - val_accuracy: 2.5807\n","\n","Epoch 00257: val_loss did not improve from 0.13843\n","Epoch 258/1000\n"," - 1s - loss: 0.6784 - accuracy: 0.5682 - val_loss: 0.1397 - val_accuracy: 2.5730\n","\n","Epoch 00258: val_loss did not improve from 0.13843\n","Epoch 259/1000\n"," - 1s - loss: 0.6785 - accuracy: 0.5684 - val_loss: 0.1396 - val_accuracy: 2.5711\n","\n","Epoch 00259: val_loss did not improve from 0.13843\n","Epoch 260/1000\n"," - 1s - loss: 0.6782 - accuracy: 0.5689 - val_loss: 0.1396 - val_accuracy: 2.5922\n","\n","Epoch 00260: val_loss did not improve from 0.13843\n","Epoch 261/1000\n"," - 1s - loss: 0.6779 - accuracy: 0.5709 - val_loss: 0.1395 - val_accuracy: 2.5999\n","\n","Epoch 00261: val_loss did not improve from 0.13843\n","Epoch 262/1000\n"," - 1s - loss: 0.6780 - accuracy: 0.5708 - val_loss: 0.1396 - val_accuracy: 2.5903\n","\n","Epoch 00262: val_loss did not improve from 0.13843\n","Epoch 263/1000\n"," - 1s - loss: 0.6782 - accuracy: 0.5701 - val_loss: 0.1396 - val_accuracy: 2.5961\n","\n","Epoch 00263: val_loss did not improve from 0.13843\n","Epoch 264/1000\n"," - 1s - loss: 0.6783 - accuracy: 0.5692 - val_loss: 0.1395 - val_accuracy: 2.6038\n","\n","Epoch 00264: val_loss did not improve from 0.13843\n","Epoch 265/1000\n"," - 1s - loss: 0.6780 - accuracy: 0.5712 - val_loss: 0.1395 - val_accuracy: 2.6038\n","\n","Epoch 00265: val_loss did not improve from 0.13843\n","Epoch 266/1000\n"," - 1s - loss: 0.6783 - accuracy: 0.5687 - val_loss: 0.1395 - val_accuracy: 2.6018\n","\n","Epoch 00266: val_loss did not improve from 0.13843\n","Epoch 267/1000\n"," - 1s - loss: 0.6777 - accuracy: 0.5695 - val_loss: 0.1395 - val_accuracy: 2.6076\n","\n","Epoch 00267: val_loss did not improve from 0.13843\n","Epoch 268/1000\n"," - 1s - loss: 0.6780 - accuracy: 0.5686 - val_loss: 0.1395 - val_accuracy: 2.5903\n","\n","Epoch 00268: val_loss did not improve from 0.13843\n","Epoch 269/1000\n"," - 1s - loss: 0.6779 - accuracy: 0.5704 - val_loss: 0.1395 - val_accuracy: 2.6076\n","\n","Epoch 00269: val_loss did not improve from 0.13843\n","Epoch 270/1000\n"," - 1s - loss: 0.6782 - accuracy: 0.5683 - val_loss: 0.1395 - val_accuracy: 2.6057\n","\n","Epoch 00270: val_loss did not improve from 0.13843\n","Epoch 271/1000\n"," - 1s - loss: 0.6780 - accuracy: 0.5701 - val_loss: 0.1395 - val_accuracy: 2.6191\n","\n","Epoch 00271: val_loss did not improve from 0.13843\n","Epoch 272/1000\n"," - 1s - loss: 0.6778 - accuracy: 0.5719 - val_loss: 0.1396 - val_accuracy: 2.5999\n","\n","Epoch 00272: val_loss did not improve from 0.13843\n","Epoch 273/1000\n"," - 1s - loss: 0.6778 - accuracy: 0.5713 - val_loss: 0.1396 - val_accuracy: 2.5961\n","\n","Epoch 00273: val_loss did not improve from 0.13843\n","Epoch 274/1000\n"," - 1s - loss: 0.6777 - accuracy: 0.5722 - val_loss: 0.1396 - val_accuracy: 2.5865\n","\n","Epoch 00274: val_loss did not improve from 0.13843\n","Epoch 275/1000\n"," - 1s - loss: 0.6779 - accuracy: 0.5690 - val_loss: 0.1395 - val_accuracy: 2.6057\n","\n","Epoch 00275: val_loss did not improve from 0.13843\n","Epoch 276/1000\n"," - 1s - loss: 0.6775 - accuracy: 0.5718 - val_loss: 0.1396 - val_accuracy: 2.6018\n","\n","Epoch 00276: val_loss did not improve from 0.13843\n","Epoch 277/1000\n"," - 1s - loss: 0.6779 - accuracy: 0.5695 - val_loss: 0.1396 - val_accuracy: 2.6018\n","\n","Epoch 00277: val_loss did not improve from 0.13843\n","Epoch 278/1000\n"," - 1s - loss: 0.6777 - accuracy: 0.5715 - val_loss: 0.1396 - val_accuracy: 2.6038\n","\n","Epoch 00278: val_loss did not improve from 0.13843\n","Epoch 279/1000\n"," - 1s - loss: 0.6777 - accuracy: 0.5708 - val_loss: 0.1396 - val_accuracy: 2.5903\n","\n","Epoch 00279: val_loss did not improve from 0.13843\n","Epoch 280/1000\n"," - 1s - loss: 0.6776 - accuracy: 0.5712 - val_loss: 0.1396 - val_accuracy: 2.5961\n","\n","Epoch 00280: val_loss did not improve from 0.13843\n","Epoch 281/1000\n"," - 1s - loss: 0.6779 - accuracy: 0.5700 - val_loss: 0.1396 - val_accuracy: 2.5980\n","\n","Epoch 00281: val_loss did not improve from 0.13843\n","Epoch 282/1000\n"," - 1s - loss: 0.6777 - accuracy: 0.5707 - val_loss: 0.1396 - val_accuracy: 2.6115\n","\n","Epoch 00282: val_loss did not improve from 0.13843\n","Epoch 283/1000\n"," - 1s - loss: 0.6773 - accuracy: 0.5727 - val_loss: 0.1397 - val_accuracy: 2.6018\n","\n","Epoch 00283: val_loss did not improve from 0.13843\n","Epoch 284/1000\n"," - 1s - loss: 0.6775 - accuracy: 0.5721 - val_loss: 0.1396 - val_accuracy: 2.5999\n","\n","Epoch 00284: val_loss did not improve from 0.13843\n","Epoch 285/1000\n"," - 1s - loss: 0.6774 - accuracy: 0.5738 - val_loss: 0.1397 - val_accuracy: 2.5903\n","\n","Epoch 00285: val_loss did not improve from 0.13843\n","Epoch 286/1000\n"," - 1s - loss: 0.6771 - accuracy: 0.5725 - val_loss: 0.1396 - val_accuracy: 2.6326\n","\n","Epoch 00286: val_loss did not improve from 0.13843\n","Epoch 287/1000\n"," - 1s - loss: 0.6773 - accuracy: 0.5717 - val_loss: 0.1396 - val_accuracy: 2.6038\n","\n","Epoch 00287: val_loss did not improve from 0.13843\n","Epoch 288/1000\n"," - 1s - loss: 0.6774 - accuracy: 0.5726 - val_loss: 0.1397 - val_accuracy: 2.5980\n","\n","Epoch 00288: val_loss did not improve from 0.13843\n","Epoch 289/1000\n"," - 1s - loss: 0.6772 - accuracy: 0.5727 - val_loss: 0.1398 - val_accuracy: 2.5903\n","\n","Epoch 00289: val_loss did not improve from 0.13843\n","Epoch 290/1000\n"," - 1s - loss: 0.6774 - accuracy: 0.5716 - val_loss: 0.1398 - val_accuracy: 2.5673\n","\n","Epoch 00290: val_loss did not improve from 0.13843\n","Epoch 291/1000\n"," - 1s - loss: 0.6774 - accuracy: 0.5716 - val_loss: 0.1398 - val_accuracy: 2.5846\n","\n","Epoch 00291: val_loss did not improve from 0.13843\n","Epoch 292/1000\n"," - 1s - loss: 0.6771 - accuracy: 0.5730 - val_loss: 0.1398 - val_accuracy: 2.5711\n","\n","Epoch 00292: val_loss did not improve from 0.13843\n","Epoch 293/1000\n"," - 1s - loss: 0.6772 - accuracy: 0.5726 - val_loss: 0.1398 - val_accuracy: 2.5788\n","\n","Epoch 00293: val_loss did not improve from 0.13843\n","Epoch 294/1000\n"," - 1s - loss: 0.6773 - accuracy: 0.5714 - val_loss: 0.1399 - val_accuracy: 2.5653\n","\n","Epoch 00294: val_loss did not improve from 0.13843\n","Epoch 295/1000\n"," - 1s - loss: 0.6768 - accuracy: 0.5735 - val_loss: 0.1399 - val_accuracy: 2.5692\n","\n","Epoch 00295: val_loss did not improve from 0.13843\n","Epoch 296/1000\n"," - 1s - loss: 0.6770 - accuracy: 0.5725 - val_loss: 0.1399 - val_accuracy: 2.5692\n","\n","Epoch 00296: val_loss did not improve from 0.13843\n","Epoch 297/1000\n"," - 1s - loss: 0.6771 - accuracy: 0.5740 - val_loss: 0.1399 - val_accuracy: 2.5442\n","\n","Epoch 00297: val_loss did not improve from 0.13843\n","Epoch 298/1000\n"," - 1s - loss: 0.6769 - accuracy: 0.5728 - val_loss: 0.1399 - val_accuracy: 2.5749\n","\n","Epoch 00298: val_loss did not improve from 0.13843\n","Epoch 299/1000\n"," - 1s - loss: 0.6770 - accuracy: 0.5726 - val_loss: 0.1399 - val_accuracy: 2.5903\n","\n","Epoch 00299: val_loss did not improve from 0.13843\n","Epoch 300/1000\n"," - 1s - loss: 0.6771 - accuracy: 0.5731 - val_loss: 0.1398 - val_accuracy: 2.5846\n","\n","Epoch 00300: val_loss did not improve from 0.13843\n","Epoch 301/1000\n"," - 1s - loss: 0.6767 - accuracy: 0.5753 - val_loss: 0.1398 - val_accuracy: 2.5961\n","\n","Epoch 00301: val_loss did not improve from 0.13843\n","Epoch 302/1000\n"," - 1s - loss: 0.6770 - accuracy: 0.5737 - val_loss: 0.1398 - val_accuracy: 2.6038\n","\n","Epoch 00302: val_loss did not improve from 0.13843\n","Epoch 303/1000\n"," - 1s - loss: 0.6765 - accuracy: 0.5737 - val_loss: 0.1398 - val_accuracy: 2.5788\n","\n","Epoch 00303: val_loss did not improve from 0.13843\n","Epoch 304/1000\n"," - 1s - loss: 0.6769 - accuracy: 0.5740 - val_loss: 0.1399 - val_accuracy: 2.5846\n","\n","Epoch 00304: val_loss did not improve from 0.13843\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"KIk0rYDvKfse"},"source":["### load model"]},{"cell_type":"code","metadata":{"id":"3jwBpTS0Kgt0"},"source":["ckpt_path = current_path + 'ckpt/'\n","board_path = current_path + 'graph/'\n","\n","# model_name = 'classifier_%s_lstm_close_updown_pr_robust_trial_%s_timesplit.h5' % (period, symbol_name)\n","\n","model = keras.models.load_model(ckpt_path + model_name)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cPz7DZxeJc4P"},"source":["### test"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":838},"id":"baSlNaUxJd2r","executionInfo":{"status":"ok","timestamp":1621943481236,"user_tz":-540,"elapsed":7907,"user":{"displayName":"7th June","photoUrl":"","userId":"08178289703395036410"}},"outputId":"c1716967-797f-4eac-89af-263bf31b0f4c"},"source":["# x_test = new_input_x\n","# y_test = np.where(new_input_pr > 1, 1, 0)\n","# pr_test = new_input_pr\n","\n","# x_test = org_x_test\n","# y_test = org_y_test\n","# pr_test = org_pr_test\n","\n","# x_test = x_train\n","# y_test = y_train\n","# pr_test = pr_train\n","\n","# x_test = x_val\n","# y_test = y_val\n","# pr_test = pr_val\n","\n","# x_test = concat_x\n","# y_test = np.where(concat_pr > 1, 1, 0)\n","# pr_test = concat_pr\n","\n","test_result = model.predict(x_test)\n","# test_result = model.predict(test_set)\n","\n","print('test_result.shape :', test_result.shape)\n","# print('pr_val.shape :', pr_val.shape)\n","\n","y_score = test_result[:, [1]]\n","print('y_test[:5] :', y_test.reshape(-1,)[:5])\n","# print('np.unique(y_test) :', np.unique(y_test, return_counts=True))\n","print('y_score[:5] :', y_score[:5])\n","# print('np.unique(y_score) :', np.unique(y_score, return_counts=True))\n","\n","print('y_test.shape :', y_test.shape)\n","print('y_score.shape :', y_score.shape)\n","\n","print('len(y_test) :', len(y_test))\n","\n","#     precision recall curve   #\n","precision, recall, threshold = precision_recall_curve(y_test, y_score)\n","precision, recall = precision[:-1], recall[:-1]\n","\n","plt.plot(threshold, precision, label='precision')\n","plt.plot(threshold, recall, label='recall')\n","plt.legend()\n","plt.title('precision recall')\n","plt.show()\n","# print(y_pred)\n","\n","\n","# threshold = [0.65]\n","# print('threshold :', threshold)\n","# break\n","\n","acc_pr_bythr = []\n","new_thresh = []\n","\n","for thresh in threshold:\n","  \n","  # if thresh < 0.5:\n","  #     continue\n","\n","  y_pred = np.where(y_score[:, -1] > thresh, 1, 0)\n","  # print('y_pred.shape :', y_pred.shape)\n","  # print('y_pred :', y_pred)\n","\n","  #     compare precision     #\n","\n","  # print('precision :', precision_score(y_test, y_pred))\n","  # print('recall :', recall_score(y_test, y_pred))\n","  # print()\n","\n","  # print('np.isnan(np.sum(x_test)) :', np.isnan(np.sum(x_test)))\n","  # print('np.isnan(np.sum(y_test)) :', np.isnan(np.sum(y_test)))\n","\n","  # # plot_confusion_matrix(best_model, x_test, y_test, normalize=None)\n","  # # plt.show()  \n","  # print()\n","\n","  #     check win-ratio improvement     #\n","  cmat = confusion_matrix(y_test, y_pred)\n","  # print(cmat)\n","  # print(np.sum(cmat, axis=1))\n","\n","  test_size = len(y_test)\n","  test_pr_list = pr_test\n","  # print('origin ac_pr :', np.cumprod(test_pr_list)[-1])\n","\n","  org_wr = np.sum(cmat, axis=1)[-1] / sum(np.sum(cmat, axis=1))\n","  ml_wr = cmat[1][1] / np.sum(cmat, axis=0)[-1]\n","  # print('win ratio improvement %.2f --> %.2f' % (org_wr, ml_wr))\n","\n","  # print('pr_test.shape :', pr_test.shape)\n","\n","  # print(y_pred)\n","  # print(test_pr_list)\n","\n","  pred_pr_list = np.where(y_pred == 1, test_pr_list.reshape(-1, ), 1.0)\n","  # pred_pr_list = np.where(np.isnan(pred_pr_list), 1.0, pred_pr_list)\n","  # pred_pr_list = np.where(pred_pr_list == 0.0, 1.0, pred_pr_list)\n","  # print('pred_pr_list.shape :', pred_pr_list.shape)\n","\n","  # if np.cumprod(test_pr_list)[-1] < np.cumprod(pred_pr_list)[-1]:\n","  #   print('accum_pr increased ! : %.3f --> %.3f' % (np.cumprod(test_pr_list)[-1], np.cumprod(pred_pr_list)[-1]))\n","  #   print('thresh :', thresh)\n","    \n","  # if len(threshold) == 1:\n","#   plt.figure(figsize=(10, 5))\n","#   plt.subplot(121)\n","#   plt.plot(np.cumprod(test_pr_list))\n","#   plt.title('%.3f' % (np.cumprod(test_pr_list)[-1]))\n","# # plt.show()\n","\n","#   plt.subplot(122)\n","#   plt.plot(np.cumprod(pred_pr_list))\n","#   plt.title('%.3f' % (np.cumprod(pred_pr_list)[-1]))\n","#   # plt.axvline(len(org_pr_test), linestyle='--', color='r')\n","#   plt.show()\n","\n","\n","  acc_pr_bythr.append(np.cumprod(pred_pr_list)[-1])\n","  new_thresh.append(thresh)\n","\n","\n","print('acc_pr_bythr :', acc_pr_bythr)\n","\n","plt.figure(figsize=(10, 5))\n","plt.subplot(121)\n","plt.plot(threshold, precision, label='precision')\n","plt.plot(threshold, recall, label='recall')\n","plt.legend()\n","plt.title('precision recall')\n","# plt.show()\n","plt.subplot(122)\n","plt.plot(new_thresh, acc_pr_bythr)\n","plt.axhline(np.cumprod(test_pr_list)[-1], linestyle='--', color='r')\n","plt.title(symbol_name)\n","plt.show()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["test_result.shape : (2603, 2)\n","y_test[:5] : [1 0 0 0 1]\n","y_score[:5] : [[0.42088738]\n"," [0.5014606 ]\n"," [0.4910003 ]\n"," [0.51449823]\n"," [0.4367385 ]]\n","y_test.shape : (2603, 1)\n","y_score.shape : (2603, 1)\n","len(y_test) : 2603\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:89: RuntimeWarning: invalid value encountered in long_scalars\n"],"name":"stderr"},{"output_type":"stream","text":["acc_pr_bythr : [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 3.7032485, 3.2765114, 3.1270857, 3.2260287, 3.465345, 3.3623445, 3.3198562, 3.280708, 3.1728182, 3.0980055, 3.1455245, 3.307006, 3.3948565, 2.9057145, 3.0631065, 2.953962, 2.9628494, 3.0692441, 2.7523713, 2.8060627, 2.7972806, 2.7685962, 2.8319967, 3.176758, 3.1670341, 3.0853467, 3.363135, 3.2380266, 3.1386585, 3.283307, 3.8936365, 3.6079721, 4.8327494, 4.8302503, 5.388807, 5.4050226, 5.5010657, 4.320297, 4.170579, 4.2277136, 4.079335, 4.140118, 3.9487543, 3.7488704, 3.6627836, 3.582028, 3.5563185, 3.2920084, 3.0666418, 2.8506887, 2.713311, 2.4784405, 2.243222, 2.1376734, 2.1057959, 1.7450807, 1.6853168, 1.6403815, 1.6223221, 1.669257, 1.5715758, 1.3977066, 1.6139226, 1.5690964, 1.5041066, 1.4599171, 1.2055184, 1.2231964, 1.2799335, 1.1823974, 1.3287609, 1.2606635, 1.2057, 1.3078759, 1.1773114, 1.2257504, 1.0846583, 1.0]\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 720x360 with 2 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"code","metadata":{"id":"mVDwWWvE45Wr"},"source":["plt.subplot(122)\n","plt.plot(new_thresh, acc_pr_bythr)\n","plt.axhline(np.cumprod(test_pr_list)[-1], linestyle='--', color='r')\n","plt.axvline(new_thresh[np.argmax(acc_pr_bythr)], linestyle='--', color='r')\n","plt.title(symbol_name + '\\n' + str(new_thresh[np.argmax(acc_pr_bythr)]))\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":265},"id":"zejEkNZI4Kyv","executionInfo":{"status":"ok","timestamp":1620614941396,"user_tz":-540,"elapsed":1104,"user":{"displayName":"7th June","photoUrl":"","userId":"08178289703395036410"}},"outputId":"80ef829a-8d2e-468d-afb7-8a3423d979ef"},"source":["thresh = 0.5866\n","y_pred = np.where(y_score[:600, -1] > thresh, 1, 0)\n","# print('y_pred.shape :', y_pred.shape)\n","# print('y_pred :', y_pred)\n","\n","#     compare precision     #\n","\n","# print('precision :', precision_score(y_test, y_pred))\n","# print('recall :', recall_score(y_test, y_pred))\n","# print()\n","\n","# print('np.isnan(np.sum(x_test)) :', np.isnan(np.sum(x_test)))\n","# print('np.isnan(np.sum(y_test)) :', np.isnan(np.sum(y_test)))\n","\n","# # plot_confusion_matrix(best_model, x_test, y_test, normalize=None)\n","# # plt.show()  \n","# print()\n","\n","#     check win-ratio improvement     #\n","cmat = confusion_matrix(y_test[:len(y_pred)], y_pred)\n","# print(cmat)\n","# print(np.sum(cmat, axis=1))\n","\n","test_size = len(y_test[:len(y_pred)])\n","test_pr_list = pr_test[:len(y_pred)]\n","# print('origin ac_pr :', np.cumprod(test_pr_list)[-1])\n","\n","org_wr = np.sum(cmat, axis=1)[-1] / sum(np.sum(cmat, axis=1))\n","ml_wr = cmat[1][1] / np.sum(cmat, axis=0)[-1]\n","# print('win ratio improvement %.2f --> %.2f' % (org_wr, ml_wr))\n","\n","# print('pr_test.shape :', pr_test.shape)\n","\n","# print(y_pred)\n","# print(test_pr_list)\n","pred_pr_list = np.where(y_pred == 1, test_pr_list.reshape(-1, ), 1.0)\n","pred_pr_list = np.where(np.isnan(pred_pr_list), 1.0, pred_pr_list)\n","pred_pr_list = np.where(pred_pr_list == 0.0, 1.0, pred_pr_list)\n","\n","plt.plot(np.cumprod(pred_pr_list))\n","plt.show()"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"code","metadata":{"id":"QYwWJxCSLF62"},"source":["min_max = MinMaxScaler()\n","cbo1 = min_max.fit_transform(cbo.values.reshape(-1, 1))\n","bbw1 = min_max.fit_transform(bbw.values.reshape(-1, 1))\n","\n","min_max = MinMaxScaler()\n","bbw2 = min_max.fit_transform(bbw.values.reshape(-1, 1))\n","\n","print(bbw1[-10:])\n","print(bbw2[-10:])\n","plt.plot(bbw1)\n","plt.plot(bbw2)\n","\n","plt.show()\n","\n","print(len(bbw1))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6XibtKgphXyQ"},"source":["### Check shuffled index"]},{"cell_type":"code","metadata":{"id":"KH8eEW8ChZtV","colab":{"base_uri":"https://localhost:8080/","height":244},"executionInfo":{"status":"error","timestamp":1621939962590,"user_tz":-540,"elapsed":415,"user":{"displayName":"7th June","photoUrl":"","userId":"08178289703395036410"}},"outputId":"4e77ff24-c30b-4b2d-d516-cdb6b60466b1"},"source":["# print(index_val)\n","from datetime import datetime\n","\n","# print(index_test)\n","# print(index_train)\n","# print(index_val)\n","total_stamp = list(map(lambda x: datetime.timestamp(x[0]), input_index)) \n","timestamp_train = list(map(lambda x: datetime.timestamp(x[0]), index_train)) \n","timestamp_val = list(map(lambda x: datetime.timestamp(x[0]), index_val)) \n","timestamp_test = list(map(lambda x: datetime.timestamp(x[0]), index_test)) \n","# print(total_stamp)\n","# print(timestamp_train)\n","plt.figure(figsize=(40, 4))\n","plt.scatter(range(len(timestamp_train)), timestamp_train, label='train')\n","plt.scatter(range(len(timestamp_val)), timestamp_val, color='orange', label='val')\n","plt.scatter(range(len(timestamp_test)), timestamp_test, color='red', label='test')\n","plt.ylim(min(total_stamp), max(total_stamp))\n","plt.legend(fontsize=20)\n","\n","# print(new_input_index)"],"execution_count":null,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-10-72a922dd9685>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# print(index_train)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# print(index_val)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mtotal_stamp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimestamp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mtimestamp_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimestamp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mtimestamp_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimestamp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'input_index' is not defined"]}]},{"cell_type":"markdown","metadata":{"id":"b1UEFg1GVSLS"},"source":["### Load Data"]},{"cell_type":"code","metadata":{"id":"oa0CYY1zKH0l","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1620033617416,"user_tz":-540,"elapsed":6824,"user":{"displayName":"7th June","photoUrl":"","userId":"08178289703395036410"}},"outputId":"54203fc6-893d-4a9f-d7f2-cc35eb640a39"},"source":["period = 45\n","\n","x_save_path = current_path + 'npy/' + '%s_rnn_close_updown_x_train_neo_timesplit.npy' % period\n","x_train = np.load(x_save_path)\n","x_val = np.load(x_save_path.replace('x_train', 'x_val'))\n","x_test = np.load(x_save_path.replace('x_train', 'x_test'))\n","print('x series loaded !')\n","\n","pr_save_path = current_path + 'npy/' + '%s_rnn_close_updown_pr_train_neo_timesplit.npy' % period\n","pr_train = np.load(pr_save_path)\n","pr_val = np.load(pr_save_path.replace('pr_train', 'pr_val'))\n","pr_test = np.load(pr_save_path.replace('pr_train', 'pr_test'))\n","print('y series loaded !')\n","\n","_, row, col = x_train.shape\n","\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["x series loaded !\n","y series loaded !\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"zscZynIgMbAq","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1620051746733,"user_tz":-540,"elapsed":5243,"user":{"displayName":"7th June","photoUrl":"","userId":"08178289703395036410"}},"outputId":"c236475b-70f6-4ba9-e175-72abc31c6800"},"source":["print(keras.__version__)\n","print(tf.__version__)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["2.3.1\n","1.15.2\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"-jo3k5MdhFyg"},"source":["#### **clustering output**"]},{"cell_type":"code","metadata":{"id":"njxxm-TJ-RP-"},"source":["# x_train_for_k = test_result.flatten().reshape(-1, 1)\n","x_train_for_k = test_result\n","print(x_train_for_k[:10])\n","# x_train_for_k = test_result[:, [1]]\n","pr_train = pr_test\n","\n","print('x_train_for_k.shape :', x_train_for_k.shape)\n","print('pr_train.shape :', pr_train.shape)\n","\n","K = range(2, 10)\n","s_dist = []\n","sil = []\n","for k in K:\n","  # if cen_data.shape[0] < k:\n","  #   break\n","\n","  km = KMeans(n_clusters=k)\n","  km = km.fit(x_train_for_k)\n","\n","  labels = km.labels_\n","  # print('len(labels) :', len(labels))\n","  # print('labels[:10] :', labels[:10])\n","  sil.append(silhouette_score(x_train_for_k, labels, metric='euclidean'))\n","\n","  # inertia = km.inertia_\n","  # s_dist.append(inertia)\n","\n","best_k = K[np.argmax(np.array(sil))]\n","\n","plt.figure(figsize=(10, 5))\n","plt.plot(K, sil)\n","plt.axvline(best_k, linestyle='--')\n","# plt.plot(K, s_dist)\n","plt.show()\n","\n","\n","\n","\n","\n","#   with best_k, label 별 pr_list 확인\n","km = KMeans(n_clusters=best_k)\n","km = km.fit(x_train_for_k)\n","\n","labels = km.labels_\n","\n","print(km.score(x_train_for_k))\n","print(len(labels), len(pr_train))\n","\n","\n","\n","\n","\n","#   label 별로 profit 을 저장, 승률을 확인한다\n","label_types = np.unique(labels, return_counts=False)\n","\n","label_pr_dict = {}\n","#   init dict   #\n","for label in label_types:\n","  label_pr_dict[label] = []\n","print(label_pr_dict)\n","# break\n","\n","for i, (label, pr) in enumerate(zip(labels, pr_train)):\n","  label_pr_dict[label].append(pr[0])\n","\n","  \n","# for label in label_types:\n","print(label_pr_dict)\n","\n","\n","\n","\n","\n","def win_ratio(list_x):\n","\n","  win_cnt = np.sum(np.array(list_x) > 1)\n","  return win_cnt / len(list_x)\n","\n","\n","def acc_pr(list_x):\n","\n","  return np.cumprod(np.array(list_x))[-1]\n","\n","\n","for key in label_pr_dict:\n","  \n","  print(key, ':', 'win_ratio : %.2f' % (win_ratio(label_pr_dict[key])), 'acc_pr : %.2f' % (acc_pr(label_pr_dict[key])))\n","\n","\n","\n","\n","\n","#     predict test && test 의 라벨에 따른 win_ratio 확인\n","# test_labels = km.predict(x_test)\n","# # print(test_labels)\n","\n","# label_pr_dict = {}\n","# #   init dict   #\n","# for label in label_types:\n","#   label_pr_dict[label] = []\n","# print(label_pr_dict)\n","# # break\n","\n","# for i, (label, pr) in enumerate(zip(test_labels, pr_test)):\n","#   label_pr_dict[label].append(pr[0])\n","\n","# for key in label_pr_dict:\n","\n","#   print(key, ':', 'win_ratio : %.2f' % (win_ratio(label_pr_dict[key])), 'acc_pr : %.2f' % (acc_pr(label_pr_dict[key])))\n","\n"],"execution_count":null,"outputs":[]}]}