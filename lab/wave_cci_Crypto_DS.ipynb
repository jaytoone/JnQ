{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8uqYv5StTazo"
   },
   "source": [
    "# Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 41909,
     "status": "ok",
     "timestamp": 1666567834950,
     "user": {
      "displayName": "7th June",
      "userId": "08178289703395036410"
     },
     "user_tz": -540
    },
    "id": "6rmQpzEGXfCw",
    "outputId": "6e7fec9e-910c-4f49-bc35-5e99ac69bfd8"
   },
   "outputs": [],
   "source": [
    "import os, sys\n",
    "\n",
    "pkg_path = 'D:\\\\Projects\\\\SystemTrading\\\\JnQ\\\\'\n",
    "\n",
    "os.chdir(pkg_path)\n",
    "\n",
    "# mpl_finance_path = 'D:\\\\python\\\\python38_1\\\\projects\\\\JnQ\\\\mpl_finance'\n",
    "# ta_lib_path = 'D:\\\\python\\\\python38_1\\\\projects\\\\JnQ\\\\ta_lib'\n",
    "funcs_path = pkg_path + 'funcs'\n",
    "\n",
    "if funcs_path not in sys.path:\n",
    "\n",
    "  try:\n",
    "    # sys.path.insert(0, '/content/drive/My Drive/Colab Notebooks/JnQ')\n",
    "    sys.path.insert(0, pkg_path + 'Bank')\n",
    "    sys.path.insert(0, funcs_path)\n",
    "    # sys.path.insert(0, mpl_finance_path)\n",
    "    # sys.path.insert(0, ta_lib_path)\n",
    "    \n",
    "  except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 20652,
     "status": "ok",
     "timestamp": 1666567855597,
     "user": {
      "displayName": "7th June",
      "userId": "08178289703395036410"
     },
     "user_tz": -540
    },
    "id": "9qGt60DKTZmf"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import talib\n",
    "from funcs.public.idep import *\n",
    "from funcs.public.plot_check import *\n",
    "from funcs.public.en_ex_pairing import *\n",
    "from funcs.public.indicator import *\n",
    "from funcs.public.broker import *\n",
    "from funcs.public.ds import *\n",
    "from ast import literal_eval\n",
    "import logging\n",
    "import importlib\n",
    "\n",
    "from matplotlib import rcParams\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import gridspec\n",
    "\n",
    "import torch\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# import bz2\n",
    "import pickle\n",
    "# import _pickle as cPickle\n",
    "import shutil\n",
    "import json\n",
    "from easydict import EasyDict\n",
    "import copy\n",
    "\n",
    "import datetime\n",
    "from datetime import datetime\n",
    "import random\n",
    "import time\n",
    "# import warnings\n",
    "\n",
    "from IPython.display import clear_output\n",
    "# warnings.simplefilter(\"ignore\", category=RuntimeWarning)\n",
    "\n",
    "np.seterr(invalid=\"ignore\")\n",
    "np.set_printoptions(suppress=True)\n",
    "np.set_printoptions(linewidth=2000) \n",
    "\n",
    "pd.set_option('mode.chained_assignment',  None)\n",
    "\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ic1mfmwWCIBu",
    "tags": []
   },
   "source": [
    "# Data work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AUSBU7T8Suzi",
    "tags": []
   },
   "source": [
    "## sync_check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QmhLikYlSuzi"
   },
   "outputs": [],
   "source": [
    "def sync_check_make(df):\n",
    "\n",
    "    make_itv_list = ['3T', '5T', '15T', '30T', 'H', '4H', 'D']\n",
    "    offset_list = ['1h', '1h', '1h', '1h', '1h', '1h', '9h']\n",
    "\n",
    "    assert len(make_itv_list) == len(offset_list), \"length of itv & offset_list should be equal\"\n",
    "        \n",
    "    htf_df_list = [to_htf(df, itv=itv_, offset=offset_) for itv_, offset_ in zip(make_itv_list, offset_list)]\n",
    "\n",
    "    df_3T, df_5T, df_15T, df_30T, df_H, df_4H, df_D = htf_df_list\n",
    "\n",
    "    for htf_df in htf_df_list:\n",
    "      print(\"{} -> \".format(pd.infer_freq(htf_df.index)), htf_df.tail(1))\n",
    "\n",
    "    # heikinashi_v2(res_df_)\n",
    "    \n",
    "    # h_candle_v3(df, df_5T, '5T')\n",
    "    \n",
    "    # df = h_candle_v4(df, df_5T)\n",
    "    # df = h_candle_v4(df, df_15T)\n",
    "    # df = h_candle_v4(df, df_30T)\n",
    "    # df = h_candle_v4(df, df_45T)\n",
    "    # df = h_candle_v4(df, df_H)\n",
    "    # df = h_candle_v4(df, df_4H)\n",
    "    # df = h_candle_v4(df, 'D')\n",
    "\n",
    "    # df = candle_pattern_pkg(df, df_5T)\n",
    "    # df = candle_pattern_pkg(df, df_30T)\n",
    "    # df = candle_pattern_pkg(df, df_H)\n",
    "    # df = candle_pattern_pkg(df, df_4H)\n",
    "    \n",
    "    # --------------- stochastic --------------- #\n",
    "    # df = stoch_v2(df)\n",
    "    # df_5T['stoch'] = stoch(df_5T, 13, 3, 3)\n",
    "    # df = df.join(pd.DataFrame(index=df.index, data=to_lower_tf_v2(df, df_5T, [-1], backing_i=-1), columns=['stoch_5m']))\n",
    "\n",
    "    # print(\"stoch phase done\")\n",
    "    \n",
    "    # --------------- supertrend --------------- #\n",
    "#     df = st_price_line(df, df_15T)\n",
    "#     df = st_level(df, '15T', 1)\n",
    "    \n",
    "#     df = st_price_line(df, df_4H)\n",
    "#     df = st_level(df, '4H', 1)\n",
    "\n",
    "#     print(\"supertrend phase done\")\n",
    "\n",
    "    # --------------- ma --------------- #  \n",
    "    # df = ma(df, 60)\n",
    "    # print(\"ma phase done\")\n",
    "\n",
    "    # df = macd_hist(df, 5, 35, 15)\n",
    "    # print(\"macd_hist phase done\")\n",
    "    \n",
    "    # df = enough_space(df, '15T', 1)\n",
    "    \n",
    "    # --------------- dc --------------- #  \n",
    "    dc_period = 500\n",
    "    # df = donchian_channel_v4(df, dc_period)\n",
    "    # df = dc_line(df, df_5T, '5T')  # join 사용시에만 return df 허용함\n",
    "    # df = dc_line(df, df_15T, '15T')\n",
    "    # df = dc_line_v2(df, df_H, 'H', dc_period=5)\n",
    "\n",
    "    # df = dc_line_v4(df, df, dc_period=10)\n",
    "    # df = dc_line_v4(df, df, dc_period=20)\n",
    "    # df = dc_line_v4(df, df, dc_period=100)\n",
    "    # df = dc_line_v4(df, df_5T, dc_period=20)\n",
    "    # df = dc_line_v4(df, df_15T, dc_period=20)\n",
    "    # df = dc_line_v4(df, df_H, dc_period=20)\n",
    "    # df = dc_line_v4(df, df_4H, dc_period=20)\n",
    "    # print(\"dc phase done\")\n",
    "\n",
    "    # --------------- bb --------------- #  \n",
    "    bb_period = 200\n",
    "\n",
    "    # upper, base, lower = talib.BBANDS(res_df_.close, timeperiod=20, nbdevup=1, nbdevdn=1, matype=0)\n",
    "        \n",
    "#     df = bb_width_v3(df, period=bb_period, multiple=1, itv='T')\n",
    "#     df = bb_level_v2(df, 'T', bb_period)\n",
    "        \n",
    "#     # 1. Stock 을 위한 bb setting 아직 정립되지 않음. to_lower_tf 라든지.\n",
    "#     # df = bb_line_v3(df, df_15T, 20)    \n",
    "#     # df = bb_line_v3(df, df, bb_period)\n",
    "#     print(\"bb phase done\")\n",
    "\n",
    "    c_itv = '5T'\n",
    "\n",
    "    # df =  wick_ratio(df, c_itv)\n",
    "    # df =  wick_ratio(df, c_itv)\n",
    "\n",
    "    bb_itv= 'T'\n",
    "\n",
    "    # df = candle_range_ratio(df, c_itv, bb_itv, bb_period)\n",
    "    # # candle_pumping_ratio(df, c_itv, bb_itv, bb_period)\n",
    "\n",
    "    dc_itv= '15T'\n",
    "    dc_period = 4\n",
    "    # df = candle_pumping_ratio_v2(df, c_itv, dc_itv, dc_period)\n",
    "    # print(\"candle_pumping_ratio_v2 phase done\")\n",
    "\n",
    "    # df = dc_over_body_ratio(df, c_itv, dc_itv, dc_period)\n",
    "    # print(\"dc_over_body_ratio phase done\")\n",
    "\n",
    "    # df = body_rel_ratio(df, c_itv)\n",
    "    # print(\"body_rel_ratio phase done\")\n",
    "\n",
    "    # --------------- cbline --------------- #    \n",
    "    # cloud_bline(df_3T, 20)\n",
    "    # df = df.join(to_lower_tf_v2(df, df_3T, [-1]), how='inner')\n",
    "    # # cloud_bline(df_5T, 20)\n",
    "    # # df = df.join(to_lower_tf_v2(df, df_5T, [-1]), how='inner')\n",
    "    # cloud_bline(df_15T, 20)\n",
    "    # df = df.join(to_lower_tf_v2(df, df_15T, [-1]), how='inner')\n",
    "    # cloud_bline(df_30T, 20)\n",
    "    # df = df.join(to_lower_tf_v2(df, df_30T, [-1]), how='inner')\n",
    "    # cloud_bline(df_H, 20)\n",
    "    # df = df.join(to_lower_tf_v2(df, df_H, [-1]), how='inner')\n",
    "    # cloud_bline(df_4H, 20)\n",
    "    # df = df.join(to_lower_tf_v2(df, df_4H, [-1]), how='inner')\n",
    "\n",
    "    # print(\"cbline phase done\")\n",
    "\n",
    "\n",
    "\n",
    "    # --------------- sd_dc --------------- #\n",
    "    # df = sd_dc(df, 20, 40)\n",
    "    # df = sd_dc(df, 20, 20)\n",
    "    # df = sd_dc(df_5T, 20, 40, df)\n",
    "    # df = sd_dc(df_H, 20, 40, df)\n",
    "\n",
    "    # print(\"sd_dc phase done\")\n",
    "\n",
    "    # --------------- imb_ratio --------------- #\n",
    "    # imb_ratio(df, '5T')\n",
    "    # imb_ratio_v3(df, \"5T\")\n",
    "    # imb_ratio_v4(df, \"5T\")\n",
    "\n",
    "    # imb_ratio(df, 'H')\n",
    "    # imb_ratio_v2(df, '5T')\n",
    "    \n",
    "    # print(\"imb_ratio phase done\")\n",
    "\n",
    "    # --------------- rel_abs_ratio --------------- #\n",
    "    # rel_abs_ratio(df, '5T', norm_period=120)\n",
    "\n",
    "    # --------------- normalize data --------------- #\n",
    "    # itv = 'T'\n",
    "    # lb_period = 15\n",
    "    # target_col = 'close_{}{}'.format(itv, lb_period)\n",
    "    # target_data = df['close'].diff(lb_period).to_numpy()\n",
    "    # norm_data(df, target_data, target_col)    \n",
    "    # print(\"normalize data phase done !\")\n",
    "\n",
    "    # --------------- lucid sar --------------- #\n",
    "    # lucid_sar_v2(df)\n",
    "    # lucid_sar_v2(df_3T)\n",
    "    # df = df.join(to_lower_tf_v2(df, df_3T, [-2, -1]), how='inner')\n",
    "    # lucid_sar_v2(df_5T)\n",
    "    # df = df.join(to_lower_tf_v2(df, df_5T, [-2, -1]), how='inner')\n",
    "    # lucid_sar_v2(df_15T)\n",
    "    # df = df.join(to_lower_tf_v2(df, df_15T, [-2, -1]), how='inner')\n",
    "    # lucid_sar_v2(df_30T)\n",
    "    # df = df.join(to_lower_tf_v2(df, df_30T, [-2, -1]), how='inner')       \n",
    "\n",
    "    # print(\"sar phase done\")\n",
    "\n",
    "\n",
    "    # --------------- rsi --------------- #  \n",
    "    # df['rsi_1m'] = rsi(df, 14)    \n",
    "    # df_5T['rsi_5m'] = rsi(df_5T, 14)\n",
    "    # df = df.join(pd.DataFrame(index=df.index, data=to_lower_tf_v2(df, df_5T, [-1]), columns=['rsi_5m']))\n",
    "    \n",
    "    # print(\"rsi phase done\")\n",
    "\n",
    "\n",
    "    # --------------- cci --------------- #  \n",
    "    # df['cci_1m'] = cci(df, 20)\n",
    "\n",
    "    # print(\"cci phase done\")\n",
    "\n",
    "    # --------------- ema --------------- #      \n",
    "    # df_5T['ema_5m'] = ema(df_5T['close'], 195)\n",
    "    # df = df.join(pd.DataFrame(index=df.index, data=to_lower_tf_v2(df, df_5T, [-1]), columns=['ema_5m']))\n",
    "    \n",
    "    # print(\"ema phase done\")        \n",
    "\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 620
    },
    "executionInfo": {
     "elapsed": 3099,
     "status": "ok",
     "timestamp": 1662020383911,
     "user": {
      "displayName": "7th June",
      "userId": "08178289703395036410"
     },
     "user_tz": -540
    },
    "id": "odqVwQHpYo1M",
    "outputId": "a847fe3a-8ef4-42ef-8f5b-42a30e227a8c"
   },
   "outputs": [],
   "source": [
    "res_df_ = sync_check_make(res_df_)  # suffix duplication 유의\n",
    "res_df_.tail().iloc[:, -10:]\n",
    "# res_df_.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3SKglsQCj5_x"
   },
   "outputs": [],
   "source": [
    "# test_df_ = sync_check_make(res_df_.iloc[-4000:])  # suffix duplication 유의\n",
    "# test_df_.tail().iloc[:, -10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gOQxwYqK0jCS"
   },
   "outputs": [],
   "source": [
    "# ------ validation ------ #\n",
    "# res_df_.cppr_15T.describe()\n",
    "print((res_df_.open_15T.to_numpy() - res_df_.close_15T.to_numpy())[-10:])\n",
    "print((res_df_.dc_upper_15T4.to_numpy() - res_df_.dc_lower_15T4.to_numpy())[-10:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RmiB5VU5DN6B"
   },
   "outputs": [],
   "source": [
    "# np.where(res_df_.CDL3LINESTRIKE_15T) #.iloc[-1000:,]\n",
    "\n",
    "# CDL3LINESTRIKE = talib.CDL3LINESTRIKE(df_15T.open, df_15T.high, df_15T.low, df_15T.close)\n",
    "for col in talib.get_function_groups()['Pattern Recognition']:  \n",
    "  print(np.unique(res_df_[col + '_15T'].to_numpy(), return_counts=True))\n",
    "\n",
    "# CDLCLOSINGMARUBOZU = talib.CDLCLOSINGMARUBOZU(df_15T.open, df_15T.high, df_15T.low, df_15T.close)\n",
    "# print(np.unique(CDLCLOSINGMARUBOZU.to_numpy(), return_counts=True))\n",
    "# print(CDLCLOSINGMARUBOZU.tail(50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2407,
     "status": "ok",
     "timestamp": 1662020394112,
     "user": {
      "displayName": "7th June",
      "userId": "08178289703395036410"
     },
     "user_tz": -540
    },
    "id": "XrgJPQRuisCa",
    "outputId": "b136b481-cf11-40ed-9bcc-1c12bad3129d"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "save current res_df \n",
    "\"\"\"\n",
    "\n",
    "res_df_.reset_index().to_feather(data_path, compression='lz4')  # key 잘 확인하고 저장\n",
    "print(data_path, 'saved !')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jTN3M842Suzl",
    "tags": []
   },
   "source": [
    "## concatenation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WVAKq3i8Suzm",
    "tags": []
   },
   "source": [
    "### Row concatenation (Feather version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_path = 'database/binance/'\n",
    "\n",
    "ticker = \"AAVEUSDT_1m.ftr\"\n",
    "\n",
    "old_dir = 'non_cum'\n",
    "old_date = '2023-12-20'  # earlier\n",
    "\n",
    "new_dir = 'non_cum'\n",
    "new_date = '2024-02-15'  # latest\n",
    "\n",
    "old_path = os.path.join(pkg_path, db_path,  old_dir, old_date, \" \".join([old_date, ticker]))\n",
    "new_path = os.path.join(pkg_path, db_path, new_dir, new_date, \" \".join([new_date, ticker]))\n",
    "\n",
    "old_df = pd.read_feather(old_path, columns=None, use_threads=True).set_index(\"index\")   # key 에 new_date 담겨있음\n",
    "new_df = pd.read_feather(new_path, columns=None, use_threads=True).set_index(\"index\")\n",
    "\n",
    "sum_df = pd.concat([old_df, new_df])\n",
    "sum_df = sum_df[~sum_df.index.duplicated(keep='last')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_idx_ts = np.array(list(map(lambda x: datetime.timestamp(x), sum_df.index)))\n",
    "ideal_ts_gap = 60 # * itv_num\n",
    "\n",
    "for ts_i in range(len(np_idx_ts)):\n",
    "\n",
    "  if ts_i != 0:\n",
    "    ts_gap = np_idx_ts[ts_i] - np_idx_ts[ts_i - 1]\n",
    "    \n",
    "    if ts_gap > ideal_ts_gap or ts_gap < ideal_ts_gap:\n",
    "    # if ts_gap == ideal_ts_gap:  # logic 정상성 확인을 위함.\n",
    "        \n",
    "      print(\"unideal ts_gap : {} {}\".format(sum_df.index[ts_i - 1], sum_df.index[ts_i]))\n",
    "\n",
    "print(\"continuity check done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "database_dir_save_path = os.path.join(pkg_path, db_path,  \"cum\", new_date)\n",
    "os.makedirs(database_dir_save_path, exist_ok=True)\n",
    "\n",
    "data_save_path = os.path.join(database_dir_save_path, \" \".join([new_date, ticker]))\n",
    "sum_df.reset_index().to_feather(data_save_path, compression='lz4')\n",
    "\n",
    "print(\"{} saved\".format(data_save_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### on multiple ticker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def row_concate(db_path, ticker, old_dir, new_dir, old_date, new_date):    \n",
    "    \n",
    "    old_path = os.path.join(pkg_path, db_path,  old_dir, old_date, \" \".join([old_date, ticker]))\n",
    "    new_path = os.path.join(pkg_path, db_path, new_dir, new_date, \" \".join([new_date, ticker]))\n",
    "\n",
    "    try:\n",
    "        old_df = pd.read_feather(old_path, columns=None, use_threads=True).set_index(\"index\")   # key 에 new_date 담겨있음\n",
    "        new_df = pd.read_feather(new_path, columns=None, use_threads=True).set_index(\"index\")\n",
    "        \n",
    "        old_df.index = pd.DatetimeIndex(list(map(lambda x: str(x).replace('59.999000', '00'), old_df.index)))\n",
    "        # print(list(map(lambda x: pd.to_datetime(str(x).replace('59.999000', '00')), old_df.index[:5])))\n",
    "        # print( pd.DatetimeIndex(list(map(lambda x: str(x).replace('59.999000', '00'), old_df.index[:5]))))       \n",
    "        # print(new_df.index[:5])\n",
    "        # return\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return\n",
    "\n",
    "    sum_df = pd.concat([old_df, new_df])\n",
    "    sum_df = sum_df[~sum_df.index.duplicated(keep='last')]\n",
    "    \n",
    "    np_idx_ts = np.array(list(map(lambda x: datetime.timestamp(x), sum_df.index)))\n",
    "    ideal_ts_gap = 60 # * itv_num\n",
    "\n",
    "    for ts_i in range(len(np_idx_ts)):\n",
    "\n",
    "      if ts_i != 0:\n",
    "        ts_gap = np_idx_ts[ts_i] - np_idx_ts[ts_i - 1]\n",
    "\n",
    "        if ts_gap > ideal_ts_gap or ts_gap < ideal_ts_gap:\n",
    "        # if ts_gap == ideal_ts_gap:  # logic 정상성 확인을 위함.\n",
    "\n",
    "          print(\"unideal ts_gap : {} {}\".format(sum_df.index[ts_i - 1], sum_df.index[ts_i]))\n",
    "\n",
    "    print(\"continuity check done\")\n",
    "    \n",
    "    database_dir_save_path = os.path.join(pkg_path, db_path,  \"cum\", new_date)\n",
    "    os.makedirs(database_dir_save_path, exist_ok=True)\n",
    "\n",
    "    data_save_path = os.path.join(database_dir_save_path, \" \".join([new_date, ticker]))\n",
    "    sum_df.reset_index().to_feather(data_save_path, compression='lz4')\n",
    "\n",
    "    print(\"{} saved\".format(data_save_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_path = 'database/binance/'\n",
    "\n",
    "old_dir = 'cum'\n",
    "old_date = '2023-10-17'  # earlier\n",
    "\n",
    "new_dir = 'cum'\n",
    "new_date = '2024-02-15'  # latest\n",
    "\n",
    "ticker_list = [ticker_.split(\" \")[-1] for ticker_ in os.listdir(os.path.join(pkg_path, db_path,  old_dir, old_date))]\n",
    "\n",
    "for ticker in ticker_list:\n",
    "    row_concate(db_path, ticker, old_dir, new_dir, old_date, new_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nUs4fjVHSuzl",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Column concatenation (Feather version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6cu-Y82iSuzl"
   },
   "outputs": [],
   "source": [
    "new_dir_path = \"st3m_backi2\"\n",
    "base_dir_path = \"bb1d_backi2\"\n",
    "\n",
    "# new_date = \"2021-11-17\"\n",
    "new_date = \"2022-01-10\"\n",
    "\n",
    "#     save to (new) concat dir    #\n",
    "#      1. if dir. not exists, makedir\n",
    "save_path = './candlestick_concated/res_df/'\n",
    "save_path = os.path.join(save_path, new_dir_path, \"concat/cum\", new_date)   \n",
    "# save_path = os.path.join(save_path, new_dir_path, \"concat/non_cum\", new_date)   # row col 하려면 concat 맞음, noncum 사용\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "\n",
    "#     load ftr list    #\n",
    "# ftr_list = [s for s in os.listdir(os.path.join(save_path, new_dir_path)) if \"ftr\" in s]\n",
    "\n",
    "noncat_path = save_path.replace(\"concat/\", \"noncat/\")\n",
    "ftr_list = [s for s in os.listdir(noncat_path) if \"ftr\" in s]\n",
    "print(ftr_list)\n",
    "# break\n",
    "\n",
    "\n",
    "for key in ftr_list:\n",
    "\n",
    "  if new_date not in key:\n",
    "    continue\n",
    "\n",
    "  try:\n",
    "\n",
    "    #       read from base postfix's directory    #\n",
    "    base_df = pd.read_feather(os.path.join(save_path.replace(new_dir_path, base_dir_path), key), columns=None, use_threads=True).set_index(\"index\")\n",
    "    res_df = pd.read_feather(os.path.join(noncat_path, key), columns=None, use_threads=True).set_index(\"index\")\n",
    "\n",
    "    # print(base_df.head())\n",
    "    # print(res_df.head())\n",
    "    # break\n",
    "\n",
    "    new_res_df = pd.concat([base_df, res_df], axis=1) # df_tot.drop_duplicates()\n",
    "    # new_res_df.head()\n",
    "\n",
    "    droped_new_res_df = new_res_df.loc[:,~new_res_df.columns.duplicated(keep='last')]\n",
    "    # droped_new_res_df.head()\n",
    "    # break\n",
    "\n",
    "    droped_new_res_df.reset_index().to_feather(os.path.join(save_path, key), compression='lz4')\n",
    "\n",
    "    # res_df_dict[key] = res_df\n",
    "    # res_df_dict[key] = droped_new_res_df\n",
    "    print(os.path.join(save_path, key), \"saved !\")\n",
    "  \n",
    "  except Exception as e:\n",
    "    print(\"error occured ! :\", e)\n",
    "  \n",
    "\n",
    "  # sample_cnt -= 1\n",
    "\n",
    "  # if sample_cnt <= 0:\n",
    "  #   break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L7l5CTJfSuzn",
    "tags": []
   },
   "source": [
    "### Check continuity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w-5jn9opBl73"
   },
   "outputs": [],
   "source": [
    "droped_new_res_df = res_df_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nGzMGyC3Suzn"
   },
   "outputs": [],
   "source": [
    "# print(droped_new_res_df.columns)\n",
    "\n",
    "print(droped_new_res_df.iloc[[0, -1]])\n",
    "\n",
    "np_idx_ts = np.array(list(map(lambda x: datetime.timestamp(x), droped_new_res_df.index)))\n",
    "\n",
    "print(np_idx_ts[:10])\n",
    "for ts_i in range(len(np_idx_ts)):\n",
    "  \n",
    "  if ts_i != 0:\n",
    "    ts_gap = np_idx_ts[ts_i] - np_idx_ts[ts_i - 1]\n",
    "\n",
    "    if ts_gap > 60 or ts_gap < 60:\n",
    "\n",
    "      print(\"invalid ts_gap found !\")\n",
    "    # if ts_gap == 60:\n",
    "      print(droped_new_res_df.index[ts_i - 1])\n",
    "      print(droped_new_res_df.index[ts_i])\n",
    "      # print(ts_gap)\n",
    "      print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x_XGJqBi8Jex",
    "tags": []
   },
   "source": [
    "### Check length of front missing value & middle_data non_missing validity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "14chOHeXh6JD",
    "tags": []
   },
   "source": [
    "## Lab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O87s8_EUakqS",
    "tags": []
   },
   "source": [
    "### instant indi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Fg4JnLY6i99D"
   },
   "outputs": [],
   "source": [
    "def get_wave_time_ratio(res_df, wave_itv1, wave_period1):\n",
    "\n",
    "  wave_cu_post_idx_fill_ = res_df['wave_cu_post_idx_fill_{}{}'.format(wave_itv1, wave_period1)].to_numpy().astype(int)\n",
    "  wave_co_post_idx_fill_ = res_df['wave_co_post_idx_fill_{}{}'.format(wave_itv1, wave_period1)].to_numpy().astype(int)\n",
    "\n",
    "  wave_cu_idx_fill_ = res_df['wave_cu_idx_fill_{}{}'.format(wave_itv1, wave_period1)].to_numpy().astype(int)\n",
    "  wave_co_idx_fill_ = res_df['wave_co_idx_fill_{}{}'.format(wave_itv1, wave_period1)].to_numpy().astype(int)\n",
    "\n",
    "  wave_valid_cu_post_idx_fill_ = np.where(wave_cu_post_idx_fill_ < 0, 0, wave_cu_post_idx_fill_)\n",
    "  wave_valid_co_post_idx_fill_ = np.where(wave_co_post_idx_fill_ < 0, 0, wave_co_post_idx_fill_)\n",
    "\n",
    "  wave_valid_cu_idx_fill_ = np.where(wave_cu_idx_fill_ < 0, 0, wave_cu_idx_fill_)\n",
    "  wave_valid_co_idx_fill_ = np.where(wave_co_idx_fill_ < 0, 0, wave_co_idx_fill_)\n",
    "\n",
    "  res_df['short_wave_time_ratio_{}{}'.format(wave_itv1, wave_period1)] = (wave_valid_co_post_idx_fill_ - wave_valid_cu_post_idx_fill_[wave_valid_co_post_idx_fill_[wave_valid_cu_idx_fill_]]) / (wave_valid_cu_idx_fill_ - wave_valid_co_post_idx_fill_)\n",
    "  res_df['long_wave_time_ratio_{}{}'.format(wave_itv1, wave_period1)] = (wave_valid_cu_post_idx_fill_ - wave_valid_co_post_idx_fill_[wave_valid_cu_post_idx_fill_[wave_valid_co_idx_fill_]]) / (wave_valid_co_idx_fill_ - wave_valid_cu_post_idx_fill_)\n",
    "\n",
    "  return res_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XkrbzNFKZhl0"
   },
   "outputs": [],
   "source": [
    "def wave_range_cci_v4_1(t_df, wave_period):\n",
    "    t_df = cci_v2(t_df, wave_period)\n",
    "    itv = pd.infer_freq(t_df.index)\n",
    "\n",
    "    cci_ = t_df['cci_{}{}'.format(itv, wave_period)].to_numpy()\n",
    "    b1_cci_ = t_df['cci_{}{}'.format(itv, wave_period)].shift(1).to_numpy()\n",
    "\n",
    "    baseline = 0\n",
    "    band_width = 100\n",
    "    upper_band = baseline + band_width\n",
    "    lower_band = baseline - band_width\n",
    "\n",
    "    data_cols = ['open', 'high', 'low', 'close']\n",
    "    ohlc_list = [t_df[col_].to_numpy() for col_ in data_cols]\n",
    "    open, high, low, close = ohlc_list\n",
    "\n",
    "    # ============ modules ============ #\n",
    "    # ------ define co, cu ------ # <- point missing 과 관련해 정교해아함\n",
    "    cu_bool = (b1_cci_ > upper_band) & (upper_band > cci_)\n",
    "    co_bool = (b1_cci_ < lower_band) & (lower_band < cci_)\n",
    "\n",
    "    return wave_publics_v2(t_df, cu_bool, co_bool, ohlc_list, wave_period)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FdDkSxbaX4AI"
   },
   "outputs": [],
   "source": [
    "def wave_publics_v2(t_df, cu_bool, co_bool, ohlc_list, wave_period):\n",
    "    itv = pd.infer_freq(t_df.index)\n",
    "\n",
    "    len_df = len(t_df)\n",
    "    len_df_range = np.arange(len_df).astype(int)\n",
    "\n",
    "    cu_idx = get_index_bybool(cu_bool, len_df_range)\n",
    "    co_idx = get_index_bybool(co_bool, len_df_range)\n",
    "\n",
    "    open, high, low, close = ohlc_list\n",
    "\n",
    "    cu_fill_idx, co_fill_idx, cu_prime_idx, co_prime_idx, cu_prime_fill_idx, co_prime_fill_idx, valid_cu_bool, valid_co_bool = get_terms_info_v4(\n",
    "        cu_idx, co_idx, len_df, len_df_range)\n",
    "    # cu_fill_idx, co_fill_idx, cu_prime_idx, co_prime_idx, cu_prime_fill_idx, co_prime_fill_idx, \\\n",
    "    #   cu_post_idx, co_post_idx, cu_post_fill_idx, co_post_fill_idx, valid_cu_bool, valid_co_bool = get_terms_info_v5(cu_idx, co_idx, len_df, len_df_range)\n",
    "\n",
    "    # ------ get post_terms ------ #\n",
    "    high_post_terms = np.vstack((co_fill_idx[valid_cu_bool], cu_idx[valid_cu_bool])).T.astype(int)\n",
    "    low_post_terms = np.vstack((cu_fill_idx[valid_co_bool], co_idx[valid_co_bool])).T.astype(int)\n",
    "\n",
    "    high_post_terms_cnt = high_post_terms[:, 1] - high_post_terms[:, 0]\n",
    "    low_post_terms_cnt = low_post_terms[:, 1] - low_post_terms[:, 0]\n",
    "\n",
    "    # ------ get post_idx ------ #\n",
    "    paired_cu_post_idx = high_post_terms[:, 1]  # Todo, 여기는 cross_idx (위에서 vstack 으로 cross_idx 입력함)\n",
    "    paired_co_post_idx = low_post_terms[:, 1]\n",
    "\n",
    "    cu_post_idx = np.full(len_df, np.nan)  # --> Todo, unavailable : not cross_idx\n",
    "    co_post_idx = np.full(len_df, np.nan)\n",
    "\n",
    "    cu_post_idx[paired_cu_post_idx] = paired_cu_post_idx\n",
    "    co_post_idx[paired_co_post_idx] = paired_co_post_idx\n",
    "\n",
    "    cu_post_fill_idx = fill_arr(cu_post_idx)\n",
    "    co_post_fill_idx = fill_arr(co_post_idx)\n",
    "\n",
    "    # ------ get prime_terms ------ # # 기본은 아래 logic 으로 수행하고, update_hl 도 해당 term 구간의 hl 이 더 작거나 클경우 적용 가능할 것\n",
    "    # high_prime_terms = np.vstack((co_prime_fill_idx[valid_cu_bool], cu_idx[valid_cu_bool])).T.astype(int)\n",
    "    # low_prime_terms = np.vstack((cu_prime_fill_idx[valid_co_bool], co_idx[valid_co_bool])).T.astype(int)\n",
    "\n",
    "    # high_prime_terms_cnt = high_prime_terms[:, 1] - high_prime_terms[:, 0]\n",
    "    # low_prime_terms_cnt = low_prime_terms[:, 1] - low_prime_terms[:, 0]\n",
    "\n",
    "    # paired_prime_cu_idx = high_prime_terms[:, 1]\n",
    "    # paired_prime_co_idx = low_prime_terms[:, 1]\n",
    "\n",
    "    # ====== get wave_hl & terms ====== #\n",
    "    wave_high_ = np.full(len_df, np.nan)\n",
    "    wave_low_ = np.full(len_df, np.nan)\n",
    "\n",
    "    wave_highs = np.array([high[iin:iout + 1].max() for iin, iout in high_post_terms])\n",
    "    wave_lows = np.array([low[iin:iout + 1].min() for iin, iout in low_post_terms])\n",
    "\n",
    "    wave_high_[paired_cu_post_idx] = wave_highs\n",
    "    wave_low_[paired_co_post_idx] = wave_lows\n",
    "\n",
    "    wave_high_fill_ = fill_arr(wave_high_)\n",
    "    wave_low_fill_ = fill_arr(wave_low_)\n",
    "\n",
    "    # ------ Todo, update_hl 에 대해서, post_terms_hl 적용 ------ #\n",
    "    wave_high_terms_low_ = np.full(len_df, np.nan)\n",
    "    wave_low_terms_high_ = np.full(len_df, np.nan)\n",
    "\n",
    "    wave_high_terms_lows = np.array([low[iin:iout + 1].min() for iin, iout in high_post_terms])  # for point rejection, Todo, min_max 설정 항상 주의\n",
    "    wave_low_terms_highs = np.array([high[iin:iout + 1].max() for iin, iout in low_post_terms])\n",
    "\n",
    "    wave_high_terms_low_[paired_cu_post_idx] = wave_high_terms_lows\n",
    "    wave_low_terms_high_[paired_co_post_idx] = wave_low_terms_highs\n",
    "\n",
    "    update_low_cu_bool = wave_high_terms_low_ < wave_low_fill_\n",
    "    update_high_co_bool = wave_low_terms_high_ > wave_high_fill_\n",
    "\n",
    "    # ------ term cnt ------ #\n",
    "    wave_high_terms_cnt_ = np.full(len_df, np.nan)\n",
    "    wave_low_terms_cnt_ = np.full(len_df, np.nan)\n",
    "\n",
    "    wave_high_terms_cnt_[paired_cu_post_idx] = high_post_terms_cnt\n",
    "    wave_low_terms_cnt_[paired_co_post_idx] = low_post_terms_cnt\n",
    "\n",
    "    wave_high_terms_cnt_fill_ = fill_arr(wave_high_terms_cnt_)\n",
    "    wave_low_terms_cnt_fill_ = fill_arr(wave_low_terms_cnt_)\n",
    "\n",
    "    # ------ hl_fill 의 prime_idx 를 찾아야함 ------ #\n",
    "    # b1_wave_high_fill_ = pd.Series(wave_high_fill_).shift(1).to_numpy()\n",
    "    # b1_wave_low_fill_ = pd.Series(wave_low_fill_).shift(1).to_numpy()\n",
    "    # wave_high_prime_idx = np.where((wave_high_fill_ != b1_wave_high_fill_) & ~np.isnan(wave_high_fill_), len_df_range, np.nan)\n",
    "    # wave_low_prime_idx = np.where((wave_low_fill_ != b1_wave_low_fill_) & ~np.isnan(wave_low_fill_), len_df_range, np.nan)\n",
    "    #\n",
    "    # high_prime_idx_fill_ = fill_arr(wave_high_prime_idx)\n",
    "    # low_prime_idx_fill_ = fill_arr(wave_low_prime_idx)\n",
    "\n",
    "    # ============ enlist to df_cols ============ #\n",
    "    t_df['wave_high_fill_{}{}'.format(itv, wave_period)] = wave_high_fill_\n",
    "    t_df['wave_low_fill_{}{}'.format(itv, wave_period)] = wave_low_fill_\n",
    "    t_df['wave_high_terms_cnt_fill_{}{}'.format(itv, wave_period)] = wave_high_terms_cnt_fill_\n",
    "    t_df['wave_low_terms_cnt_fill_{}{}'.format(itv, wave_period)] = wave_low_terms_cnt_fill_\n",
    "\n",
    "    t_df['wave_update_low_cu_bool_{}{}'.format(itv, wave_period)] = update_low_cu_bool  # temporary, for plot_check\n",
    "    t_df['wave_update_high_co_bool_{}{}'.format(itv, wave_period)] = update_high_co_bool\n",
    "\n",
    "    t_df['wave_cu_{}{}'.format(itv, wave_period)] = cu_bool  # * ~update_low_cu_bool\n",
    "    t_df['wave_co_{}{}'.format(itv, wave_period)] = co_bool  # * ~update_high_co_bool\n",
    "    \n",
    "    t_df['wave_cu_idx_fill_{}{}'.format(itv, wave_period)] = cu_fill_idx\n",
    "    t_df['wave_co_idx_fill_{}{}'.format(itv, wave_period)] = co_fill_idx\n",
    "\n",
    "    t_df['wave_co_post_idx_{}{}'.format(itv, wave_period)] = co_post_idx  # paired_\n",
    "    t_df['wave_cu_post_idx_{}{}'.format(itv, wave_period)] = cu_post_idx  # paired_\n",
    "    t_df['wave_co_post_idx_fill_{}{}'.format(itv, wave_period)] = co_post_fill_idx\n",
    "    t_df['wave_cu_post_idx_fill_{}{}'.format(itv, wave_period)] = cu_post_fill_idx\n",
    "\n",
    "    # Todo, idx 저장은 sync. 가 맞는 tf_df 에 대하여 적용하여야함\n",
    "    # ------ for roll prev_hl ------ #\n",
    "    # high_post_idx 를 위해 co_prime_idx 입력 = 뜻 : high_term's prime co_idx (high_prime_idx = wave_high 를 만들기 위한 가장 앞단의 co_idx)\n",
    "    t_df['wave_co_prime_idx_{}{}'.format(itv,\n",
    "                                         wave_period)] = co_prime_idx  # co_prime_idx wave_high_prime_idx  # high 갱신을 고려해, prev_hl 는 prime_idx 기준으로 진행\n",
    "    t_df['wave_cu_prime_idx_{}{}'.format(itv,\n",
    "                                         wave_period)] = cu_prime_idx  # cu_prime_idx wave_low_prime_idx  # cu_prime_idx's low 를 사용하겠다라는 의미, 즉 roll_prev 임\n",
    "    t_df['wave_co_prime_idx_fill_{}{}'.format(itv, wave_period)] = co_prime_fill_idx  # co_prime_fill_idx high_prime_idx_fill_\n",
    "    t_df['wave_cu_prime_idx_fill_{}{}'.format(itv, wave_period)] = cu_prime_fill_idx  # cu_prime_fill_idx low_prime_idx_fill_\n",
    "\n",
    "    # ------ for plot_checking ------ #\n",
    "    t_df['wave_cu_marker_{}{}'.format(itv, wave_period)] = get_line(cu_idx, close)\n",
    "    t_df['wave_co_marker_{}{}'.format(itv, wave_period)] = get_line(co_idx, close)\n",
    "\n",
    "    return t_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LyGnAMvLYvOZ",
    "tags": []
   },
   "source": [
    "### wave_range"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O1uu9vQnY5dn",
    "tags": []
   },
   "source": [
    "#### plot_check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YqBXjVPzdccC"
   },
   "outputs": [],
   "source": [
    "i = random.randint(0, len(res_df_))\n",
    "# i = 235290, 512385\n",
    "# i = 74470\n",
    "# i = 82533\n",
    "# i = 387103\n",
    "# i = 370055\n",
    "# i = 687581\n",
    "\n",
    "data_size = 300 # 1500 150\n",
    "assert i > data_size\n",
    "# t_df = res_df.iloc[i - data_size:i + data_size]\n",
    "# t_df = res_df.iloc[i - data_size:i].astype(float)\n",
    "t_df = res_df_.iloc[i - data_size:i].astype(float)\n",
    "a_data = t_df.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PgaNnempXRd_"
   },
   "outputs": [],
   "source": [
    "# wave_itv1, wave_period1 = '15T', 20\n",
    "wave_itv1, wave_period1 = 'T', 20\n",
    "roll_hl_cnt = 3\n",
    "\n",
    "if wave_itv1 != 'T':\n",
    "    offset = '1h' if wave_itv1 != 'D' else '9h'\n",
    "    htf_df_ = to_htf(t_df, wave_itv1, offset=offset)  # to_htf 는 ohlc, 4개의 col 만 존재 (현재까지)\n",
    "    htf_df = htf_df_[~pd.isnull(htf_df_.close)]\n",
    "    \n",
    "    # htf_df = wave_range_bb_v1(htf_df, wave_period1, itv=wave_itv1)\n",
    "    htf_df = wave_range_cci_v4_1(htf_df, wave_period1)\n",
    "    # htf_df = wave_range_cci_v3(htf_df, wave_period1)\n",
    "    # htf_df = wave_range_dc_envel_v1(htf_df, wave_period1)\n",
    "    \n",
    "    cols = list(htf_df.columns[4:])  # 15T_ohlc 를 제외한 wave_range_cci_v4 로 추가된 cols, 다 넣어버리기 (추후 혼란 방지)\n",
    "    \n",
    "    valid_co_prime_idx, valid_cu_prime_idx, roll_co_idx_arr, roll_cu_idx_arr = roll_wave_hl_idx_v5(htf_df, wave_itv1, wave_period1,\n",
    "                                                                                                           roll_hl_cnt=roll_hl_cnt)\n",
    "    \n",
    "    \"\"\" \n",
    "    1. wave_bb 의 경우 roll_hl 의 기준이 co <-> cu 변경됨 (cci 와 비교)\n",
    "    2. wave_bb : high_fill_ -> cu_prime_idx 사용\n",
    "    \"\"\"\n",
    "    htf_df = get_roll_wave_data_v2(htf_df, valid_co_prime_idx, roll_co_idx_arr, 'wave_high_fill_{}{}'.format(wave_itv1, wave_period1),\n",
    "                                   roll_hl_cnt)\n",
    "    cols += list(htf_df.columns[-roll_hl_cnt:])\n",
    "\n",
    "    htf_df = get_roll_wave_data_v2(htf_df, valid_cu_prime_idx, roll_cu_idx_arr, 'wave_low_fill_{}{}'.format(wave_itv1, wave_period1), roll_hl_cnt)\n",
    "    cols += list(htf_df.columns[-roll_hl_cnt:])\n",
    "\n",
    "    htf_df = wave_range_ratio_v4_2(htf_df, wave_itv1, wave_period1, roll_hl_cnt=roll_hl_cnt)\n",
    "    \n",
    "    cols += list(htf_df.columns[-4:])\n",
    "    \n",
    "#     cols = list(htf_df.columns)  # 그냥 다 넣어버리기 (추후 혼란 방지)\n",
    "\n",
    "#     valid_high_prime_idx, valid_low_prime_idx, roll_prev_high_idx_arr, roll_prev_low_idx_arr = roll_wave_hl_idx_v5(htf_df, wave_itv1, wave_period1, roll_hl_cnt=roll_hl_cnt)\n",
    "\n",
    "#     htf_df = get_roll_wave_data_v2(htf_df, valid_high_prime_idx, roll_prev_high_idx_arr, 'wave_high_fill_{}{}'.format(wave_itv1, wave_period1), roll_hl_cnt)\n",
    "#     cols += list(htf_df.columns[-roll_hl_cnt:])\n",
    "\n",
    "#     htf_df = get_roll_wave_data_v2(htf_df, valid_low_prime_idx, roll_prev_low_idx_arr, 'wave_low_fill_{}{}'.format(wave_itv1, wave_period1), roll_hl_cnt)\n",
    "#     cols += list(htf_df.columns[-roll_hl_cnt:])\n",
    "\n",
    "#     htf_df = wave_range_ratio_v4_2(htf_df, wave_itv1, wave_period1, roll_hl_cnt=3)\n",
    "#     cols += list(htf_df.columns[-4:])  # wrr 은 4개의 cols\n",
    "\n",
    "    # ------ 필요한 cols 만 join (htf's idx 정보는 ltf 와 sync. 가 맞지 않음 - join 불가함) ------ #\n",
    "    t_df.drop(cols, inplace=True, axis=1, errors='ignore')    \n",
    "    t_df = t_df.join(to_lower_tf_v3(t_df, htf_df, cols, backing_i=0, ltf_itv='T').loc[t_df.index], how='inner')\n",
    "    \n",
    "    \n",
    "\n",
    "else:  \n",
    "    # t_df = wave_range_bb_v1(t_df, wave_period1, itv=wave_itv1)\n",
    "    t_df = wave_range_cci_v4_1(t_df, wave_period1, itv=wave_itv1)\n",
    "    # t_df = wave_range_stoch_v1(t_df, wave_period1)\n",
    "    # t_df = wave_range_dc_envel_v1(t_df, wave_period1)\n",
    "\n",
    "    \"\"\" \n",
    "    1. wave_bb 의 경우 roll_hl 의 기준이 co <-> cu 변경됨 (cci 와 비교)\n",
    "    2. wave_bb : high_fill_ -> cu_prime_idx 사용\n",
    "    \"\"\"\n",
    "    valid_co_prime_idx, valid_cu_prime_idx, roll_co_idx_arr, roll_cu_idx_arr = roll_wave_hl_idx_v5(t_df, wave_itv1, wave_period1, roll_hl_cnt=roll_hl_cnt)\n",
    "    t_df = get_roll_wave_data_v2(t_df, valid_co_prime_idx, roll_co_idx_arr, 'wave_high_fill_{}{}'.format(wave_itv1, wave_period1), roll_hl_cnt)\n",
    "    t_df = get_roll_wave_data_v2(t_df, valid_cu_prime_idx, roll_cu_idx_arr, 'wave_low_fill_{}{}'.format(wave_itv1, wave_period1), roll_hl_cnt)\n",
    "\n",
    "    t_df = wave_range_ratio_v4_2(t_df, wave_itv1, wave_period1, roll_hl_cnt=roll_hl_cnt)\n",
    "\n",
    "# t_df = wave_range_v11(t_df, config)\n",
    "# t_df = wave_range_v11_2(t_df, config)\n",
    "# t_df = wave_range_dcbase_v11_3(t_df, config, over_period=2)\n",
    "# t_df = wave_range_cci_v1(t_df, wave_itv1, wave_period1)\n",
    "# t_df = wave_range_v12(t_df, config, ltf_df=None)\n",
    "# t_df = wave_range_v13(t_df, config, ltf_df=None, term_thresh=1)\n",
    "# t_df = wave_range_v14(t_df, config, ltf_df=None, term_thresh1=1, term_thresh2=3)\n",
    "# t_df = wave_range_v15(t_df, config, term_thresh1=2, term_thresh2=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 862
    },
    "executionInfo": {
     "elapsed": 2482,
     "status": "ok",
     "timestamp": 1660482394219,
     "user": {
      "displayName": "7th June",
      "userId": "08178289703395036410"
     },
     "user_tz": -540
    },
    "id": "-Dr_tTk9csFm",
    "outputId": "cbbc017d-db06-458d-ce7d-d3ad9d67a7ad"
   },
   "outputs": [],
   "source": [
    "col_idx_dict = \\\n",
    "{\n",
    "  \"ohlc_col_idxs\": get_col_idxs(t_df, ['open', 'high', 'low', 'close']),\n",
    "}   \n",
    "\n",
    "\n",
    "plt.style.use(['dark_background', 'fast'])\n",
    "fig = plt.figure(figsize=(15, 15), dpi=65)\n",
    "nrows, ncols = 2, 1\n",
    "gs = gridspec.GridSpec(nrows=nrows,  # row 부터 index 채우고 col 채우는 순서임 (gs_idx)\n",
    "                        ncols=ncols,\n",
    "                        height_ratios=[3, 1]\n",
    "                        )\n",
    "\n",
    "ax = fig.add_subplot(gs[0])\n",
    "\n",
    "# ------ candles ------ #\n",
    "# candle_plot(a_data[:, col_idx_dict['ohlc_col_idxs']], ax, alpha=1.0, wickwidth=1.0)\n",
    "# candle_plot(a_data[:, col_idx_dict['ohlc_col_idxs']], ax, alpha=1.0)\n",
    "candle_plot_v2(ax, a_data[:, col_idx_dict['ohlc_col_idxs']], alpha=1.0, wickwidth=1.0)\n",
    "# _ = [step_col_plot(a_data[:, params[0]], *params[1:]) for params in col_idx_dict['step_col_info']]\n",
    "\n",
    "len_df = len(t_df)   \n",
    "len_df_range = np.arange(len_df).astype(int)\n",
    "\n",
    "# ============ ============ ============ #\n",
    "plot_size = 100\n",
    "plot_size = len_df\n",
    "# ============ ============ ============ #\n",
    "\n",
    "wave_high_fill_ = t_df['wave_high_fill_{}{}'.format(wave_itv1, wave_period1)].to_numpy()\n",
    "wave_low_fill_ = t_df['wave_low_fill_{}{}'.format(wave_itv1, wave_period1)].to_numpy()\n",
    "wave_high_terms_cnt_fill_ = t_df['wave_high_terms_cnt_fill_{}{}'.format(wave_itv1, wave_period1)].to_numpy()\n",
    "wave_low_terms_cnt_fill_ = t_df['wave_low_terms_cnt_fill_{}{}'.format(wave_itv1, wave_period1)].to_numpy()\n",
    "\n",
    "wave_cu_idx_ = get_index_bybool(t_df['wave_cu_{}{}'.format(wave_itv1, wave_period1)].to_numpy(), len_df_range)\n",
    "wave_co_idx_ = get_index_bybool(t_df['wave_co_{}{}'.format(wave_itv1, wave_period1)].to_numpy(), len_df_range)\n",
    "# wave_cu_bool_idx_ = get_index_bybool(t_df['wave_cu_bool_{}{}'.format(wave_itv1, wave_period1)].to_numpy(), len_df_range)\n",
    "# wave_co_bool_idx_ = get_index_bybool(t_df['wave_co_bool_{}{}'.format(wave_itv1, wave_period1)].to_numpy(), len_df_range)\n",
    "wave_update_low_cu_bool_idx_ = get_index_bybool(t_df['wave_update_low_cu_bool_{}{}'.format(wave_itv1, wave_period1)].to_numpy(), len_df_range)\n",
    "wave_update_high_co_bool_idx_ = get_index_bybool(t_df['wave_update_high_co_bool_{}{}'.format(wave_itv1, wave_period1)].to_numpy(), len_df_range)\n",
    "\n",
    "wave_cu_prime_idx_ = t_df['wave_cu_prime_idx_{}{}'.format(wave_itv1, wave_period1)].to_numpy()\n",
    "wave_co_prime_idx_ = t_df['wave_co_prime_idx_{}{}'.format(wave_itv1, wave_period1)].to_numpy()\n",
    "wave_cu_prime_idx_fill_ = t_df['wave_cu_prime_idx_fill_{}{}'.format(wave_itv1, wave_period1)].to_numpy()\n",
    "wave_co_prime_idx_fill_ = t_df['wave_co_prime_idx_fill_{}{}'.format(wave_itv1, wave_period1)].to_numpy()\n",
    "\n",
    "wave_cu_post_idx_ = t_df['wave_cu_post_idx_{}{}'.format(wave_itv1, wave_period1)].to_numpy()\n",
    "wave_co_post_idx_ = t_df['wave_co_post_idx_{}{}'.format(wave_itv1, wave_period1)].to_numpy()\n",
    "wave_cu_post_idx_fill_ = t_df['wave_cu_post_idx_fill_{}{}'.format(wave_itv1, wave_period1)].to_numpy()\n",
    "wave_co_post_idx_fill_ = t_df['wave_co_post_idx_fill_{}{}'.format(wave_itv1, wave_period1)].to_numpy()\n",
    "\n",
    "wave_cu_marker_ = t_df['wave_cu_marker_{}{}'.format(wave_itv1, wave_period1)].to_numpy()\n",
    "wave_co_marker_ = t_df['wave_co_marker_{}{}'.format(wave_itv1, wave_period1)].to_numpy()\n",
    "\n",
    "# ============ plot_check ============ #\n",
    "# dc_base_ = t_df['dc_base_{}{}'.format(wave_itv1, wave_period1)].to_numpy()\n",
    "# plt.step(len_df_range, dc_base_, alpha=1.0, color='cyan', linewidth=1)\n",
    "\n",
    "plt.step(len_df_range, wave_cu_marker_, \"o\", alpha=1.0, color='#ff00ff', markersize=5)\n",
    "plt.step(len_df_range, wave_co_marker_, \"o\", alpha=1.0, color='#00ff00', markersize=5)\n",
    "\n",
    "# plt.step(len_df_range, t_df['dc_upper_{}{}'.format(wave_itv1, wave_period1)], color='#ffeb3b')\n",
    "# plt.step(len_df_range, t_df['dc_lower_{}{}'.format(wave_itv1, wave_period1)], color='#ffeb3b')\n",
    "\n",
    "# plt.step(len_df_range, t_df['bb_upper_{}{}'.format(wave_itv1, wave_period1)], color='#ffeb3b')\n",
    "# plt.step(len_df_range, t_df['bb_lower_{}{}'.format(wave_itv1, wave_period1)], color='#ffeb3b')\n",
    "\n",
    "# [plt.axvline(int(idx_), color=\"#ff0000\") for idx_ in wave_cu_bool_idx_ if not np.isnan(idx_)]\n",
    "# [plt.axvline(int(idx_), color=\"#0000ff\") for idx_ in wave_co_bool_idx_ if not np.isnan(idx_)]\n",
    "[plt.axvline(int(idx_), color=\"#ff0000\") for idx_ in wave_update_low_cu_bool_idx_ if not np.isnan(idx_)]\n",
    "[plt.axvline(int(idx_), color=\"#0000ff\") for idx_ in wave_update_high_co_bool_idx_ if not np.isnan(idx_)]\n",
    "\n",
    "# [plt.axvline(int(idx_), color=\"#ff00ff\") for idx_ in wave_cu_idx_ if not np.isnan(idx_)]\n",
    "# [plt.axvline(int(idx_), color=\"#00ff00\") for idx_ in wave_co_idx_ if not np.isnan(idx_)]\n",
    "\n",
    "[plt.axvline(int(idx_), color=\"#00ff00\") for idx_ in wave_co_prime_idx_ if not np.isnan(idx_)]\n",
    "[plt.axvline(int(idx_), color=\"#ff00ff\") for idx_ in wave_cu_prime_idx_ if not np.isnan(idx_)]\n",
    "\n",
    "plt.step(len_df_range, wave_high_fill_, \"*\", alpha=1.0, color='#00ff00', markersize=6)\n",
    "plt.step(len_df_range, wave_low_fill_, \"*\", alpha=1.0, color='#ff00ff', markersize=6)\n",
    "\n",
    "# ------ data check in gs[0] ------ #\n",
    "# plt.axvline(wave_cu_post_idx_fill_[230], color='r')\n",
    "# plt.axvline(wave_cu_prime_idx_fill_[230])\n",
    "window_idx = 182\n",
    "plt.axvline(window_idx)\n",
    "\n",
    "plt.xlim(0, plot_size)\n",
    "\n",
    "plt.subplot(gs[1])\n",
    "\n",
    "# --- cci --- #\n",
    "cci_ = t_df['cci_{}{}'.format(wave_itv1, wave_period1)].to_numpy()\n",
    "plt.step(len_df_range, cci_, alpha=1.0, color='yellow', linewidth=2)\n",
    "plt.axhline(100, color=\"#ffffff\")\n",
    "plt.axhline(-100, color=\"#ffffff\")\n",
    "\n",
    "# --- stoch --- #\n",
    "# stoch_ = t_df['stoch_{}{}33'.format(wave_itv1, wave_period1)].to_numpy()\n",
    "# plt.step(len_df_range, stoch_, alpha=1.0, color='yellow', linewidth=2)\n",
    "# plt.axhline(67, color=\"#ffffff\")\n",
    "# plt.axhline(33, color=\"#ffffff\")\n",
    "\n",
    "[plt.axvline(int(idx_), color=\"#ff00ff\") for idx_ in wave_cu_idx_ if not np.isnan(idx_)]\n",
    "[plt.axvline(int(idx_), color=\"#00ff00\") for idx_ in wave_co_idx_ if not np.isnan(idx_)]   # long 이라서 초록색임\n",
    "\n",
    "# plt.step(len_df_range, wave_high_terms_cnt_fill_, alpha=1.0, color='yellow', linewidth=2)\n",
    "# plt.step(len_df_range, wave_low_terms_cnt_fill_, alpha=1.0, color='yellow', linewidth=2)\n",
    "# plt.step(len_df_range, wave_high_terms_cnt_fill_, \"*\", alpha=1.0, color='#00ff00', markersize=6)\n",
    "# plt.step(len_df_range, wave_low_terms_cnt_fill_, \"*\", alpha=1.0, color='#ff00ff', markersize=6)\n",
    "\n",
    "\n",
    "plt.xlim(0, plot_size)  # for sync. with gs[0]\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c2yVTn1tnxMn",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### data_window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(window_idx)\n",
    "print(t_df['wave_high_fill_{}{}_-{}'.format(wave_itv1, wave_period1, 1)][window_idx])\n",
    "print(t_df['wave_high_fill_{}{}_-{}'.format(wave_itv1, wave_period1, 2)][window_idx])\n",
    "print(t_df['wave_high_fill_{}{}_-{}'.format(wave_itv1, wave_period1, 3)][window_idx])\n",
    "print(t_df['wave_low_fill_{}{}_-{}'.format(wave_itv1, wave_period1, 1)][window_idx])\n",
    "print(t_df['wave_low_fill_{}{}_-{}'.format(wave_itv1, wave_period1, 2)][window_idx])\n",
    "print(t_df['wave_low_fill_{}{}_-{}'.format(wave_itv1, wave_period1, 3)][window_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# valid_co_prime_idx, valid_cu_prime_idx, roll_co_idx_arr, roll_cu_idx_arr = roll_wave_hl_idx_v5(t_df, wave_itv1, wave_period1,\n",
    "#                                                                                                            roll_hl_cnt=roll_hl_cnt)\n",
    "        \n",
    "# t_df = get_roll_wave_data_v2(t_df, valid_cu_prime_idx, roll_cu_idx_arr, 'wave_high_fill_{}{}'.format(wave_itv1, wave_period1),\n",
    "#                                roll_hl_cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wave_co_prime_idx_[~pd.isnull(wave_co_prime_idx_)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_co_prime_idx\n",
    "roll_co_idx_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 351,
     "status": "ok",
     "timestamp": 1660483201485,
     "user": {
      "displayName": "7th June",
      "userId": "08178289703395036410"
     },
     "user_tz": -540
    },
    "id": "6bra-Br3lq1h",
    "outputId": "8c90e368-24c7-439e-9733-567d4848859a"
   },
   "outputs": [],
   "source": [
    "# print(valid_high_prime_idx)  # = valid_co_prime_idx\n",
    "# print(roll_prev_high_idx_arr)   # = roll_prev_co_idx_arr\n",
    "print(valid_low_prime_idx)  # = valid_co_prime_idx\n",
    "print(roll_prev_low_idx_arr)   # = roll_prev_co_idx_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 345,
     "status": "ok",
     "timestamp": 1660484147094,
     "user": {
      "displayName": "7th June",
      "userId": "08178289703395036410"
     },
     "user_tz": -540
    },
    "id": "0DgdFydfB1f-",
    "outputId": "1677cb32-6a71-4292-b49a-204f7da8ed20"
   },
   "outputs": [],
   "source": [
    "idx = 239\n",
    "# print(wave_cu_post_idx_fill_[idx])\n",
    "# print(wave_co_prime_idx_fill_[idx - 1])\n",
    "# print(wave_co_post_idx_fill_[idx - 1])\n",
    "# print(wave_co_prime_idx_fill_[idx])\n",
    "print(wave_co_post_idx_fill_[int(wave_cu_post_idx_fill_[idx])])\n",
    "print(wave_co_idx_[idx])\n",
    "\n",
    "# print(len(t_df))\n",
    "# len(wave_co_prime_idx_fill_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1MNVIExLULhJ",
    "tags": []
   },
   "source": [
    "### legacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xpyP5t8Ht_pE",
    "tags": []
   },
   "source": [
    "#### calc recursive indi's min_period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N1Hbm4OA4Tkk"
   },
   "outputs": [],
   "source": [
    "ticker_prcn = get_precision_by_price(res_df.close.iloc[-1]) + 2\n",
    "\n",
    "target_len = 300000\n",
    "slice_len_list = list(range(50, 10000, 100))\n",
    "slice_len_list.insert(0, target_len)\n",
    "\n",
    "start_time = time.time()\n",
    "# prev_int_, prev_pnts_ = None, None\n",
    "offset = 1\n",
    "\n",
    "for sl_idx, sample_len in enumerate(slice_len_list):\n",
    "\n",
    "  sample_df = res_df.iloc[-sample_len -offset:-offset]\n",
    "  sample_len2 = sample_len\n",
    "\n",
    "  # --------- input using indi.s --------- #\n",
    "  # res = ema_v0(sample_df['close'], 190)\n",
    "  res = rsi(sample_df, 14)\n",
    "\n",
    "    #    to_htf()    #\n",
    "  # df_5T = to_htf(sample_df, itv_='5T', offset='1h')\n",
    "  # sample_len2 = len(df_5T)\n",
    "\n",
    "  # # --------- input using htf_indi. --------- #\n",
    "  # res = ema(df_5T['close'], 195)\n",
    "  # -------------------------------------- #\n",
    "\n",
    "  res_last_row = res.iloc[-1]\n",
    "  if pd.isnull(res_last_row):\n",
    "    continue\n",
    "\n",
    "  # print(res_last_row)\n",
    "  # break\n",
    "\n",
    "  # sample_df = sample_df.join(to_lower_tf_v2(sample_df, df_5T, [-1]), how='inner')\n",
    "\n",
    "\n",
    "  #   자리수 분할 계산    #\n",
    "  int_, points_ = str(res_last_row).split('.')\n",
    "  pnts_ = points_[:ticker_prcn]\n",
    "\n",
    "  if sl_idx == 0:\n",
    "    target_int_ = int_\n",
    "    target_pnts_ = pnts_\n",
    "    print(\"target {} ({}) -> {} {}\".format(sample_len, sample_len2, int_, points_))\n",
    "\n",
    "  else:\n",
    "    if target_int_ == int_ and target_pnts_ == pnts_:\n",
    "      # print(sample_len, \"({})\".format(sample_len2), '->', int_, pnts_, end='\\n\\n')\n",
    "      print(\"{} ({}) -> {} {}\\n\".format(sample_len, sample_len2, int_, points_))\n",
    "      break\n",
    "\n",
    "print(time.time() - start_time)  # (1301)(1361)(1301)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EOXQbXixiQcK",
    "tags": []
   },
   "source": [
    "#### volume_profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7pFuazxpgx9B"
   },
   "outputs": [],
   "source": [
    "session_df = res_df_.iloc[-1440:] # 0.159 -> 0.024 (14400 -> 1440)\n",
    "volume = session_df['volume'].to_numpy()\n",
    "close = session_df['close'].to_numpy()\n",
    "# px.histogram(session_df, x='volume', y='close', nbins=150, orientation='h').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SoPJkiyKiXLM"
   },
   "outputs": [],
   "source": [
    "kde_factor = 0.05\n",
    "num_samples = 100\n",
    "\n",
    "start_time = time.time()\n",
    "kde = stats.gaussian_kde(close,weights=volume,bw_method=kde_factor)\n",
    "kdx = np.linspace(close.min(),close.max(),num_samples)\n",
    "kdy = kde(kdx)\n",
    "ticks_per_sample = (kdx.max() - kdx.min()) / num_samples\n",
    "print(\"ticks_per_sample :\", ticks_per_sample)  # sample 당 가격\n",
    "print(\"kdy elapsed_time :\", time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mK2jBddAxJ14"
   },
   "outputs": [],
   "source": [
    "peaks,_ = signal.find_peaks(kdy)\n",
    "pkx = kdx[peaks]\n",
    "pky = kdy[peaks]\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "# plt.hist(close, bins=num_samples, weights=volume, alpha=.8, color='#1592e6')\n",
    "# plt.plot(kdx, kdy, color='white')\n",
    "# plt.plot(pkx, pky, 'bo', color='yellow')\n",
    "plt.plot(kdy, kdx, color='white')\n",
    "plt.plot(pky, pkx, 'bo', color='yellow')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a8tpQZCy0SO1"
   },
   "outputs": [],
   "source": [
    "pkx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gfA946s8UgS0"
   },
   "outputs": [],
   "source": [
    "# ------ vp 의 indi. 화가 필요함 -> point 설정 ------ #\n",
    "# 1. 4 level 은 미리 만들어놓는게 맞는걸로 보임 -> 추종하는 function 이 많음 (utils_tr, ep_out ...)\n",
    "#   a. 4 level 에 국한하는게 아니라, 모든 peaks 에 대해 levels 설정\n",
    "#   b. 각 session 별로 peak_list 가 주어질 것\n",
    "#     i. prev_data 사용해야하는점 주의 (session vp 는 future_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3T-9FwWFXR4f",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "##### prominence_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zqj944n-WzyZ"
   },
   "outputs": [],
   "source": [
    "# peaks  # ndarray\n",
    "# kdx  # ndarray\n",
    "# kdy  # ndarray\n",
    "# kdx.min()\n",
    "left_base * ticks_per_sample\n",
    "# volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Rvqv0RGojo9h"
   },
   "outputs": [],
   "source": [
    "print(peak_y)\n",
    "print(peak_props['prominences'])\n",
    "peak_props"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c2729DJ6h720",
    "tags": []
   },
   "source": [
    "#### imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Rucj-iepiR_-"
   },
   "outputs": [],
   "source": [
    "t_df = res_df_.iloc[-120:-100]\n",
    "a_data = t_df.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ktm1aB-Bh7GH"
   },
   "outputs": [],
   "source": [
    "plt.style.use(['dark_background', 'fast'])\n",
    "fig = plt.figure(figsize=(15, 9))\n",
    "nrows, ncols = 1, 1\n",
    "gs = gridspec.GridSpec(nrows=nrows,  # row 부터 index 채우고 col 채우는 순서임 (gs_idx)\n",
    "                        ncols=ncols\n",
    "                        # height_ratios=[3, 1]\n",
    "                        )\n",
    "\n",
    "ax = fig.add_subplot(gs[0])\n",
    "\n",
    "# ------ candles ------ #\n",
    "candle_plot(a_data[:, col_idx_dict['ohlc_col_idxs']], ax, alpha=1.0, wickwidth=1.0)\n",
    "_ = [step_col_plot(a_data[:, params[0]], *params[1:]) for params in col_idx_dict['step_col_info']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jMQBhQ1Ljt5Y"
   },
   "outputs": [],
   "source": [
    "def imb_ratio_v4(df, itv):\n",
    "\n",
    "  itv_num = itv_to_number(itv)\n",
    "\n",
    "  close = df['close_{}'.format(itv)].to_numpy()\n",
    "  open = df['open_{}'.format(itv)].to_numpy()\n",
    "\n",
    "  b1_close = df['close_{}'.format(itv)].shift(itv_num).to_numpy()\n",
    "  b1_open = df['open_{}'.format(itv)].shift(itv_num).to_numpy()\n",
    "  b1_high = df['high_{}'.format(itv)].shift(itv_num).to_numpy()\n",
    "  b1_low = df['low_{}'.format(itv)].shift(itv_num).to_numpy()\n",
    "\n",
    "  body_range = abs(close - open)\n",
    "  b1_body_range = abs(b1_close - b1_open)\n",
    "\n",
    "  df['body_rel_ratio_{}'.format(itv)] = body_range / b1_body_range\n",
    "\n",
    "  short_body_range = np.where(close <= b1_low, body_range, b1_body_range)\n",
    "  long_body_range = np.where(close >= b1_high, body_range, b1_body_range)\n",
    "\n",
    "  # 추후에 통계 측정해야함 -> bir 에 따른 개별 trader 의 epout / tpep 이라던가 => short 에 양봉은 취급안함 (why use np.nan)\n",
    "  df['short_ir_{}'.format(itv)] = np.where(close < open, (b1_low - close) / short_body_range, np.nan) # close < open & close < b1_low\n",
    "  df['long_ir_{}'.format(itv)] = np.where(close > open, (close - b1_high) / long_body_range, np.nan) # close > open & close > b1_high\n",
    "\n",
    "  return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I1Vq_BiKpY3P"
   },
   "outputs": [],
   "source": [
    "def imb_ratio_v3(df, itv):\n",
    "\n",
    "  itv_num = itv_to_number(itv)\n",
    "\n",
    "  # high = df['high_{}'.format(itv)].to_numpy()\n",
    "  # low = df['low_{}'.format(itv)].to_numpy()\n",
    "  # candle_range = high - low\n",
    "\n",
    "  close = df['close_{}'.format(itv)].to_numpy()\n",
    "  open = df['open_{}'.format(itv)].to_numpy()\n",
    "\n",
    "  b1_close = df['close_{}'.format(itv)].shift(itv_num).to_numpy()\n",
    "  b1_open = df['open_{}'.format(itv)].shift(itv_num).to_numpy()\n",
    "  b1_high = df['high_{}'.format(itv)].shift(itv_num).to_numpy()\n",
    "  b1_low = df['low_{}'.format(itv)].shift(itv_num).to_numpy()\n",
    "\n",
    "  body_range = abs(close - open)\n",
    "  b1_body_range = abs(b1_close - b1_open)\n",
    "\n",
    "  df['body_rel_ratio_{}'.format(itv)] = body_range / b1_body_range\n",
    "\n",
    "  short_body_ratio = np.where(close <= b1_low, body_range, b1_body_range)\n",
    "  long_body_range = np.where(close >= b1_high, body_range, b1_body_range)\n",
    "\n",
    "  # 추후에 통계 측정해야함 -> bir 에 따른 개별 trader 의 epout / tpep 이라던가 => short 에 양봉은 취급안함 (why use np.nan)\n",
    "  df['short_ir_{}'.format(itv)] = np.where(close < open, (b1_low - close) / body_range, np.nan) # close < open & close < b1_low\n",
    "  df['long_ir_{}'.format(itv)] = np.where(close > open, (close - b1_high) / body_range, np.nan) # close > open & close > b1_high\n",
    "\n",
    "  return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mtnMUkhwkdrE"
   },
   "outputs": [],
   "source": [
    "imb_ratio(t_df, \"5T\")\n",
    "# imb_ratio_v3(t_df, \"5T\")\n",
    "# imb_ratio_v4(t_df, \"5T\")\n",
    "\n",
    "t_df.tail(100).short_ir_5T  # .461871\n",
    "# t_df.iloc[:, -10:]\n",
    "# t_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RVUs1YE_pgBI"
   },
   "outputs": [],
   "source": [
    "# imb_ratio(t_df, \"5T\")\n",
    "# imb_ratio_v3(t_df, \"5T\")\n",
    "imb_ratio_v4(t_df, \"5T\")\n",
    "\n",
    "t_df.tail(100).short_ir_5T  # .461871\n",
    "# t_df.iloc[:, -10:]\n",
    "# t_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bieHiKjBkuFL"
   },
   "outputs": [],
   "source": [
    "# ------ rtc 1, 0 개념 ------ #\n",
    "# short_rtc_1 = close\n",
    "# short_rtc_0 = b1_low\n",
    "\n",
    "# long_rtc_1 = close\n",
    "# long_rtc_0 = b1_high\n",
    "\n",
    "# rtc 로 활용하려면, col 로 추가해야할 것 -> 추가할만한 col_name 은 아님\n",
    "# 1. h_candle 인 경우 -> ?\n",
    "#   a. h_candle_v3 먹이고, open_{}.shift(num_itv).to_numpy() 진행 -> ex. res_df['close_{}'.format(hc_itv)].shift(itv_num).to_numpy() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3X6bMXJIjPYy"
   },
   "outputs": [],
   "source": [
    "# 1. 현재 종가 - 이전 고가 = imb_range (long)\n",
    "long_imb_range = t_df.close - t_df.high.shift(1)\n",
    "# 2. 이전 저가 - 현재 종가 - imb_range (short)\n",
    "short_imb_range = t_df.low.shift(1) - t_df.close"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qsyPhNR8yP1c",
    "tags": []
   },
   "source": [
    "### olds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q1nEA19v7Qpj"
   },
   "outputs": [],
   "source": [
    "from math import floor\n",
    "\n",
    "\n",
    "def _calc_dev(base_price, price):\n",
    "    return 100 * (price - base_price) / base_price\n",
    "\n",
    "\n",
    "def zigzag(highs, lows, depth=10, dev_threshold=5):\n",
    "    def pivots(src_raw, length, isHigh):\n",
    "        src = list(reversed(src_raw))\n",
    "        bar_index = list(range(len(src)))\n",
    "        for start in range(0, len(src)):\n",
    "            if start + 2 * length + 1 > len(src) - 1:\n",
    "                return\n",
    "            p = 0\n",
    "            if length < len(src) - start:\n",
    "                p = src[start + length]\n",
    "            if length == 0:\n",
    "                yield 0, p\n",
    "            else:\n",
    "                isFound = True\n",
    "                for i in range(start, start + length):\n",
    "                    if isHigh and src[i] > p:\n",
    "                        isFound = False\n",
    "                    if not isHigh and src[i] < p:\n",
    "                        isFound = False\n",
    "                for i in range(start + length + 1, start + 2 * length + 1):\n",
    "                    if isHigh and src[i] >= p:\n",
    "                        isFound = False\n",
    "                    c = not isHigh and src[i] <= p\n",
    "                    if c:\n",
    "                        isFound = False\n",
    "                if isFound:\n",
    "                    yield (bar_index[start + length], p)\n",
    "                else:\n",
    "                    yield None, None\n",
    "\n",
    "    data_highs = [x for x in pivots(highs, floor(depth / 2), True) if x[0]]\n",
    "    data_lows = [x for x in pivots(lows, floor(depth / 2), False) if x[0]]\n",
    "\n",
    "    raw_pairs = []\n",
    "\n",
    "    for i, (ind, p) in enumerate(data_highs):\n",
    "        lows_d = sorted([(ind_l, p_l) for ind_l, p_l in data_lows if ind > ind_l], key=lambda x: x[0])\n",
    "        if lows_d:\n",
    "            lows = lows_d[-1]\n",
    "\n",
    "            if abs(_calc_dev(lows[1], p)) >= dev_threshold:\n",
    "                raw_pairs.append(\n",
    "                    ((ind, p),\n",
    "                     (lows[0], lows[1]))\n",
    "                )\n",
    "\n",
    "    result = []\n",
    "\n",
    "    for (i_h, p_h),(i_l, p_l) in raw_pairs:\n",
    "        if not result:\n",
    "            result.append(((i_h, p_h),(i_l, p_l)))\n",
    "            continue\n",
    "\n",
    "        if i_l == result[-1][1][0]:\n",
    "            if p_h > result[-1][0][1]:\n",
    "                result = result[:-1]\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "        result.append(((i_h, p_h),(i_l, p_l)))\n",
    "\n",
    "    return result\n",
    "\n",
    "# highs, lows = t_df.high.to_numpy(), t_df.low.to_numpy()\n",
    "# zigzag(highs, lows, depth=5, dev_threshold=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fGlmdyb97j4Q"
   },
   "outputs": [],
   "source": [
    "t_df = res_df_.iloc[-120:]\n",
    "a_data = t_df.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ropzIp0wUPAA"
   },
   "outputs": [],
   "source": [
    "# import plotly.express as px\n",
    "# import plotly.graph_objects as go\n",
    "\n",
    "def get_dist_plot(c, v, kx, ky):\n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(go.Histogram(name='Vol Profile', x=c, y=v, nbinsx=150, \n",
    "                               histfunc='sum', histnorm='probability density',\n",
    "                               marker_color='#B0C4DE'))\n",
    "    fig.add_trace(go.Scatter(name='KDE', x=kx, y=ky, mode='lines', marker_color='#D2691E'))    \n",
    "\n",
    "    peaks,_ = signal.find_peaks(kdy)\n",
    "    pkx = kdx[peaks]\n",
    "    pky = kdy[peaks]\n",
    "    pk_marker_args=dict(size=10, color='black')\n",
    "    fig.add_trace(go.Scatter(name=\"Peaks\", x=pkx, y=pky, mode='markers', marker=pk_marker_args))\n",
    "    fig.show()\n",
    "\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nLWAP1Cl2Hvu"
   },
   "outputs": [],
   "source": [
    "def wave_range_ratio(res_df, config, bb_itv, bb_period):\n",
    "\n",
    "  wave_itv = pd.infer_freq(res_df.index)\n",
    "  wave_period = config.tr_set.wave_period\n",
    "\n",
    "  bb_upper_ = res_df['bb_upper_{}{}'.format(bb_itv, bb_period)].to_numpy()\n",
    "  bb_lower_ = res_df['bb_lower_{}{}'.format(bb_itv, bb_period)].to_numpy()\n",
    "  \n",
    "  cu_prime_idx_fill_ = res_df['wave_cu_prime_idx_fill_{}{}'.format(wave_itv, wave_period)].to_numpy()\n",
    "  co_prime_idx_fill_ = res_df['wave_co_prime_idx_fill_{}{}'.format(wave_itv, wave_period)].to_numpy()\n",
    "\n",
    "  cu_bb_range = get_line(co_prime_idx_fill_, bb_upper_) - get_line(co_prime_idx_fill_, bb_lower_)  # cu 에서 co_prime 의 bb_range 사용\n",
    "  co_bb_range = get_line(cu_prime_idx_fill_, bb_upper_) - get_line(cu_prime_idx_fill_, bb_lower_)\n",
    "\n",
    "  wave_range = res_df['wave_high_fill_{}{}'.format(wave_itv, wave_period)].to_numpy() - res_df['wave_low_fill_{}{}'.format(wave_itv, wave_period)].to_numpy()\n",
    "  \n",
    "  res_df['cu_wrr_{}{}'.format(wave_itv, wave_period)] = wave_range / cu_bb_range   # for cu (currently, long)\n",
    "  res_df['co_wrr_{}{}'.format(wave_itv, wave_period)] = wave_range / co_bb_range\n",
    "\n",
    "  return res_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H_zPYIshbZgP"
   },
   "outputs": [],
   "source": [
    "# Todo, future_data\n",
    "def enough_space(res_df, itv, period):\n",
    "\n",
    "  dc_upper_ = res_df['dc_upper_{}{}'.format(itv, period)].to_numpy()\n",
    "  dc_base_ = res_df['dc_base_{}{}'.format(itv, period)].to_numpy()\n",
    "  dc_lower_ = res_df['dc_lower_{}{}'.format(itv, period)].to_numpy()  \n",
    "  high_ = res_df['high_{}'.format(config.loc_set.point.tf_entry)].to_numpy()\n",
    "  low_ = res_df['low_{}'.format(config.loc_set.point.tf_entry)].to_numpy()\n",
    "\n",
    "  half_dc_gap = dc_upper_ - dc_base_\n",
    "\n",
    "  res_df['cu_es_{}{}'.format(itv, period)] = (low_ - dc_lower_) / half_dc_gap\n",
    "  res_df['co_es_{}{}'.format(itv, period)] = (dc_upper_ - high_) / half_dc_gap\n",
    "\n",
    "  return res_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o3IUBc59VR5D"
   },
   "outputs": [],
   "source": [
    "# Todo, future_data\n",
    "def candle_range_ratio(res_df, c_itv, bb_itv, bb_period):\n",
    "\n",
    "  itv_num = itv_to_number(c_itv)\n",
    "\n",
    "  b1_bb_upper_ = res_df['bb_upper_{}{}'.format(bb_itv, bb_period)].shift(itv_num).to_numpy()\n",
    "  b1_bb_lower_ = res_df['bb_lower_{}{}'.format(bb_itv, bb_period)].shift(itv_num).to_numpy()\n",
    "  bb_range = b1_bb_upper_ - b1_bb_lower_   # <-- h_candle's open_idx 의 bb_gap 사용\n",
    "\n",
    "  high_ = res_df['high_{}'.format(c_itv)].to_numpy()\n",
    "  low_ = res_df['low_{}'.format(c_itv)].to_numpy()\n",
    "  candle_range = high_ - low_  # 부호로 양 / 음봉 구분 (양봉 > 0)\n",
    "  \n",
    "  res_df['crr_{}'.format(c_itv)] = candle_range / bb_range\n",
    "\n",
    "  return res_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u3yFd8Dcok5m"
   },
   "outputs": [],
   "source": [
    "def body_rel_ratio(res_df, c_itv):\n",
    "\n",
    "  itv_num = itv_to_number(c_itv)\n",
    "  \n",
    "  b1_close_ = res_df['close_{}'.format(c_itv)].shift(itv_num).to_numpy()\n",
    "  b1_open_ = res_df['open_{}'.format(c_itv)].shift(itv_num).to_numpy()\n",
    "  b1_body_range = abs(b1_close_ - b1_open_)\n",
    "\n",
    "  close_ = res_df['close_{}'.format(c_itv)].to_numpy()\n",
    "  open_ = res_df['open_{}'.format(c_itv)].to_numpy()\n",
    "  body_range = abs(close_ - open_)\n",
    "  \n",
    "  res_df['body_rel_ratio_{}'.format(c_itv)] = body_range / b1_body_range\n",
    "\n",
    "  return res_df\n",
    "\n",
    "def dc_over_body_ratio(res_df, c_itv, dc_itv, dc_period):\n",
    "  close_ = res_df['close_{}'.format(c_itv)].to_numpy()\n",
    "  open_ = res_df['open_{}'.format(c_itv)].to_numpy()\n",
    "  body_range = abs(close_ - open_)\n",
    "  \n",
    "  dc_upper_ = res_df['dc_upper_{}{}'.format(dc_itv, dc_period)].to_numpy()\n",
    "  dc_lower_ = res_df['dc_lower_{}{}'.format(dc_itv, dc_period)].to_numpy() \n",
    "\n",
    "  res_df['dc_upper_{}{}_br'.format(dc_itv, dc_period)] = (close_ - dc_upper_) / body_range\n",
    "  res_df['dc_lower_{}{}_br'.format(dc_itv, dc_period)] = (dc_lower_ - close_) / body_range\n",
    "\n",
    "  return res_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JWj02LLGbnji"
   },
   "outputs": [],
   "source": [
    "def candle_pumping_ratio_v2(res_df, c_itv, dc_itv, period):\n",
    "\n",
    "  res_df = dc_line_v3(res_df, dc_itv, dc_period=period)\n",
    "\n",
    "  dc_upper_ = res_df['dc_upper_{}{}'.format(dc_itv, period)].to_numpy()\n",
    "  dc_lower_ = res_df['dc_lower_{}{}'.format(dc_itv, period)].to_numpy()\n",
    "  dc_range = dc_upper_ - dc_lower_\n",
    " \n",
    "  open_ = res_df['open_{}'.format(c_itv)].to_numpy()\n",
    "  close_ = res_df['close_{}'.format(c_itv)].to_numpy()\n",
    "  body = close_ - open_  # 부호로 양 / 음봉 구분 (양봉 > 0)\n",
    "  \n",
    "  res_df['cppr_{}'.format(c_itv)] = body / dc_range\n",
    "\n",
    "  return res_df\n",
    "\n",
    "\n",
    "# Todo, future_data\n",
    "def candle_pumping_ratio(res_df, c_itv, bb_itv, period):\n",
    "\n",
    "  itv_num = itv_to_number(c_itv)\n",
    "\n",
    "  # 여기에도 v2 처럼 bb_indi. 추가 (자동화)\n",
    "\n",
    "  b1_bb_upper_ = res_df['bb_upper_{}{}'.format(bb_itv, period)].shift(itv_num).to_numpy()\n",
    "  b1_bb_lower_ = res_df['bb_lower_{}{}'.format(bb_itv, period)].shift(itv_num).to_numpy()\n",
    "  bb_range = b1_bb_upper_ - b1_bb_lower_\n",
    "\n",
    "  open_ = res_df['open_{}'.format(c_itv)].to_numpy()\n",
    "  close_ = res_df['close_{}'.format(c_itv)].to_numpy()\n",
    "  body = close_ - open_  # 부호로 양 / 음봉 구분 (양봉 > 0)\n",
    "  \n",
    "  res_df['cppr_{}'.format(c_itv)] = body / bb_range\n",
    "\n",
    "  return res_df\n",
    "\n",
    "\n",
    "def pumping_ratio(res_df, config, itv, period1, period2):\n",
    "\n",
    "  bb_lower_5T = res_df['bb_lower_5T'].to_numpy()\n",
    "  bb_upper_5T = res_df['bb_upper_5T'].to_numpy()\n",
    "  bb_range = bb_upper_5T - bb_lower_5T\n",
    "\n",
    "  selection_id = config.selection_id\n",
    "  \n",
    "  res_df['short_ppr_{}'.format(selection_id)] = res_df['short_tp_gap_{}'.format(selection_id)].to_numpy() / get_line(res_df['short_wave_high_idx_{}{}{}'.format(itv, period1, period2)].to_numpy(), bb_range)\n",
    "  res_df['long_ppr_{}'.format(selection_id)] = res_df['long_tp_gap_{}'.format(selection_id)].to_numpy() / get_line(res_df['long_wave_low_idx_{}{}{}'.format(itv, period1, period2)].to_numpy(), bb_range)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OpkclLSzazZ-"
   },
   "outputs": [],
   "source": [
    "def wave_body_ratio(res_df_, itv, period):\n",
    "  dc_upper_ = res_df_['dc_upper_{}{}'.format(itv, period)].to_numpy()\n",
    "  dc_lower_ = res_df_['dc_lower_{}{}'.format(itv, period)].to_numpy()\n",
    "  close_ = res_df_['close_{}'.format(itv)].to_numpy()\n",
    "  open_ = res_df_['open_{}'.format(itv)].to_numpy()\n",
    "\n",
    "  dc_range = dc_upper_ - dc_lower_\n",
    "  body_range = abs(close_ - open_)\n",
    "\n",
    "  res_df_['wave_body_ratio'] = body_range / dc_range\n",
    "  res_df_['dc_upper_body_ratio'] = (np.maximum(close_, open_) - dc_upper_) / body_range\n",
    "  res_df_['dc_lower_body_ratio'] = (dc_lower_) - np.minimum(close_, open_) / body_range\n",
    "\n",
    "  return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0-zzitQRbnz8"
   },
   "outputs": [],
   "source": [
    "# res_df_['wave_body_ratio'].tail(200)\n",
    "\n",
    "itv = 'H'\n",
    "period = 5\n",
    "wave_body_ratio(res_df_, itv, period)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_second(index_row):\n",
    "    splited_time = str(index_row).split(':') # [-2:] = '59.999000'\n",
    "    splited_time[-1] = '59.999000'\n",
    "    return pd.to_datetime(':'.join(splited_time))\n",
    "\n",
    "print(res_df_.index[0])\n",
    "print(change_second(res_df_.index[0]))\n",
    "res_df_.index = list(map(change_second, res_df_.index))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pd.infer_freq(to_htf(res_df_, '15T', '1h').index)) # --> None 인 이유는, 폐장 시간 때문에 (중간에 공백기간)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(res_df_.head())\n",
    "\n",
    "# print(pd.to_datetime(res_df_.index)) #, format='%Y-%M-%D'))\n",
    "# Todo, int64Index -> timeIndex 로 변환하는 작업이 먼저 필요할 것\n",
    "# to_htf(res_df_, itv_='30T', offset='1H')\n",
    "str_index = res_df_.index.astype(str)\n",
    "type(str_index[0])\n",
    "str_index\n",
    "\n",
    "def str_to_date(str_date):\n",
    "    year = str_date[:4]\n",
    "    month = str_date[4:6]\n",
    "    day = str_date[6:8]\n",
    "    hour = str_date[8:10]\n",
    "    min_ = str_date[10:12]\n",
    "    \n",
    "    new_str_date = \"%s-%s-%s %s:%s:59.999000\".format(year, month, day, hour, min_)\n",
    "    new_str_date = \"%s-%s-%s %s:%s:59\" % (year, month, day, hour, min_)\n",
    "    \n",
    "    # return new_str_date\n",
    "    return datetime.strptime(new_str_date, '%Y-%m-%d %H:%M:%S')\n",
    "    \n",
    "    \n",
    "    \n",
    "list(map(lambda x: str_to_date(x), str_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MSUY4nnku3s9",
    "tags": []
   },
   "source": [
    "## legacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mEKyVbHWSuzi",
    "tags": []
   },
   "source": [
    "### Database utility (file extension conversion & else)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bw5JibDKSuzj",
    "tags": []
   },
   "source": [
    "#### xlsx to feather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VA-_gcA7Suzj"
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "\n",
    "save_path = './candlestick_concated/res_df/'\n",
    "\n",
    "exist_list = os.listdir(save_path)\n",
    "\n",
    "\n",
    "a_day = 3600 * 24\n",
    "\n",
    "for i in tqdm(range(len(file_list))):\n",
    "\n",
    "  keys = [file_list[i]]\n",
    "\n",
    "  # if 'neo'.upper() not in file_list[i]:\n",
    "    # continue\n",
    "\n",
    "  # if '2021-04-30'.upper() not in file_list[i]:\n",
    "  # if '2021-07-01'.upper() not in file_list[i]:\n",
    "  if '2021-10-10'.upper() not in file_list[i]:\n",
    "    continue\n",
    "\n",
    "\n",
    "  for key in keys:      \n",
    "\n",
    "    # if 'eth'.upper() not in key:\n",
    "    #   continue\n",
    "\n",
    "    feather_name = key.replace(\".xlsx\", \".ftr\")\n",
    "    # feather_path = save_path + feather_name\n",
    "\n",
    "    if feather_name in exist_list:\n",
    "      print(feather_name, \"already exist !\")\n",
    "      continue\n",
    "    \n",
    "    open_index = []\n",
    "    \n",
    "    df = pd.read_excel(date_path + key, index_col=0)\n",
    "    second_df = pd.read_excel(date_path2 + key, index_col=0)\n",
    "    third_df = pd.read_excel(date_path3 + key, index_col=0)\n",
    "    fourth_df = pd.read_excel(date_path4 + key, index_col=0)\n",
    "    fifth_df = pd.read_excel(date_path5 + key, index_col=0)\n",
    "    \n",
    "    print(df.index[[0, -1]])\n",
    "    print(second_df.index[[0, -1]])\n",
    "    print(third_df.index[[0, -1]])\n",
    "    print(fourth_df.index[[0, -1]])\n",
    "    print(fifth_df.index[[0, -1]])\n",
    "\n",
    "    open_index.append(df.index[0])\n",
    "    open_index.append(second_df.index[0])\n",
    "    open_index.append(third_df.index[0])\n",
    "    open_index.append(fourth_df.index[0])\n",
    "    open_index.append(fifth_df.index[0])\n",
    "    \n",
    "    try:\n",
    "      #     Todo    #\n",
    "      #      1. 1m 마지막 timeindex 의 date 기준, 08:59:59.999000 를 last timestamp 로 설정\n",
    "      #      2. 시작 timestamp 는 모든 tf 의 가장 최근 시작 index,\n",
    "      #       a. 1m 의 시작 timeindex 는 최소, htf 의 시작 timeindex 보다 interval 만큼 앞서야함\n",
    "      #         i. 따라서 1m open_index, latest_open_index + 1d 를 하면 댐\n",
    "      #           1. timestamp 으로 변환후 1day 를 더하고 datetime 으로 변환\n",
    "      sixth_df = pd.read_excel(date_path6 + key, index_col=0)\n",
    "      seventh_df = pd.read_excel(date_path7 + key, index_col=0)\n",
    "\n",
    "      print(sixth_df.index[[0, -1]])\n",
    "      print(seventh_df.index[[0, -1]])\n",
    "      print()\n",
    "\n",
    "      open_index.append(sixth_df.index[0])\n",
    "      open_index.append(seventh_df.index[0])\n",
    "\n",
    "    except Exception as e:\n",
    "      print(e)\n",
    "\n",
    "\n",
    "    df.reset_index().to_feather(date_path + feather_name, compression='lz4')\n",
    "    second_df.reset_index().to_feather(date_path2 + feather_name, compression='lz4')\n",
    "    third_df.reset_index().to_feather(date_path3 + feather_name, compression='lz4')\n",
    "    fourth_df.reset_index().to_feather(date_path4 + feather_name, compression='lz4')\n",
    "    fifth_df.reset_index().to_feather(date_path5 + feather_name, compression='lz4')\n",
    "    sixth_df.reset_index().to_feather(date_path6 + feather_name, compression='lz4')\n",
    "    seventh_df.reset_index().to_feather(date_path7 + feather_name, compression='lz4')\n",
    "\n",
    "    print(\"xlsx converted to feather !\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pe0QpnORSuzk",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### add itv_name to ftr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M-gl30KxSuzk"
   },
   "outputs": [],
   "source": [
    "save_path = './candlestick_concated/res_df/'\n",
    "\n",
    "# dir_path = \"bbdc3m_backi2\"\n",
    "# date = '2021-10-10'\n",
    "date = '2021-07-01'\n",
    "\n",
    "db_path = './candlestick_concated/database_bn/non_cum/%s/' % date\n",
    "os.makedirs(os.path.join(db_path), exist_ok=True)\n",
    "\n",
    "# exist_list = os.listdir(os.path.join(save_path, dir_path))\n",
    "# break\n",
    "\n",
    "\n",
    "a_day = 3600 * 24\n",
    "\n",
    "for i in tqdm(range(len(file_list))):\n",
    "\n",
    "  keys = [file_list[i]]\n",
    "\n",
    "  # if 'neo'.upper() not in file_list[i]:\n",
    "    # continue\n",
    "\n",
    "  if date not in file_list[i]:\n",
    "    continue\n",
    "\n",
    "\n",
    "  for key in keys:      \n",
    "\n",
    "    # if 'eth'.upper() not in key:\n",
    "    #   continue\n",
    "    # print(key)\n",
    "    \n",
    "    if \".ftr\" not in key:\n",
    "      continue\n",
    "        \n",
    "    df = shutil.copy(date_path + key, db_path + key.replace(\".ftr\", \"_%s.ftr\" % interval))\n",
    "    second_df = shutil.copy(date_path2 + key, db_path + key.replace(\".ftr\", \"_%s.ftr\" % interval2))\n",
    "    third_df = shutil.copy(date_path3 + key, db_path + key.replace(\".ftr\", \"_%s.ftr\" % interval3))\n",
    "    fourth_df = shutil.copy(date_path4 + key, db_path + key.replace(\".ftr\", \"_%s.ftr\" % interval4))\n",
    "    fifth_df = shutil.copy(date_path5 + key, db_path + key.replace(\".ftr\", \"_%s.ftr\" % interval5))\n",
    "    sixth_df = shutil.copy(date_path6 + key, db_path + key.replace(\".ftr\", \"_%s.ftr\" % interval6))\n",
    "    seventh_df = shutil.copy(date_path7 + key, db_path + key.replace(\".ftr\", \"_%s.ftr\" % interval7))\n",
    "\n",
    "    print(\"copied to\" + db_path + key.replace(\".ftr\", \"_%s.ftr\" % interval))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4oZ1ohTtSuzk"
   },
   "source": [
    "#### feather ver. (database to res_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vgVHpnUsSuzk"
   },
   "outputs": [],
   "source": [
    "# db_path = './candlestick_concated/database_ub/' # upbit\n",
    "db_path = './candlestick_concated/database_bn/'   # binance\n",
    "\n",
    "save_path = './candlestick_concated/res_df/'\n",
    "\n",
    "save_dir_path = \"bb1d_backi2\"\n",
    "date = '2022-02-17'\n",
    "\n",
    "# concat_path = 'noncat' # 새로운 cols 를 기존 cum/concat 에 붙이려는 경우\n",
    "concat_path = 'concat'\n",
    "cum_path = \"cum\"\n",
    "# cum_path = \"non_cum\"  # non_cum 으로 진행하는 경우, row concat 용도이기 때문에 noncat -> concat 으로 변경 (base cols 를 모두 담고 있음)\n",
    "\n",
    "load_path = os.path.join(db_path, cum_path, date)\n",
    "save_path = os.path.join(save_path, save_dir_path, concat_path, cum_path, date)\n",
    "\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "file_list = os.listdir(load_path)\n",
    "exist_list = os.listdir(save_path)\n",
    "# break\n",
    "\n",
    "a_day = 3600 * 24\n",
    "\n",
    "for i in tqdm(range(len(file_list))):\n",
    "\n",
    "  keys = [file_list[i]]\n",
    "\n",
    "  # if 'neo'.upper() not in file_list[i]:\n",
    "    # continue\n",
    "\n",
    "  if date not in file_list[i]:\n",
    "    continue\n",
    "\n",
    "\n",
    "  for key in keys:      \n",
    "\n",
    "    # if 'eth'.upper() not in key:\n",
    "    #   continue\n",
    "    # print(key)\n",
    "    \n",
    "    if \".ftr\" not in key:\n",
    "      continue\n",
    "\n",
    "    if \"_1m\" not in key:\n",
    "      continue\n",
    "\n",
    "    # feather_name = key.replace(\".ftr\", \"_%.ftr\" % save_dir_path)\n",
    "    feather_name = key.replace(\"_1m\", \"\")\n",
    "    feather_path = os.path.join(save_path, feather_name)\n",
    "\n",
    "    if feather_name in exist_list:\n",
    "      print(feather_name, \"already exist !\")\n",
    "      continue\n",
    "    \n",
    "    df = pd.read_feather(os.path.join(load_path, key), columns=None, use_threads=True).set_index(\"index\")\n",
    "\n",
    "    res_df = sync_check_make(df)\n",
    "\n",
    "    res_df.reset_index().to_feather(feather_path, compression='lz4')\n",
    "    print(feather_path, \"saved succesfully !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E0n53hflJbnp",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### htf candle check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6xW0yugCWvGz"
   },
   "outputs": [],
   "source": [
    "itv_list = ['3T', '5T', '15T', '30T', '1H', '4H']\n",
    "comp_df_list = [second_df, third_df, fourth_df, fifth_df, sixth_df, seventh_df]\n",
    "offset_list = ['1h', '2min', '2min', '2min', '2min', '2min']\n",
    "# itv_list = ['4H']\n",
    "# comp_df_list = [seventh_df]\n",
    "\n",
    "slice_len = 100\n",
    "for itv_, comp_df_, offset in zip(itv_list, comp_df_list, offset_list):\n",
    "\n",
    "  print(\"itv_ :\", itv_)\n",
    "\n",
    "  # df = h_candle_v2(df, '3T')\n",
    "  # end_ts = \n",
    "  h_res_df = df.resample(itv_, offset=offset).agg({\n",
    "          'open': 'first',\n",
    "          'high': 'max',\n",
    "          'low': 'min',\n",
    "          'close': 'last'\n",
    "      })\n",
    "\n",
    "  #   앞은 길이가 다르고, 뒤에서부터 잘라서 비교    #\n",
    "  #   last_row 빼고는 동일, 4h 제외\n",
    "  # print(df.tail())\n",
    "  print(h_res_df.tail())\n",
    "  print(comp_df_.tail())\n",
    "  # # print(h_res_df.head())\n",
    "  # # print(second_df.head())\n",
    "\n",
    "  # print(len(h_res_df))\n",
    "  # print(len(second_df))\n",
    "\n",
    "  # print(h_res_df.values[-slice_len:])\n",
    "  # print(second_df.iloc[:, :4].values[-slice_len:])\n",
    "  # print(np.argwhere(h_res_df.values[-slice_len:] != comp_df_.iloc[:, :4].values[-slice_len:]))\n",
    "  # print()\n",
    "  break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yRNwqVeAu8X8"
   },
   "outputs": [],
   "source": [
    "#       1. new_date 의 시작 timeidx 와 base_date end timeidx 의 최소 days' gap     #\n",
    "#       2. new_date 의 시작 부분 indi. value 는 np.nan 으로 채워질 거기 때문에 계산해야함    #\n",
    "\n",
    "df_count = droped_new_res_df.count()\n",
    "len_missing = df_count.max() - df_count.min()\n",
    "print(len_missing / 1440)\n",
    "\n",
    "#       3. \n",
    "missing_sliced_df = droped_new_res_df.iloc[len_missing:]\n",
    "df_count2 = missing_sliced_df.count()\n",
    "# print(df_count2)\n",
    "print((df_count2.max() - df_count2.min()))    # this value should be zero !\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AY7E2_hTBsyM"
   },
   "outputs": [],
   "source": [
    "# df_count2.index[df_count2.argmin()]\n",
    "# missing_sliced_df.head(5)\n",
    "\n",
    "stay_missed = np.sum(pd.isnull(missing_sliced_df), axis=0)\n",
    "print(stay_missed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p9yqewOw9g33"
   },
   "outputs": [],
   "source": [
    "stay_missed_cols = stay_missed[stay_missed != 0].index\n",
    "\n",
    "for sm_col in stay_missed_cols:\n",
    "  \n",
    "  row_idx = np.argwhere(pd.isnull(missing_sliced_df[sm_col].values))\n",
    "\n",
    "  plt.figure(figsize=(3,3))\n",
    "  plt.plot(row_idx)\n",
    "  plt.ylim(0, len(missing_sliced_df))\n",
    "  plt.title(sm_col)\n",
    "\n",
    "  plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## olds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### old data sync_check : mtf to T df (very old..)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "khKb9nhlSuzj"
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "\n",
    "save_path = './candlestick_concated/res_df/'\n",
    "\n",
    "exist_list = os.listdir(save_path)\n",
    "\n",
    "\n",
    "a_day = 3600 * 24\n",
    "\n",
    "for i in tqdm(range(len(file_list))):\n",
    "\n",
    "  keys = [file_list[i]]\n",
    "\n",
    "  # if 'neo'.upper() not in file_list[i]:\n",
    "    # continue\n",
    "\n",
    "  # if '2021-04-30'.upper() not in file_list[i]:\n",
    "  if '2021-07-01'.upper() not in file_list[i]:\n",
    "  # if '2021-10-10'.upper() not in file_list[i]:\n",
    "    continue\n",
    "\n",
    "\n",
    "  for key in keys:      \n",
    "\n",
    "    # if 'eth'.upper() not in key:\n",
    "    #   continue\n",
    "\n",
    "    excel_name = key.replace(\".xlsx\", \"_st1h_backi2.xlsx\")\n",
    "    excel_path = save_path + excel_name\n",
    "\n",
    "    if excel_name in exist_list:\n",
    "      print(excel_name, \"already exist !\")\n",
    "      continue\n",
    "    \n",
    "    open_index = []\n",
    "    \n",
    "    df = pd.read_excel(date_path + key, index_col=0)\n",
    "    second_df = pd.read_excel(date_path2 + key, index_col=0)\n",
    "    third_df = pd.read_excel(date_path3 + key, index_col=0)\n",
    "    fourth_df = pd.read_excel(date_path4 + key, index_col=0)\n",
    "    fifth_df = pd.read_excel(date_path5 + key, index_col=0)\n",
    "    \n",
    "    print(df.index[[0, -1]])\n",
    "    print(second_df.index[[0, -1]])\n",
    "    print(third_df.index[[0, -1]])\n",
    "    print(fourth_df.index[[0, -1]])\n",
    "    print(fifth_df.index[[0, -1]])\n",
    "\n",
    "    open_index.append(df.index[0])\n",
    "    open_index.append(second_df.index[0])\n",
    "    open_index.append(third_df.index[0])\n",
    "    open_index.append(fourth_df.index[0])\n",
    "    open_index.append(fifth_df.index[0])\n",
    "    \n",
    "    try:\n",
    "      #     Todo    #\n",
    "      #      1. 1m 마지막 timeindex 의 date 기준, 08:59:59.999000 를 last timestamp 로 설정\n",
    "      #      2. 시작 timestamp 는 모든 tf 의 가장 최근 시작 index,\n",
    "      #       a. 1m 의 시작 timeindex 는 최소, htf 의 시작 timeindex 보다 interval 만큼 앞서야함\n",
    "      #         i. 따라서 1m open_index, latest_open_index + 1d 를 하면 댐\n",
    "      #           1. timestamp 으로 변환후 1day 를 더하고 datetime 으로 변환\n",
    "      sixth_df = pd.read_excel(date_path6 + key, index_col=0)\n",
    "      seventh_df = pd.read_excel(date_path7 + key, index_col=0)\n",
    "\n",
    "      print(sixth_df.index[[0, -1]])\n",
    "      print(seventh_df.index[[0, -1]])\n",
    "      print()\n",
    "\n",
    "      open_index.append(sixth_df.index[0])\n",
    "      open_index.append(seventh_df.index[0])\n",
    "\n",
    "    except Exception as e:\n",
    "      print(e)\n",
    "\n",
    "    latest_open_index = sorted(open_index)[-1]\n",
    "    \n",
    "    open_ts = datetime.timestamp(latest_open_index)\n",
    "    latest_open_index_1m = datetime.fromtimestamp(open_ts + a_day)\n",
    "\n",
    "    #   str 로 만들어 접근하면 불가함  #\n",
    "    end_index = pd.to_datetime(str(df.index[-1]).split(\" \")[0] + \" 08:59:59.999000\")\n",
    "    # break\n",
    "\n",
    "    sliced_df = df.loc[latest_open_index_1m:end_index] # to_lower_tf 의 기준 ltf\n",
    "    sliced_second_df = second_df.loc[latest_open_index:end_index]\n",
    "    sliced_third_df = third_df.loc[latest_open_index:end_index]\n",
    "    sliced_fourth_df = fourth_df.loc[latest_open_index:end_index]\n",
    "    sliced_fifth_df = fifth_df.loc[latest_open_index:end_index]\n",
    "\n",
    "    print(\"sliced index\")\n",
    "    print(sliced_df.index[[0, -1]])\n",
    "    print(sliced_second_df.index[[0, -1]])\n",
    "    print(sliced_third_df.index[[0, -1]])\n",
    "    print(sliced_fourth_df.index[[0, -1]])\n",
    "    print(sliced_fifth_df.index[[0, -1]])\n",
    "\n",
    "    try:\n",
    "      sliced_sixth_df = sixth_df.loc[latest_open_index:end_index]\n",
    "      sliced_seventh_df = seventh_df.loc[latest_open_index:end_index]\n",
    "\n",
    "      print(sliced_sixth_df.index[[0, -1]])\n",
    "      print(sliced_seventh_df.index[[0, -1]])\n",
    "\n",
    "      res_df = sync_check(sliced_df, sliced_second_df, sliced_third_df, sliced_fourth_df, sliced_fifth_df, sliced_sixth_df, sliced_seventh_df)\n",
    "    \n",
    "    except:\n",
    "      res_df = sync_check(sliced_df, sliced_second_df, sliced_third_df, sliced_fourth_df, sliced_fifth_df)\n",
    "\n",
    "\n",
    "\n",
    "    res_df.to_excel(excel_path)\n",
    "    print(excel_name, \"saved succesfully !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### row concatenation (excel version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U-3QkfbFSuzl"
   },
   "outputs": [],
   "source": [
    "save_path = './candlestick_concated/res_df/'\n",
    "\n",
    "dict_name = \"2021-07-01 ETHUSDT_bb15m_backi2_res_dfs.pkl\"\n",
    "\n",
    "#     load with pickle    #\n",
    "with open(save_path + dict_name, 'rb') as f:\n",
    "  saved_res_df_dict = pickle.load(f)\n",
    "\n",
    "print(dict_name, \"loaded !\")\n",
    "res_df_files = os.listdir(save_path)\n",
    "res_df_files.reverse()\n",
    "\n",
    "print(res_df_files)\n",
    "\n",
    "res_df_dict = {}\n",
    "\n",
    "base_postfix = '_bb15m_backi2.xlsx'\n",
    "new_postfix = '_st1h_backi2.xlsx'\n",
    "\n",
    "max_cnt = 10\n",
    "sample_cnt = max_cnt\n",
    "\n",
    "for k_i, key in enumerate(res_df_files):\n",
    "\n",
    "  if '2021-07-01'.upper() not in key:\n",
    "  # if '2021-10-10'.upper() not in key:\n",
    "    continue\n",
    "\n",
    "  # if \"link\".upper() not in key:\n",
    "  # if \"btc\".upper() not in key:\n",
    "  #   continue\n",
    "\n",
    "  if new_postfix not in key:\n",
    "    continue\n",
    "\n",
    "  # if key in \n",
    "\n",
    "  if sample_cnt == max_cnt:\n",
    "    dict_name = \"%s_res_dfs.pkl\" % key.split(\".\")[0]\n",
    "    print(\"dict_name :\", dict_name)\n",
    "\n",
    "  base_df = saved_res_df_dict[key.replace(new_postfix, base_postfix)]\n",
    "  # base_df = pd.read_excel(save_path + key.replace(new_postfix, base_postfix), index_col=0)  \n",
    "  res_df = pd.read_excel(save_path + key, index_col=0)  \n",
    "\n",
    "  # print(base_df.head())\n",
    "  # print(res_df.head())\n",
    "  # break\n",
    "\n",
    "  new_res_df = pd.concat([base_df, res_df], axis=1) # df_tot.drop_duplicates()\n",
    "  # new_res_df.head()\n",
    "\n",
    "  droped_new_res_df = new_res_df.loc[:,~new_res_df.columns.duplicated(keep='last')]\n",
    "  droped_new_res_df.head()\n",
    "  # break\n",
    "\n",
    "  # res_df_dict[key] = res_df\n",
    "  res_df_dict[key] = droped_new_res_df\n",
    "  print(key, \"saved to dict !\")\n",
    "\n",
    "  #     save with pickle    #\n",
    "  with open(save_path + dict_name, 'wb') as f:\n",
    "    pickle.dump(res_df_dict, f)\n",
    "\n",
    "  sample_cnt -= 1\n",
    "\n",
    "  if sample_cnt <= 0:\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XG2p9OhhSuzm"
   },
   "outputs": [],
   "source": [
    "# save_path = './candlestick_concated/res_df/'        # cols 추가된 cum db 에 new_row's cols 기준으로 합치는 경우\n",
    "save_path = './candlestick_concated/database_bn/'   # ohlcv cum db 만들 경우\n",
    "\n",
    "base_date = '2022-04-25'\n",
    "# new_date = '2022-02-17'\n",
    "new_date = '2022-04-27'\n",
    "\n",
    "# ------ load ftr list ------ #\n",
    "if \"database\" in save_path:\n",
    "  base_dir_path = \"\"\n",
    "  new_dir_path = \"\"\n",
    "  concat_dir = \"\"\n",
    "else:\n",
    "  base_dir_path = \"sar_backi2\"\n",
    "  new_dir_path = \"bb4h_backi2\"  # dir_path 가 base / new 서로 달라질 수 있어서 분할함\n",
    "  concat_dir = \"concat\"\n",
    "\n",
    "base_date_path = os.path.join(save_path, base_dir_path, concat_dir, \"cum\", base_date)      # 기존 cum db 와 new_date db 를 cum 진행\n",
    "# base_date_path = os.path.join(save_path, base_dir_path, concat_dir, \"non_cum\", base_date)    # non_cum db 와 new_date db 를 cum 진행\n",
    "\n",
    "# new_date_path = os.path.join(save_path, new_dir_path, concat_dir, \"cum\", new_date)      # 상황별로 직접 선택해야할 듯\n",
    "new_date_path = os.path.join(save_path, new_dir_path, concat_dir, \"non_cum\", new_date)\n",
    "\n",
    "\n",
    "\n",
    "# ------ save to (new) concat dir ------ #\n",
    "#      1. if dir. not exists, makedir\n",
    "save_path = new_date_path.replace(\"non_cum\", \"cum\")   # non_cum 아니여도 무관\n",
    "os.makedirs(save_path, exist_ok=True)   # noncat / concat 두가지 경우 존재가능할 것\n",
    "# os.makedirs(os.path.join(save_path, dir_path, \"noncat/cum\", new_date), exist_ok=True)\n",
    "\n",
    "\n",
    "ftr_list = [s for s in os.listdir(new_date_path) if \"ftr\" in s]\n",
    "exist_list = os.listdir(save_path)\n",
    "print(ftr_list)\n",
    "# break\n",
    "\n",
    "\n",
    "for key in ftr_list:\n",
    "\n",
    "  if new_date not in key:   # date rejection\n",
    "    continue\n",
    "  if '1m' not in key:  # itv rejection\n",
    "    continue\n",
    "\n",
    "  # if key in exist_list:\n",
    "  #   print(key, \"already exist !\")\n",
    "  #   continue\n",
    "\n",
    "  #       read from base postfix's directory    #\n",
    "  base_df = pd.read_feather(os.path.join(base_date_path, key.replace(new_date, base_date)), columns=None, use_threads=True).set_index(\"index\")   # key 에 new_date 담겨있음\n",
    "  res_df = pd.read_feather(os.path.join(new_date_path, key), columns=None, use_threads=True).set_index(\"index\")\n",
    "\n",
    "  # print(base_df.head())\n",
    "  # print(res_df.head())\n",
    "  # break\n",
    "\n",
    "  new_res_df = pd.concat([base_df, res_df], axis=0) # df_tot.drop_duplicates()\n",
    "  # new_res_df.head()\n",
    "\n",
    "  intersection_cols = res_df.columns.intersection(base_df.columns)\n",
    "\n",
    "  droped_new_res_df = new_res_df.loc[~new_res_df.index.duplicated(keep='last'),intersection_cols]\n",
    "  # droped_new_res_df = new_res_df.loc[:,~new_res_df.columns.duplicated(keep='last')]\n",
    "  # droped_new_res_df = new_res_df.loc[:,~new_res_df.index.duplicated(keep='last')]\n",
    "  # droped_new_res_df.head()\n",
    "  # break  \n",
    "  \n",
    "  print(droped_new_res_df.iloc[[0, -1]])  \n",
    "\n",
    "  # ------------- verify df continuity directly itv by itv ------------- #\n",
    "  true_continue = True\n",
    "  if \"_\" in key:\n",
    "\n",
    "    # interval = key.split(\".\")[0].split(\"_\")[-1] \n",
    "    # itv_num = itv_to_number(interval)\n",
    "\n",
    "    # verified_df = consecutive_df(droped_new_res_df, itv_to_number(interval))\n",
    "    # verified_df.reset_index().to_feather(os.path.join(save_path, key), compression='lz4')\n",
    "\n",
    "    # res_df_dict[key] = res_df\n",
    "    # res_df_dict[key] = droped_new_res_df  \n",
    "\n",
    "    np_idx_ts = np.array(list(map(lambda x: datetime.timestamp(x), droped_new_res_df.index)))\n",
    "    ideal_ts_gap = 60 # * itv_num\n",
    "\n",
    "    for ts_i in range(len(np_idx_ts)):\n",
    "      \n",
    "      if ts_i != 0:\n",
    "        ts_gap = np_idx_ts[ts_i] - np_idx_ts[ts_i - 1]\n",
    "        if ts_gap > ideal_ts_gap or ts_gap < ideal_ts_gap:\n",
    "        # if ts_gap == ideal_ts_gap:\n",
    "          print(droped_new_res_df.index[ts_i - 1])\n",
    "          print(droped_new_res_df.index[ts_i])\n",
    "          # print(ts_gap)\n",
    "          print(\"------------------ unideal ts_gap ------------------\")\n",
    "          true_continue = False\n",
    "\n",
    "    print(\"continuity checked !\")\n",
    "\n",
    "  if true_continue:\n",
    "    droped_new_res_df.reset_index().to_feather(os.path.join(save_path, key), compression='lz4')\n",
    "\n",
    "  print(os.path.join(save_path, key), \"saved !\")\n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t1E_eAyPSuzm",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### new col to latest feather (1m_indi. only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xyI5NrM7Suzm"
   },
   "outputs": [],
   "source": [
    "save_path = './candlestick_concated/res_df/'\n",
    "\n",
    "cum_dir = \"cum\"\n",
    "\n",
    "new_dir_path = \"rsi_backi2\"\n",
    "base_dir_path = \"bbdc3m_backi2\"\n",
    "\n",
    "new_date = '2021-11-17'\n",
    "\n",
    "\n",
    "\n",
    "#     load ftr list    #\n",
    "base_save_path = os.path.join(save_path, base_dir_path, \"concat/cum\", new_date)\n",
    "new_save_path = base_save_path.replace(base_dir_path, new_dir_path)\n",
    "\n",
    "#     save to (new) cum dir    #\n",
    "#      1. if dir. not exists, makedir\n",
    "os.makedirs(new_save_path, exist_ok=True)\n",
    "\n",
    "ftr_list = [s for s in os.listdir(base_save_path) if \"ftr\" in s]\n",
    "print(ftr_list)\n",
    "# break\n",
    "\n",
    "\n",
    "max_cnt = 10\n",
    "sample_cnt = max_cnt\n",
    "\n",
    "for key in ftr_list:\n",
    "\n",
    "  if new_date not in key:\n",
    "    continue\n",
    "\n",
    "\n",
    "  #       read from base postfix's directory    #\n",
    "  base_df = pd.read_feather(os.path.join(base_save_path, key), columns=None, use_threads=True).set_index(\"index\")\n",
    "  # print(base_df.head())\n",
    "  # print(res_df.head())\n",
    "  # break\n",
    "\n",
    "  droped_new_res_df = sync_check(base_df)\n",
    "\n",
    "  # new_res_df = pd.concat([base_df, res_df], axis=0) # df_tot.drop_duplicates()\n",
    "  # # new_res_df.head()\n",
    "\n",
    "  # intersection_cols = res_df.columns.intersection(base_df.columns)\n",
    "\n",
    "  # droped_new_res_df = new_res_df.loc[~new_res_df.index.duplicated(keep='last'),intersection_cols]\n",
    "  # droped_new_res_df = new_res_df.loc[:,~new_res_df.columns.duplicated(keep='last')]\n",
    "  # droped_new_res_df = new_res_df.loc[:,~new_res_df.index.duplicated(keep='last')]\n",
    "  # droped_new_res_df.head()\n",
    "  # break\n",
    "\n",
    "  droped_new_res_df.reset_index().to_feather(os.path.join(new_save_path, key), compression='lz4')\n",
    "\n",
    "  print(os.path.join(new_save_path, key), \"saved !\")\n",
    "\n",
    "  # sample_cnt -= 1\n",
    "\n",
    "  # if sample_cnt <= 0:\n",
    "  #   break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "epgS5Dksu-HX",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### mv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aJcVpEdrslA5"
   },
   "outputs": [],
   "source": [
    "df_path = './candlestick_concated/survey_df_v2'\n",
    "files_ = os.listdir(df_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cEKyEYkotFDy"
   },
   "outputs": [],
   "source": [
    "dirs = [file_ for file_ in files_ if not file_.endswith('.ftr')]\n",
    "files = [file_ for file_ in files_ if file_.endswith('.ftr')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IgM79tcxtPVZ"
   },
   "outputs": [],
   "source": [
    "dirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M767iRtwtRQP"
   },
   "outputs": [],
   "source": [
    "def move_fn(dir_, file_):\n",
    "  src_path = os.path.join(df_path, file_)\n",
    "  dst_path = os.path.join(df_path, dir_, file_)\n",
    "  shutil.move(src_path, dst_path)\n",
    "  print(\"moved to {}\".format(dst_path))\n",
    "\n",
    "_ = [move_fn('2022-01-10 ETHUSDT_all', file_) for file_ in files if 'eth'.upper() in file_]\n",
    "# sols\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Iy76iO7gztne",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### move legacy files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hMRht32Czwry"
   },
   "outputs": [],
   "source": [
    "# print()\n",
    "cur_dir_list = os.listdir('.')\n",
    "for f in cur_dir_list:\n",
    "  if 'legacy' in f :\n",
    "    # print(f)\n",
    "    if os.path.isdir(pkg_path + f,):\n",
    "      continue\n",
    "\n",
    "    shutil.move(pkg_path + f, pkg_path + 'legacy/' + f)\n",
    "    print(\"moved to\" + pkg_path + 'legacy/' +  f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5duWn8t4BRyv",
    "tags": []
   },
   "source": [
    "# IDEP\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rrIGjmUzqU-D",
    "tags": []
   },
   "source": [
    "## Load utils & config paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bank.traders.trader_v1_4 import *\n",
    "\n",
    "# ------- input params ------- #\n",
    "paper_name = \"wave_cci_wrr32_spread\"\n",
    "bank_id_list = [1]\n",
    "\n",
    "bank = Trader(paper_name=paper_name, id_list=bank_id_list, config_type=\"realtrade\", mode=\"IDEP\")\n",
    "\n",
    "id_arr = np.array(bank_id_list)\n",
    "utils_arr = np.array(bank.utils_list)\n",
    "config_arr = np.array(bank.config_list)  # Todo, 이거 사용하려면, bank.__init__ 에서 read_write_config 한번해야할 것."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "leSQlImg4_9L"
   },
   "source": [
    "### utils paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 506,
     "status": "ok",
     "timestamp": 1666570072396,
     "user": {
      "displayName": "7th June",
      "userId": "08178289703395036410"
     },
     "user_tz": -540
    },
    "id": "CB2yZdQ95Cdg"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import logging\n",
    "from funcs.public.broker import itv_to_number\n",
    "from datetime import datetime\n",
    "\n",
    "sys_log = logging.getLogger()\n",
    "\n",
    "\n",
    "def enlist_tr(res_df, config, np_timeidx, mode='OPEN', env='BANK', show_detail=True):\n",
    "    selection_id = config.selection_id\n",
    "\n",
    "    len_df = len(res_df)\n",
    "    len_df_range = np.arange(len_df)\n",
    "\n",
    "    # if config.tr_set.check_hlm == 2:  # 동일한 param 으로도 p2_hlm 시도를 충분히 할 수 있음 (csdbox 와 같은)\n",
    "    #   assert not (wave_itv1 == wave_itv2 and wave_period1 == wave_period2)\n",
    "\n",
    "    \"\"\"\n",
    "    1. get data for tr_set\n",
    "    \"\"\"\n",
    "    wave_itv1 = config.tr_set.wave_itv1\n",
    "    wave_period1 = config.tr_set.wave_period1\n",
    "    wave_itv2 = config.tr_set.wave_itv2\n",
    "    wave_period2 = config.tr_set.wave_period2\n",
    "    tc_period = config.tr_set.tc_period\n",
    "    roll_hl_cnt = 3\n",
    "\n",
    "    roll_highs1 = [res_df['wave_high_fill_{}{}_-{}'.format(wave_itv1, wave_period1, cnt_ + 1)].to_numpy() for cnt_ in reversed(range(roll_hl_cnt))]\n",
    "    roll_lows1 = [res_df['wave_low_fill_{}{}_-{}'.format(wave_itv1, wave_period1, cnt_ + 1)].to_numpy() for cnt_ in reversed(range(roll_hl_cnt))]\n",
    "\n",
    "    wave_high_fill1_ = res_df['wave_high_fill_{}{}'.format(wave_itv1, wave_period1)].to_numpy()\n",
    "    wave_low_fill1_ = res_df['wave_low_fill_{}{}'.format(wave_itv1, wave_period1)].to_numpy()\n",
    "\n",
    "    #     roll_highs2 = [res_df['wave_high_fill_{}{}_-{}'.format(wave_itv2, wave_period2, cnt_ + 1)].to_numpy() for cnt_ in reversed(range(roll_hl_cnt))]\n",
    "    #     roll_lows2 = [res_df['wave_low_fill_{}{}_-{}'.format(wave_itv2, wave_period2, cnt_ + 1)].to_numpy() for cnt_ in reversed(range(roll_hl_cnt))]\n",
    "\n",
    "    #     wave_high_fill2_ = res_df['wave_high_fill_{}{}'.format(wave_itv2, wave_period2)].to_numpy()\n",
    "    #     wave_low_fill2_ = res_df['wave_low_fill_{}{}'.format(wave_itv2, wave_period2)].to_numpy()\n",
    "\n",
    "    # res_df['short_wave_spread_fill_{}{}'.format(wave_itv1, wave_period1)] = roll_highs1[-1] / wave_low_fill1_\n",
    "    # res_df['long_wave_spread_fill_{}{}'.format(wave_itv1, wave_period1)] = wave_high_fill1_ / roll_lows1[-1]\n",
    "    # res_df['short_wave_spread_fill_{}{}'.format(wave_itv1, wave_period1)] = wave_high_fill1_ / wave_low_fill1_\n",
    "    # res_df['long_wave_spread_fill_{}{}'.format(wave_itv1, wave_period1)] = wave_high_fill1_ / wave_low_fill1_\n",
    "\n",
    "    # itvnum = itv_to_number(wave_itv1)\n",
    "    # itvnum2 = itvnum * 2\n",
    "\n",
    "    # high_ = res_df['high_{}'.format(wave_itv1)].to_numpy()\n",
    "    # low_ = res_df['low_{}'.format(wave_itv1)].to_numpy()\n",
    "\n",
    "    # b1_close_ = res_df['close_{}'.format(wave_itv1)].shift(itvnum).to_numpy()\n",
    "    # b1_open_ = res_df['open_{}'.format(wave_itv1)].shift(itvnum).to_numpy()\n",
    "    # b1_high_ = res_df['high_{}'.format(wave_itv1)].shift(itvnum).to_numpy()\n",
    "    # b1_low_ = res_df['low_{}'.format(wave_itv1)].shift(itvnum).to_numpy()\n",
    "    # b2_high_ = res_df['high_{}'.format(wave_itv1)].shift(itvnum2).to_numpy()\n",
    "    # b2_low_ = res_df['low_{}'.format(wave_itv1)].shift(itvnum2).to_numpy()\n",
    "\n",
    "    # max_high_ = np.maximum(b1_high_, b2_high_)\n",
    "    # min_low_ = np.minimum(b1_low_, b2_low_)\n",
    "\n",
    "    # res_df['b1_close_{}'.format(wave_itv1)] = b1_close_\n",
    "    # res_df['b1_open_{}'.format(wave_itv1)] = b1_open_\n",
    "    # res_df['b1_high_{}'.format(wave_itv1)] = b1_high_\n",
    "    # res_df['b1_low_{}'.format(wave_itv1)] = b1_low_\n",
    "    # res_df['b2_high_{}'.format(wave_itv1)] = b2_high_\n",
    "    # res_df['b2_low_{}'.format(wave_itv1)] = b2_low_\n",
    "    # res_df['max_high_{}'.format(wave_itv1)] = max_high_\n",
    "    # res_df['min_low_{}'.format(wave_itv1)] = min_low_\n",
    "\n",
    "    \"\"\"\n",
    "    2. set tr_set's 1, 0 & spread\n",
    "    \"\"\"\n",
    "    # cu's roll_high_[:, -1] = prev_high & cu's roll_low_[:, -1] = current_low\n",
    "    # co's roll_low_[:, -1] = prev_low & co's roll_high_[:, -1] = current_high\n",
    "    # 2 사용하는 이유 : tp_1 을 p2_box 기준으로 설정하가 위함 --> enex_pairing_v4 function 과 호환되지 않음\n",
    "    res_df['short_tp_1_{}'.format(selection_id)] = wave_low_fill1_  # wave_low_fill1_ b2_low_5T\n",
    "    res_df['short_tp_0_{}'.format(selection_id)] = roll_highs1[-1]  # roll_highs1[-1] wave_high_fill1_\n",
    "    res_df['long_tp_1_{}'.format(selection_id)] = wave_high_fill1_  # wave_high_fill1_ b2_high_5T\n",
    "    res_df['long_tp_0_{}'.format(selection_id)] = roll_lows1[-1]  # roll_lows1[-1]  wave_low_fill1_\n",
    "\n",
    "    res_df['short_ep1_1_{}'.format(selection_id)] = wave_low_fill1_  # wave_low_fill1_   # b2_low_5T\n",
    "    res_df['short_ep1_0_{}'.format(selection_id)] = wave_high_fill1_  # wave_high_fill1_\n",
    "    res_df['long_ep1_1_{}'.format(selection_id)] = wave_high_fill1_  # wave_high_fill1_   # b2_high_5T\n",
    "    res_df['long_ep1_0_{}'.format(selection_id)] = wave_low_fill1_  # wave_low_fill1_\n",
    "\n",
    "    # --> p2's ep use p1's ep\n",
    "    res_df['short_ep2_1_{}'.format(selection_id)] = wave_low_fill1_  # wave_low_fill2_   # b2_low_5T\n",
    "    res_df['short_ep2_0_{}'.format(selection_id)] = wave_high_fill1_  # wave_high_fill2_\n",
    "    res_df['long_ep2_1_{}'.format(selection_id)] = wave_high_fill1_  # wave_high_fill2_   # b2_high_5T\n",
    "    res_df['long_ep2_0_{}'.format(selection_id)] = wave_low_fill1_  # wave_low_fill2_\n",
    "\n",
    "    # --> out use p1's low, (allow prev_low as out for p1_hhm only)\n",
    "    res_df['short_out_1_{}'.format(selection_id)] = wave_low_fill1_  # wave_low_fill1_   # wave_low_fill2_   # b2_low_5T\n",
    "    res_df['short_out_0_{}'.format(selection_id)] = roll_highs1[-1]  # roll_highs1[-1] if not config.tr_set.check_hlm else wave_high_fill1_   # roll_highs2[-1]  # roll_high_[:, -2]\n",
    "    res_df['long_out_1_{}'.format(selection_id)] = wave_high_fill1_  # wave_high_fill1_   # wave_high_fill2_   # b2_high_5T\n",
    "    res_df['long_out_0_{}'.format(selection_id)] = roll_lows1[-1]  # roll_lows1[-1] if not config.tr_set.check_hlm else wave_low_fill1_   # roll_lows2[-1]\n",
    "\n",
    "    res_df['short_tp_gap_{}'.format(selection_id)] = abs(res_df['short_tp_1_{}'.format(selection_id)] - res_df['short_tp_0_{}'.format(selection_id)])\n",
    "    res_df['long_tp_gap_{}'.format(selection_id)] = abs(res_df['long_tp_1_{}'.format(selection_id)] - res_df['long_tp_0_{}'.format(selection_id)])\n",
    "    res_df['short_ep1_gap_{}'.format(selection_id)] = abs(res_df['short_ep1_1_{}'.format(selection_id)] - res_df['short_ep1_0_{}'.format(selection_id)])\n",
    "    res_df['long_ep1_gap_{}'.format(selection_id)] = abs(res_df['long_ep1_1_{}'.format(selection_id)] - res_df['long_ep1_0_{}'.format(selection_id)])\n",
    "\n",
    "    res_df['short_out_gap_{}'.format(selection_id)] = abs(res_df['short_out_1_{}'.format(selection_id)] - res_df['short_out_0_{}'.format(selection_id)])\n",
    "    res_df['long_out_gap_{}'.format(selection_id)] = abs(res_df['long_out_1_{}'.format(selection_id)] - res_df['long_out_0_{}'.format(selection_id)])\n",
    "    res_df['short_ep2_gap_{}'.format(selection_id)] = abs(res_df['short_ep2_1_{}'.format(selection_id)] - res_df['short_ep2_0_{}'.format(selection_id)])\n",
    "    res_df['long_ep2_gap_{}'.format(selection_id)] = abs(res_df['long_ep2_1_{}'.format(selection_id)] - res_df['long_ep2_0_{}'.format(selection_id)])\n",
    "\n",
    "    res_df['short_spread_{}'.format(selection_id)] = (res_df['short_tp_0_{}'.format(selection_id)].to_numpy() / res_df['short_tp_1_{}'.format(selection_id)].to_numpy() - 1) / 2\n",
    "    res_df['long_spread_{}'.format(selection_id)] = (res_df['long_tp_1_{}'.format(selection_id)].to_numpy() / res_df['long_tp_0_{}'.format(selection_id)].to_numpy() - 1) / 2\n",
    "        \n",
    "\n",
    "    res_df['short_lvrg_needed_{}'.format(selection_id)] = (1 / res_df['short_spread_{}'.format(selection_id)]) * config.loc_set.point1.lvrg_k\n",
    "    res_df['long_lvrg_needed_{}'.format(selection_id)] = (1 / res_df['long_spread_{}'.format(selection_id)]) * config.loc_set.point1.lvrg_k\n",
    "    \n",
    "    if config.loc_set.point1.lvrg_ceiling:\n",
    "        res_df['short_lvrg_needed_{}'.format(selection_id)][res_df['short_lvrg_needed_{}'.format(selection_id)] > config.loc_set.point1.lvrg_max_short] = config.loc_set.point1.lvrg_max_short\n",
    "        res_df['long_lvrg_needed_{}'.format(selection_id)][res_df['long_lvrg_needed_{}'.format(selection_id)] > config.loc_set.point1.lvrg_max_long] = config.loc_set.point1.lvrg_max_long     \n",
    "        \n",
    "    # Todo - public_indi 이전에 해야할지도 모름 # 'close', 'haopen', 'hahigh', 'halow', 'haclose'\n",
    "    open_, high, low, close = [res_df[col_].to_numpy() for col_ in ['open', 'high', 'low', 'close']]\n",
    "\n",
    "    \"\"\"\n",
    "    3. set tp / ep / out\n",
    "    \"\"\"\n",
    "\n",
    "    #     a. tp\n",
    "    tpg = config.tr_set.tp_gap\n",
    "\n",
    "    #         i. magnetic level\n",
    "    #     cu_wrr_32_ = res_df['cu_wrr_32_{}{}'.format(wave_itv1, wave_period1)].to_numpy()\n",
    "    #     co_wrr_32_ = res_df['co_wrr_32_{}{}'.format(wave_itv1, wave_period1)].to_numpy()\n",
    "\n",
    "    #     short_magnetic_tpg = np.vectorize(get_next_wave_level)(cu_wrr_32_, 'tp')\n",
    "    #     long_magnetic_tpg = np.vectorize(get_next_wave_level)(co_wrr_32_, 'tp')\n",
    "\n",
    "    #     res_df['short_tp_{}'.format(selection_id)] = res_df['short_tp_1_{}'.format(selection_id)].to_numpy() - res_df['short_tp_gap_{}'.format(selection_id)].to_numpy() * short_magnetic_tpg  # ep 와 마찬가지로, tpg 기준 가능\n",
    "    #     res_df['long_tp_{}'.format(selection_id)] = res_df['long_tp_1_{}'.format(selection_id)].to_numpy() + res_df['long_tp_gap_{}'.format(selection_id)].to_numpy() * long_magnetic_tpg\n",
    "\n",
    "    #         ii. 기준 : tp_1, gap : tp_box\n",
    "    res_df['short_tp_{}'.format(selection_id)] = res_df['short_tp_1_{}'.format(selection_id)].to_numpy() - res_df['short_tp_gap_{}'.format(selection_id)].to_numpy() * tpg\n",
    "    res_df['long_tp_{}'.format(selection_id)] = res_df['long_tp_1_{}'.format(selection_id)].to_numpy() + res_df['long_tp_gap_{}'.format(selection_id)].to_numpy() * tpg\n",
    "\n",
    "    # res_df['short_tp_{}'.format(selection_id)] = res_df['short_out_1_{}'.format(selection_id)].to_numpy() - res_df[\n",
    "    #     'short_out_gap_{}'.format(selection_id)].to_numpy() * tpg\n",
    "    # res_df['long_tp_{}'.format(selection_id)] = res_df['long_out_1_{}'.format(selection_id)].to_numpy() + res_df[\n",
    "    #     'long_out_gap_{}'.format(selection_id)].to_numpy() * tpg\n",
    "\n",
    "    # res_df['short_tp_{}'.format(selection_id)] = short_tp_1 - short_epout_gap * tpg\n",
    "    # res_df['long_tp_{}'.format(selection_id)] = long_tp_1 + long_epout_gap * tpg\n",
    "\n",
    "    #     b. ep\n",
    "    #             i. limit_ep1\n",
    "    if config.ep_set.entry_type == \"LIMIT\":\n",
    "        epg1 = config.tr_set.ep1_gap\n",
    "\n",
    "        #              1. magnetic level\n",
    "        # cu_wrr_32_ = res_df['cu_wrr_32_{}{}'.format(wave_itv1, wave_period1)].to_numpy()\n",
    "        # co_wrr_32_ = res_df['co_wrr_32_{}{}'.format(wave_itv1, wave_period1)].to_numpy()\n",
    "\n",
    "        #         short_magnetic_outg = np.vectorize(get_next_wave_level)(cu_wrr_32_)\n",
    "        #         long_magnetic_outg = np.vectorize(get_next_wave_level)(co_wrr_32_)\n",
    "\n",
    "        #         res_df['short_ep1_{}'.format(selection_id)] = res_df['short_tp_1_{}'.format(selection_id)].to_numpy() - res_df['short_tp_gap_{}'.format(selection_id)].to_numpy() * short_magnetic_outg  # ep 와 마찬가지로, tpg 기준 가능\n",
    "        #         res_df['long_ep1_{}'.format(selection_id)] = res_df['long_tp_1_{}'.format(selection_id)].to_numpy() + res_df['long_tp_gap_{}'.format(selection_id)].to_numpy() * long_magnetic_outg\n",
    "\n",
    "        #              2. 기준 : ep1_0, gap : ep1_box\n",
    "        res_df['short_ep1_{}'.format(selection_id)] = res_df['short_ep1_1_{}'.format(selection_id)].to_numpy() + res_df['short_ep1_gap_{}'.format(selection_id)].to_numpy() * epg1\n",
    "        res_df['long_ep1_{}'.format(selection_id)] = res_df['long_ep1_1_{}'.format(selection_id)].to_numpy() - res_df['long_ep1_gap_{}'.format(selection_id)].to_numpy() * epg1\n",
    "\n",
    "        #              3. 기준 : ep1_0, gap : tp_box\n",
    "        # p1_hlm 을 위해선, tp_0 를 기준할 수 없음 --> ep1 & ep2 를 기준으로 진행\n",
    "        # res_df['short_ep1_{}'.format(selection_id)] = res_df['short_ep1_0_{}'.format(selection_id)].to_numpy() + res_df['short_tp_gap_{}'.format(selection_id)].to_numpy() * epg1  # fibonacci 고려하면, tp / out gap 기준이 맞지 않을까\n",
    "        # res_df['long_ep1_{}'.format(selection_id)] = res_df['long_ep1_0_{}'.format(selection_id)].to_numpy() - res_df['long_tp_gap_{}'.format(selection_id)].to_numpy() * epg1\n",
    "\n",
    "        #              4. 기준 : tp_1, gap : tp_box [ fibo_ep ]\n",
    "        # res_df['short_ep1_{}'.format(selection_id)] = res_df['short_tp_1_{}'.format(selection_id)].to_numpy() + res_df['short_tp_gap_{}'.format(selection_id)].to_numpy() * epg1  # fibonacci 고려하면, tp / out gap 기준이 맞지 않을까\n",
    "        # res_df['long_ep1_{}'.format(selection_id)] = res_df['long_tp_1_{}'.format(selection_id)].to_numpy() - res_df['long_tp_gap_{}'.format(selection_id)].to_numpy() * epg1\n",
    "\n",
    "\n",
    "    #             ii. market_ep1\n",
    "    else:\n",
    "        res_df['short_ep1_{}'.format(selection_id)] = close\n",
    "        res_df['long_ep1_{}'.format(selection_id)] = close\n",
    "\n",
    "    #             iii. limit_ep2\n",
    "    if config.ep_set.point2.entry_type == \"LIMIT\":\n",
    "        epg2 = config.tr_set.ep2_gap\n",
    "\n",
    "        #              1. magnetic level\n",
    "        # cu_wrr_32_ = res_df['cu_wrr_32_{}{}'.format(wave_itv1, wave_period1)].to_numpy()\n",
    "        # co_wrr_32_ = res_df['co_wrr_32_{}{}'.format(wave_itv1, wave_period1)].to_numpy()\n",
    "\n",
    "        #         short_magnetic_outg = np.vectorize(get_next_wave_level)(cu_wrr_32_)\n",
    "        #         long_magnetic_outg = np.vectorize(get_next_wave_level)(co_wrr_32_)\n",
    "\n",
    "        #         res_df['short_ep2_{}'.format(selection_id)] = res_df['short_tp_1_{}'.format(selection_id)].to_numpy() - res_df['short_tp_gap_{}'.format(selection_id)].to_numpy() * short_magnetic_outg  # ep 와 마찬가지로, tpg 기준 가능\n",
    "        #         res_df['long_ep2_{}'.format(selection_id)] = res_df['long_tp_1_{}'.format(selection_id)].to_numpy() + res_df['long_tp_gap_{}'.format(selection_id)].to_numpy() * long_magnetic_outg\n",
    "\n",
    "        #              2. 기준 : ep2_0, gap : ep2_box\n",
    "        res_df['short_ep2_{}'.format(selection_id)] = res_df['short_ep2_1_{}'.format(selection_id)].to_numpy() + res_df['short_ep2_gap_{}'.format(selection_id)].to_numpy() * epg2\n",
    "        res_df['long_ep2_{}'.format(selection_id)] = res_df['long_ep2_1_{}'.format(selection_id)].to_numpy() - res_df['long_ep2_gap_{}'.format(selection_id)].to_numpy() * epg2\n",
    "\n",
    "        #              3. 기준 : ep2_0, gap : out_box\n",
    "        # res_df['short_ep2_{}'.format(selection_id)] = res_df['short_ep2_0_{}'.format(selection_id)].to_numpy() + res_df['short_out_gap_{}'.format(selection_id)].to_numpy() * epg2\n",
    "        # res_df['long_ep2_{}'.format(selection_id)] = res_df['long_ep2_0_{}'.format(selection_id)].to_numpy() - res_df['long_out_gap_{}'.format(selection_id)].to_numpy() * epg2\n",
    "\n",
    "        #              4. 기준 : tp_1, gap : tp_box [ fibo_ep ]\n",
    "        # res_df['short_ep2_{}'.format(selection_id)] = res_df['short_tp_1_{}'.format(selection_id)].to_numpy() - res_df['short_tp_gap_{}'.format(selection_id)].to_numpy() * epg2  # fibonacci 고려하면, tp / out gap 기준이 맞지 않을까\n",
    "        # res_df['long_ep2_{}'.format(selection_id)] = res_df['long_tp_1_{}'.format(selection_id)].to_numpy() + res_df['long_tp_gap_{}'.format(selection_id)].to_numpy() * epg2\n",
    "\n",
    "        #              5. 기준 : out_0, gap : out_box [ fibo_ep ]\n",
    "        # res_df['short_ep2_{}'.format(selection_id)] = res_df['short_out_0_{}'.format(selection_id)].to_numpy() + res_df[\n",
    "        #     'short_out_gap_{}'.format(selection_id)].to_numpy() * epg2  # fibonacci 고려하면, tp / out gap 기준이 맞지 않을까\n",
    "        # res_df['long_ep2_{}'.format(selection_id)] = res_df['long_out_0_{}'.format(selection_id)].to_numpy() - res_df[\n",
    "        #     'long_out_gap_{}'.format(selection_id)].to_numpy() * epg2\n",
    "\n",
    "\n",
    "    #             iv. market_ep2\n",
    "    else:\n",
    "        res_df['short_ep2_{}'.format(selection_id)] = close\n",
    "        res_df['long_ep2_{}'.format(selection_id)] = close\n",
    "\n",
    "    #     c. out\n",
    "    outg = config.tr_set.out_gap\n",
    "    # res_df['short_out_{}'.format(selection_id)] = short_tp_0 + short_tp_gap * outg            # 1. for hhm check -> 규칙성과 wave_range 기반 거래 기준의 hhm 확인\n",
    "    # res_df['long_out_{}'.format(selection_id)] = long_tp_0 - long_tp_gap * outg\n",
    "\n",
    "    if config.tr_set.check_hlm == 0:\n",
    "\n",
    "        # cu_wrr_32_ = res_df['cu_wrr_32_{}{}'.format(wave_itv1, wave_period1)].to_numpy()\n",
    "        # co_wrr_32_ = res_df['co_wrr_32_{}{}'.format(wave_itv1, wave_period1)].to_numpy()\n",
    "\n",
    "        #     i. magnetic level\n",
    "        #         short_magnetic_outg = np.vectorize(get_next_wave_level)(cu_wrr_32_, 'out')\n",
    "        #         long_magnetic_outg = np.vectorize(get_next_wave_level)(co_wrr_32_, 'out')\n",
    "\n",
    "        #         res_df['short_out_{}'.format(selection_id)] = res_df['short_tp_1_{}'.format(selection_id)].to_numpy() - res_df['short_tp_gap_{}'.format(selection_id)].to_numpy() * short_magnetic_outg  # ep 와 마찬가지로, tpg 기준 가능\n",
    "        #         res_df['long_out_{}'.format(selection_id)] = res_df['long_tp_1_{}'.format(selection_id)].to_numpy() + res_df['long_tp_gap_{}'.format(selection_id)].to_numpy() * long_magnetic_outg\n",
    "\n",
    "        #     ii. 기준 : out_0, gap : out_box\n",
    "        # res_df['short_out_{}'.format(selection_id)] = res_df['short_out_0_{}'.format(selection_id)].to_numpy() + res_df['short_out_gap_{}'.format(selection_id)].to_numpy() * outg\n",
    "        # res_df['long_out_{}'.format(selection_id)] = res_df['long_out_0_{}'.format(selection_id)].to_numpy() - res_df['long_out_gap_{}'.format(selection_id)].to_numpy() * outg\n",
    "\n",
    "        # 1 : 1 RR_ratio\n",
    "        res_df['short_out_{}'.format(selection_id)] = res_df['short_ep1_1_{}'.format(selection_id)].to_numpy() + res_df['short_ep1_gap_{}'.format(selection_id)].to_numpy() * 2 * epg1\n",
    "        res_df['long_out_{}'.format(selection_id)] = res_df['long_ep1_1_{}'.format(selection_id)].to_numpy() - res_df['long_ep1_gap_{}'.format(selection_id)].to_numpy() * 2 * epg1\n",
    "\n",
    "        #     iii. 기준 : tp_0, gap : tp_box\n",
    "        # res_df['short_out_{}'.format(selection_id)] = res_df['short_tp_0_{}'.format(selection_id)].to_numpy() + res_df['short_tp_gap_{}'.format(selection_id)].to_numpy() * outg\n",
    "        # res_df['long_out_{}'.format(selection_id)] = res_df['long_tp_0_{}'.format(selection_id)].to_numpy() - res_df['long_tp_gap_{}'.format(selection_id)].to_numpy() * outg\n",
    "\n",
    "        #     iv. 기준 : ep1_0, gap : ep1_box\n",
    "        # res_df['short_out_{}'.format(selection_id)] = res_df['short_ep1_0_{}'.format(selection_id)].to_numpy() + res_df['short_ep1_gap_{}'.format(selection_id)].to_numpy() * outg\n",
    "        # res_df['long_out_{}'.format(selection_id)] = res_df['long_ep1_0_{}'.format(selection_id)].to_numpy() - res_df['long_ep1_gap_{}'.format(selection_id)].to_numpy() * outg\n",
    "\n",
    "        # res_df['short_out_{}'.format(selection_id)] = res_df['short_ep1_1_{}'.format(selection_id)].to_numpy() + res_df['short_ep1_gap_{}'.format(selection_id)].to_numpy() * outg\n",
    "        # res_df['long_out_{}'.format(selection_id)] = res_df['long_ep1_1_{}'.format(selection_id)].to_numpy() - res_df['long_ep1_gap_{}'.format(selection_id)].to_numpy() * outg\n",
    "\n",
    "        #     v. 기준 : temporary refix for flexible p1_hhm\n",
    "        # res_df['short_tp_0_{}'.format(selection_id)] = res_df['short_out_{}'.format(selection_id)]\n",
    "        # res_df['long_tp_0_{}'.format(selection_id)] = res_df['long_out_{}'.format(selection_id)]\n",
    "\n",
    "\n",
    "    elif config.tr_set.check_hlm == 1:  # for p1_hlm\n",
    "        # ------ ep1box as outg ------ #\n",
    "        res_df['short_out_{}'.format(selection_id)] = res_df['short_ep1_0_{}'.format(selection_id)].to_numpy() + res_df['short_ep1_gap_{}'.format(selection_id)].to_numpy() * outg\n",
    "        res_df['long_out_{}'.format(selection_id)] = res_df['long_ep1_0_{}'.format(selection_id)].to_numpy() - res_df['long_ep1_gap_{}'.format(selection_id)].to_numpy() * outg\n",
    "\n",
    "        # ------ 1_tr - ep1box as outg ------ #\n",
    "        # res_df['short_out_{}'.format(selection_id)] = res_df['short_ep1_0_{}'.format(selection_id)].to_numpy() + res_df['short_ep1_gap_{}'.format(selection_id)].to_numpy() * outg\n",
    "        # res_df['long_out_{}'.format(selection_id)] = res_df['long_ep1_0_{}'.format(selection_id)].to_numpy() - res_df['long_ep1_gap_{}'.format(selection_id)].to_numpy() * outg\n",
    "\n",
    "        # ------ 1_tr - auto_calculation by ep1 ------ #\n",
    "        # res_df['short_out_{}'.format(selection_id)] = res_df['short_ep1_{}'.format(selection_id)] + (res_df['short_ep1_{}'.format(selection_id)].to_numpy() - res_df['short_tp_{}'.format(selection_id)].to_numpy())\n",
    "        # res_df['long_out_{}'.format(selection_id)] = res_df['long_ep1_{}'.format(selection_id)].to_numpy() - (res_df['long_tp_{}'.format(selection_id)].to_numpy() - res_df['long_ep1_{}'.format(selection_id)].to_numpy())\n",
    "\n",
    "        # ------ tpbox as outg ------ #\n",
    "        # res_df['short_out_{}'.format(selection_id)] = res_df['short_ep1_0_{}'.format(selection_id)].to_numpy() + res_df['short_tp_gap_{}'.format(selection_id)].to_numpy() * outg  # ep 와 마찬가지로, tpg 기준 가능\n",
    "        # res_df['long_out_{}'.format(selection_id)] = res_df['long_ep1_0_{}'.format(selection_id)].to_numpy() - res_df['long_tp_gap_{}'.format(selection_id)].to_numpy() * outg\n",
    "\n",
    "        # ------ [fibo_out] = 기준 : tp_0, gap = tp_box ------ #\n",
    "        # res_df['short_out_{}'.format(selection_id)] = res_df['short_tp_0_{}'.format(selection_id)].to_numpy() + res_df['short_tp_gap_{}'.format(selection_id)].to_numpy() * outg  # ep 와 마찬가지로, tpg 기준 가능\n",
    "        # res_df['long_out_{}'.format(selection_id)] = res_df['long_tp_0_{}'.format(selection_id)].to_numpy() - res_df['long_tp_gap_{}'.format(selection_id)].to_numpy() * outg\n",
    "\n",
    "    else:  # p2_hlm\n",
    "        #     i. 기준 : out_0, gap : out_box\n",
    "        # res_df['short_out_{}'.format(selection_id)] = res_df['short_out_0_{}'.format(selection_id)].to_numpy() + res_df['short_out_gap_{}'.format(selection_id)].to_numpy() * outg\n",
    "        # res_df['long_out_{}'.format(selection_id)] = res_df['long_out_0_{}'.format(selection_id)].to_numpy() - res_df['long_out_gap_{}'.format(selection_id)].to_numpy() * outg\n",
    "\n",
    "        #     ii. 기준 : ep2_0, gap : ep2_box\n",
    "        res_df['short_out_{}'.format(selection_id)] = res_df['short_ep2_1_{}'.format(selection_id)].to_numpy() + res_df['short_ep2_gap_{}'.format(selection_id)].to_numpy() * epg2 # * 5\n",
    "        res_df['long_out_{}'.format(selection_id)] = res_df['long_ep2_1_{}'.format(selection_id)].to_numpy() - res_df['long_ep2_gap_{}'.format(selection_id)].to_numpy() * epg2 # * 5\n",
    "\n",
    "        # res_df['short_out_{}'.format(selection_id)] = res_df['short_tp_0_{}'.format(selection_id)].to_numpy() + res_df['short_tp_gap_{}'.format(selection_id)].to_numpy() * outg\n",
    "        # res_df['long_out_{}'.format(selection_id)] = res_df['long_tp_0_{}'.format(selection_id)].to_numpy() - res_df['long_tp_gap_{}'.format(selection_id)].to_numpy() * outg\n",
    "\n",
    "        #     iii. ep2box as out\n",
    "        # res_df['short_out_{}'.format(selection_id)] = res_df['short_ep2_0_{}'.format(selection_id)].to_numpy() + res_df['short_ep2_gap_{}'.format(selection_id)].to_numpy() * outg   # p2's ep_box 를 out 으로 사용한다?\n",
    "        # res_df['long_out_{}'.format(selection_id)] = res_df['long_ep2_0_{}'.format(selection_id)].to_numpy() - res_df['long_ep2_gap_{}'.format(selection_id)].to_numpy() * outg\n",
    "\n",
    "        \n",
    "    if env == 'IDEP':\n",
    "        calc_with_hoga_unit_vecto = np.vectorize(calc_with_precision)\n",
    "        res_df['short_tp_{}'.format(selection_id)] = calc_with_hoga_unit_vecto(res_df['short_tp_{}'.format(selection_id)].to_numpy(), 2)\n",
    "        res_df['long_tp_{}'.format(selection_id)] = calc_with_hoga_unit_vecto(res_df['long_tp_{}'.format(selection_id)].to_numpy(), 2)\n",
    "        res_df['short_ep1_{}'.format(selection_id)] = calc_with_hoga_unit_vecto(res_df['short_ep1_{}'.format(selection_id)].to_numpy(), 2)\n",
    "        res_df['long_ep1_{}'.format(selection_id)] = calc_with_hoga_unit_vecto(res_df['long_ep1_{}'.format(selection_id)].to_numpy(), 2)\n",
    "        res_df['short_ep2_{}'.format(selection_id)] = calc_with_hoga_unit_vecto(res_df['short_ep2_{}'.format(selection_id)].to_numpy(), 2)\n",
    "        res_df['long_ep2_{}'.format(selection_id)] = calc_with_hoga_unit_vecto(res_df['long_ep2_{}'.format(selection_id)].to_numpy(), 2)\n",
    "        res_df['short_out_{}'.format(selection_id)] = calc_with_hoga_unit_vecto(res_df['short_out_{}'.format(selection_id)].to_numpy(), 2)\n",
    "        res_df['long_out_{}'.format(selection_id)] = calc_with_hoga_unit_vecto(res_df['long_out_{}'.format(selection_id)].to_numpy(), 2)\n",
    "        \n",
    "        \n",
    "    if mode == \"OPEN\":\n",
    "\n",
    "        #    a. init open_res\n",
    "        short_open_res1 = np.ones(len_df)  # .astype(object)\n",
    "        long_open_res1 = np.ones(len_df)  # .astype(object)\n",
    "        short_open_res2 = np.ones(len_df)  # .astype(object)\n",
    "        long_open_res2 = np.ones(len_df)  # .astype(object)\n",
    "\n",
    "        if show_detail:\n",
    "            sys_log.warning(\"init open_res\")\n",
    "            sys_log.warning(\"np.sum(short_open_res1 == 1) : {}\".format(np.sum(short_open_res1 == 1)))\n",
    "            sys_log.warning(\"np.sum(long_open_res1 == 1) : {}\".format(np.sum(long_open_res1 == 1)))\n",
    "\n",
    "        #    b. bars in a hour.\n",
    "#         timestamp_index = np.array(list(map(lambda x: int(datetime.timestamp(x)), pd.Series(res_df.index))))\n",
    "#         timestamp_index_shifted = np.array(list(map(lambda x: int(datetime.timestamp(x)) if not pd.isnull(x) else 0, pd.Series(res_df.index).shift(60))))\n",
    "#         bars_in_hour = timestamp_index - timestamp_index_shifted < 4800\n",
    "\n",
    "#         short_open_res1 *= bars_in_hour\n",
    "#         long_open_res1 *= bars_in_hour\n",
    "\n",
    "#         if show_detail:\n",
    "#             sys_log.warning(\"bars in a hour\")\n",
    "#             sys_log.warning(\"np.sum(short_open_res1 == 1) : {}\".format(np.sum(short_open_res1 == 1)))\n",
    "#             sys_log.warning(\"np.sum(long_open_res1 == 1) : {}\".format(np.sum(long_open_res1 == 1)))\n",
    "\n",
    "        \"\"\"\n",
    "        4. wave points\n",
    "        \"\"\"\n",
    "        #    a. wave_point\n",
    "        # notnan_short_tc = ~pd.isnull(res_df['short_tc_{}{}'.format(wave_itv1, wave_period1)].to_numpy())  # isnull for object\n",
    "        # notnan_long_tc = ~pd.isnull(res_df['long_tc_{}{}'.format(wave_itv1, wave_period1)].to_numpy())  # isnull for object\n",
    "\n",
    "        notnan_cu = ~pd.isnull(res_df['wave_cu_{}{}'.format(wave_itv1, wave_period1)].to_numpy())  # isnull for object\n",
    "        notnan_co = ~pd.isnull(res_df['wave_co_{}{}'.format(wave_itv1, wave_period1)].to_numpy())\n",
    "        # notnan_cu2 = ~pd.isnull(res_df['wave_cu_{}{}'.format(wave_itv2, wave_period2)].to_numpy())  # isnull for object\n",
    "        # notnan_co2 = ~pd.isnull(res_df['wave_co_{}{}'.format(wave_itv2, wave_period2)].to_numpy())\n",
    "\n",
    "        short_open_res1 *= res_df['wave_cu_{}{}'.format(wave_itv1, wave_period1)].to_numpy().astype(bool) * notnan_cu  # object로 변환되는 경우에 대응해, bool 로 재정의\n",
    "        long_open_res1 *= res_df['wave_co_{}{}'.format(wave_itv1, wave_period1)].to_numpy().astype(bool) * notnan_co  # np.nan = bool type 으로 True 임..\n",
    "        # short_open_res1 *= res_df['short_tc_{}{}'.format(wave_itv1, wave_period1)].to_numpy().astype(bool) * notnan_short_tc\n",
    "        # long_open_res1 *= res_df['long_tc_{}{}'.format(wave_itv1, wave_period1)].to_numpy().astype(bool) * notnan_long_tc\n",
    "\n",
    "        # short_open_res2 *= res_df['wave_cu_{}{}'.format(wave_itv2, wave_period2)].to_numpy().astype(bool) * notnan_cu2  # object로 변환되는 경우에 대응해, bool 로 재정의\n",
    "        # long_open_res2 *= res_df['wave_co_{}{}'.format(wave_itv2, wave_period2)].to_numpy().astype(bool) * notnan_co2  # np.nan = bool type 으로 True 임..\n",
    "        # short_open_res2 *= res_df['short_tc_{}{}'.format(wave_itv2, tc_period)].to_numpy()\n",
    "        # long_open_res2 *= res_df['long_tc_{}{}'.format(wave_itv2, tc_period)].to_numpy()\n",
    "\n",
    "        # sys_log.warning => for pretty logging, 개행 문자를 추가하지 않고 이쁘게 log_file 에만 write 하는 법.\n",
    "        if show_detail:\n",
    "            sys_log.warning(\"wave_point\")\n",
    "            sys_log.warning(\"np.sum(short_open_res1 == 1) : {}\".format(np.sum(short_open_res1 == 1)))\n",
    "            sys_log.warning(\"np.sum(long_open_res1 == 1) : {}\".format(np.sum(long_open_res1 == 1)))\n",
    "            # sys_log.warning(\"np.sum(short_open_res2 == 1) : {}\".format(np.sum(short_open_res2 == 1)))\n",
    "            # sys_log.warning(\"np.sum(long_open_res2 == 1) : {}\".format(np.sum(long_open_res2 == 1)))\n",
    "\n",
    "        #     b. reject wave_update_hl\n",
    "        notnan_update_low_cu = ~pd.isnull(res_df['wave_update_low_cu_bool_{}{}'.format(wave_itv1, wave_period1)].to_numpy())\n",
    "        notnan_update_high_co = ~pd.isnull(res_df['wave_update_high_co_bool_{}{}'.format(wave_itv1, wave_period1)].to_numpy())\n",
    "        # notnan_update_low_cu2 = ~pd.isnull(res_df['wave_update_low_cu_bool_{}{}'.format(wave_itv2, wave_period2)].to_numpy())\n",
    "        # notnan_update_high_co2 = ~pd.isnull(res_df['wave_update_high_co_bool_{}{}'.format(wave_itv2, wave_period2)].to_numpy())\n",
    "\n",
    "        short_open_res1 *= ~(res_df['wave_update_low_cu_bool_{}{}'.format(wave_itv1, wave_period1)].to_numpy().astype(bool)) * notnan_update_low_cu\n",
    "        long_open_res1 *= ~(res_df['wave_update_high_co_bool_{}{}'.format(wave_itv1, wave_period1)].to_numpy().astype(bool)) * notnan_update_high_co\n",
    "        # short_open_res2 *= ~(res_df['wave_update_low_cu_bool_{}{}'.format(wave_itv2, wave_period2)].to_numpy().astype(bool)) * notnan_update_low_cu2\n",
    "        # long_open_res2 *= ~(res_df['wave_update_high_co_bool_{}{}'.format(wave_itv2, wave_period2)].to_numpy().astype(bool)) * notnan_update_high_co2\n",
    "\n",
    "        if show_detail:\n",
    "            sys_log.warning(\"reject update_hl\")\n",
    "            sys_log.warning(\"np.sum(short_open_res1 == 1) : {}\".format(np.sum(short_open_res1 == 1)))\n",
    "            sys_log.warning(\"np.sum(long_open_res1 == 1) : {}\".format(np.sum(long_open_res1 == 1)))\n",
    "        #     sys_log.warning(\"np.sum(short_open_res2 == 1) : {}\".format(np.sum(short_open_res2 == 1)))\n",
    "        #     sys_log.warning(\"np.sum(long_open_res2 == 1) : {}\".format(np.sum(long_open_res2 == 1)))\n",
    "\n",
    "        #     c. wave_itv\n",
    "        if wave_itv1 != 'T':\n",
    "            wave_itv1_num = itv_to_number(wave_itv1)\n",
    "            short_open_res1 *= np_timeidx % wave_itv1_num == (wave_itv1_num - 1)\n",
    "            long_open_res1 *= np_timeidx % wave_itv1_num == (wave_itv1_num - 1)\n",
    "\n",
    "            if show_detail:\n",
    "                sys_log.warning(\"wave_itv1\")\n",
    "                sys_log.warning(\"np.sum(short_open_res1 == 1) : {}\".format(np.sum(short_open_res1 == 1)))\n",
    "                sys_log.warning(\"np.sum(long_open_res1 == 1) : {}\".format(np.sum(long_open_res1 == 1)))\n",
    "\n",
    "        if wave_itv2 != 'T':\n",
    "            wave_itv2_num = itv_to_number(wave_itv2)\n",
    "            short_open_res2 *= np_timeidx % wave_itv2_num == (wave_itv2_num - 1)\n",
    "            long_open_res2 *= np_timeidx % wave_itv2_num == (wave_itv2_num - 1)\n",
    "\n",
    "            if show_detail:\n",
    "                sys_log.warning(\"wave_itv2\")\n",
    "                sys_log.warning(\"np.sum(short_open_res2 == 1) : {}\".format(np.sum(short_open_res2 == 1)))\n",
    "                sys_log.warning(\"np.sum(long_open_res2 == 1) : {}\".format(np.sum(long_open_res2 == 1)))\n",
    "\n",
    "        #     d. wave_mm\n",
    "        wave_high_terms_cnt_fill1_ = res_df['wave_high_terms_cnt_fill_{}{}'.format(wave_itv1, wave_period1)].to_numpy()\n",
    "        wave_low_terms_cnt_fill1_ = res_df['wave_low_terms_cnt_fill_{}{}'.format(wave_itv1, wave_period1)].to_numpy()\n",
    "\n",
    "        short_open_res1 *= (wave_high_terms_cnt_fill1_ > config.tr_set.wave_greater2) & (\n",
    "                wave_low_terms_cnt_fill1_ > config.tr_set.wave_greater1)\n",
    "        long_open_res1 *= (wave_low_terms_cnt_fill1_ > config.tr_set.wave_greater2) & (\n",
    "                wave_high_terms_cnt_fill1_ > config.tr_set.wave_greater1)\n",
    "\n",
    "        # wave_high_terms_cnt_fill2_ = res_df['wave_high_terms_cnt_fill_{}{}'.format(wave_itv2, wave_period2)].to_numpy()\n",
    "        # wave_low_terms_cnt_fill2_ = res_df['wave_low_terms_cnt_fill_{}{}'.format(wave_itv2, wave_period2)].to_numpy()\n",
    "\n",
    "        # short_open_res2 *= (wave_high_terms_cnt_fill2_ > config.tr_set.wave_greater2) & (wave_low_terms_cnt_fill2_ > config.tr_set.wave_greater1)\n",
    "        # long_open_res2 *= (wave_low_terms_cnt_fill2_ > config.tr_set.wave_greater2) & (wave_high_terms_cnt_fill2_ > config.tr_set.wave_greater1)\n",
    "\n",
    "        if show_detail:\n",
    "            sys_log.warning(\"wave_mm\")\n",
    "            sys_log.warning(\"np.sum(short_open_res1 == 1) : {}\".format(np.sum(short_open_res1 == 1)))\n",
    "            sys_log.warning(\"np.sum(long_open_res1 == 1) : {}\".format(np.sum(long_open_res1 == 1)))\n",
    "            # sys_log.warning(\"np.sum(short_open_res2 == 1) : {}\".format(np.sum(short_open_res2 == 1)))\n",
    "            # sys_log.warning(\"np.sum(long_open_res2 == 1) : {}\".format(np.sum(long_open_res2 == 1)))\n",
    "\n",
    "        #     e. wave_length\n",
    "        if config.tr_set.wave_length_min_short1 != \"None\":\n",
    "            short_wave_length_fill_ = res_df['short_wave_length_fill_{}{}'.format(wave_itv1, wave_period1)].to_numpy()\n",
    "            short_open_res1 *= short_wave_length_fill_ >= config.tr_set.wave_length_min_short1\n",
    "            if show_detail:\n",
    "                sys_log.warning(\"wave_length_min_short1\")\n",
    "                sys_log.warning(\"np.sum(short_open_res1 == 1) : {}\".format(np.sum(short_open_res1 == 1)))\n",
    "                \n",
    "        if config.tr_set.wave_length_max_short1 != \"None\":\n",
    "            short_wave_length_fill_ = res_df['short_wave_length_fill_{}{}'.format(wave_itv1, wave_period1)].to_numpy()\n",
    "            short_open_res1 *= short_wave_length_fill_ <= config.tr_set.wave_length_max_short1\n",
    "            if show_detail:\n",
    "                sys_log.warning(\"wave_length_max_short1\")\n",
    "                sys_log.warning(\"np.sum(short_open_res1 == 1) : {}\".format(np.sum(short_open_res1 == 1)))\n",
    "                \n",
    "        if config.tr_set.wave_length_min_long1 != \"None\":\n",
    "            long_wave_length_fill_ = res_df['long_wave_length_fill_{}{}'.format(wave_itv1, wave_period1)].to_numpy()\n",
    "            long_open_res1 *= long_wave_length_fill_ >= config.tr_set.wave_length_min_long1\n",
    "            if show_detail:\n",
    "                sys_log.warning(\"wave_length_min_long1\")\n",
    "                sys_log.warning(\"np.sum(long_open_res1 == 1) : {}\".format(np.sum(long_open_res1 == 1)))                \n",
    "       \n",
    "        if config.tr_set.wave_length_max_long1 != \"None\":     \n",
    "            long_wave_length_fill_ = res_df['long_wave_length_fill_{}{}'.format(wave_itv1, wave_period1)].to_numpy()\n",
    "            long_open_res1 *= long_wave_length_fill_ <= config.tr_set.wave_length_max_long1\n",
    "            if show_detail:\n",
    "                sys_log.warning(\"wave_length_max_long1\")\n",
    "                sys_log.warning(\"np.sum(long_open_res1 == 1) : {}\".format(np.sum(long_open_res1 == 1)))\n",
    "\n",
    "        #     #     f. wave_spread\n",
    "        #     if config.tr_set.wave_spread1 != \"None\":\n",
    "        #       short_wave_spread_fill = res_df['short_wave_spread_fill_{}{}'.format(wave_itv1, wave_period1)].to_numpy()\n",
    "        #       long_wave_spread_fill = res_df['long_wave_spread_fill_{}{}'.format(wave_itv1, wave_period1)].to_numpy()\n",
    "\n",
    "        #       short_open_res1 *= short_wave_spread_fill >= config.tr_set.wave_spread1\n",
    "        #       long_open_res1 *= long_wave_spread_fill >= config.tr_set.wave_spread1\n",
    "\n",
    "        #       if show_detail:\n",
    "        #         sys_log.warning(\"wave_spread\")\n",
    "        #         sys_log.warning(\"np.sum(short_open_res1 == 1) : {}\".format(np.sum(short_open_res1 == 1)))\n",
    "        #         sys_log.warning(\"np.sum(long_open_res1 == 1) : {}\".format(np.sum(long_open_res1 == 1)))\n",
    "\n",
    "        #     #     g. wave_time_ratio\n",
    "        #     if config.tr_set.wave_time_ratio1 != \"None\":\n",
    "        #       short_wave_time_ratio = res_df['short_wave_time_ratio_{}{}'.format(wave_itv1, wave_period1)].to_numpy()\n",
    "        #       long_wave_time_ratio = res_df['long_wave_time_ratio_{}{}'.format(wave_itv1, wave_period1)].to_numpy()\n",
    "\n",
    "        #       short_open_res1 *= short_wave_time_ratio >= config.tr_set.wave_time_ratio1\n",
    "        #       long_open_res1 *= long_wave_time_ratio >= config.tr_set.wave_time_ratio1\n",
    "\n",
    "        #       if show_detail:\n",
    "        #         sys_log.warning(\"wave_time_ratio\")\n",
    "        #         sys_log.warning(\"np.sum(short_open_res1 == 1) : {}\".format(np.sum(short_open_res1 == 1)))\n",
    "        #         sys_log.warning(\"np.sum(long_open_res1 == 1) : {}\".format(np.sum(long_open_res1 == 1)))\n",
    "\n",
    "        \"\"\"\n",
    "        5. bb points\n",
    "        \"\"\"\n",
    "        #     a. wave prime_idx : 연속된 wave_point 에 대해서 prime_idx 만 허용함 (wave_bb 에서 파생됨)\n",
    "        #     short_open_res1 *= res_df['wave_cu_idx_{}{}'.format(wave_itv1, wave_period1)] == res_df['wave_cu_prime_idx_fill_{}{}'.format(wave_itv1, wave_period1)]\n",
    "        #     long_open_res1 *= res_df['wave_co_idx_{}{}'.format(wave_itv1, wave_period1)] == res_df['wave_co_prime_idx_fill_{}{}'.format(wave_itv1, wave_period1)]\n",
    "\n",
    "        #     if show_detail:\n",
    "        #         sys_log.warning(\"wave prime_idx\")\n",
    "        #         sys_log.warning(\"np.sum(short_open_res1 == 1) : {}\".format(np.sum(short_open_res1 == 1)))\n",
    "        #         sys_log.warning(\"np.sum(long_open_res1 == 1) : {}\".format(np.sum(long_open_res1 == 1)))\n",
    "\n",
    "        #     b. inner_triangle\n",
    "        #     # short_open_res1 *= (roll_highs1[-2] > roll_highs1[-1]) & (roll_lows1[-2] < roll_lows1[-1])\n",
    "        #     # long_open_res1 *= (roll_lows1[-2] < roll_lows1[-1]) & (roll_highs1[-2] > roll_highs1[-1])\n",
    "        #     short_open_res1 *= (roll_highs1[-2] > roll_highs1[-1])\n",
    "        #     long_open_res1 *= (roll_lows1[-2] < roll_lows1[-1])\n",
    "        #     # short_open_res1 *= (roll_lows1[-2] < roll_lows1[-1])\n",
    "        #     # long_open_res1 *= (roll_highs1[-2] > roll_highs1[-1])\n",
    "\n",
    "        #     if show_detail:\n",
    "        #         sys_log.warning(\"inner_triangle\")\n",
    "        #         sys_log.warning(\"np.sum(short_open_res1 == 1) : {}\".format(np.sum(short_open_res1 == 1)))\n",
    "        #         sys_log.warning(\"np.sum(long_open_res1 == 1) : {}\".format(np.sum(long_open_res1 == 1)))\n",
    "\n",
    "        \"\"\"\n",
    "        6. point validation\n",
    "        \"\"\"\n",
    "        short_tp_ = res_df['short_tp_{}'.format(selection_id)].to_numpy()\n",
    "        short_ep1_ = res_df['short_ep1_{}'.format(selection_id)].to_numpy()\n",
    "        short_ep2_ = res_df['short_ep2_{}'.format(selection_id)].to_numpy()\n",
    "        short_out_ = res_df['short_out_{}'.format(selection_id)].to_numpy()\n",
    "\n",
    "        long_tp_ = res_df['long_tp_{}'.format(selection_id)].to_numpy()\n",
    "        long_ep1_ = res_df['long_ep1_{}'.format(selection_id)].to_numpy()\n",
    "        long_ep2_ = res_df['long_ep2_{}'.format(selection_id)].to_numpy()\n",
    "        long_out_ = res_df['long_out_{}'.format(selection_id)].to_numpy()\n",
    "\n",
    "        #     a. p1 point_validation\n",
    "        #             i. tr_set validation reject nan data\n",
    "        #             ii. 정상 거래 위한 tp > ep\n",
    "        short_open_res1 *= (short_tp_ < short_ep1_) & (short_ep1_ < short_out_)\n",
    "        #             iii. reject hl_out open_execution -> close always < ep1_0 at wave_p1\n",
    "        short_open_res1 *= close < short_out_  # res_df['short_ep1_0_{}'.format(selection_id)].to_numpy()\n",
    "        # short_open_res1 *= close < short_ep1_   # reject entry open_execution\n",
    "        # short_out_  res_df['short_tp_0_{}'.format(selection_id)].to_numpy() res_df['short_ep1_0_{}'.format(selection_id)].to_numpy()\n",
    "\n",
    "        long_open_res1 *= (long_tp_ > long_ep1_) & (long_ep1_ > long_out_)  # (long_tp_ > long_ep_)\n",
    "        long_open_res1 *= close > long_out_  # res_df['long_ep1_0_{}'.format(selection_id)].to_numpy()\n",
    "        # long_open_res1 *= close > long_ep1_  # reject entry open_execution\n",
    "        # long_out_ res_df['long_tp_0_{}'.format(selection_id)].to_numpy() res_df['long_ep1_0_{}'.format(selection_id)].to_numpy()\n",
    "\n",
    "        #     b. p2 point_validation => deprecated => now, executed in en_ex_pairing() function.\n",
    "        # short_open_res2 *= (short_ep2_ < short_out_) # tr_set validation (short_tp_ < short_ep_) # --> p2_box location (cannot be vectorized)\n",
    "        # short_open_res2 *= close < short_out_    # reject hl_out open_execution\n",
    "        # long_open_res2 *= (long_ep2_ > long_out_)  # tr_set validation (long_tp_ > long_ep_) &   # p2's ep & out cannot be vectorized\n",
    "        # long_open_res2 *= close > long_out_    # reject hl_out open_execution\n",
    "\n",
    "        res_df['short_open1_{}'.format(selection_id)] = short_open_res1 * (not config.pos_set.short_ban)\n",
    "        res_df['long_open1_{}'.format(selection_id)] = long_open_res1 * (not config.pos_set.long_ban)\n",
    "\n",
    "        res_df['short_open2_{}'.format(selection_id)] = short_open_res2\n",
    "        res_df['long_open2_{}'.format(selection_id)] = long_open_res2\n",
    "\n",
    "        if show_detail:\n",
    "            sys_log.warning(\"point validation\")\n",
    "            sys_log.warning(\"np.sum(short_open_res1 == 1) : {}\".format(np.sum(short_open_res1 == 1)))\n",
    "            sys_log.warning(\"np.sum(long_open_res1 == 1) : {}\".format(np.sum(long_open_res1 == 1)))\n",
    "            # sys_log.warning(\"np.sum(short_open_res2 == 1) : {}\".format(np.sum(short_open_res2 == 1)))\n",
    "            # sys_log.warning(\"np.sum(long_open_res2 == 1) : {}\".format(np.sum(long_open_res2 == 1)))\n",
    "\n",
    "        #     c. tr\n",
    "        if config.tr_set.check_hlm == 2:\n",
    "            res_df['short_tr_{}'.format(selection_id)] = np.nan\n",
    "            res_df['long_tr_{}'.format(selection_id)] = np.nan\n",
    "        else:\n",
    "            # res_df['short_tr_{}'.format(selection_id)] = abs((short_ep1_ / short_tp_ - config.trader_set.limit_fee - 1) / (short_ep1_ / short_out_ - config.trader_set.market_fee - 1))\n",
    "            # res_df['long_tr_{}'.format(selection_id)] = abs((long_tp_ / long_ep1_ - config.trader_set.limit_fee - 1) / (long_out_ / long_ep1_ - config.trader_set.market_fee - 1))\n",
    "\n",
    "            # tr renewal. (more accurate)\n",
    "            fee = config.trader_set.limit_fee + config.trader_set.market_fee\n",
    "            profit_gain = short_ep1_ / short_tp_ * (1 - fee)\n",
    "            profit_loss = short_ep1_ / short_out_ * (1 - fee)\n",
    "            res_df['short_tr_{}'.format(selection_id)] = abs((profit_gain - 1) / (profit_loss - 1))\t\n",
    "            \n",
    "            profit_gain = long_tp_ / long_ep1_ * (1 - fee)\n",
    "            profit_loss = long_out_ / long_ep1_ * (1 - fee)\n",
    "            res_df['long_tr_{}'.format(selection_id)] = abs((profit_gain - 1) / (profit_loss - 1))\t\n",
    "\n",
    "    return res_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HuCb0phoPN83",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### legacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-DNPumVZi0xs"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "    # ------ wave_point 분리 ------ #\n",
    "    # cci_ = res_df['cci_{}{}'.format(wave_itv1, wave_period1)].to_numpy()\n",
    "    # b1_cci_ = res_df['cci_{}{}'.format(wave_itv1, wave_period1)].shift(1).to_numpy()        \n",
    "    \n",
    "    # band_width = 100\n",
    "    # upper_band = band_width\n",
    "    # lower_band = -band_width\n",
    "\n",
    "    # update_low_cu_bool = res_df['update_low_cu_bool_{}{}'.format(wave_itv1, wave_period1)]\n",
    "    # update_high_co_bool = res_df['update_high_co_bool_{}{}'.format(wave_itv1, wave_period1)]\n",
    "\n",
    "    # short_open_res1 *= (b1_cci_ > upper_band) & (upper_band > cci_) & ~update_low_cu_bool\n",
    "    # long_open_res1 *= (b1_cci_ < lower_band) & (lower_band < cci_) & ~update_high_co_bool   \n",
    "\n",
    "    \n",
    "# ------------ csd ------------ #\n",
    "    # ------ dc ------ #\n",
    "    # dc_upper_ = res_df['dc_upper_T30'].to_numpy()    \n",
    "    # dc_lower_ = res_df['dc_lower_T30'].to_numpy()    \n",
    "\n",
    "    # # Todo, post_cu ~ co 의 dc_lower == low (=touched) 여부 조사\n",
    "    # short_open_idx1 = get_index_bybool(short_open_res1, len_df_range)\n",
    "    # long_open_idx1 = get_index_bybool(long_open_res1, len_df_range)\n",
    "    # wave_co_post_idx_fill_ = res_df['wave_co_post_idx_fill_{}{}'.format(wave_itv1, wave_period1)].to_numpy()\n",
    "    # wave_cu_post_idx_fill_ = res_df['wave_cu_post_idx_fill_{}{}'.format(wave_itv1, wave_period1)].to_numpy()   # co_prime_idx (wave_high 정보를 지정하기 위한 front co_idx 지정)\n",
    "\n",
    "    # # 1. dc_lower == low 여부 조사, np.nan 덕분에 vectorize 불가하다고 봄\n",
    "    # short_valid_idx_bool = ~(pd.isnull(wave_co_post_idx_fill_) | pd.isnull(short_open_idx1)) # get_index_bybool\n",
    "    # dc_upper_touch = dc_upper_ <= high\n",
    "    # dc_upper_touch_span = np.full(len_df, np.nan)\n",
    "    # dc_upper_touch_span[short_valid_idx_bool] = [dc_upper_touch[int(iin):int(iout) + 1].sum() for iin, iout in zip(wave_co_post_idx_fill_, short_open_idx1) if not pd.isnull(iin) if not pd.isnull(iout)]\n",
    "\n",
    "    # long_valid_idx_bool = ~(pd.isnull(wave_cu_post_idx_fill_) | pd.isnull(long_open_idx1)) # get_index_bybool\n",
    "    # dc_lower_touch = dc_lower_ >= low\n",
    "    # dc_lower_touch_span = np.full(len_df, np.nan)\n",
    "    # dc_lower_touch_span[long_valid_idx_bool] = [dc_lower_touch[int(iin):int(iout) + 1].sum() for iin, iout in zip(wave_cu_post_idx_fill_, long_open_idx1) if not pd.isnull(iin) if not pd.isnull(iout)]\n",
    "\n",
    "    # short_open_res1 *= dc_upper_touch_span == 0\n",
    "    # long_open_res1 *= dc_lower_touch_span == 0\n",
    "\n",
    "    # if show_detail:\n",
    "    #   sys_log.warning(\"csd - dc\")\n",
    "    #   sys_log.warning(\"np.sum(short_open_res1 == 1) : {}\".format(np.sum(short_open_res1 == 1)))\n",
    "    #   sys_log.warning(\"np.sum(long_open_res1 == 1) : {}\".format(np.sum(long_open_res1 == 1)))\n",
    "    #   # sys_log.warning(\"np.sum(short_open_res2 == 1) : {}\".format(np.sum(short_open_res2 == 1)))\n",
    "    #   # sys_log.warning(\"np.sum(long_open_res2 == 1) : {}\".format(np.sum(long_open_res2 == 1)))   \n",
    "    \n",
    "    # ================== pattern depiction ================== #  \n",
    "    # bb_upper_ = res_df['bb_upper_{}{}'.format('T', 60)].to_numpy()\n",
    "    # bb_lower_ = res_df['bb_lower_{}{}'.format('T', 60)].to_numpy()\n",
    "    # bb_upper2_ = res_df['bb_upper2_{}{}'.format('T', 60)].to_numpy()\n",
    "    # bb_lower2_ = res_df['bb_lower2_{}{}'.format('T', 60)].to_numpy()\n",
    "    # bb_upper3_ = res_df['bb_upper3_{}{}'.format('T', 60)].to_numpy()\n",
    "    # bb_lower3_ = res_df['bb_lower3_{}{}'.format('T', 60)].to_numpy()\n",
    "    \n",
    "    # ------ 양 / 음봉 (long) ------ #\n",
    "    # short_open_res1 *= close < open\n",
    "    # long_open_res1 *= close > open\n",
    "\n",
    "    # if show_detail:\n",
    "    #   sys_log.warning(\"close > open\")\n",
    "    #   sys_log.warning(\"np.sum(short_open_res1 == 1) : {}\".format(np.sum(short_open_res1 == 1)))\n",
    "    #   sys_log.warning(\"np.sum(long_open_res1 == 1) : {}\".format(np.sum(long_open_res1 == 1)))\n",
    "\n",
    "    \n",
    "    # # ------ even_break, hhhl (long) ------ #        \n",
    "    # # cu's roll_high_[:, -1] = prev_high & cu's roll_low_[:, -1] = current_low\n",
    "    # # co's roll_low_[:, -1] = prev_low & co's roll_high_[:, -1] = current_high\n",
    "    # short_open_res1 *= (roll_low_[:, -2] > roll_low_[:, -1]) # & (roll_high_[:, -2] > roll_high_[:, -1])\n",
    "    # long_open_res1 *= (roll_high_[:, -2] < roll_high_[:, -1]) # & (roll_low_[:, -2] < roll_low_[:, -1])\n",
    "\n",
    "    # if show_detail:\n",
    "    #   sys_log.warning(\"even_break\")\n",
    "    #   sys_log.warning(\"np.sum(short_open_res1 == 1) : {}\".format(np.sum(short_open_res1 == 1)))\n",
    "    #   sys_log.warning(\"np.sum(long_open_res1 == 1) : {}\".format(np.sum(long_open_res1 == 1)))\n",
    "\n",
    "    # # ------ roll_high < bb_upper2 (long) ------ #\n",
    "    # short_open_res1 *= (roll_low_[:, -4] > roll_low_bb_lower2_[:, -4])\n",
    "    # short_open_res1 *= (roll_low_[:, -3] > roll_low_bb_lower2_[:, -3])\n",
    "    # short_open_res1 *= (roll_low_[:, -2] > roll_low_bb_lower2_[:, -2])\n",
    "    # short_open_res1 *= (roll_low_[:, -1] > roll_low_bb_lower2_[:, -1])\n",
    "\n",
    "    # long_open_res1 *= (roll_high_[:, -4] < roll_high_bb_upper2_[:, -4])\n",
    "    # long_open_res1 *= (roll_high_[:, -3] < roll_high_bb_upper2_[:, -3])\n",
    "    # long_open_res1 *= (roll_high_[:, -2] < roll_high_bb_upper2_[:, -2])\n",
    "    # long_open_res1 *= (roll_high_[:, -1] < roll_high_bb_upper2_[:, -1])\n",
    "    \n",
    "    # if show_detail:\n",
    "    #   sys_log.warning(\"roll_high < bb_upper2\")\n",
    "    #   sys_log.warning(\"np.sum(short_open_res1 == 1) : {}\".format(np.sum(short_open_res1 == 1)))\n",
    "    #   sys_log.warning(\"np.sum(long_open_res1 == 1) : {}\".format(np.sum(long_open_res1 == 1)))\n",
    "\n",
    "\n",
    "    # ------ get candle_lastidx ------ #        \n",
    "    # tf_entry = itv_to_number(config.loc_set.point.tf_entry)\n",
    "    # b1_shift = np_timeidx % tf_entry + 1  # dynamic\n",
    "    # b1_candle_lastidx = (len_df_range - b1_shift).astype(float)\n",
    "    # b2_candle_lastidx = (len_df_range - (b1_shift + tf_entry)).astype(float)\n",
    "    # b3_candle_lastidx = (len_df_range - (b1_shift + 2 * tf_entry)).astype(float)\n",
    "    # b1_candle_lastidx[b1_candle_lastidx < 0] = np.nan\n",
    "    # b2_candle_lastidx[b2_candle_lastidx < 0] = np.nan\n",
    "    # b3_candle_lastidx[b3_candle_lastidx < 0] = np.nan\n",
    "\n",
    "    # high_5T = res_df['high_5T'].to_numpy()\n",
    "    # low_5T = res_df['low_5T'].to_numpy()\n",
    "\n",
    "    # b2_high_5T = get_line(b2_candle_lastidx, high_5T)\n",
    "    # b2_low_5T = get_line(b2_candle_lastidx, low_5T)\n",
    "    \n",
    "        # # ------ bb_stream ------ #        \n",
    "        # roll_high_bb_upper_ = get_roll_wave_data(valid_high_prime_idx, roll_high_idx_arr, len_df, bb_upper_, roll_hl_cnt)\n",
    "        # roll_low_bb_upper_ = get_roll_wave_data(valid_low_prime_idx, roll_low_idx_arr, len_df, bb_upper_, roll_hl_cnt)\n",
    "        # roll_high_bb_upper2_ = get_roll_wave_data(valid_high_prime_idx, roll_high_idx_arr, len_df, bb_upper2_, roll_hl_cnt)\n",
    "        # roll_low_bb_upper2_ = get_roll_wave_data(valid_low_prime_idx, roll_low_idx_arr, len_df, bb_upper2_, roll_hl_cnt)\n",
    "\n",
    "        # roll_high_bb_lower_ = get_roll_wave_data(valid_high_prime_idx, roll_high_idx_arr, len_df, bb_lower_, roll_hl_cnt)\n",
    "        # roll_low_bb_lower_ = get_roll_wave_data(valid_low_prime_idx, roll_low_idx_arr, len_df, bb_lower_, roll_hl_cnt)\n",
    "        # roll_high_bb_lower2_ = get_roll_wave_data(valid_high_prime_idx, roll_high_idx_arr, len_df, bb_lower2_, roll_hl_cnt)\n",
    "        # roll_low_bb_lower2_ = get_roll_wave_data(valid_low_prime_idx, roll_low_idx_arr, len_df, bb_lower2_, roll_hl_cnt)\n",
    "\n",
    "        # wave_base_ = res_df['dc_base_{}{}'.format(wave_itv, wave_period)].to_numpy()\n",
    "\n",
    "        # roll_high_wave_base_ = get_roll_wave_data(valid_high_prime_idx, roll_high_idx_arr, len_df, wave_base_, roll_hl_cnt)\n",
    "        # roll_low_wave_base_ = get_roll_wave_data(valid_low_prime_idx, roll_low_idx_arr, len_df, wave_base_, roll_hl_cnt)\n",
    "\n",
    "        # short_open_res *= (roll_high_bb_lower_[:, -2] > roll_high_wave_base_[:, -2]) & (roll_high_wave_base_[:, -2] > roll_high_bb_lower2_[:, -2])\n",
    "        # short_open_res *= (roll_low_bb_lower_[:, -1] > roll_low_wave_base_[:, -1]) & (roll_low_wave_base_[:, -1] > roll_low_bb_lower2_[:, -1])\n",
    "        # short_open_res *= (roll_high_bb_lower_[:, -1] > roll_high_wave_base_[:, -1]) & (roll_high_wave_base_[:, -1] > roll_high_bb_lower2_[:, -1])\n",
    "\n",
    "        # long_open_res *= (roll_low_bb_upper_[:, -2] < roll_low_wave_base_[:, -2]) & (roll_low_wave_base_[:, -2] < roll_low_bb_upper2_[:, -2])\n",
    "        # long_open_res *= (roll_high_bb_upper_[:, -1] < roll_high_wave_base_[:, -1]) & (roll_high_wave_base_[:, -1] < roll_high_bb_upper2_[:, -1])\n",
    "        # long_open_res *= (roll_low_bb_upper_[:, -1] < roll_low_wave_base_[:, -1]) & (roll_low_wave_base_[:, -1] < roll_low_bb_upper2_[:, -1])\n",
    "\n",
    "        # if show_detail:\n",
    "        #   sys_log.warning(\"bb_stream\")\n",
    "        #   sys_log.warning(\"np.sum(short_open_res == 1) : {}\".format(np.sum(short_open_res == 1)))\n",
    "        #   sys_log.warning(\"np.sum(long_open_res == 1) : {}\".format(np.sum(long_open_res == 1)))  \n",
    "\n",
    "        # ------ candle_pattern  ------ #   \n",
    "        # b3_bb_upper_ = get_line(b3_candle_lastidx, bb_upper_)\n",
    "        # b3_bb_lower_ = get_line(b3_candle_lastidx, bb_lower_)\n",
    "        # b3_bb_upper2_ = get_line(b3_candle_lastidx, bb_upper2_)\n",
    "        # b3_bb_lower2_ = get_line(b3_candle_lastidx, bb_lower2_)\n",
    "        # b3_close = get_line(b3_candle_lastidx, close)\n",
    "\n",
    "        # b2_bb_upper2_ = get_line(b2_candle_lastidx, bb_upper2_)\n",
    "        # b2_bb_lower2_ = get_line(b2_candle_lastidx, bb_lower2_)\n",
    "        # b2_bb_upper3_ = get_line(b2_candle_lastidx, bb_upper3_)\n",
    "        # b2_bb_lower3_ = get_line(b2_candle_lastidx, bb_lower3_)\n",
    "        # b2_close = get_line(b2_candle_lastidx, close)\n",
    "\n",
    "        # b1_bb_upper_ = get_line(b1_candle_lastidx, bb_upper_)\n",
    "        # b1_bb_lower_ = get_line(b1_candle_lastidx, bb_lower_)\n",
    "        # b1_bb_upper2_ = get_line(b1_candle_lastidx, bb_upper2_)\n",
    "        # b1_bb_lower2_ = get_line(b1_candle_lastidx, bb_lower2_)\n",
    "        # b1_close = get_line(b1_candle_lastidx, close)\n",
    "        # b1_high_5T = get_line(b1_candle_lastidx, high_5T)\n",
    "        # b1_low_5T = get_line(b1_candle_lastidx, low_5T)\n",
    "\n",
    "        # short_open_res *= (b3_bb_lower_ > b3_close) & (b3_close > b3_bb_lower2_)\n",
    "        # short_open_res *= (b2_bb_lower2_ > b2_close) & (b2_close > b2_bb_lower3_)\n",
    "        # short_open_res *= (b1_bb_lower_ > b1_close) & (b1_close > b1_bb_lower2_)\n",
    "\n",
    "        # long_open_res *= (b3_bb_upper_ < b3_close) & (b3_close < b3_bb_upper2_)\n",
    "        # long_open_res *= (b2_bb_upper2_ < b2_close) & (b2_close < b2_bb_upper3_)\n",
    "        # long_open_res *= (b1_bb_upper_ < b1_close) & (b1_close < b1_bb_upper2_)\n",
    "\n",
    "        # # short_open_res *= (b2_low_5T < b1_low_5T) & (b2_close < b1_close)\n",
    "        # # long_open_res *= (b2_high_5T > b1_high_5T) & (b2_close > b1_close)\n",
    "\n",
    "        # if show_detail:\n",
    "        #   sys_log.warning(\"candle_pattern\")\n",
    "        #   sys_log.warning(\"np.sum(short_open_res == 1) : {}\".format(np.sum(short_open_res == 1)))\n",
    "        #   sys_log.warning(\"np.sum(long_open_res == 1) : {}\".format(np.sum(long_open_res == 1))) \n",
    "\n",
    "\n",
    "        # ------ low_confirm ------ #\n",
    "        # short_open_res *= b1_high_5T > wave_high_fill_\n",
    "        # long_open_res *= b1_low_5T < wave_low_fill_\n",
    "\n",
    "        # if show_detail:\n",
    "        #   sys_log.warning(\"low_confirm\")\n",
    "        #   sys_log.warning(\"np.sum(short_open_res == 1) : {}\".format(np.sum(short_open_res == 1)))\n",
    "        #   sys_log.warning(\"np.sum(long_open_res == 1) : {}\".format(np.sum(long_open_res == 1))) \n",
    "        \n",
    "        # ------ candle_ratio ------ #\n",
    "        # b2_candle_range_5T = b2_high_5T - b2_low_5T\n",
    "        # b1_candle_range_5T = b1_high_5T - b1_low_5T\n",
    "        \n",
    "        # short_open_res *= b1_candle_range_5T / b2_candle_range_5T < config.loc_set.point.crr\n",
    "        # long_open_res *= b1_candle_range_5T / b2_candle_range_5T < config.loc_set.point.crr\n",
    "\n",
    "        # if show_detail:\n",
    "        #   sys_log.warning(\"candle_ratio\")\n",
    "        #   sys_log.warning(\"np.sum(short_open_res == 1) : {}\".format(np.sum(short_open_res == 1)))\n",
    "        #   sys_log.warning(\"np.sum(long_open_res == 1) : {}\".format(np.sum(long_open_res == 1))) \n",
    "\n",
    "        # ------ wick_ratio ------ #\n",
    "        # upper_wick_ratio_ = res_df['upper_wick_ratio_{}'.format(config.loc_set.point.wick_itv)].to_numpy()\n",
    "        # lower_wick_ratio_ = res_df['lower_wick_ratio_{}'.format(config.loc_set.point.wick_itv)].to_numpy()\n",
    "        # b2_upper_wick_ratio_ = get_line(b2_candle_lastidx, upper_wick_ratio_)\n",
    "        # b2_lower_wick_ratio_ = get_line(b2_candle_lastidx, lower_wick_ratio_)\n",
    "\n",
    "        # short_open_res *= b2_upper_wick_ratio_ < config.loc_set.point.short_wick_ratio\n",
    "        # long_open_res *= b2_upper_wick_ratio_ < config.loc_set.point.long_wick_ratio\n",
    "\n",
    "        # if show_detail:\n",
    "        #   sys_log.warning(\"wick_ratio\")\n",
    "        #   sys_log.warning(\"np.sum(short_open_res == 1) : {}\".format(np.sum(short_open_res == 1)))\n",
    "        #   sys_log.warning(\"np.sum(long_open_res == 1) : {}\".format(np.sum(long_open_res == 1))) \n",
    "\n",
    "        # ------ large wave1_range ------ #          \n",
    "        # roll_bb_upper_ = get_roll_wave_data(valid_high_prime_idx, roll_high_idx_arr, len_df, bb_upper_, roll_hl_cnt)\n",
    "        # roll_bb_lower_ = get_roll_wave_data(valid_low_prime_idx, roll_low_idx_arr, len_df, bb_lower_, roll_hl_cnt)\n",
    "        # roll_bb_upper3_ = get_roll_wave_data(valid_high_prime_idx, roll_high_idx_arr, len_df, bb_upper3_, roll_hl_cnt)\n",
    "        # roll_bb_lower3_ = get_roll_wave_data(valid_low_prime_idx, roll_low_idx_arr, len_df, bb_lower3_, roll_hl_cnt)\n",
    "\n",
    "        # short_open_res *= (roll_bb_upper_[:, -2] < roll_high_[:, -2]) & (roll_low_[:, -1] < roll_bb_lower_[:, -1]) & (roll_low_[:, -1] > roll_bb_lower3_[:, -1])\n",
    "        # long_open_res *= (roll_bb_upper_[:, -1] < roll_high_[:, -1]) & (roll_low_[:, -2] < roll_bb_lower_[:, -2]) & (roll_high_[:, -1] < roll_bb_upper3_[:, -1])\n",
    "\n",
    "        # if show_detail:\n",
    "        #   sys_log.warning(\"np.sum(short_open_res == 1) : {}\".format(np.sum(short_open_res == 1)))\n",
    "        #   sys_log.warning(\"np.sum(long_open_res == 1) : {}\".format(np.sum(long_open_res == 1)))\n",
    "        \n",
    "        # ------ low in bb_level3 ------ #  Todo, idx sync 맞춰야할 것\n",
    "        # short_open_res *= (bb_upper2_ < wave_high_fill_) & (wave_high_fill_ < bb_upper3_)\n",
    "        # long_open_res *= (bb_lower3_ < wave_low_fill_) & (wave_low_fill_ < bb_lower2_)\n",
    "\n",
    "        # if show_detail:\n",
    "        #   sys_log.warning(\"np.sum(short_open_res == 1) : {}\".format(np.sum(short_open_res == 1)))\n",
    "        #   sys_log.warning(\"np.sum(long_open_res == 1) : {}\".format(np.sum(long_open_res == 1)))\n",
    "\n",
    "        # ------ first_high ------ #        \n",
    "        # wave_high_prime_idx_fill_= res_df['wave_high_prime_idx_fill_{}{}'.format(itv, period1)].to_numpy()\n",
    "        # wave_low_prime_idx_fill_= res_df['wave_low_prime_idx_fill_{}{}'.format(itv, period1)].to_numpy()\n",
    "        # cu_prime_wave_base = get_line(cu_prime_idx_fill_, wave_base_)\n",
    "        # co_prime_wave_base = get_line(co_prime_idx_fill_, wave_base_)\n",
    "\n",
    "        # # short_open_res *= (co_roll_high_[:, -1] > dc_base_) & (dc_base_ > co_roll_low_[:, -1])\n",
    "        # # long_open_res *= (cu_roll_low_[:, -1] < dc_base_) & (dc_base_ < cu_roll_high_[:, -1])\n",
    "\n",
    "        # short_open_res *= (cu_prime_wave_base > dc_base_) & (dc_base_ > co_prime_wave_base)\n",
    "        # long_open_res *= (co_prime_wave_base < dc_base_) & (dc_base_ < cu_prime_wave_base)\n",
    "\n",
    "        # if show_detail:\n",
    "        #   sys_log.warning(\"np.sum(short_open_res == 1) : {}\".format(np.sum(short_open_res == 1)))\n",
    "        #   sys_log.warning(\"np.sum(long_open_res == 1) : {}\".format(np.sum(long_open_res == 1)))\n",
    "\n",
    "        # cu_prime_dc_base = get_line(cu_prime_idx_fill_, dc_base_)        \n",
    "        # co_prime_dc_base = get_line(co_prime_idx_fill_, dc_base_)\n",
    "        \n",
    "        # shift_size = itv_to_number(p1_itv1)\n",
    "        # b1_dc_base_ = res_df['dc_base_{}{}'.format(p1_itv1, p1_period1)].shift(shift_size).to_numpy()\n",
    "\n",
    "        # short_open_res *= (b1_dc_base_ < dc_base_)\n",
    "        # long_open_res *= (b1_dc_base_ > dc_base_)\n",
    "\n",
    "        # if show_detail:\n",
    "        #   sys_log.warning(\"np.sum(short_open_res == 1) : {}\".format(np.sum(short_open_res == 1)))\n",
    "        #   sys_log.warning(\"np.sum(long_open_res == 1) : {}\".format(np.sum(long_open_res == 1)))\n",
    "\n",
    "        # ------ b1_base_15T < wave_high ------ #\n",
    "        # short_open_res *= (b1_dc_base_ > co_roll_low_[:, -1])\n",
    "        # long_open_res *= (b1_dc_base_ < cu_roll_high_[:, -1])\n",
    "\n",
    "        # if show_detail:\n",
    "        #   sys_log.warning(\"np.sum(short_open_res == 1) : {}\".format(np.sum(short_open_res == 1)))\n",
    "        #   sys_log.warning(\"np.sum(long_open_res == 1) : {}\".format(np.sum(long_open_res == 1)))   \n",
    "        \n",
    "        \n",
    "    # b1_itv_num = itv_to_number(config.tr_set.p2_itv0)\n",
    "    # b2_itv_num = itv_to_number(config.tr_set.p2_itv0) * 2  # multi 2 for imb_v2\n",
    "\n",
    "    # res_df[short_tp_1_] = res_df['dc_lower_{}{}'.format(itv, period1)]\n",
    "    # res_df[short_tp_0_] = res_df['dc_upper_{}{}'.format(itv, period2)]\n",
    "    # # res_df[short_tp_0_] = res_df['dc_upper_15T4']\n",
    "    # res_df[long_tp_1_] = res_df['dc_upper_{}{}'.format(itv, period1)]\n",
    "    # res_df[long_tp_0_] = res_df['dc_lower_{}{}'.format(itv, period2)]\n",
    "    # # res_df[long_tp_0_] = res_df['dc_lower_15T4']\n",
    "    \n",
    "        # ------ base_cc ------ #\n",
    "        dc_base_ = res_df['dc_base_{}{}'.format(p1_itv1, p1_period1)].to_numpy()\n",
    "\n",
    "        # close_ = res_df['close_{}'.format(config.loc_set.point.tf_entry)].to_numpy()\n",
    "        # b1_dc_base_ = res_df['dc_base_{}{}'.format(p1_itv1, p1_period1)].shift(tf_entry).to_numpy()\n",
    "        # b1_close_ = res_df['close_{}'.format(config.loc_set.point.tf_entry)].shift(tf_entry).to_numpy()\n",
    "\n",
    "        # short_open_res *= ((b1_close_ > dc_base_) & (dc_base_ > close_)) | ((b1_close_ > b1_dc_base_) & (dc_base_ > close_))\n",
    "        # long_open_res *= ((b1_close_ < dc_base_) & (dc_base_ < close_)) | ((b1_close_ < b1_dc_base_) & (dc_base_ < close_))\n",
    "\n",
    "        # if show_detail:\n",
    "        #   sys_log.warning(\"np.sum(short_open_res == 1) : {}\".format(np.sum(short_open_res == 1)))\n",
    "        #   sys_log.warning(\"np.sum(long_open_res == 1) : {}\".format(np.sum(long_open_res == 1)))\n",
    "        \n",
    "        # ------ tf_entry ------ #\n",
    "        # tf_entry = itv_to_number(config.loc_set.point.tf_entry)\n",
    "        \n",
    "        # short_open_res *= np_timeidx % tf_entry == (tf_entry - 1)\n",
    "        # long_open_res *= np_timeidx % tf_entry == (tf_entry - 1) \n",
    "\n",
    "        # if show_detail:\n",
    "        #   sys_log.warning(\"np.sum(short_open_res == 1) : {}\".format(np.sum(short_open_res == 1)))\n",
    "        #   sys_log.warning(\"np.sum(long_open_res == 1) : {}\".format(np.sum(long_open_res == 1)))\n",
    "\n",
    "    \n",
    "        # ------ cppr 로 양음봉 check ------ # \n",
    "        # tf_entry = itv_to_number(config.loc_set.point.tf_entry)\n",
    "\n",
    "        # res_df['b1_cppr_{}'.format(config.loc_set.point.tf_entry)] = res_df['cppr_{}'.format(config.loc_set.point.tf_entry)].shift(tf_entry)\n",
    "        # b1_cppr_ = res_df['b1_cppr_{}'.format(config.loc_set.point.tf_entry)].to_numpy()  # check b1's cppr in ep_loc\n",
    "        # cppr_ = res_df['cppr_{}'.format(config.loc_set.point.tf_entry)].to_numpy()\n",
    "        # short_open_res *= (b1_cppr_ > 0) & (cppr_ < 0)\n",
    "        # long_open_res *= (b1_cppr_ < 0) & (cppr_ > 0)\n",
    "\n",
    "        # res_df['b1_updbr'] = res_df['dc_upper_15T4_br'].shift(tf_entry).to_numpy()\n",
    "        # res_df['b1_lwdbr'] = res_df['dc_lower_15T4_br'].shift(tf_entry).to_numpy()\n",
    "        \n",
    "        # res_df['b1_updbr_cppr'] = b1_cppr_ * res_df['b1_updbr'].to_numpy()\n",
    "        # res_df['b1_lwdbr_cppr'] = b1_cppr_ * res_df['b1_lwdbr'].to_numpy()\n",
    "\n",
    "        # if show_detail:\n",
    "        #   sys_log.warning(\"np.sum(short_open_res == 1) : {}\".format(np.sum(short_open_res == 1)))\n",
    "        #   sys_log.warning(\"np.sum(long_open_res == 1) : {}\".format(np.sum(long_open_res == 1)))\n",
    "\n",
    "        # ------ dc_cross ------ #\n",
    "        # b1_dc_upper_15T4 = res_df['dc_upper_15T4'].shift(tf_entry).to_numpy()\n",
    "        # b1_dc_lower_15T4 = res_df['dc_lower_15T4'].shift(tf_entry).to_numpy()\n",
    "        # b1_high_ = res_df['high_{}'.format(config.loc_set.point.tf_entry)].shift(tf_entry).to_numpy()\n",
    "        # b1_low_ = res_df['low_{}'.format(config.loc_set.point.tf_entry)].shift(tf_entry).to_numpy()\n",
    "        # short_open_res *= b1_high_ > b1_dc_upper_15T4\n",
    "        # long_open_res *= b1_low_ < b1_dc_lower_15T4\n",
    "        \n",
    "        # ------ dc_cc ------ #\n",
    "        # b1_close_ = res_df['close_{}'.format(config.loc_set.point.tf_entry)].shift(tf_entry).to_numpy()\n",
    "        # short_open_res *= b1_close_ > b1_dc_upper_15T4\n",
    "        # long_open_res *= b1_close_ < b1_dc_lower_15T4\n",
    "\n",
    "        # if show_detail:\n",
    "        #   sys_log.warning(\"np.sum(short_open_res == 1) : {}\".format(np.sum(short_open_res == 1)))\n",
    "        #   sys_log.warning(\"np.sum(long_open_res == 1) : {}\".format(np.sum(long_open_res == 1)))\n",
    "\n",
    "        # ------ empty_space ------ #       \n",
    "        # dc_upper_15T4 = res_df['dc_upper_15T4'].to_numpy()\n",
    "        # dc_lower_15T4 = res_df['dc_lower_15T4'].to_numpy() \n",
    "        # high_ = res_df['high_{}'.format(config.loc_set.point.tf_entry)].to_numpy()\n",
    "        # low_ = res_df['low_{}'.format(config.loc_set.point.tf_entry)].to_numpy()\n",
    "        # short_open_res *= high_ < dc_upper_15T4\n",
    "        # long_open_res *= low_ > dc_lower_15T4\n",
    "        \n",
    "        # ------ candle_pattern ------ #  \n",
    "        # pattern_column = \"{}_{}\".format(config.loc_set.point.candle_pattern, config.loc_set.point.tf_entry)\n",
    "        # short_open_res *= res_df[pattern_column].to_numpy() < 0\n",
    "        # long_open_res *= res_df[pattern_column].to_numpy() > 0\n",
    "\n",
    "        # if show_detail:\n",
    "        #   sys_log.warning(\"np.sum(short_open_res == 1) : {}\".format(np.sum(short_open_res == 1)))\n",
    "        #   sys_log.warning(\"np.sum(long_open_res == 1) : {}\".format(np.sum(long_open_res == 1)))\n",
    "\n",
    "\n",
    "        # ------ lower_touch > upper_touch (long) ------ #\n",
    "        # short_open_res *= res_df['short_upper_touch_idx_{}{}{}'.format(p1_itv1, p1_period1, p1_period2)].to_numpy() > res_df['short_lower_touch_idx_{}{}{}'.format(p1_itv1, p1_period1, p1_period2)].to_numpy()\n",
    "        # long_open_res *= res_df['long_lower_touch_idx_{}{}{}'.format(p1_itv1, p1_period1, p1_period2)].to_numpy() > res_df['long_upper_touch_idx_{}{}{}'.format(p1_itv1, p1_period1, p1_period2)].to_numpy()\n",
    "\n",
    "        # ------ base_3T cross ------ #\n",
    "        # dc_base_3T = res_df['dc_base_3T'].to_numpy()\n",
    "        # b1_close = res_df['close'].shift(1).to_numpy()\n",
    "        # short_open_res *= (b1_close > dc_base_3T) & (dc_base_3T > close)\n",
    "        # long_open_res *= (b1_close < dc_base_3T) & (dc_base_3T < close)\n",
    "\n",
    "        # ------ wave_low < base_5T (long) ------ #\n",
    "        # dc_base_5T = res_df['dc_base_5T'].to_numpy()        \n",
    "        # short_open_res *= res_df['short_upper_touch_line_{}{}{}'.format(p1_itv1, p1_period1, p1_period2)].to_numpy() > dc_base_5T\n",
    "        # long_open_res *= res_df['long_lower_touch_line_{}{}{}'.format(p1_itv1, p1_period1, p1_period2)].to_numpy() < dc_base_5T      \n",
    "\n",
    "        # ------ ppr ------ #\n",
    "        # pumping_ratio(res_df, config, p1_itv1, p1_period1, p1_period2)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aTV4h3LjTZBp",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### olds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dc49JPmoTaPQ"
   },
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "    csd_period = 40\n",
    "    res_df = dc_line_v4(res_df, res_df, dc_period=csd_period)\n",
    "    \n",
    "    dc_upper_ = res_df['dc_upper_T{}'.format(csd_period)].to_numpy()    \n",
    "    dc_lower_ = res_df['dc_lower_T{}'.format(csd_period)].to_numpy()  \n",
    "\n",
    "    short_open_res2 *= dc_upper_touch_span == 0\n",
    "    long_open_res2 *= dc_lower_touch_span == 0\n",
    "\n",
    "    # ------ csdbox ------ # --> 결국 dc's upper & lower\n",
    "    if show_detail:\n",
    "      sys_log.warning(\"csdbox\")\n",
    "      # sys_log.warning(\"np.sum(short_open_res1 == 1) : {}\".format(np.sum(short_open_res1 == 1)))\n",
    "      # sys_log.warning(\"np.sum(long_open_res1 == 1) : {}\".format(np.sum(long_open_res1 == 1)))\n",
    "      sys_log.warning(\"np.sum(short_open_res2 == 1) : {}\".format(np.sum(short_open_res2 == 1)))\n",
    "      sys_log.warning(\"np.sum(long_open_res2 == 1) : {}\".format(np.sum(long_open_res2 == 1)))\n",
    "      \n",
    "    # olds,\n",
    "    # np.nan - np.nan = np.nan -> vectorize 가능할 것 => xx\n",
    "    # 1. wave_high_prime_idx_ ~ long_open_idx1 의 valid(not_non) 한 idx 를 max_dc_lower 와 min_low 의 비교값으로 채워넣음\n",
    "    # valid_idx = ~(pd.isnull(wave_cu_post_idx_fill_) | pd.isnull(long_open_idx1))\n",
    "    # max_dc_lower_ = [dc_lower_[int(iin):int(iout)].max() for iin, iout in zip(wave_cu_post_idx_fill_, long_open_idx1) if not pd.isnull(iin) if not pd.isnull(iout)]\n",
    "    # min_low = [low[int(iin):int(iout)].min() for iin, iout in zip(wave_high_prime_idx_, long_open_idx1) if not pd.isnull(iin) if not pd.isnull(iout)]\n",
    "    \n",
    "    short_tp_1_, long_tp_1_ = 'short_tp_1_{}'.format(selection_id), 'long_tp_1_{}'.format(selection_id)\n",
    "    short_tp_0_, long_tp_0_ = 'short_tp_0_{}'.format(selection_id), 'long_tp_0_{}'.format(selection_id)\n",
    "    short_tp_gap_, long_tp_gap_ = 'short_tp_gap_{}'.format(selection_id), 'long_tp_gap_{}'.format(selection_id)\n",
    "\n",
    "    short_ep_1_, long_ep_1_ = 'short_ep_1_{}'.format(selection_id), 'long_ep_1_{}'.format(selection_id)\n",
    "    short_ep_0_, long_ep_0_ = 'short_ep_0_{}'.format(selection_id), 'long_ep_0_{}'.format(selection_id)\n",
    "    short_ep_gap_, long_ep_gap_ = 'short_ep_gap_{}'.format(selection_id), 'long_ep_gap_{}'.format(selection_id)\n",
    "\n",
    "    short_epout_1_, long_epout_1_ = 'short_epout_1_{}'.format(selection_id), 'long_epout_1_{}'.format(selection_id)\n",
    "    short_epout_0_, long_epout_0_ = 'short_epout_0_{}'.format(selection_id), 'long_epout_0_{}'.format(selection_id)\n",
    "    short_epout_gap_, long_epout_gap_ = 'short_epout_gap_{}'.format(selection_id), 'long_epout_gap_{}'.format(selection_id)\n",
    "    \n",
    "    # ================== convert unit -> numpy ================== #   \n",
    "    # tp_cols = [short_tp_1_, short_tp_0_, short_tp_gap_, long_tp_1_, long_tp_0_, long_tp_gap_]\n",
    "    # epout_cols = [short_epout_1_, short_epout_0_, short_epout_gap_, long_epout_1_, long_epout_0_, long_epout_gap_]\n",
    "\n",
    "    # short_tp_1, short_tp_0, short_tp_gap, long_tp_1, long_tp_0, long_tp_gap = [res_df[col_].to_numpy() for col_ in tp_cols]\n",
    "    # short_epout_1, short_epout_0, short_epout_gap, long_epout_1, long_epout_0, long_epout_gap = [res_df[col_].to_numpy() for col_ in epout_cols]\n",
    "\n",
    "    if p2_itv1 != \"None\":  # vectorized point2\n",
    "        short_point1_on2_idx = pd.Series(\n",
    "            np.where(res_df['short_wave_point_{}{}{}'.format(p1_itv1, p1_period1, p1_period2)], len_df_range, np.nan)).rolling(point1_to2_period,\n",
    "                                                                                                                                    min_periods=1).max().to_numpy()  # period 내의 max_point1_idx\n",
    "        long_point1_on2_idx = pd.Series(\n",
    "            np.where(res_df['long_wave_point_{}{}{}'.format(p1_itv1, p1_period1, p1_period2)], len_df_range, np.nan)).rolling(point1_to2_period,\n",
    "                                                                                                                                   min_periods=1).max().to_numpy()\n",
    "\n",
    "        short_point2_idx = pd.Series(\n",
    "            np.where(res_df['short_wave_point_{}{}{}'.format(p2_itv1, p2_period1, p2_period2)], len_df_range, np.nan)).to_numpy()\n",
    "        long_point2_idx = pd.Series(\n",
    "            np.where(res_df['long_wave_point_{}{}{}'.format(p2_itv1, p2_period1, p2_period2)], len_df_range, np.nan)).to_numpy()\n",
    "\n",
    "        res_df['short_point_idxgap_{}'.format(selection_id)] = short_point2_idx - short_point1_on2_idx\n",
    "        res_df['long_point_idxgap_{}'.format(selection_id)] = long_point2_idx - long_point1_on2_idx\n",
    "\n",
    "        # ------ p1 & p2 ------ #\n",
    "        short_open_res *= ~np.isnan(res_df['short_point_idxgap_{}'.format(selection_id)].to_numpy())\n",
    "        long_open_res *= ~np.isnan(res_df['long_point_idxgap_{}'.format(selection_id)].to_numpy())\n",
    "\n",
    "        if show_detail:\n",
    "          sys_log.warning(\"np.sum(short_open_res == 1) : {}\".format(np.sum(short_open_res == 1)))\n",
    "          sys_log.warning(\"np.sum(long_open_res == 1) : {}\".format(np.sum(long_open_res == 1)))\n",
    "\n",
    "        # ------ p2 amax > p1_idx (long) ------ #\n",
    "        short_open_res *= res_df['short_upper_touch_idx_{}{}{}'.format(p2_itv1, p2_period1, p2_period2)].to_numpy() > short_point1_on2_idx\n",
    "        long_open_res *= res_df['long_lower_touch_idx_{}{}{}'.format(p2_itv1, p2_period1, p2_period2)].to_numpy() > long_point1_on2_idx\n",
    "\n",
    "        if show_detail:\n",
    "          sys_log.warning(\"np.sum(short_open_res == 1) : {}\".format(np.sum(short_open_res == 1)))\n",
    "          sys_log.warning(\"np.sum(long_open_res == 1) : {}\".format(np.sum(long_open_res == 1)))\n",
    "\n",
    "        # ------ higher low (long) ------ #\n",
    "        # short_a_line1_on2_ = get_line(short_point1_on2_idx, res_df['short_a_line_{}{}{}'.format(p1_itv1, p1_period1, p1_period2)].to_numpy())\n",
    "        # long_a_line1_on2_ = get_line(long_point1_on2_idx, res_df['long_a_line_{}{}{}'.format(p1_itv1, p1_period1, p1_period2)].to_numpy())\n",
    "\n",
    "        # short_a_line2_ = res_df['short_a_line_{}{}{}'.format(p2_itv1, p2_period1, p2_period2)].to_numpy()\n",
    "        # long_a_line2_ = res_df['long_a_line_{}{}{}'.format(p2_itv1, p2_period1, p2_period2)].to_numpy()\n",
    "\n",
    "        # short_open_res *= short_a_line1_on2_ >= short_a_line2_\n",
    "        # long_open_res *= long_a_line1_on2_ <= long_a_line2_\n",
    "\n",
    "        # print(np.sum(long_open_res == 1))\n",
    "\n",
    "        # ------ higher high (long) ------ #\n",
    "        # short_open_res *= co_roll_low_[:, -2] > co_roll_low_[:, -1]\n",
    "        # long_open_res *= cu_roll_high_[:, -2] < cu_roll_high_[:, -1]\n",
    "\n",
    "        # short_open_res *= co_roll_low_[:, -3] > co_roll_low_[:, -2]\n",
    "        # long_open_res *= cu_roll_high_[:, -3] < cu_roll_high_[:, -2]\n",
    "\n",
    "        # if show_detail:\n",
    "        #   sys_log.warning(\"np.sum(short_open_res == 1) : {}\".format(np.sum(short_open_res == 1)))\n",
    "        #   sys_log.warning(\"np.sum(long_open_res == 1) : {}\".format(np.sum(long_open_res == 1)))\n",
    "\n",
    "        # ------ higher low (long) ------ # \n",
    "        # short_open_res *= co_roll_high_[:, -2] > co_roll_high_[:, -1]\n",
    "        # long_open_res *= cu_roll_low_[:, -2] < cu_roll_low_[:, -1]\n",
    "\n",
    "        # if show_detail:\n",
    "        #   sys_log.warning(\"np.sum(short_open_res == 1) : {}\".format(np.sum(short_open_res == 1)))\n",
    "        #   sys_log.warning(\"np.sum(long_open_res == 1) : {}\".format(np.sum(long_open_res == 1)))  \n",
    "\n",
    "        # ------ tf2_base < wave_low ------ #\n",
    "        # short_open_res *= (dc_base_ > co_roll_high_[:, -1])\n",
    "        # long_open_res *= (dc_base_ < cu_roll_low_[:, -1])\n",
    "\n",
    "        # if show_detail:\n",
    "        #   sys_log.warning(\"np.sum(short_open_res == 1) : {}\".format(np.sum(short_open_res == 1)))\n",
    "        #   sys_log.warning(\"np.sum(long_open_res == 1) : {}\".format(np.sum(long_open_res == 1)))  \n",
    "\n",
    "        # short_open_res *= (cu_prime_wave_base > cu_prime_dc_base) & (dc_base_ > co_prime_wave_base)\n",
    "        # long_open_res *= (co_prime_wave_base < co_prime_dc_base) & (dc_base_ < cu_prime_wave_base)\n",
    "\n",
    "        short_open_res *= (co_prime_dc_base < co_roll_low_[:, -2])\n",
    "        long_open_res *= (cu_prime_dc_base > cu_roll_high_[:, -2])  #  b1_cu_prime_idx’s tf2_base > b1_high -> b1 이 아님 (error)\n",
    "\n",
    "\n",
    "        res_df[short_epout_1_] = res_df['short_wave_low_{}{}{}'.format(itv, period1, period2)]\n",
    "        res_df[short_epout_0_] = res_df['short_new_wave_high_{}{}{}'.format(itv, period1, period2)]\n",
    "        res_df[long_epout_1_] = res_df['long_wave_high_{}{}{}'.format(itv, period1, period2)]\n",
    "        res_df[long_epout_0_] = res_df['long_new_wave_low_{}{}{}'.format(itv, period1, period2)]\n",
    "        \n",
    "        # ------ get candle_lastidx ------ #        \n",
    "        tf_entry = itv_to_number(config.loc_set.point.tf_entry)\n",
    "        b1_candle_shift = np_timeidx % tf_entry + 1\n",
    "        b2_candle_shift = b1_candle_shift + tf_entry\n",
    "        b3_candle_shift = b1_candle_shift + 2 * tf_entry\n",
    "\n",
    "        print(b3_candle_shift)\n",
    "\n",
    "        # bb_upper_ = res_df['bb_upper_{}{}'.format('T', 60)].to_numpy()\n",
    "        # bb_lower_ = res_df['bb_lower_{}{}'.format('T', 60)].to_numpy()\n",
    "        # bb_upper2_ = res_df['bb_upper2_{}{}'.format('T', 60)].to_numpy()\n",
    "        # bb_lower2_ = res_df['bb_lower2_{}{}'.format('T', 60)].to_numpy()\n",
    "        # bb_upper3_ = res_df['bb_upper3_{}{}'.format('T', 60)].to_numpy()\n",
    "        # bb_lower3_ = res_df['bb_lower3_{}{}'.format('T', 60)].to_numpy()\n",
    "\n",
    "        # ------ compare by back_idx  ------ #   \n",
    "        b3_bb_upper_ = res_df['bb_upper_{}{}'.format('T', 60)].shift(b3_candle_shift).to_numpy()\n",
    "        b3_bb_lower_ = res_df['bb_lower_{}{}'.format('T', 60)].shift(b3_candle_shift).to_numpy()\n",
    "        b3_bb_upper2_ = res_df['bb_upper2_{}{}'.format('T', 60)].shift(b3_candle_shift).to_numpy()\n",
    "        b3_bb_lower2_ = res_df['bb_lower2_{}{}'.format('T', 60)].shift(b3_candle_shift).to_numpy()\n",
    "        b3_close = res_df['close'].shift(b3_candle_shift).to_numpy()\n",
    "\n",
    "        b2_bb_upper2_ = res_df['bb_upper2_{}{}'.format('T', 60)].shift(b2_candle_shift).to_numpy()\n",
    "        b2_bb_lower2_ = res_df['bb_lower2_{}{}'.format('T', 60)].shift(b2_candle_shift).to_numpy()\n",
    "        b2_bb_upper3_ = res_df['bb_upper3_{}{}'.format('T', 60)].shift(b2_candle_shift).to_numpy()\n",
    "        b2_bb_lower3_ = res_df['bb_lower3_{}{}'.format('T', 60)].shift(b2_candle_shift).to_numpy()\n",
    "        b2_close = res_df['close'].shift(b2_candle_shift).to_numpy()\n",
    "\n",
    "        b1_bb_upper_ = res_df['bb_upper_{}{}'.format('T', 60)].shift(b1_candle_shift).to_numpy()\n",
    "        b1_bb_lower_ = res_df['bb_lower_{}{}'.format('T', 60)].shift(b1_candle_shift).to_numpy()\n",
    "        b1_bb_upper2_ = res_df['bb_upper2_{}{}'.format('T', 60)].shift(b1_candle_shift).to_numpy()\n",
    "        b1_bb_lower2_ = res_df['bb_lower2_{}{}'.format('T', 60)].shift(b1_candle_shift).to_numpy()\n",
    "        b1_close = res_df['close'].shift(b1_candle_shift).to_numpy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HKdUKKl-483N",
    "tags": []
   },
   "source": [
    "### public paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 358,
     "status": "ok",
     "timestamp": 1666567971345,
     "user": {
      "displayName": "7th June",
      "userId": "08178289703395036410"
     },
     "user_tz": -540
    },
    "id": "nzOYWA2kqZ0d"
   },
   "outputs": [],
   "source": [
    "from funcs.public.indicator import *\n",
    "from funcs.public.broker import *\n",
    "from funcs.public.constant import *\n",
    "import logging\n",
    "from ast import literal_eval\n",
    "\n",
    "pd.set_option('mode.chained_assignment', None)\n",
    "sys_log = logging.getLogger()\n",
    "\n",
    "\n",
    "def lvrg_liqd_set_v4(res_df, config, open_side, ep_, out_, fee, limit_leverage=50):\n",
    "\n",
    "    \"\"\"\n",
    "    v2_1 --> v4\n",
    "        1. set for DS_ML version.\n",
    "\n",
    "    \"\"\"\n",
    "    selection_id = config.selection_id\n",
    "    leverage = config.lvrg_set.leverage\n",
    "    loss = None\n",
    "    \n",
    "#     if not pd.isnull(out_):\n",
    "#         if not config.lvrg_set.static_lvrg_short:\n",
    "#         # 이 phase 가 정석, 윗 phase 는 결과가 수익 극대화라 사용함\n",
    "#             if open_side == OrderSide.SELL:\n",
    "#                 loss = ep_ / out_\n",
    "            \n",
    "#         if not config.lvrg_set.static_lvrg_long:\n",
    "#             if open_side == OrderSide.BUY:\n",
    "#                 loss = out_ / ep_\n",
    "\n",
    "#         if loss is not None:\n",
    "#             leverage = config.lvrg_set.target_pct / abs(loss - 1 - (fee + config.trader_set.market_fee))\n",
    "\n",
    "    # 감당하기 힘든 fluc. 의 경우 진입하지 않음 - dynamic_lvrg 사용 경우\n",
    "    # if leverage < 1 and config.lvrg_set.lvrg_rejection:\n",
    "    #     # if config.lvrg_set.leverage >= 1 and config.lvrg_set.lvrg_rejection:\n",
    "    #     return None, None\n",
    "\n",
    "    if not config.lvrg_set.allow_float:\n",
    "        leverage = int(leverage)\n",
    "\n",
    "    leverage = min(limit_leverage, max(leverage, 1))\n",
    "\n",
    "    if open_side == OrderSide.SELL:\n",
    "        liqd_p = ep_ / (1 + fee + config.trader_set.market_fee - 1 / leverage)\n",
    "    else:\n",
    "        liqd_p = ep_ * (1 + fee + config.trader_set.market_fee - 1 / leverage)\n",
    "\n",
    "    return leverage, liqd_p\n",
    "\n",
    "\n",
    "def lvrg_liqd_set_v2_1(res_df, config, open_side, ep_, out_, fee, limit_leverage=50):\n",
    "\n",
    "    \"\"\"\n",
    "    v2 --> v2_1\n",
    "        1. math.ceil leverage.\n",
    "\n",
    "    \"\"\"\n",
    "    selection_id = config.selection_id\n",
    "    leverage = config.lvrg_set.leverage\n",
    "    loss = None\n",
    "\n",
    "    if not pd.isnull(out_):\n",
    "        if not config.lvrg_set.static_lvrg_short:\n",
    "            # 이 phase 가 정석, 윗 phase 는 결과가 수익 극대화라 사용함\n",
    "            if open_side == OrderSide.SELL:\n",
    "                loss = ep_ / out_\n",
    "\n",
    "        if not config.lvrg_set.static_lvrg_long:\n",
    "            if open_side == OrderSide.BUY:\n",
    "                loss = out_ / ep_\n",
    "\n",
    "        if loss is not None:\n",
    "            leverage = config.lvrg_set.target_pct / abs(loss - 1 - (fee + config.trader_set.market_fee))\n",
    "\n",
    "    # ------------ leverage rejection ------------ #\n",
    "    # 감당하기 힘든 fluc. 의 경우 진입하지 않음 - dynamic_lvrg 사용 경우\n",
    "    if leverage < 1 and config.lvrg_set.lvrg_rejection:\n",
    "        # if config.lvrg_set.leverage >= 1 and config.lvrg_set.lvrg_rejection:\n",
    "        return None, None\n",
    "\n",
    "    if not config.lvrg_set.allow_float:\n",
    "        leverage = math.ceil(leverage)\n",
    "\n",
    "    leverage = min(limit_leverage, max(leverage, 1))\n",
    "\n",
    "    if open_side == OrderSide.SELL:\n",
    "        liqd_p = ep_ / (1 + fee + config.trader_set.market_fee - 1 / leverage)\n",
    "    else:\n",
    "        liqd_p = ep_ * (1 + fee + config.trader_set.market_fee - 1 / leverage)\n",
    "\n",
    "    return leverage, liqd_p\n",
    "\n",
    "\n",
    "def lvrg_liqd_set_v2(res_df, config, open_side, ep_, out_, fee, limit_leverage=50):\n",
    "    \n",
    "    selection_id = config.selection_id\n",
    "    leverage = config.lvrg_set.leverage\n",
    "    loss = None\n",
    "    \n",
    "    if not pd.isnull(out_):\n",
    "        if not config.lvrg_set.static_lvrg_short:\n",
    "        # 이 phase 가 정석, 윗 phase 는 결과가 수익 극대화라 사용함\n",
    "            if open_side == OrderSide.SELL:\n",
    "                loss = ep_ / out_\n",
    "            \n",
    "        if not config.lvrg_set.static_lvrg_long:\n",
    "            if open_side == OrderSide.BUY:\n",
    "                loss = out_ / ep_\n",
    "\n",
    "        if loss is not None:\n",
    "            leverage = config.lvrg_set.target_pct / abs(loss - 1 - (fee + config.trader_set.market_fee))\n",
    "\n",
    "    # ------------ leverage rejection ------------ #\n",
    "    # 감당하기 힘든 fluc. 의 경우 진입하지 않음 - dynamic_lvrg 사용 경우\n",
    "    if leverage < 1 and config.lvrg_set.lvrg_rejection:\n",
    "        # if config.lvrg_set.leverage >= 1 and config.lvrg_set.lvrg_rejection:\n",
    "        return None, None\n",
    "\n",
    "    if not config.lvrg_set.allow_float:\n",
    "        leverage = int(leverage)\n",
    "\n",
    "    leverage = min(limit_leverage, max(leverage, 1))\n",
    "\n",
    "    if open_side == OrderSide.SELL:\n",
    "        liqd_p = ep_ / (1 + fee + config.trader_set.market_fee - 1 / leverage)\n",
    "    else:\n",
    "        liqd_p = ep_ * (1 + fee + config.trader_set.market_fee - 1 / leverage)\n",
    "\n",
    "    return leverage, liqd_p\n",
    "\n",
    "\n",
    "def lvrg_liqd_set(res_df, config, open_side, ep_, out_, fee, limit_leverage=50):\n",
    "    selection_id = config.selection_id\n",
    "    leverage = config.lvrg_set.leverage\n",
    "\n",
    "    if not pd.isnull(out_) and not config.lvrg_set.static_lvrg:\n",
    "        # 이 phase 가 정석, 윗 phase 는 결과가 수익 극대화라 사용함\n",
    "        if open_side == OrderSide.SELL:\n",
    "            loss = ep_ / out_\n",
    "        else:\n",
    "            loss = out_ / ep_\n",
    "\n",
    "        leverage = config.lvrg_set.target_pct / abs(loss - 1 - (fee + config.trader_set.market_fee))\n",
    "\n",
    "    # ------------ leverage rejection ------------ #\n",
    "    # 감당하기 힘든 fluc. 의 경우 진입하지 않음 - dynamic_lvrg 사용 경우\n",
    "    if leverage < 1 and config.lvrg_set.lvrg_rejection:\n",
    "        # if config.lvrg_set.leverage >= 1 and config.lvrg_set.lvrg_rejection:\n",
    "        return None, None\n",
    "\n",
    "    if not config.lvrg_set.allow_float:\n",
    "        leverage = int(leverage)\n",
    "\n",
    "    leverage = min(limit_leverage, max(leverage, 1))\n",
    "\n",
    "    if open_side == OrderSide.SELL:\n",
    "        liqd_p = ep_ / (1 + fee + config.trader_set.market_fee - 1 / leverage)\n",
    "    else:\n",
    "        liqd_p = ep_ * (1 + fee + config.trader_set.market_fee - 1 / leverage)\n",
    "\n",
    "    return leverage, liqd_p\n",
    "\n",
    "\n",
    "def sync_check(df_, config, mode=\"OPEN\", row_slice=True):\n",
    "    try:\n",
    "        make_itv_list = [m_itv.replace('m', 'T') for m_itv in literal_eval(config.trader_set.itv_list)]\n",
    "        row_list = literal_eval(config.trader_set.row_list)\n",
    "        rec_row_list = literal_eval(config.trader_set.rec_row_list)\n",
    "        offset_list = literal_eval(config.trader_set.offset_list)\n",
    "\n",
    "        assert len(make_itv_list) == len(offset_list), \"length of itv & offset_list should be equal\"\n",
    "        htf_df_list = [to_htf(df_, itv=itv_, offset=offset_) for itv_idx, (itv_, offset_)\n",
    "                       in enumerate(zip(make_itv_list, offset_list)) if itv_idx != 0]  #\n",
    "        htf_df_list.insert(0, df_)\n",
    "\n",
    "        # for htf_df_ in htf_df_list:\n",
    "        #     print(htf_df_.tail())\n",
    "\n",
    "        #       Todo        #\n",
    "        #        1. row_list calc.\n",
    "        #           a. indi. 를 만들기 위한 최소 period 가 존재하고, 그 indi. 를 사용한 lb_period 가 존재함\n",
    "        #           b. => default_period + lb_period\n",
    "        #               i. from sync_check, public_indi, ep_point2, ep_dur 의 tf 별 max lb_period check\n",
    "        #                   1. default_period + max lb_period check\n",
    "        #                       a. 현재까지 lb_period_list\n",
    "        #                           h_prev_idx (open / close) 60\n",
    "        #                           dc_period 135\n",
    "        #                           zone_dc_period 135\n",
    "\n",
    "        # --------- slicing (in trader phase only) --------- #\n",
    "        #               --> latency 영향도가 높은 곳은 이곳\n",
    "        if row_slice:  # recursive 가 아닌 indi. 의 latency 를 고려한 slicing\n",
    "            df, df_3T, df_5T, df_15T, df_30T, df_H, df_4H = [df_s.iloc[-row_list[row_idx]:].copy() for row_idx, df_s in\n",
    "                                                             enumerate(htf_df_list)]\n",
    "            rec_df, rec_df_3T, rec_df_5T, rec_df_15T, rec_df_30T, rec_df_H, rec_df_4H = [\n",
    "                df_s.iloc[-rec_row_list[row_idx]:].copy() for row_idx, df_s\n",
    "                in\n",
    "                enumerate(htf_df_list)]\n",
    "        else:\n",
    "            df, df_3T, df_5T, df_15T, df_30T, df_H, df_4H = htf_df_list\n",
    "            rec_df, rec_df_3T, rec_df_5T, rec_df_15T, rec_df_30T, rec_df_H, rec_df_4H = htf_df_list\n",
    "\n",
    "        # --------- add indi. --------- #\n",
    "\n",
    "        #        1. 필요한 indi. 는 enlist_epouttp & mr_check 보면서 삽입\n",
    "        #        2. min use_rows 계산을 위해서, tf 별로 gathering 함        #\n",
    "        # start_time = time.time()\n",
    "\n",
    "        # ------ T ------ #\n",
    "        # df = dc_line(df, None, 'T', dc_period=20)\n",
    "        # df = bb_line(df, None, 'T')\n",
    "        #\n",
    "        # ------ 3T ------ #\n",
    "        # df = dc_line(df, df_3T, '3T')\n",
    "\n",
    "        # ------ 5T ------ #\n",
    "        # h_candle_v3(df, '5T')\n",
    "        # df = dc_line(df, df_5T, '5T')\n",
    "        # df = bb_line(df, df_5T, '5T')\n",
    "        #\n",
    "        # ------ 15T ------ #\n",
    "        # h_candle_v3(df, '15T')\n",
    "        # df = dc_line(df, df_15T, '15T')\n",
    "        # df = bb_line(df, df_15T, '15T')\n",
    "        #\n",
    "        # ------ 30T ------ #\n",
    "        # df = bb_line(df, df_30T, '30T')\n",
    "        #\n",
    "        # ------ H ------ #\n",
    "        # h_candle_v3(df, 'H')\n",
    "        # df = dc_line(df, df_H, 'H')\n",
    "\n",
    "        # ------ 4H ------ #\n",
    "        # df = bb_line(df, df_4H, '4H')\n",
    "\n",
    "        # rec_df['rsi_1m'] = rsi(rec_df, 14)  # Todo - recursive, 250 period\n",
    "        # df = df.join(to_lower_tf_v2(df, rec_df.iloc[:, [-1]], [-1], backing_i=0), how='inner')  # <-- join same_tf manual\n",
    "        #\n",
    "        # if order_side in [\"OPEN\"]:\n",
    "        #     rec_df_5T['ema_5T'] = ema(rec_df_5T['close'], 195)  # Todo - recursive, 1100 period (5T)\n",
    "        #     df = df.join(to_lower_tf_v2(df, rec_df_5T, [-1]), how='inner')\n",
    "\n",
    "    except Exception as e:\n",
    "        sys_log.error(\"error in sync_check :\", e)\n",
    "    else:\n",
    "        return df\n",
    "\n",
    "\n",
    "def public_indi(res_df, config, np_timeidx, mode=\"OPEN\"):\n",
    "    selection_id = config.selection_id\n",
    "\n",
    "    wave_itv1 = config.tr_set.wave_itv1\n",
    "    wave_itv2 = config.tr_set.wave_itv2\n",
    "    wave_period1 = config.tr_set.wave_period1\n",
    "    wave_period2 = config.tr_set.wave_period2\n",
    "    roll_hl_cnt = 3\n",
    "\n",
    "    # assert itv_to_number(wave_itv1) > 1  # wave_itv2 == 'T' and\n",
    "    # ====== public ====== #\n",
    "    # res_df = wave_range_dcbase_v11_3(res_df, config, over_period=2)\n",
    "\n",
    "    try:\n",
    "        # ------------ wave_period1 ------------ #\n",
    "        if itv_to_number(wave_itv1) > 1:\n",
    "            offset = '1h' if wave_itv1 != 'D' else '9h'\n",
    "            htf_df_ = to_htf(res_df, wave_itv1, offset=offset)  # to_htf 는 ohlc, 4개의 col 만 존재 (현재까지)\n",
    "            htf_df = htf_df_[~pd.isnull(htf_df_.close)]\n",
    "\n",
    "            htf_df = wave_range_cci_v4_1(htf_df, wave_period1, itv=wave_itv1)\n",
    "\n",
    "            cols = list(htf_df.columns[4:])  # 15T_ohlc 를 제외한 wave_range_cci_v4 로 추가된 cols, 다 넣어버리기 (추후 혼란 방지)\n",
    "\n",
    "            valid_co_prime_idx, valid_cu_prime_idx, roll_co_idx_arr, roll_cu_idx_arr = roll_wave_hl_idx_v5(htf_df,\n",
    "                                                                                                           wave_itv1,\n",
    "                                                                                                           wave_period1,\n",
    "                                                                                                           roll_hl_cnt=roll_hl_cnt)\n",
    "\n",
    "            \"\"\" \n",
    "            1. wave_bb 의 경우 roll_hl 의 기준이 co <-> cu 변경됨 (cci 와 비교)\n",
    "            2. wave_bb : high_fill_ -> cu_prime_idx 사용\n",
    "            \"\"\"\n",
    "            htf_df = get_roll_wave_data_v2(htf_df, valid_co_prime_idx, roll_co_idx_arr, 'wave_high_fill_{}{}'.format(wave_itv1, wave_period1), roll_hl_cnt)\n",
    "            cols += list(htf_df.columns[-roll_hl_cnt:])\n",
    "\n",
    "            htf_df = get_roll_wave_data_v2(htf_df, valid_cu_prime_idx, roll_cu_idx_arr, 'wave_low_fill_{}{}'.format(wave_itv1, wave_period1), roll_hl_cnt)\n",
    "            cols += list(htf_df.columns[-roll_hl_cnt:])\n",
    "\n",
    "            htf_df = wave_range_ratio_v4_2(htf_df, wave_itv1, wave_period1, roll_hl_cnt=roll_hl_cnt)\n",
    "            cols += list(htf_df.columns[-4:])\n",
    "            # print(cols)\n",
    "\n",
    "            htf_df = get_wave_length(htf_df, valid_co_prime_idx, valid_cu_prime_idx, roll_co_idx_arr, roll_cu_idx_arr, wave_itv1, wave_period1, roll_hl_cnt=roll_hl_cnt)\n",
    "            cols += list(htf_df.columns[-4:])\n",
    "            # print(cols)\n",
    "\n",
    "            # ------ 필요한 cols 만 join (htf's idx 정보는 ltf 와 sync. 가 맞지 않음 - join 불가함) ------ #\n",
    "            res_df.drop(cols, inplace=True, axis=1, errors='ignore')\n",
    "            res_df = res_df.join(to_lower_tf_v4(res_df, htf_df, cols, backing_i=0, ltf_itv='T').loc[res_df.index], how='inner')\n",
    "\n",
    "        else:\n",
    "            res_df = wave_range_cci_v4_1(res_df, wave_period1, itv=wave_itv1)\n",
    "            # res_df = wave_range_fisher_v1(res_df, wave_period1, itv=wave_itv1)\n",
    "            \n",
    "\n",
    "            valid_co_prime_idx, valid_cu_prime_idx, roll_co_idx_arr, roll_cu_idx_arr = roll_wave_hl_idx_v5(res_df,\n",
    "                                                                                                           wave_itv1,\n",
    "                                                                                                           wave_period1,\n",
    "                                                                                                           roll_hl_cnt=roll_hl_cnt)\n",
    "\n",
    "            res_df = get_roll_wave_data_v2(res_df, valid_co_prime_idx, roll_co_idx_arr, 'wave_high_fill_{}{}'.format(wave_itv1, wave_period1), roll_hl_cnt)\n",
    "            res_df = get_roll_wave_data_v2(res_df, valid_cu_prime_idx, roll_cu_idx_arr, 'wave_low_fill_{}{}'.format(wave_itv1, wave_period1), roll_hl_cnt)\n",
    "\n",
    "            res_df = wave_range_ratio_v4_2(res_df, wave_itv1, wave_period1, roll_hl_cnt=roll_hl_cnt)\n",
    "\n",
    "            res_df = get_wave_length(res_df, valid_co_prime_idx, valid_cu_prime_idx, roll_co_idx_arr, roll_cu_idx_arr, wave_itv1, wave_period1, roll_hl_cnt=roll_hl_cnt)\n",
    "\n",
    "            # res_df = cci_v2(res_df, 120, itv=wave_itv1)\n",
    "            \n",
    "            \n",
    "        # ------------ wave_period2 ------------ #\n",
    "#         if wave_itv1 != wave_itv2 or wave_period1 != wave_period2:\n",
    "#             if itv_to_number(wave_itv2) > 1:\n",
    "#                 offset = '1h' if wave_itv2 != 'D' else '9h'\n",
    "#                 htf_df = to_htf(res_df, wave_itv2, offset=offset)\n",
    "#                 htf_df = wave_range_cci_v4_1(htf_df, wave_period2, itv=wave_itv2)\n",
    "#                 # htf_df = wave_range_bb_v1(htf_df, wave_period2, itv=wave_itv2)\n",
    "\n",
    "#                 # cols = list(htf_df.columns[-15:-4])  # except idx col\n",
    "#                 cols = list(htf_df.columns[4:])  # 15T_ohlc 를 제외한 wave_range_cci_v4 로 추가된 cols, 다 넣어버리기 (추후 혼란 방지)\n",
    "\n",
    "#                 # ------ 필요한 cols 만 join (htf's idx 정보는 ltf 와 sync. 가 맞지 않음 - join 불가함) ------ #\n",
    "#                 res_df.drop(cols, inplace=True, axis=1, errors='ignore')\n",
    "#                 res_df = res_df.join(to_lower_tf_v4(res_df, htf_df, cols, backing_i=0, ltf_itv='T'), how='inner')  # tf_entry 진입이면, backing_i = 0 가 가능한 것 아닌가.\n",
    "\n",
    "#             else:\n",
    "#                 res_df = wave_range_cci_v4_1(res_df, wave_period2, itv=wave_itv2)\n",
    "#                 res_df = wave_range_bb_v1(res_df, wave_period2, itv=wave_itv2)\n",
    "\n",
    "\n",
    "        \"\"\"\n",
    "        # 2. extra.\n",
    "        \"\"\"\n",
    "        # res_df = fisher_v2(res_df, 120, itv='T')\n",
    "        # res_df = ma_v2(res_df, 50, itv='T')\n",
    "        # res_df = ma_v2(res_df, 200, itv='T')\n",
    "        # res_df = ma_v2(res_df, 800, itv='T')\n",
    "\n",
    "        # valid_co_prime_idx, valid_cu_prime_idx, roll_co_idx_arr, roll_cu_idx_arr = roll_wave_hl_idx_v5(res_df, wave_itv2, wave_period2,\n",
    "        #                                                                                                roll_hl_cnt=roll_hl_cnt)\n",
    "        # res_df = get_roll_wave_data_v2(res_df, valid_co_prime_idx, roll_co_idx_arr, 'wave_high_fill_{}{}'.format(wave_itv2, wave_period2),\n",
    "        #                                roll_hl_cnt)\n",
    "        # res_df = get_roll_wave_data_v2(res_df, valid_cu_prime_idx, roll_cu_idx_arr, 'wave_low_fill_{}{}'.format(wave_itv2, wave_period2), roll_hl_cnt)\n",
    "        #\n",
    "        # res_df = wave_range_ratio_v4_2(res_df, wave_itv2, wave_period2, roll_hl_cnt=roll_hl_cnt)\n",
    "\n",
    "        # ------ wave_loc_pct (bb) ------ #\n",
    "        # res_df = wave_loc_pct_v2(res_df, config, 'T', 60)\n",
    "        # res_df = wave_loc_pct(res_df, config, 'T', 60)\n",
    "\n",
    "        # future_cols = ['cu_es_15T1', 'co_es_15T1', 'upper_wick_ratio_15T', 'lower_wick_ratio_15T']\n",
    "        # itv_list = ['15T', '15T', '15T', '15T']\n",
    "        # res_df = backing_future_data(res_df, future_cols, itv_list)\n",
    "\n",
    "        # ====== intervaly ====== #\n",
    "        # ------ 5T ------ #\n",
    "        # res_df = dc_level(res_df, '5T', 1)\n",
    "        # res_df = bb_level(res_df, '5T', 1)\n",
    "\n",
    "        # res_df = st_level(res_df, '5T', 1)\n",
    "\n",
    "        # ------ 15T ------ #\n",
    "        # res_df = wick_ratio(res_df, '15T')\n",
    "        # res_df = dc_level(res_df, '15T', 1)\n",
    "        # res_df = bb_level(res_df, '15T', 1)\n",
    "        # res_df = dtk_plot(res_df, dtk_itv2='15T', hhtf_entry=15, use_dtk_line=config.loc_set.zone.use_dtk_line, np_timeidx=np_timeidx)\n",
    "\n",
    "        # res_df = st_level(res_df, '15T', 1)\n",
    "\n",
    "        # ------ 30T ------ #\n",
    "        # res_df = wick_ratio(res_df, '30T')\n",
    "        # res_df = dc_level(res_df, '30T', 1)\n",
    "        # res_df = bb_level(res_df, '30T', 1)\n",
    "        # res_df = st_level(res_df, '30T', 1)\n",
    "\n",
    "        # ------ H ------ #\n",
    "        # res_df = wick_ratio(res_df, 'H')\n",
    "        # res_df = bb_level(res_df, 'H', 1)\n",
    "\n",
    "        # ------ 4H ------ #\n",
    "        # res_df = wick_ratio(res_df, '4H')\n",
    "        # res_df = bb_level(res_df, '4H', 1)\n",
    "\n",
    "        # res_df['dc_upper_v2'.format(selection_id)] = res_df['high'].rolling(config.loc_set.zone.dc_period).max()   # Todo, consider dc_period\n",
    "        # res_df['dc_lower_v2'.format(selection_id)] = res_df['low'].rolling(config.loc_set.zone.dc_period).min()\n",
    "\n",
    "        # res_df['zone_dc_upper_v2'.format(selection_id)] = res_df['high'].rolling(config.loc_set.zone.zone_dc_period).max()   # Todo, consider zone_dc_period\n",
    "        # res_df['zone_dc_lower_v2'.format(selection_id)] = res_df['low'].rolling(config.loc_set.zone.zone_dc_period).min()\n",
    "\n",
    "        # if order_side in [\"OPEN\"]:\n",
    "        # candle_score_v3(res_df, 'T', unsigned=False)\n",
    "        # candle_score_v3(res_df, config.loc_set.point1.exp_itv, unsigned=False)\n",
    "\n",
    "        #     temp indi.    #\n",
    "        # res_df[\"ma30_1m\"] = res_df['close'].rolling(30).mean()\n",
    "        # res_df[\"ma60_1m\"] = res_df['close'].rolling(60).mean()\n",
    "        # res_df = dtk_plot(res_df, dtk_itv2='15T', hhtf_entry=15, use_dtk_line=config.loc_set.zone.use_dtk_line, np_timeidx=np_timeidx)\n",
    "\n",
    "    except Exception as e:\n",
    "        sys_log.error(\"error in public : {}\".format(e))\n",
    "    else:\n",
    "        return res_df\n",
    "\n",
    "\n",
    "def expiry_v0(res_df, config, op_idx, e_j, tp_j, np_datas, open_side):\n",
    "    high, low = np_datas\n",
    "    selection_id = config.selection_id\n",
    "    expire = 0\n",
    "\n",
    "    if config.tr_set.expire_tick != \"None\":\n",
    "        if e_j - op_idx >= config.tr_set.expire_tick:\n",
    "            expire = 1\n",
    "\n",
    "    if config.tr_set.expire_k1 != \"None\":\n",
    "        if open_side == OrderSide.SELL:\n",
    "            short_tp_1_ = res_df['short_tp_1_{}'.format(selection_id)].to_numpy()  # id 에 따라 dynamic 변수라 이곳에서 numpy 화 진행\n",
    "            short_tp_gap_ = res_df['short_tp_gap_{}'.format(selection_id)].to_numpy()\n",
    "            if low[e_j] <= short_tp_1_[tp_j] - short_tp_gap_[tp_j] * config.tr_set.expire_k1:\n",
    "                expire = 1\n",
    "        else:\n",
    "            long_tp_1_ = res_df['long_tp_1_{}'.format(\n",
    "                selection_id)].to_numpy()  # iloc 이 빠를까, to_numpy() 가 빠를까  # 3.94 ms --> 5.34 ms (iloc)\n",
    "            long_tp_gap_ = res_df['long_tp_gap_{}'.format(selection_id)].to_numpy()\n",
    "            if high[e_j] >= long_tp_1_[tp_j] + long_tp_gap_[tp_j] * config.tr_set.expire_k1:\n",
    "                expire = 1\n",
    "\n",
    "    return expire\n",
    "\n",
    "\n",
    "def expiry_tp(res_df, config, op_idx, e_j, tp_j, np_datas, open_side):\n",
    "    \"\"\"\n",
    "    tp, tp_gap 기준 expiry\n",
    "    \"\"\"\n",
    "\n",
    "    high, low = np_datas\n",
    "    selection_id = config.selection_id\n",
    "    expire = 0\n",
    "\n",
    "    if config.tr_set.expire_tick != \"None\":\n",
    "        if e_j - op_idx >= config.tr_set.expire_tick:\n",
    "            expire = 1\n",
    "\n",
    "    if config.tr_set.expire_k1 != \"None\":\n",
    "        if open_side == OrderSide.SELL:\n",
    "            short_tp_ = res_df['short_tp_{}'.format(selection_id)].to_numpy()  # id 에 따라 dynamic 변수라 이곳에서 numpy 화 진행\n",
    "            short_tp_gap_ = res_df['short_tp_gap_{}'.format(selection_id)].to_numpy()\n",
    "            if low[e_j] <= short_tp_[tp_j] + short_tp_gap_[tp_j] * config.tr_set.expire_k1:\n",
    "                expire = 1\n",
    "        else:\n",
    "            long_tp_ = res_df['long_tp_{}'.format(\n",
    "                selection_id)].to_numpy()  # iloc 이 빠를까, to_numpy() 가 빠를까  # 3.94 ms --> 5.34 ms (iloc)\n",
    "            long_tp_gap_ = res_df['long_tp_gap_{}'.format(selection_id)].to_numpy()\n",
    "            if high[e_j] >= long_tp_[tp_j] - long_tp_gap_[tp_j] * config.tr_set.expire_k1:\n",
    "                expire = 1\n",
    "\n",
    "    return expire\n",
    "\n",
    "\n",
    "def expiry_wave(res_df, config, op_idx, e_j, wave1, wave_gap, np_datas, open_side):\n",
    "    \"\"\"\n",
    "    wave_1, wave_gap 기준 expiry\n",
    "    \"\"\"\n",
    "\n",
    "    high, low = np_datas\n",
    "    selection_id = config.selection_id\n",
    "    expire = 0\n",
    "\n",
    "    if config.tr_set.expire_tick != \"None\":\n",
    "        if e_j - op_idx >= config.tr_set.expire_tick:\n",
    "            expire = 1\n",
    "\n",
    "    if config.tr_set.expire_k2 != \"None\":\n",
    "        if open_side == OrderSide.SELL:\n",
    "            if low[e_j] <= wave1 + wave_gap * config.tr_set.expire_k2:\n",
    "                expire = 1\n",
    "        else:\n",
    "            if high[e_j] >= wave1 - wave_gap * config.tr_set.expire_k2:\n",
    "                expire = 1\n",
    "\n",
    "    return expire\n",
    "\n",
    "\n",
    "def expiry_p1p2(res_df, config, op_idx1, op_idx2, tp1, tp0, tp_gap, np_datas, open_side):\n",
    "    \"\"\"\n",
    "    op_idx1 과 op_idx2 사이의 high / low 통해 expiration survey 라고 보면 됨\n",
    "    \"\"\"\n",
    "\n",
    "    high, low = np_datas\n",
    "    selection_id = config.selection_id\n",
    "    expire = 0\n",
    "    touch_idx = None\n",
    "\n",
    "    # if config.tr_set.expire_tick != \"None\":\n",
    "    #     if e_j - op_idx >= config.tr_set.expire_tick:\n",
    "    #         expire = 1\n",
    "\n",
    "    # Todo, p1's tp1, 0 cannot be vectorized\n",
    "    #   a. expiration 의 조건은 wave1, 0 의 broken\n",
    "    idx_range = np.arange(op_idx1, op_idx2)\n",
    "    if config.tr_set.expire_k1 != \"None\":\n",
    "        if open_side == OrderSide.SELL:\n",
    "            touch_idx = np.where((low[op_idx1:op_idx2] <= tp1 + tp_gap * config.tr_set.expire_k1) | \\\n",
    "                                 (high[op_idx1:op_idx2] >= tp0 - tp_gap * config.tr_set.expire_k1),\n",
    "                                 idx_range, np.nan)\n",
    "            # if op_idx1 >= 16353:\n",
    "            #   print(\"high[16353], tp0 :\", high[16353], tp0)\n",
    "            if np.sum(~np.isnan(touch_idx)) > 0:  # touch 가 존재하면,\n",
    "                # if low[op_idx1:op_idx2].min() <= tp1 + tp_gap * config.tr_set.expire_k1 or \\\n",
    "                # high[op_idx1:op_idx2].max() >= tp0 - tp_gap * config.tr_set.expire_k1:   # p2_box loc. 이 있어서, op_idx2 + 1 안함\n",
    "                expire = 1\n",
    "        else:\n",
    "            touch_idx = np.where((high[op_idx1:op_idx2] >= tp1 - tp_gap * config.tr_set.expire_k1) | \\\n",
    "                                 (low[op_idx1:op_idx2] <= tp0 + tp_gap * config.tr_set.expire_k1),\n",
    "                                 idx_range, np.nan)\n",
    "            if np.sum(~np.isnan(touch_idx)) > 0:\n",
    "                # if high[op_idx1:op_idx2].max() >= tp1 - tp_gap * config.tr_set.expire_k1 or \\\n",
    "                # low[op_idx1:op_idx2].min() <= tp0 + tp_gap * config.tr_set.expire_k1:\n",
    "                expire = 1\n",
    "\n",
    "    return expire, np.nanmin(touch_idx)\n",
    "\n",
    "\n",
    "def ep_loc_p1_v3(res_df, config, np_timeidx, show_detail=True, ep_loc_side=OrderSide.SELL):\n",
    "\n",
    "    \"\"\"\n",
    "    vectorized calc.\n",
    "        1. multi-stem 에 따라 dynamic vars.가 입력되기 때문에 class 내부 vars. 로 종속시키지 않음\n",
    "        2. min & max variables 사용\n",
    "    \"\"\"\n",
    "\n",
    "    # 0. param init\n",
    "    selection_id = config.selection_id\n",
    "    wave_itv1 = config.tr_set.wave_itv1\n",
    "    wave_period1 = config.tr_set.wave_period1\n",
    "    c_i = config.trader_set.complete_index\n",
    "\n",
    "    len_df = len(res_df)\n",
    "    mr_res = np.ones(len_df)\n",
    "    zone_arr = np.full(len_df, 0)\n",
    "\n",
    "    \n",
    "    # 1. wrr\n",
    "    if config.loc_set.point1.wrr_32_min_short != \"None\":\n",
    "        if ep_loc_side == OrderSide.SELL:\n",
    "            cu_wrr_32_ = res_df['cu_wrr_32_{}{}'.format(wave_itv1, wave_period1)].to_numpy()\n",
    "            mr_res *= cu_wrr_32_ >= config.loc_set.point1.wrr_32_min_short\n",
    "            if show_detail:\n",
    "                sys_log.warning(\n",
    "                    \"cu_wrr_32_ >= config.loc_set.point1.wrr_32_min_short : {:.5f} {:.5f} ({})\".format(cu_wrr_32_[c_i],\n",
    "                                                                                                 config.loc_set.point1.wrr_32_min_short,\n",
    "                                                                                                 mr_res[c_i]))\n",
    "    if config.loc_set.point1.wrr_32_max_short != \"None\":\n",
    "        if ep_loc_side == OrderSide.SELL:\n",
    "            cu_wrr_32_ = res_df['cu_wrr_32_{}{}'.format(wave_itv1, wave_period1)].to_numpy()\n",
    "            mr_res *= cu_wrr_32_ <= config.loc_set.point1.wrr_32_max_short\n",
    "            if show_detail:\n",
    "                sys_log.warning(\n",
    "                    \"cu_wrr_32_ <= config.loc_set.point1.wrr_32_max_short : {:.5f} {:.5f} ({})\".format(cu_wrr_32_[c_i],\n",
    "                                                                                                 config.loc_set.point1.wrr_32_max_short,\n",
    "                                                                                                 mr_res[c_i]))                \n",
    "    if config.loc_set.point1.wrr_32_min_long != \"None\":\n",
    "        if ep_loc_side == OrderSide.BUY:\n",
    "            co_wrr_32_ = res_df['co_wrr_32_{}{}'.format(wave_itv1, wave_period1)].to_numpy()\n",
    "            mr_res *= co_wrr_32_ >= config.loc_set.point1.wrr_32_min_long\n",
    "            if show_detail:\n",
    "                sys_log.warning(\n",
    "                    \"co_wrr_32_ >= config.loc_set.point1.wrr_32_min_long : {:.5f} {:.5f} ({})\".format(co_wrr_32_[c_i],\n",
    "                                                                                                 config.loc_set.point1.wrr_32_min_long,\n",
    "                                                                                                 mr_res[c_i]))\n",
    "    if config.loc_set.point1.wrr_32_max_long != \"None\":\n",
    "        if ep_loc_side == OrderSide.BUY:\n",
    "            co_wrr_32_ = res_df['co_wrr_32_{}{}'.format(wave_itv1, wave_period1)].to_numpy()\n",
    "            mr_res *= co_wrr_32_ <= config.loc_set.point1.wrr_32_max_long\n",
    "            if show_detail:\n",
    "                sys_log.warning(\n",
    "                    \"co_wrr_32_ <= config.loc_set.point1.wrr_32_max_long : {:.5f} {:.5f} ({})\".format(co_wrr_32_[c_i],\n",
    "                                                                                                 config.loc_set.point1.wrr_32_max_long,\n",
    "                                                                                                 mr_res[c_i]))\n",
    "\n",
    "                \n",
    "    # 2. spread\n",
    "    if config.loc_set.point1.spread_min_short != \"None\": \n",
    "        if ep_loc_side == OrderSide.SELL:\n",
    "            short_spread_ = res_df['short_spread_{}'.format(selection_id)].to_numpy()\n",
    "            mr_res *= short_spread_ >= config.loc_set.point1.spread_min_short\n",
    "            if show_detail:\n",
    "                sys_log.warning(\n",
    "                    \"short_spread_ >= config.loc_set.point1.spread_min_short : {:.5f} {:.5f} ({})\".format(short_spread_[c_i], \n",
    "                                                                                                          config.loc_set.point1.spread_min_short,\n",
    "                                                                                                          mr_res[c_i]))\n",
    "\n",
    "    if config.loc_set.point1.spread_max_short != \"None\": \n",
    "        if ep_loc_side == OrderSide.SELL:\n",
    "            short_spread_ = res_df['short_spread_{}'.format(selection_id)].to_numpy()\n",
    "            mr_res *= short_spread_ <= config.loc_set.point1.spread_max_short\n",
    "            if show_detail:\n",
    "                sys_log.warning(\n",
    "                    \"short_spread_ <= config.loc_set.point1.spread_max_short : {:.5f} {:.5f} ({})\".format(short_spread_[c_i], \n",
    "                                                                                                   config.loc_set.point1.spread_max_short,\n",
    "                                                                                                   mr_res[c_i]))\n",
    "    \n",
    "    if config.loc_set.point1.spread_min_long != \"None\": \n",
    "        if ep_loc_side == OrderSide.BUY:\n",
    "            long_spread_ = res_df['long_spread_{}'.format(selection_id)].to_numpy()\n",
    "            mr_res *= long_spread_ >= config.loc_set.point1.spread_min_long\n",
    "            if show_detail:\n",
    "                sys_log.warning(\n",
    "                    \"long_spread_ >= config.loc_set.point1.spread_min_long : {:.5f} {:.5f} ({})\".format(long_spread_[c_i], \n",
    "                                                                                   config.loc_set.point1.spread_min_long,\n",
    "                                                                                   mr_res[c_i]))\n",
    "    \n",
    "    if config.loc_set.point1.spread_max_long != \"None\": \n",
    "        if ep_loc_side == OrderSide.BUY:\n",
    "            long_spread_ = res_df['long_spread_{}'.format(selection_id)].to_numpy()\n",
    "            mr_res *= long_spread_ <= config.loc_set.point1.spread_max_long\n",
    "            if show_detail:\n",
    "                sys_log.warning(\n",
    "                    \"long_spread_ <= config.loc_set.point1.spread_max_long : {:.5f} {:.5f} ({})\".format(long_spread_[c_i],\n",
    "                                                                                                        config.loc_set.point1.spread_max_long,\n",
    "                                                                                                        mr_res[c_i]))\n",
    "     \n",
    "    \n",
    "    # 3. lvrg        \n",
    "    if config.loc_set.point1.lvrg_min_short != \"None\": \n",
    "        if ep_loc_side == OrderSide.SELL:\n",
    "            short_lvrg_needed_ = res_df['short_lvrg_needed_{}'.format(selection_id)].to_numpy()\n",
    "            mr_res *= short_lvrg_needed_ >= config.loc_set.point1.lvrg_min_short\n",
    "            if show_detail:\n",
    "                sys_log.warning(\n",
    "                    \"short_lvrg_needed_ >= config.loc_set.point1.lvrg_min_short : {:.5f} {:.5f} ({})\".format(short_lvrg_needed_[c_i], \n",
    "                                                                                                             config.loc_set.point1.lvrg_min_short,\n",
    "                                                                                                             mr_res[c_i]))\n",
    "\n",
    "    if config.loc_set.point1.lvrg_max_short != \"None\": \n",
    "        if ep_loc_side == OrderSide.SELL:\n",
    "            short_lvrg_needed_ = res_df['short_lvrg_needed_{}'.format(selection_id)].to_numpy()\n",
    "            mr_res *= short_lvrg_needed_ <= config.loc_set.point1.lvrg_max_short\n",
    "            if show_detail:\n",
    "                sys_log.warning(\n",
    "                    \"short_lvrg_needed_ <= config.loc_set.point1.lvrg_max_short : {:.5f} {:.5f} ({})\".format(short_lvrg_needed_[c_i], \n",
    "                                                                                                             config.loc_set.point1.lvrg_max_short,\n",
    "                                                                                                             mr_res[c_i]))\n",
    "    \n",
    "    if config.loc_set.point1.lvrg_min_long != \"None\": \n",
    "        if ep_loc_side == OrderSide.BUY:\n",
    "            long_lvrg_needed_ = res_df['long_lvrg_needed_{}'.format(selection_id)].to_numpy()\n",
    "            mr_res *= long_lvrg_needed_ >= config.loc_set.point1.lvrg_min_long\n",
    "            if show_detail:\n",
    "                sys_log.warning(\n",
    "                    \"long_lvrg_needed_ >= config.loc_set.point1.lvrg_min_long : {:.5f} {:.5f} ({})\".format(long_lvrg_needed_[c_i], \n",
    "                                                                                                           config.loc_set.point1.lvrg_min_long,\n",
    "                                                                                                           mr_res[c_i]))\n",
    "    \n",
    "    if config.loc_set.point1.lvrg_max_long != \"None\":\n",
    "        if ep_loc_side == OrderSide.BUY:\n",
    "            long_lvrg_needed_ = res_df['long_lvrg_needed_{}'.format(selection_id)].to_numpy()\n",
    "            mr_res *= long_lvrg_needed_ <= config.loc_set.point1.lvrg_max_long\n",
    "            if show_detail:\n",
    "                sys_log.warning(\n",
    "                    \"long_lvrg_needed_ <= config.loc_set.point1.lvrg_max_long : {:.5f} {:.5f} ({})\".format(long_lvrg_needed_[c_i], \n",
    "                                                                                                           config.loc_set.point1.lvrg_max_long,\n",
    "                                                                                                           mr_res[c_i]))\n",
    "              \n",
    "    \n",
    "    # 4. tr\n",
    "    if config.loc_set.point1.tr_min_short != \"None\":\n",
    "        if ep_loc_side == OrderSide.SELL:\n",
    "            short_tr_ = res_df['short_tr_{}'.format(selection_id)].to_numpy()\n",
    "            mr_res *= short_tr_ >= config.loc_set.point1.tr_min_short\n",
    "            if show_detail:\n",
    "                sys_log.warning(\n",
    "                    \"short_tr_ >= config.loc_set.point1.tr_min_short : {:.5f} {:.5f} ({})\".format(short_tr_[c_i], \n",
    "                                                                                                  config.loc_set.point1.tr_min_short,\n",
    "                                                                                                  mr_res[c_i]))\n",
    "\n",
    "    if config.loc_set.point1.tr_max_short != \"None\":\n",
    "        if ep_loc_side == OrderSide.SELL:\n",
    "            short_tr_ = res_df['short_tr_{}'.format(selection_id)].to_numpy()\n",
    "            mr_res *= short_tr_ <= config.loc_set.point1.tr_max_short\n",
    "            if show_detail:\n",
    "                sys_log.warning(\n",
    "                    \"short_tr_ <= config.loc_set.point1.tr_max_short : {:.5f} {:.5f} ({})\".format(short_tr_[c_i], \n",
    "                                                                                           config.loc_set.point1.tr_max_short,\n",
    "                                                                                           mr_res[c_i]))\n",
    "    \n",
    "    if config.loc_set.point1.tr_min_long != \"None\":\n",
    "        if ep_loc_side == OrderSide.BUY:\n",
    "            long_tr_ = res_df['long_tr_{}'.format(selection_id)].to_numpy()\n",
    "            mr_res *= long_tr_ >= config.loc_set.point1.tr_min_long\n",
    "            if show_detail:\n",
    "                sys_log.warning(\n",
    "                    \"long_tr_ >= config.loc_set.point1.tr_min_long : {:.5f} {:.5f} ({})\".format(long_tr_[c_i], \n",
    "                                                                                                config.loc_set.point1.tr_min_long,\n",
    "                                                                                                mr_res[c_i]))\n",
    "    \n",
    "    if config.loc_set.point1.tr_max_long != \"None\":\n",
    "        if ep_loc_side == OrderSide.BUY:\n",
    "            long_tr_ = res_df['long_tr_{}'.format(selection_id)].to_numpy()\n",
    "            mr_res *= long_tr_ <= config.loc_set.point1.tr_max_long\n",
    "            if show_detail:\n",
    "                sys_log.warning(\n",
    "                    \"long_tr_ <= config.loc_set.point1.tr_max_long : {:.5f} {:.5f} ({})\".format(long_tr_[c_i], \n",
    "                                                                                                config.loc_set.point1.tr_max_long,\n",
    "                                                                                                mr_res[c_i]))\n",
    "                \n",
    "    \n",
    "            \n",
    "            \n",
    "    # 4. zone\n",
    "    #     a. config var. 이 등록되지 않은 dur. 은 selection_id 으로 조건문을 나눔 (lvrg_set 과 동일)\n",
    "    if config.loc_set.zone1.use_zone:\n",
    "        \n",
    "        # i.  on_price\n",
    "        # wave_itv1 = config.tr_set.wave_itv1\n",
    "        # wave_period1 = config.tr_set.wave_period1\n",
    "\n",
    "        # wave_high_fill1_ = res_df['wave_high_fill_{}{}'.format(wave_itv1, wave_period1)].to_numpy()\n",
    "        # wave_low_fill1_ = res_df['wave_low_fill_{}{}'.format(wave_itv1, wave_period1)].to_numpy()\n",
    "\n",
    "        # short_tp_0_ = res_df['short_tp_0_{}'.format(selection_id)].to_numpy()\n",
    "        # long_tp_0_ = res_df['long_tp_0_{}'.format(selection_id)].to_numpy()\n",
    "        \n",
    "#         short_ep_0_ = res_df['short_ep1_0_{}'.format(selection_id)].to_numpy()\n",
    "#         long_ep_0_ = res_df['long_ep1_0_{}'.format(selection_id)].to_numpy()\n",
    "        \n",
    "        # -1. ma50 & ma200\n",
    "#         lb_period = 250\n",
    "#         thresh_k = 0.5\n",
    "        \n",
    "#         high = res_df['high']\n",
    "#         low = res_df['low']\n",
    "#         ma50 = res_df['ma_T50']#.to_numpy()\n",
    "#         ma200 = res_df['ma_T200']#.to_numpy()\n",
    "#         ma50_b1 = res_df['ma_T50'].shift(lb_period).to_numpy()\n",
    "#         ma200_b1 = res_df['ma_T200'].shift(lb_period).to_numpy()\n",
    "        \n",
    "#         if ep_loc_side == OrderSide.SELL:\n",
    "#             # mr_res *= ma50 < ma200\n",
    "#             # mr_res *= ma50_b1 < ma200_b1\n",
    "#             mr_res *= (ma50 < ma200).rolling(lb_period).sum() == lb_period\n",
    "#             mr_res *= (high < ma50).rolling(lb_period).sum() >= lb_period * thresh_k\n",
    "#             if show_detail:\n",
    "#                 sys_log.warning(\"ma50 < ma200 : {:.5f} {:.5f} ({})\".format(ma50[c_i], ma200[c_i], mr_res[c_i]))\n",
    "#         else:\n",
    "#             # mr_res *= ma50 > ma200\n",
    "#             # mr_res *= ma50_b1 > ma200_b1\n",
    "#             mr_res *= (ma50 > ma200).rolling(lb_period).sum() == lb_period\n",
    "#             mr_res *= (low > ma50).rolling(lb_period).sum() >= lb_period * thresh_k\n",
    "#             if show_detail:\n",
    "#                 sys_log.warning(\"ma50 > ma200 : {:.5f} {:.5f} ({})\".format(ma50[c_i], ma200[c_i], mr_res[c_i]))\n",
    "        \n",
    "\n",
    "        #    1. sar\n",
    "        #         high_ = res_df['high_15T'].to_numpy()\n",
    "        #         low_ = res_df['low_15T'].to_numpy()\n",
    "        #         sar_ = res_df['sar_5T'].to_numpy()\n",
    "\n",
    "        #         if ep_loc_side == OrderSide.SELL:\n",
    "        #             mr_res *= high_ < sar_\n",
    "        #             if show_detail:\n",
    "        #                 sys_log.warning(\"high_ < sar_ : {:.5f} {:.5f} ({})\".format(high_[c_i], sar_[c_i], mr_res[c_i]))\n",
    "        #         else:\n",
    "        #             mr_res *= low_ > sar_\n",
    "        #             if show_detail:\n",
    "        #                 sys_log.warning(\"low_ > sar_ : {:.5f} {:.5f} ({})\".format(low_[c_i], sar_[c_i], mr_res[c_i]))\n",
    "\n",
    "        #    2. dc_base\n",
    "        # dc_base_ = res_df['dc_base_T30'].to_numpy()\n",
    "        # dc_base_T20 = res_df['dc_base_T20'].to_numpy()\n",
    "        # dc_base_5T20 = res_df['dc_base_5T20'].to_numpy()\n",
    "        # dc_base_15T20 = res_df['dc_base_15T20'].to_numpy()\n",
    "        # dc_base_H20 = res_df['dc_base_H20'].to_numpy()\n",
    "        # dc_base_4H20 = res_df['dc_base_4H20'].to_numpy()\n",
    "\n",
    "        # if ep_loc_side == OrderSide.SELL:\n",
    "        #     mr_res *= short_tp_0_ < dc_base_H20\n",
    "        #     if show_detail:\n",
    "        #         sys_log.warning(\"short_tp_0_ < dc_base_H20 : {:.5f} {:.5f} ({})\".format(short_tp_0_[c_i], dc_base_H20[c_i], mr_res[c_i]))\n",
    "        # else:\n",
    "        #     mr_res *= long_tp_0_ > dc_base_H20\n",
    "        #     if show_detail:\n",
    "        #         sys_log.warning(\"long_tp_0_ > dc_base_H20 : {:.5f} {:.5f} ({})\".format(long_tp_0_[c_i], dc_base_H20[c_i], mr_res[c_i]))\n",
    "\n",
    "        #    3. bb\n",
    "#         bb_base_T200 = res_df['bb_base_T200'].to_numpy()\n",
    "        \n",
    "#         if ep_loc_side == OrderSide.SELL:\n",
    "#             mr_res *= short_ep_0_ < bb_base_T200\n",
    "#             # mr_res *= short_tp_0_ < bb_base_T200\n",
    "#             if show_detail:\n",
    "#                 sys_log.warning(\"short_ep_0_ < bb_base_T200 : {:.5f} {:.5f} ({})\".format(short_ep_0_[c_i], bb_base_T200[c_i], mr_res[c_i]))\n",
    "#                 # sys_log.warning(\"short_tp_0_ < bb_base_T200 : {:.5f} {:.5f} ({})\".format(short_tp_0_[c_i], bb_base_T200[c_i], mr_res[c_i]))\n",
    "#         else:\n",
    "#             mr_res *= long_ep_0_ > bb_base_T200\n",
    "#             # mr_res *= long_tp_0_ > bb_base_T200\n",
    "#             if show_detail:\n",
    "#                 sys_log.warning(\"long_ep_0_ > bb_base_T200 : {:.5f} {:.5f} ({})\".format(long_ep_0_[c_i], bb_base_T200[c_i], mr_res[c_i]))\n",
    "#                 # sys_log.warning(\"long_tp_0_ > bb_base_T200 : {:.5f} {:.5f} ({})\".format(long_tp_0_[c_i], bb_base_T200[c_i], mr_res[c_i]))\n",
    "\n",
    "        \n",
    "        # ii. outer_price\n",
    "        #     -a. fisher\n",
    "        # fisher_ = res_df['fisher_T30'].to_numpy()\n",
    "        fisher2_ = res_df['fisher_T120'].to_numpy()\n",
    "        fisher_threshold = 1\n",
    "        \n",
    "        if ep_loc_side == OrderSide.SELL:\n",
    "            mr_res *= fisher2_ < -fisher_threshold\n",
    "            if show_detail:\n",
    "                sys_log.warning(\"fisher2_ < -fisher_threshold : {:.5f} {:.5f} ({})\".format(fisher2_[c_i], -fisher_threshold, mr_res[c_i]))\n",
    "                # sys_log.warning(\"cci_ < b1_cci_ : {:.5f} {:.5f} ({})\".format(cci_[c_i], b1_cci_[c_i], mr_res[c_i]))                \n",
    "        else:\n",
    "            mr_res *= fisher2_ > fisher_threshold\n",
    "            if show_detail:\n",
    "                sys_log.warning(\"fisher2_ > fisher_threshold : {:.5f} {:.5f} ({})\".format(fisher2_[c_i], fisher_threshold, mr_res[c_i]))\n",
    "            \n",
    "        #     a. cci\n",
    "#         cci_ = res_df['cci_T120'].to_numpy()\n",
    "        \n",
    "#         # cci_ = res_df['cci_30T20'].to_numpy()\n",
    "#         # b1_cci_ = res_df['cci_30T20'].shift(30).to_numpy()\n",
    "#         threshold = 100\n",
    "\n",
    "#         if ep_loc_side == OrderSide.SELL:\n",
    "#             mr_res *= cci_ < -threshold\n",
    "#             # mr_res *= cci_ < 0\n",
    "#             # mr_res *= cci_ < b1_cci_\n",
    "#             # mr_res *= (cci_ > -100) & (cci_ < -80)\n",
    "#             if show_detail:\n",
    "#                 sys_log.warning(\"cci_ < -threshold : {:.5f} {:.5f} ({})\".format(cci_[c_i], -threshold, mr_res[c_i]))\n",
    "#                 # sys_log.warning(\"cci_ < b1_cci_ : {:.5f} {:.5f} ({})\".format(cci_[c_i], b1_cci_[c_i], mr_res[c_i]))\n",
    "#         else:\n",
    "#             mr_res *= cci_ > threshold\n",
    "#             # mr_res *= cci_ < 0\n",
    "#             # mr_res *= cci_ > b1_cci_\n",
    "#             # mr_res *= (cci_ > 80) & (cci_ < 100)\n",
    "#             if show_detail:\n",
    "#                 sys_log.warning(\"cci_ > threshold : {:.5f} {:.5f} ({})\".format(cci_[c_i], threshold, mr_res[c_i]))\n",
    "#                 # sys_log.warning(\"cci_ > b1_cci_ : {:.5f} {:.5f} ({})\".format(cci_[c_i], b1_cci_[c_i], mr_res[c_i]))\n",
    "\n",
    "\n",
    "        #     b. macd\n",
    "#         # macd_ = res_df['macd_T535'].to_numpy()\n",
    "#         macd_ = res_df['macd_hist_T53515'].to_numpy()\n",
    "\n",
    "#         if ep_loc_side == OrderSide.SELL:\n",
    "#           mr_res *= macd_ < 0\n",
    "#           if show_detail:\n",
    "#             sys_log.warning(\"macd_ < 0 : {:.5f} {:.5f} ({})\".format(macd_[c_i], 0, mr_res[c_i]))\n",
    "#         else:\n",
    "#           mr_res *= macd_ > 0\n",
    "#           if show_detail:\n",
    "#             sys_log.warning(\"macd_ > 0 : {:.5f} {:.5f} ({})\".format(macd_[c_i], 0, mr_res[c_i]))\n",
    "\n",
    "    return mr_res, zone_arr  # mr_res 의 True idx 가 open signal\n",
    "\n",
    "\n",
    "def ep_loc_p2_v3(res_df, config, np_timeidx, show_detail=True, ep_loc_side=OrderSide.SELL):\n",
    "\n",
    "    # ------- param init ------- #\n",
    "    selection_id = config.selection_id\n",
    "    wave_itv2 = config.tr_set.wave_itv2\n",
    "    wave_period2 = config.tr_set.wave_period2\n",
    "    c_i = config.trader_set.complete_index\n",
    "\n",
    "    len_df = len(res_df)\n",
    "    mr_res = np.ones(len_df)\n",
    "    zone_arr = np.full(len_df, 0)\n",
    "\n",
    "    # ------------ wave_range_ratio ------------ #\n",
    "    if config.loc_set.point2.wrr_32_min != \"None\":\n",
    "        if ep_loc_side == OrderSide.SELL:\n",
    "            cu_wrr_32_ = res_df['cu_wrr_32_{}{}'.format(wave_itv2, wave_period2)].to_numpy()\n",
    "            mr_res *= cu_wrr_32_ >= config.loc_set.point2.wrr_32_min\n",
    "            if show_detail:\n",
    "                sys_log.warning(\n",
    "                    \"cu_wrr_32_ >= config.loc_set.point2.wrr_32_min : {:.5f} {:.5f} ({})\".format(cu_wrr_32_[c_i],\n",
    "                                                                                                 config.loc_set.point2.wrr_32_min,\n",
    "                                                                                                 mr_res[c_i]))\n",
    "        else:\n",
    "            co_wrr_32_ = res_df['co_wrr_32_{}{}'.format(wave_itv2, wave_period2)].to_numpy()\n",
    "            mr_res *= co_wrr_32_ >= config.loc_set.point2.wrr_32_min\n",
    "            if show_detail:\n",
    "                sys_log.warning(\n",
    "                    \"co_wrr_32_ >= config.loc_set.point2.wrr_32_min : {:.5f} {:.5f} ({})\".format(co_wrr_32_[c_i],\n",
    "                                                                                                 config.loc_set.point2.wrr_32_min,\n",
    "                                                                                                 mr_res[c_i]))\n",
    "    if config.loc_set.point2.wrr_32_max != \"None\":\n",
    "        if ep_loc_side == OrderSide.SELL:\n",
    "            cu_wrr_32_ = res_df['cu_wrr_32_{}{}'.format(wave_itv2, wave_period2)].to_numpy()\n",
    "            mr_res *= cu_wrr_32_ <= config.loc_set.point2.wrr_32_max\n",
    "            if show_detail:\n",
    "                sys_log.warning(\n",
    "                    \"cu_wrr_32_ <= config.loc_set.point2.wrr_32_max : {:.5f} {:.5f} ({})\".format(cu_wrr_32_[c_i],\n",
    "                                                                                                 config.loc_set.point2.wrr_32_max,\n",
    "                                                                                                 mr_res[c_i]))\n",
    "        else:\n",
    "            co_wrr_32_ = res_df['co_wrr_32_{}{}'.format(wave_itv2, wave_period2)].to_numpy()\n",
    "            mr_res *= co_wrr_32_ <= config.loc_set.point2.wrr_32_max\n",
    "            if show_detail:\n",
    "                sys_log.warning(\n",
    "                    \"co_wrr_32_ <= config.loc_set.point2.wrr_32_max : {:.5f} {:.5f} ({})\".format(co_wrr_32_[c_i],\n",
    "                                                                                                 config.loc_set.point2.wrr_32_max,\n",
    "                                                                                                 mr_res[c_i]))\n",
    "\n",
    "    if config.loc_set.zone2.use_zone:\n",
    "        # ------------ outer_price ------------ #\n",
    "        # ------ cci ------ #\n",
    "        cci_ = res_df['cci_30T20'].to_numpy()\n",
    "        b1_cci_ = res_df['cci_30T20'].shift(30).to_numpy()\n",
    "        # base_value = -100\n",
    "\n",
    "        if ep_loc_side == OrderSide.SELL:\n",
    "            # mr_res *= cci_ < 0\n",
    "            mr_res *= cci_ < b1_cci_\n",
    "            # mr_res *= (cci_ > -100) & (cci_ < -80)\n",
    "            if show_detail:\n",
    "                sys_log.warning(\"cci_ < b1_cci_ : {:.5f} {:.5f} ({})\".format(cci_[c_i], b1_cci_[c_i], mr_res[c_i]))\n",
    "        else:\n",
    "            # mr_res *= cci_ < 0\n",
    "            mr_res *= cci_ > b1_cci_\n",
    "            # mr_res *= (cci_ > 80) & (cci_ < 100)\n",
    "            if show_detail:\n",
    "                sys_log.warning(\"cci_ > b1_cci_ : {:.5f} {:.5f} ({})\".format(cci_[c_i], b1_cci_[c_i], mr_res[c_i]))\n",
    "\n",
    "    return mr_res, zone_arr  # mr_res 의 True idx 가 open signal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EQ63Jwpvr7qA",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### legacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "csZwxsP5r_Pz"
   },
   "outputs": [],
   "source": [
    "      \n",
    "    # ------------ tr_thresh ------------ #  # vectorize allow only for p1_hhm\n",
    "    # if config.loc_set.point1.short_tr_thresh != \"None\":  #  and not config.tr_set.check_hlm:\n",
    "    #     if ep_loc_side == OrderSide.SELL:\n",
    "    #         short_tr_ = res_df['short_tr_{}'.format(selection_id)].to_numpy()\n",
    "    #         mr_res *= short_tr_ >= config.loc_set.point1.short_tr_thresh\n",
    "    #         # mr_res *= short_tr_ <= config.loc_set.point1.short_tr_thresh + 0.1\n",
    "    #         if show_detail:\n",
    "    #             sys_log.warning(\n",
    "    #                 \"short_tr_ >= short_tr_thresh : {:.5f} {:.5f} ({})\".format(short_tr_[c_i], config.loc_set.point1.short_tr_thresh, mr_res[c_i]))\n",
    "    #     else:\n",
    "    #         long_tr_ = res_df['long_tr_{}'.format(selection_id)].to_numpy()\n",
    "    #         mr_res *= long_tr_ >= config.loc_set.point1.long_tr_thresh\n",
    "    #         # mr_res *= long_tr_ <= config.loc_set.point1.long_tr_thresh + 0.1\n",
    "    #         if show_detail:\n",
    "    #             sys_log.warning(\n",
    "    #                 \"long_tr_ >= long_tr_thresh : {:.5f} {:.5f} ({})\".format(long_tr_[c_i], config.loc_set.point1.long_tr_thresh, mr_res[c_i]))\n",
    "    \n",
    "        \"\"\"\n",
    "        future data phase\n",
    "        \"\"\"\n",
    "\n",
    "    #         close_ = res_df['close_30T'].to_numpy()\n",
    "    #         f1_close_ = res_df['close_30T'].shift(-30).to_numpy()\n",
    "\n",
    "    #         if ep_loc_side == OrderSide.SELL:\n",
    "    #             mr_res *= f1_close_ < close_\n",
    "    #             if show_detail:\n",
    "    #                 sys_log.warning(\"f1_close_ < close_ : {:.5f} {:.5f} ({})\".format(f1_close_[c_i], close_[c_i], mr_res[c_i]))\n",
    "    #         else:\n",
    "    #             mr_res *= f1_close_ > close_\n",
    "    #             if show_detail:\n",
    "    #                 sys_log.warning(\"f1_close_ < close_ : {:.5f} {:.5f} ({})\".format(f1_close_[c_i], close_[c_i], mr_res[c_i]))\n",
    "\n",
    "    #         cci_ = res_df['cci_30T20'].to_numpy()\n",
    "    #         f1_cci_ = res_df['cci_30T20'].shift(-30).to_numpy()\n",
    "\n",
    "    #         if ep_loc_side == OrderSide.SELL:\n",
    "    #             mr_res *= f1_cci_ < cci_\n",
    "    #             if show_detail:\n",
    "    #                 sys_log.warning(\"f1_cci_ < cci_ : {:.5f} {:.5f} ({})\".format(f1_cci_[c_i], cci_[c_i], mr_res[c_i]))\n",
    "    #         else:\n",
    "    #             mr_res *= f1_cci_ > cci_\n",
    "    #             if show_detail:\n",
    "    #                 sys_log.warning(\"f1_cci_ < cci_ : {:.5f} {:.5f} ({})\".format(f1_cci_[c_i], cci_[c_i], mr_res[c_i]))\n",
    "    \n",
    "    # ------ hl_loc_pct ------ #\n",
    "    if config.loc_set.zone.hl_loc_pct != \"None\":      \n",
    "      wave_high_loc_pct_ = res_df['wave_high_loc_pct_{}{}'.format(wave_itv1, wave_period1)].to_numpy()\n",
    "      wave_low_loc_pct_ = res_df['wave_low_loc_pct_{}{}'.format(wave_itv1, wave_period1)].to_numpy()\n",
    "\n",
    "      if ep_loc_side == OrderSide.SELL:\n",
    "        mr_res *= wave_high_loc_pct_ >= config.loc_set.zone.hl_loc_pct\n",
    "        mr_res *= wave_high_loc_pct_ <= config.loc_set.zone.hl_loc_pct + 0.5\n",
    "        if show_detail:\n",
    "            sys_log.warning(\"wave_high_loc_pct_ >= config.loc_set.zone.hl_loc_pct : {:.5f} {:.5f} ({})\".format(wave_high_loc_pct_[c_i], config.loc_set.zone.hl_loc_pct, mr_res[c_i]))\n",
    "      else:\n",
    "        mr_res *= wave_low_loc_pct_ >= config.loc_set.zone.hl_loc_pct\n",
    "        mr_res *= wave_low_loc_pct_ <= config.loc_set.zone.hl_loc_pct + 0.5\n",
    "        if show_detail:\n",
    "            sys_log.warning(\"wave_low_loc_pct_ >= config.loc_set.zone.hl_loc_pct : {:.5f} {:.5f} ({})\".format(wave_low_loc_pct_[c_i], config.loc_set.zone.hl_loc_pct, mr_res[c_i]))\n",
    "\n",
    "            \n",
    "        # ------------------ wave_biaser (sr_confirmer) ------------------ #\n",
    "        if selection_id in ['3_9']:     \n",
    "          itv, period1, period2 = config.tr_set.p1_itv1, config.tr_set.p1_period1, config.tr_set.p1_period2          \n",
    "\n",
    "          if ep_loc_side == OrderSide.SELL:\n",
    "            short_wave_high_ = res_df['short_wave_high_{}{}{}'.format(itv, period1, period2)]\n",
    "            bb_lower_5T_amax = get_line(res_df['short_wave_high_idx_{}{}{}'.format(itv, period1, period2)].to_numpy(), res_df['bb_lower_5T'].to_numpy())\n",
    "            mr_res *= short_wave_high_ <= bb_lower_5T_amax\n",
    "            if show_detail:\n",
    "                sys_log.warning(\"short_wave_high_ <= bb_lower_5T_amax : {:.5f} {:.5f} ({})\".format(short_wave_high_[c_i], bb_lower_5T_amax[c_i], mr_res[c_i]))\n",
    "          else:\n",
    "            long_wave_low_ = res_df['long_wave_low_{}{}{}'.format(itv, period1, period2)]\n",
    "            bb_upper_5T_amax = get_line(res_df['long_wave_low_idx_{}{}{}'.format(itv, period1, period2)].to_numpy(), res_df['bb_upper_5T'].to_numpy())\n",
    "            mr_res *= long_wave_low_ >= bb_upper_5T_amax\n",
    "            if show_detail:\n",
    "                sys_log.warning(\"long_wave_low_ >= bb_upper_5T_amax : {:.5f} {:.5f} ({})\".format(long_wave_low_[c_i], bb_upper_5T_amax[c_i], mr_res[c_i]))\n",
    "\n",
    "\n",
    "        if selection_id in ['4_3', '3_5', '3_51']:\n",
    "            dc_base_T20 = res_df['dc_base_T20'].to_numpy()\n",
    "            dc_base_3T20 = res_df['dc_base_3T20'].to_numpy()\n",
    "            # b1_dc_base_3T20 = res_df['dc_base_3T20'].shift(3).to_numpy()\n",
    "            # dc_base_5T = res_df['dc_base_5T'].to_numpy()\n",
    "            # dc_base_15T = res_df['dc_base_15T'].to_numpy()\n",
    "            # dc_base_30T = res_df['dc_base_30T'].to_numpy()\n",
    "            dc_base_H20 = res_df['dc_base_H20'].to_numpy()\n",
    "            # dc_base_4H = res_df['dc_base_4H'].to_numpy()\n",
    "            # dc_base_D = res_df['dc_base_D'].to_numpy()\n",
    "\n",
    "            itv, period1, period2 = config.tr_set.p1_itv1, config.tr_set.p1_period1, config.tr_set.p1_period2\n",
    "            if ep_loc_side == OrderSide.SELL:\n",
    "                # ------ short_base_ <= dc_base_3T20 ------ #\n",
    "                short_base_ = res_df['short_base_{}{}{}'.format(itv, period1, period2)].to_numpy()\n",
    "                mr_res *= short_base_ <= dc_base_3T20\n",
    "                if show_detail:\n",
    "                    sys_log.warning(\"short_base_ <= dc_base_3T20 : {:.5f} {:.5f} ({})\".format(short_base_[c_i], dc_base_3T20[c_i], mr_res[c_i]))\n",
    "\n",
    "                # mr_res *= short_base_ <= dc_base_T20\n",
    "                # if show_detail:\n",
    "                #     sys_log.warning(\"short_base_ <= dc_base_T20 : {:.5f} {:.5f} ({})\".format(short_base_[c_i], dc_base_T20[c_i], mr_res[c_i]))\n",
    "\n",
    "                # ------ reject csd ------ #\n",
    "                # dc_upper_ = res_df['dc_upper_{}{}'.format(itv, period1)].to_numpy()\n",
    "                # mr_res *= dc_upper_ <= dc_base_3T\n",
    "                # if show_detail:\n",
    "                #     sys_log.warning(\"dc_upper_ <= dc_base_3T20 : {:.5f} {:.5f} ({})\".format(dc_upper_[c_i], dc_base_3T[c_i], mr_res[c_i]))\n",
    "\n",
    "                # Todo, 부호 조심\n",
    "                # dc_upper2_ = res_df['dc_upper_{}{}'.format(itv, period2)].to_numpy()\n",
    "                # mr_res *= dc_upper2_ >= dc_base_H\n",
    "                # if show_detail:\n",
    "                #     sys_log.warning(\"dc_upper2_ >= dc_base_H20 : {:.5f} {:.5f} ({})\".format(dc_upper2_[c_i], dc_base_H[c_i], mr_res[c_i]))  \n",
    "\n",
    "                # long 과 동일한 dur.\n",
    "                dc_lower2_ = res_df['dc_lower_{}{}'.format(itv, period2)].to_numpy()\n",
    "                mr_res *= dc_lower2_ >= dc_base_H20\n",
    "                if show_detail:\n",
    "                    sys_log.warning(\"dc_lower2_ >= dc_base_H20 : {:.5f} {:.5f} ({})\".format(dc_lower2_[c_i], dc_base_H20[c_i], mr_res[c_i]))  \n",
    "\n",
    "                # ------ consecutive base ascender ------ #\n",
    "                # ------ 1. roll_min ------ #\n",
    "                dc_base_3T20_rollmin = res_df['dc_base_3T20'].rolling(config.loc_set.zone.base_roll_period).min().to_numpy()\n",
    "                mr_res *= dc_base_3T20_rollmin == dc_base_3T20\n",
    "                if show_detail:\n",
    "                    sys_log.warning(\n",
    "                        \"dc_base_3T20_rollmin == dc_base_3T2020 : {:.5f} {:.5f} ({})\".format(dc_base_3T20_rollmin[c_i], dc_base_3T20[c_i], mr_res[c_i]))\n",
    "            else:\n",
    "                # ------ long_base >= dc_base_3T20 ------ #\n",
    "                long_base_ = res_df['long_base_{}{}{}'.format(itv, period1, period2)].to_numpy()\n",
    "                mr_res *= long_base_ >= dc_base_3T20\n",
    "                if show_detail:\n",
    "                    sys_log.warning(\"long_base_ >= dc_base_3T20 : {:.5f} {:.5f} ({})\".format(long_base_[c_i], dc_base_3T20[c_i], mr_res[c_i]))\n",
    "\n",
    "                # mr_res *= long_base_ >= dc_base_T20\n",
    "                # if show_detail:\n",
    "                #     sys_log.warning(\"long_base_ >= dc_base_T20 : {:.5f} {:.5f} ({})\".format(long_base_[c_i], dc_base_T20[c_i], mr_res[c_i]))\n",
    "\n",
    "                # ------ reject csd ------ #\n",
    "                # dc_lower_ = res_df['dc_lower_{}{}'.format(itv, period1)].to_numpy()\n",
    "                # mr_res *= dc_lower_ >= dc_base_3T\n",
    "                # if show_detail:\n",
    "                #     sys_log.warning(\"dc_lower_ >= dc_base_3T20 : {:.5f} {:.5f} ({})\".format(dc_lower_[c_i], dc_base_3T[c_i], mr_res[c_i]))\n",
    "\n",
    "                dc_lower2_ = res_df['dc_lower_{}{}'.format(itv, period2)].to_numpy()\n",
    "                mr_res *= dc_lower2_ >= dc_base_H20\n",
    "                if show_detail:\n",
    "                    sys_log.warning(\"dc_lower2_ >= dc_base_H20 : {:.5f} {:.5f} ({})\".format(dc_lower2_[c_i], dc_base_H20[c_i], mr_res[c_i]))\n",
    "\n",
    "                # bb_lower_5T = res_df['bb_lower_5T'].to_numpy()\n",
    "                # mr_res *= dc_lower2_ >= bb_lower_5T\n",
    "                # if show_detail:\n",
    "                #     sys_log.warning(\"dc_lower2_ >= bb_lower_5T : {:.5f} {:.5f} ({})\".format(dc_lower2_[c_i], bb_lower_5T[c_i], mr_res[c_i]))\n",
    "\n",
    "                # ------ alignment ------ #\n",
    "                # mr_res *= (dc_base_3T20 > dc_base_5T) & (dc_base_5T > dc_base_15T) & (dc_base_15T > dc_base_30T)\n",
    "\n",
    "                # ------ consecutive base ascender ------ #\n",
    "                # ------ 1. roll_max ------ #\n",
    "                dc_base_3T20_rollmax = res_df['dc_base_3T20'].rolling(config.loc_set.zone.base_roll_period).max().to_numpy()\n",
    "                mr_res *= dc_base_3T20_rollmax == dc_base_3T20\n",
    "                if show_detail:\n",
    "                    sys_log.warning(\n",
    "                        \"dc_base_3T20_rollmax == dc_base_3T2020 : {:.5f} {:.5f} ({})\".format(dc_base_3T20_rollmax[c_i], dc_base_3T20[c_i], mr_res[c_i]))\n",
    "\n",
    "                # ------ 2. roll_max_v2 - ascender  ------ #\n",
    "                # dc_base_3T_ascend = (res_df['dc_base_3T'] >= res_df['dc_base_3T'].shift(3)).rolling(config.loc_set.zone.base_roll_period).sum().to_numpy()\n",
    "                # # mr_res *= dc_base_3T_ascend == config.loc_set.zone.base_roll_period\n",
    "                # mr_res *= dc_base_3T_ascend != config.loc_set.zone.base_roll_period\n",
    "                # if show_detail:\n",
    "                #     sys_log.warning(\"dc_base_3T_ascend == config.loc_set.zone.base_roll_period : {:.5f} {:.5f} ({})\".format(dc_base_3T_ascend[c_i], config.loc_set.zone.base_roll_period, mr_res[c_i]))\n",
    "\n",
    "\n",
    "    if config.loc_set.point.wrr != \"None\":            \n",
    "      wave_itv = 'T'\n",
    "      wave_period = config.tr_set.wave_period\n",
    "      co_wrr_ = res_df['co_wrr_{}{}'.format(wave_itv, wave_period)].to_numpy()\n",
    "      cu_wrr_ = res_df['cu_wrr_{}{}'.format(wave_itv, wave_period)].to_numpy()\n",
    "      if ep_loc_side == OrderSide.SELL:\n",
    "        mr_res *= co_wrr_ <= config.loc_set.point.wrr\n",
    "        if show_detail:\n",
    "            sys_log.warning(\"co_wrr_ <= config.loc_set.point.wrr : {:.5f} {:.5f} ({})\".format(co_wrr_[c_i], config.loc_set.point.wrr, mr_res[c_i]))\n",
    "      else:\n",
    "        mr_res *= cu_wrr_ <= config.loc_set.point.wrr\n",
    "        if show_detail:\n",
    "            sys_log.warning(\"cu_wrr_ <= config.loc_set.point.wrr : {:.5f} {:.5f} ({})\".format(cu_wrr_[c_i], config.loc_set.point.wrr, mr_res[c_i]))\n",
    "            \n",
    "      # if ep_loc_side == OrderSide.SELL:\n",
    "      #   mr_res *= cu_es_ >= config.loc_set.point.cu_es\n",
    "      #   mr_res *= cu_es_ <= config.loc_set.point.cu_es + 2\n",
    "      #   if show_detail:\n",
    "      #       sys_log.warning(\"cu_es_ >= config.loc_set.point.cu_es : {:.5f} {:.5f} ({})\".format(cu_es_[c_i], config.loc_set.point.cu_es, mr_res[c_i]))\n",
    "      # else:\n",
    "      #   mr_res *= co_es_ >= config.loc_set.point.co_es\n",
    "      #   mr_res *= co_es_ <= config.loc_set.point.co_es + 1\n",
    "      #   if show_detail:\n",
    "      #       sys_log.warning(\"co_es_ >= config.loc_set.point.co_es : {:.5f} {:.5f} ({})\".format(co_es_[c_i], config.loc_set.point.co_es, mr_res[c_i]))\n",
    "\n",
    "\n",
    "      if ep_loc_side == OrderSide.SELL:\n",
    "          mr_res *= lower_wick_ratio_ >= config.loc_set.point.wick_ratio\n",
    "          # mr_res *= upper_wick_ratio_ >= config.loc_set.point.wick_ratio\n",
    "          if show_detail:\n",
    "              sys_log.warning(\"upper_wick_ratio_ >= config.loc_set.point.wick_ratio : {:.5f} {:.5f} ({})\".format(upper_wick_ratio_[c_i], config.loc_set.point.wick_ratio, mr_res[c_i]))\n",
    "      else:\n",
    "          mr_res *= upper_wick_ratio_ >= config.loc_set.point.wick_ratio\n",
    "          mr_res *= upper_wick_ratio_ <= config.loc_set.point.wick_ratio + 0.1\n",
    "          # mr_res *= lower_wick_ratio_ >= config.loc_set.point.wick_ratio\n",
    "          if show_detail:\n",
    "              sys_log.warning(\"lower_wick_ratio_ >= config.loc_set.point.wick_ratio : {:.5f} {:.5f} ({})\".format(lower_wick_ratio_[c_i], config.loc_set.point.wick_ratio, mr_res[c_i]))\n",
    "              \n",
    "      crr_ = res_df['crr_{}'.format(config.loc_set.point.tf_entry)].to_numpy()\n",
    "      mr_res *= crr_ >= config.loc_set.point.crr\n",
    "     \n",
    "      if show_detail:\n",
    "          sys_log.warning(\"crr_ >= config.loc_set.point.crr : {:.5f} {:.5f} ({})\".format(crr_[c_i], config.loc_set.point.crr, mr_res[c_i]))\n",
    "\n",
    "      b1_upper_wick_ratio_ = res_df['upper_wick_ratio_{}'.format(config.loc_set.point.wick_itv)].shift(itv_num).to_numpy()\n",
    "      b1_lower_wick_ratio_ = res_df['lower_wick_ratio_{}'.format(config.loc_set.point.wick_itv)].shift(itv_num).to_numpy()\n",
    "\n",
    "      if ep_loc_side == OrderSide.SELL:\n",
    "          upper_wick_ratio_ = res_df['upper_wick_ratio_{}'.format(config.loc_set.point.wick_itv)].to_numpy()\n",
    "          mr_res *= upper_wick_ratio_ >= config.loc_set.point.wick_ratio\n",
    "          if show_detail:\n",
    "              sys_log.warning(\"upper_wick_ratio_ >= config.loc_set.point.wick_ratio : {:.5f} {:.5f} ({})\".format(upper_wick_ratio_[c_i], config.loc_set.point.wick_ratio, mr_res[c_i]))\n",
    "      else:\n",
    "          lower_wick_ratio_ = res_df['lower_wick_ratio_{}'.format(config.loc_set.point.wick_itv)].to_numpy()\n",
    "          mr_res *= lower_wick_ratio_ >= config.loc_set.point.wick_ratio\n",
    "          if show_detail:\n",
    "              sys_log.warning(\"lower_wick_ratio_ >= config.loc_set.point.wick_ratio : {:.5f} {:.5f} ({})\".format(lower_wick_ratio_[c_i], config.loc_set.point.wick_ratio, mr_res[c_i]))\n",
    "\n",
    "    if config.loc_set.point.cppr != \"None\":   \n",
    "      tf_entry = itv_to_number(config.loc_set.point.tf_entry)\n",
    "      b1_cppr_ = res_df['b1_cppr_{}'.format(config.loc_set.point.tf_entry)].to_numpy()  # check b1's cppr in ep_loc\n",
    "      if ep_loc_side == OrderSide.SELL:\n",
    "        mr_res *= b1_cppr_ >= config.loc_set.point.cppr\n",
    "        if show_detail:\n",
    "            sys_log.warning(\"b1_cppr_ >= config.loc_set.point.cppr : {:.5f} {:.5f} ({})\".format(b1_cppr_[c_i], config.loc_set.point.cppr, mr_res[c_i]))\n",
    "      else:\n",
    "        mr_res *= b1_cppr_ <= -config.loc_set.point.cppr\n",
    "        if show_detail:\n",
    "            sys_log.warning(\"b1_cppr_ <= -config.loc_set.point.cppr : {:.5f} {:.5f} ({})\".format(b1_cppr_[c_i], config.loc_set.point.cppr, mr_res[c_i]))\n",
    "            \n",
    "    # ------------ candle_score ------------ #\n",
    "    wick_score_list = literal_eval(config.loc_set.point.wick_score_list)\n",
    "    if len(wick_score_list) != 0:\n",
    "        score_itv_list = literal_eval(config.loc_set.point.score_itv_list)\n",
    "        # ------ candle_score_v0 (1m initial tick 기준임) ------ #  Todo - higher timeframe 경우 back_data 사용해야함\n",
    "        for wick_score_, score_itv_ in zip(wick_score_list, score_itv_list):\n",
    "            wick_score = res_df['wick_score_{}'.format(score_itv_)].to_numpy()\n",
    "            if ep_loc_side == OrderSide.SELL:\n",
    "                mr_res *= wick_score <= -wick_score_\n",
    "                if show_detail:\n",
    "                    sys_log.warning(\"wick_score <= -wick_score_ : {:.5f} {:.5f} ({})\".format(wick_score[c_i], -wick_score_, mr_res[c_i]))\n",
    "            else:\n",
    "                mr_res *= wick_score >= wick_score_\n",
    "                if show_detail:\n",
    "                    sys_log.warning(\"wick_score >= wick_score_ : {:.5f} {:.5f} ({})\".format(wick_score[c_i], wick_score_, mr_res[c_i]))\n",
    "            \n",
    "\n",
    "        # ------------------ swing_middle ------------------ #\n",
    "        # ------------ 1. envelope ------------ #\n",
    "\n",
    "        # ------ a. dc ------ #\n",
    "        # ep_loc check 기준 idx 가 entry 기준이라는 걸 명심\n",
    "        if selection_id in ['v3_2']:\n",
    "            hc_itv = '15T'\n",
    "            dc_itv = '15T'\n",
    "            shift_num = [0, itv_to_number(hc_itv)]\n",
    "            div_res = [1, 0]\n",
    "            for itv_num, res in zip(shift_num, div_res):\n",
    "                close_ = res_df['close_{}'.format(hc_itv)].shift(itv_num).to_numpy()  # close_bar timein 사용하는 경우, 특수로 shift(0) 사용가능\n",
    "                if ep_loc_side == OrderSide.SELL:\n",
    "                    dc_lower_ = res_df['dc_lower_%s' % dc_itv].shift(itv_num).to_numpy()\n",
    "                    mr_res *= (close_ < dc_lower_) == res\n",
    "                else:\n",
    "                    dc_upper_ = res_df['dc_upper_%s' % dc_itv].shift(itv_num).to_numpy()\n",
    "                    mr_res *= (close_ > dc_upper_) == res\n",
    "\n",
    "        # ------------ 2. degree ------------ #\n",
    "        # ------ a. norm_body_ratio ------ #\n",
    "        if config.loc_set.zone.abs_ratio != \"None\":\n",
    "            itv = config.loc_set.point.tf_entry\n",
    "            abs_ratio_ = res_df['abs_ratio_{}'.format(itv)].to_numpy()\n",
    "            mr_res *= abs_ratio_ >= config.loc_set.zone.abs_ratio\n",
    "            # mr_res *= abs_ratio_ <= config.loc_set.zone.abs_ratio\n",
    "\n",
    "    # ------------ 2. imbalance_ratio ------------ #\n",
    "    if config.loc_set.zone.ir != \"None\":\n",
    "        itv = config.loc_set.point.tf_entry\n",
    "        itv_num = itv_to_number(itv)\n",
    "        if ep_loc_side == OrderSide.SELL:\n",
    "            short_ir_ = res_df['short_ir_{}'.format(itv)].to_numpy()\n",
    "            # short_ir_ = res_df['short_ir_{}'.format(itv)].shift(itv_num).to_numpy()\n",
    "\n",
    "            # mr_res *= short_ir_ >= config.loc_set.zone.ir     # greater\n",
    "            mr_res *= short_ir_ <= config.loc_set.zone.ir  # lesser\n",
    "            if show_detail:\n",
    "                sys_log.warning(\n",
    "                    \"short_ir_ <= config.loc_set.zone.ir : {:.5f} {:.5f} ({})\".format(short_ir_[c_i], config.loc_set.zone.ir, mr_res[c_i]))\n",
    "        else:\n",
    "            long_ir_ = res_df['long_ir_{}'.format(itv)].to_numpy()\n",
    "            # long_ir_ = res_df['long_ir_{}'.format(itv)].shift(itv_num).to_numpy()\n",
    "\n",
    "            # mr_res *= long_ir_ >= config.loc_set.zone.ir\n",
    "            mr_res *= long_ir_ <= config.loc_set.zone.ir\n",
    "            if show_detail:\n",
    "                sys_log.warning(\n",
    "                    \"long_ir_ <= config.loc_set.zone.ir : {:.5f} {:.5f} ({})\".format(long_ir_[c_i], config.loc_set.zone.ir, mr_res[c_i]))\n",
    "                \n",
    "        # if selection_id in ['3_6']:\n",
    "        #   itv, period1, period2 = config.loc_set.point.p1_itv0, config.loc_set.point.p1_period1, config.loc_set.point.p1_period2          \n",
    "\n",
    "        #   if ep_loc_side == OrderSide.SELL:\n",
    "        #     high_5T = res_df['high_5T'].to_numpy()  # Todo, tf_entry - 1 open 기준이라 future_data 사용 가능\n",
    "        #     short_base_ = res_df['short_base_{}{}{}'.format(itv, period1, period2)]\n",
    "        #     mr_res *= high_5T < short_base_\n",
    "        #     if show_detail:\n",
    "        #         sys_log.warning(\n",
    "        #             \"high_5T < short_base_ : {:.5f} {:.5f} ({})\".format(high_5T[c_i], short_base_[c_i], mr_res[c_i]))\n",
    "        #   else:\n",
    "        #     low_5T = res_df['low_5T'].to_numpy()  # Todo, tf_entry - 1 open 기준이라 future_data 사용 가능\n",
    "        #     long_base_ = res_df['long_base_{}{}{}'.format(itv, period1, period2)]\n",
    "        #     mr_res *= low_5T > long_base_\n",
    "        #     if show_detail:\n",
    "        #         sys_log.warning(\n",
    "        #             \"low_5T > long_base_ : {:.5f} {:.5f} ({})\".format(low_5T[c_i], long_base_[c_i], mr_res[c_i]))'\n",
    "        \n",
    "        # ------ dc_base ------ #\n",
    "        # if selection_id in ['4']:  # 'v3_3', 'v3_4',\n",
    "        #   hc_itv = '5T'\n",
    "        #   dc_itv = '5T'\n",
    "        #   itv_num = itv_to_number(hc_itv)\n",
    "        #   close_ = res_df['close_{}'.format(hc_itv)].shift(itv_num).to_numpy()   # 따라서 future_data 사용시, shifting 필요함\n",
    "        #   if ep_loc_side == OrderSide.SELL:\n",
    "        #     dc_lower_ = res_df['dc_lower_%s' % dc_itv].shift(itv_num).to_numpy()\n",
    "        #     mr_res *= close_ < dc_lower_\n",
    "        #   else:\n",
    "        #     dc_upper_ = res_df['dc_upper_%s' % dc_itv].shift(itv_num).to_numpy()\n",
    "        #     mr_res *= close_ > dc_upper_\n",
    "\n",
    "        # ------ ema ------ #\n",
    "        # if selection_id in ['v5_2']: # 'v3'\n",
    "        #   ema_5T = res_df['ema_5T'].to_numpy()\n",
    "        #   if ep_loc_side == OrderSide.SELL:\n",
    "        #     mr_res *= close < ema_5T\n",
    "        #   else:\n",
    "        #     mr_res *= close > ema_5T\n",
    "        \n",
    "        # ------ b. bb ------ #\n",
    "        # close = res_df['close'].to_numpy()\n",
    "\n",
    "        # if selection_id in ['v3_3']:\n",
    "        #   open = res_df['open'].to_numpy()\n",
    "        #   if ep_loc_side == OrderSide.SELL:\n",
    "        #     bb_lower_1m = res_df['bb_lower_1m'].to_numpy()\n",
    "        #     # mr_res *= close <= bb_lower_1m\n",
    "        #     mr_res *= open <= bb_lower_1m\n",
    "        #   else:\n",
    "        #     bb_upper_1m = res_df['bb_upper_1m'].to_numpy()\n",
    "        #     # mr_res *= close >= bb_upper_1m\n",
    "        #     mr_res *= open >= bb_upper_1m\n",
    "\n",
    "        if selection_id in ['4_1']:\n",
    "            if ep_loc_side == OrderSide.SELL:\n",
    "                bb_lower_15T = res_df['bb_lower_15T'].to_numpy()\n",
    "                short_ep_ = res_df['short_ep_{}'.format(selection_id)].to_numpy()\n",
    "                mr_res *= bb_lower_15T >= short_ep_\n",
    "            else:\n",
    "                bb_upper_15T = res_df['bb_upper_15T'].to_numpy()\n",
    "                long_ep_ = res_df['long_ep_{}'.format(selection_id)].to_numpy()\n",
    "                mr_res *= bb_upper_15T <= long_ep_\n",
    "\n",
    "        # if selection_id in ['v5_2']:\n",
    "        #   bb_upper2_ = res_df['bb_upper2_%s' % config.loc_set.zone.bbz_itv].to_numpy()\n",
    "        #   bb_lower2_ = res_df['bb_lower2_%s' % config.loc_set.zone.bbz_itv].to_numpy()\n",
    "        #   if ep_loc_side == OrderSide.SELL:\n",
    "        #     mr_res *= bb_upper2_ < close\n",
    "        #   else:\n",
    "        #     mr_res *= bb_lower2_ > close\n",
    "\n",
    "        # degree_list = literal_eval(config.loc_set.zone.degree_list)\n",
    "        # if len(degree_list) != 0:\n",
    "        # # if selection_id in ['v3_3', 'v3_4']:\n",
    "        #   norm_close_15 = res_df['norm_close_15'].to_numpy()   # -> 이거 뭘로 만들었는지 불분명함,,\n",
    "        #   b1_norm_close_15 = res_df['norm_close_15'].shift(15).to_numpy()\n",
    "\n",
    "        #   if ep_loc_side == OrderSide.SELL:\n",
    "        #     mr_res *= norm_close_15 <= -degree_list[0]\n",
    "        #     # mr_res *= b1_norm_close_15 <= -degree_list[1]\n",
    "        #   else:\n",
    "        #     mr_res *= norm_close_15 >= degree_list[0]\n",
    "        #     # mr_res *= b1_norm_close_15 >= degree_list[1]\n",
    "\n",
    "        # ------ b. dc ------ #\n",
    "        # if selection_id in ['v3_3']:\n",
    "        #   if ep_loc_side == OrderSide.SELL:\n",
    "        #     dc_lower_ = res_df['dc_lower_1m'].to_numpy()\n",
    "        #     b1_dc_lower_ = res_df['dc_lower_1m'].shift(1).to_numpy()\n",
    "        #     mr_res *= dc_lower_ < b1_dc_lower_\n",
    "        #   else:\n",
    "        #     dc_upper_ = res_df['dc_upper_1m'].to_numpy()\n",
    "        #     b1_dc_upper_ = res_df['dc_upper_1m'].shift(1).to_numpy()\n",
    "        #     mr_res *= dc_upper_ > b1_dc_upper_\n",
    "\n",
    "        # ------ c. sar ------ #\n",
    "        # if selection_id in ['v3_3']:\n",
    "        # sar_uptrend_3T = res_df['sar_uptrend_3T'].to_numpy()\n",
    "        # if ep_loc_side == OrderSide.SELL:\n",
    "        #   mr_res *= sar_uptrend_3T == 0\n",
    "        # else:\n",
    "        #   mr_res *= sar_uptrend_3T == 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JjKHyqftzhD7"
   },
   "source": [
    "### config paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1489,
     "status": "ok",
     "timestamp": 1666567980466,
     "user": {
      "displayName": "7th June",
      "userId": "08178289703395036410"
     },
     "user_tz": -540
    },
    "id": "q_4E-zH02WJy"
   },
   "outputs": [],
   "source": [
    "\n",
    "param_dict = {\n",
    "  \"selection_id\": \"1\",\n",
    "  \"trader_set\": {\n",
    "    \"backtrade\": 1,\n",
    "    \"back_data_path\": \"D:\\\\Projects\\\\SystemTrading\\\\JnQ\\\\database\\\\binance\\\\cum\\\\2023-01-12\\\\2023-01-12 ETHUSDT_1m.ftr\",\n",
    "    \"start_datetime\": \"2022-12-12 00:00:59.999\",\n",
    "    \"run\": 1,\n",
    "    \"df_log\": 0,\n",
    "    \"show_detail\": 0,\n",
    "    \"latest_index\": -1,\n",
    "    \"complete_index\": -2,\n",
    "    \"limit_fee\": 0.0002,\n",
    "    \"market_fee\": 0.0004,\n",
    "    \"initial_asset\": 10,\n",
    "    \"profit_mode\": \"PROD\",\n",
    "    \"asset_changed\": 0,\n",
    "    \"symbol\": \"ETHUSDT\",\n",
    "    \"symbol_changed\": 0,\n",
    "    \"token\": \"5859375131:AAHPzzz_Dv2OSxFsSOLChiXhfL0jN_6fOWU\",\n",
    "    \"messenger_on\": 0,\n",
    "    \"itv_list\": \"['T', '3T', '5T', '15T', '30T', 'H', '4H']\",\n",
    "    \"row_list\": \"[500, 1, 1, 1, 1, 1, 1]\",\n",
    "    \"rec_row_list\": \"[1, 1, 1, 1, 1, 1, 1]\",\n",
    "    \"offset_list\": \"['1h', '1h', '1h', '1h', '1h', '1h', '1h']\",\n",
    "    \"loop_duration\": 3.6,\n",
    "    \"realtime_term\": 0.2,\n",
    "    \"api_term\": 1,\n",
    "    \"order_term\": 0.5,\n",
    "    \"market_check_term\": 5,\n",
    "    \"open_exec_check_term\": 5,\n",
    "    \"open_exec_qty_ratio\": 0.97,\n",
    "    \"tp_exec_check_term\": 5\n",
    "     }, \n",
    "    \"pos_set\": {\n",
    "      \"short_inversion\": 0,\n",
    "      \"long_inversion\": 0,\n",
    "      \"short_ban\": 0,\n",
    "      \"long_ban\": 0,\n",
    "      \"short_fake\": 0,\n",
    "      \"long_fake\": 0\n",
    "    }, \n",
    "    \"loc_set\": {\n",
    "      \"point1\": {\n",
    "        \"exp_itv\": \"5T\",\n",
    "        \"tf_entry\": \"15T\",\n",
    "        \"candle_pattern\": \"CDLMARUBOZU\",\n",
    "          \n",
    "        \"spread_min_short\": 0.025,\n",
    "        \"spread_max_short\": 0.5,\n",
    "        \"spread_min_long\": 0.025,\n",
    "        \"spread_max_long\": 0.5,\n",
    "          \n",
    "        \"lvrg_k\": 1,\n",
    "        \"lvrg_ceiling\": 1,\n",
    "        \"lvrg_min_short\": 0,\n",
    "        \"lvrg_max_short\": 2,\n",
    "        \"lvrg_min_long\": 0,\n",
    "        \"lvrg_max_long\": 2,\n",
    "          \n",
    "        \"tr_min_short\": 0,\n",
    "        \"tr_max_short\": 0.3,\n",
    "        \"tr_min_long\": 0,\n",
    "        \"tr_max_long\": 0.3,\n",
    "          \n",
    "        \"wick_ratio_short\": \"None\",\n",
    "        \"wick_ratio_long\": \"None\",\n",
    "        \"wick_itv\": \"5T\",\n",
    "        \"wrr_10\": \"None\",\n",
    "        \"wrr_21\": \"None\",\n",
    "        \"wrr_32_min_short\": \"None\",\n",
    "        \"wrr_32_max_short\": 0.3,\n",
    "        \"wrr_32_min_long\": \"None\",\n",
    "        \"wrr_32_max_long\": 0.3,\n",
    "          \n",
    "        \"co_es\": \"None\",\n",
    "        \"cu_es\": \"None\",\n",
    "        \"crr\": \"None\",\n",
    "        \"cppr\": \"None\",\n",
    "        \"ppr\": \"None\",\n",
    "        \"wbr\": \"None\",\n",
    "        \"dbr\": \"None\",\n",
    "        \"dbr2\": \"None\",\n",
    "        \"brr\": \"None\",\n",
    "        \"ir\": \"None\",\n",
    "        \"abs_ratio\": \"None\"\n",
    "      },\n",
    "      \"point2\": {\n",
    "        \"wrr_32_min\": \"None\",\n",
    "        \"wrr_32_max\": \"None\",\n",
    "        \"csdbox_range\": 0.3,\n",
    "        \"tr_thresh_short\": \"None\",\n",
    "        \"tr_thresh_long\": \"None\",\n",
    "        \"csd_period\": \"None\"\n",
    "      },\n",
    "      \"zone1\": {\n",
    "        \"use_zone\": 0,\n",
    "        \"base_roll_period\": 50,\n",
    "        \"degree_list\": \"[]\",\n",
    "        \"dtk_itv\": \"5T\",\n",
    "        \"dt_k\": \"None\",\n",
    "        \"dc_period\": 135,\n",
    "        \"use_dtk_line\": 0,\n",
    "        \"zone_dt_k\": 0.4,\n",
    "        \"zone_dc_period\": 135\n",
    "      },\n",
    "      \"zone2\": {\n",
    "        \"use_zone\": 0\n",
    "      }\n",
    "    }, \n",
    "    \"tr_set\": {\n",
    "      \"check_hlm\": 0,\n",
    "      \"wave_itv1\": \"T\",\n",
    "      \"wave_period1\": 20,\n",
    "      \"wave_length_min_short1\": \"None\",\n",
    "      \"wave_length_max_short1\": \"None\",\n",
    "      \"wave_length_min_long1\": \"None\",\n",
    "      \"wave_length_max_long1\": \"None\",\n",
    "      \"wave_spread1\": \"None\",\n",
    "      \"wave_time_ratio1\": \"None\",\n",
    "      \"wave_itv2\": \"T\",\n",
    "      \"wave_period2\": 20,\n",
    "      \"tc_period\": 20,\n",
    "      \"wave_greater1\": 0,\n",
    "      \"wave_greater2\": 0,\n",
    "      \"wave_lesser1\": 2,\n",
    "      \"wave_lesser2\": 2,\n",
    "      \"expire_k1\": 0.0,\n",
    "      \"expire_k2\": 0.0,\n",
    "      \"expire_tick\": \"None\",\n",
    "      \"p2_box_k1\": 0,\n",
    "      \"p2_box_k2\": 0,\n",
    "      \"p1p2_low\": 0.0,\n",
    "      \"tp_gap\": 0.0,\n",
    "      \"ep1_gap\": 0.7,\n",
    "      \"ep2_gap\": 0.3,\n",
    "      \"out_gap\": 0,\n",
    "      \"decay_gap\": \"None\",\n",
    "      \"c_ep_gap\": \"None\",\n",
    "      \"t_out_gap\": \"None\",\n",
    "      \"wb_tp_gap\": 0,\n",
    "      \"wb_out_gap\": 0,\n",
    "      \"bias_tick\": 100\n",
    "    }, \n",
    "    \"ep_set\": {\n",
    "      \"entry_type\": \"LIMIT\",\n",
    "      \"static_ep\": 1,\n",
    "      \"point2\": {\n",
    "        \"entry_type\": \"LIMIT\"\n",
    "      }\n",
    "    }, \n",
    "    \"tp_set\": {\n",
    "      \"non_tp\": 0,\n",
    "      \"static_tp\": 1,\n",
    "      \"tp_onexec\": 0,\n",
    "      \"decay_term\": 60,\n",
    "      \"partial_ranges\": \"[1]\",\n",
    "      \"partial_qty_ratio\": \"[1]\"\n",
    "    }, \n",
    "    \"out_set\": {\n",
    "      \"non_out\": 0,\n",
    "      \"hl_out\": 1,\n",
    "      \"static_out\": 1,\n",
    "      \"out_onexec\": 0,\n",
    "      \"tf_exit\": \"None\",\n",
    "      \"fisher_exit\": \"None\",\n",
    "      \"rsi_exit\": 0,\n",
    "      \"cci_exit\": 0\n",
    "    }, \n",
    "    \"lvrg_set\": {\n",
    "      \"static_lvrg_short\": 1,\n",
    "      \"static_lvrg_long\": 1,\n",
    "      \"limit_leverage\": \"None\",\n",
    "      \"leverage\": 1,\n",
    "      \"target_pct\": 0.05,\n",
    "      \"allow_float\": 0,\n",
    "      \"lvrg_rejection\": 0\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MuD_2vY7TI_8",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### legacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EKag94Y2TMCO"
   },
   "outputs": [],
   "source": [
    ",\n",
    "      \"hc_itv\": 60,\n",
    "      \"osc_band\": 20\n",
    "      \n",
    "      \"wick_score_list\": \"[]\",\n",
    "      \"body_score_list\": \"[]\",\n",
    "      \"score_itv_list\": \"[]\",,\n",
    "\n",
    "      \"wick_score_list\": \"[]\",\n",
    "      \"body_score_list\": \"[]\",\n",
    "      \"score_itv_list\": \"['T']\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6HOjnZjSgzk1"
   },
   "source": [
    "## Load database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_name = '2023-01-12 ETHUSDT'  # 2023-01-12 ETHUSDT '2023-09-19 ETHUSDT' / 2023-02-21 FTMUSDT  2023-02-21 ALICEUSDT /2022-04-27 ETH / 2023-02-20 BTC 2023-03-23 KRW-ETH_1m 2023-03-23 KRW-SSX_1m\n",
    "# data_name = '2023-09-19 ETHUSDT'\n",
    "# data_name = '2023-11-02 ETHUSDT'\n",
    "# data_name = '2023-10-17 ADAUSDT'\n",
    "data_name = '2024-02-14 ADAUSDT'\n",
    "data_name = '2024-02-15 ADAUSDT'\n",
    "\n",
    "\n",
    "date, ticker = data_name.split(\" \")\n",
    "\n",
    "mode = \"CRYPTO\"\n",
    "database_type = 'database/binance/'  # 'binance/' kiwoom upbit\n",
    "file_system = \"ftr\" if mode == \"CRYPTO\" else \"pkl\"\n",
    "\n",
    "\"\"\"\n",
    "database 는 JnQ 내부로 통일할 것.\n",
    "\"\"\"\n",
    "database_dir_path = os.path.join(pkg_path, database_type, \"cum\", date).replace(\"JnQ_32bit\", \"JnQ\")  # cum non_cum -> use, non_cum data for backtrade validation\n",
    "# database_dir_path = os.path.join(pkg_path, database_type, \"non_cum\", date).replace(\"JnQ_32bit\", \"JnQ\")\n",
    "data_list = [s for s in os.listdir(database_dir_path) if file_system in s if date in s if ticker in s]\n",
    "print(data_list)\n",
    "\n",
    "start_time = time.time()\n",
    "key = data_list[0]  # tempoaray use single key\n",
    "data_path = os.path.join(database_dir_path, key)\n",
    "\n",
    "if mode == \"CRYPTO\":\n",
    "    res_df_ = pd.read_feather(data_path, columns=None, use_threads=True).set_index(\"index\")\n",
    "else:\n",
    "    res_df_ = pd.read_pickle(data_path)\n",
    "    \n",
    "# print(res_df_.head())\n",
    "print(data_path, \"loaded !\")\n",
    "print(\"load res_df_ elapsed time :\", time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RQH_l4opEh_O"
   },
   "outputs": [],
   "source": [
    "print(res_df_.dtypes)\n",
    "print(res_df_.index[[0, 1, -1]]) # '2021-12-01 09:00:00'\n",
    "# len(res_df_)\n",
    "# res_df_.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x2yj2SwAXDLp",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### edit cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1657898275247,
     "user": {
      "displayName": "7th June",
      "userId": "08178289703395036410"
     },
     "user_tz": -540
    },
    "id": "x9wkWw15XCAm",
    "outputId": "cf3fee46-d616-45e7-a3e2-3e0683513582"
   },
   "outputs": [],
   "source": [
    "col_list = list(res_df_.columns)\n",
    "\n",
    "# ------ check target cols ------ #\n",
    "# print([col_ for col_ in col_list if 'stoch' in col_])\n",
    "\n",
    "# ------ drop ------ #\n",
    "# res_df_.drop([col_ for col_ in col_list if 'open_15T' in col_], inplace=True, axis=1)\n",
    "res_df_.drop([col_ for col_ in col_list if 'bb' in col_], inplace=True, axis=1)\n",
    "# res_df_.drop([col_ for col_ in col_list if 'min' in col_], inplace=True, axis=1)\n",
    "# res_df_.drop([col_ for col_ in col_list if 'ma' in col_], inplace=True, axis=1)\n",
    "# res_df_.drop([col_ for col_ in col_list if 'long_base' in col_], inplace=True, axis=1)\n",
    "# res_df_.drop([col_ for col_ in col_list[5:]], inplace=True, axis=1)\n",
    "\n",
    "# ------ replace ------ #\n",
    "# for c_i, col_ in enumerate(col_list):\n",
    "#   if 'basis' in col_:\n",
    "# #   # if col_[-1] in ['m', 'h', 'd', 'H'] and '_' in col_:eTa_5T\n",
    "# #   # if col_[0] in ['h'] and '_' in col_:\n",
    "# #   if 'bir_' in col_:\n",
    "\n",
    "#     col_list[c_i] = col_.replace('basis', 'base')\n",
    "# #     # col_list[c_i] = col_.replace('m', 'T').replace('h', 'H').replace('1T', 'T')\n",
    "# #     # col_list[c_i] = col_.replace('1d', 'D')\n",
    "# #     # col_list[c_i] = col_.replace('eTa_5T', 'ema_5T')\n",
    "# #     # col_list[c_i] = col_list[c_i][1:]\n",
    "# #     # print(col_list[c_i][0])\n",
    "# res_df_.columns = col_list\n",
    "# col_list[-2:] = ['resi_T', 'sup_T']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rLI8unIyroiC"
   },
   "source": [
    "## Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 479,
     "status": "ok",
     "timestamp": 1666570076577,
     "user": {
      "displayName": "7th June",
      "userId": "08178289703395036410"
     },
     "user_tz": -540
    },
    "id": "qBJfPsmJzVIr",
    "outputId": "d0fabcac-50f2-416c-ec2b-5ee2353bdb1e"
   },
   "outputs": [],
   "source": [
    "config = EasyDict(param_dict)\n",
    "\n",
    "\"\"\"\n",
    "1. get_open_info_df_v2 에서 id_list, config_list를 유지하기 위해서는, override 형식을 유지하는게 맞고, override 한 내용만을 확인하기 위해서\n",
    "특정 idx 만을 선택하는 현재의 구조를 유지하는게 옳다고 봄.\n",
    "    a. 따라서, len(config_list) == 1 로 제한을 둔 것\n",
    "\"\"\"\n",
    "\n",
    "id_idx_list = [0]  # IDEP 에서 편집할 idx 선택, 원본 파일에 영향 주지는 않음\n",
    "public_override = 1\n",
    "utils_override = 1\n",
    "config_override = 1\n",
    "\n",
    "# ------ config_list 와 같은 org_var 에 override 하는거 다시 생각하기 ------ #\n",
    "id_list = id_arr[id_idx_list]\n",
    "utils_list = utils_arr[id_idx_list]\n",
    "config_list = config_arr[id_idx_list]\n",
    "\n",
    "if config_override or utils_override:\n",
    "  assert len(config_list) == 1\n",
    "  if config_override:    \n",
    "    config_list[0] = config\n",
    "    id_list[0] = config.selection_id  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RqRF1eyZ0xBL"
   },
   "source": [
    "### Edit configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 386,
     "status": "ok",
     "timestamp": 1666570021879,
     "user": {
      "displayName": "7th June",
      "userId": "08178289703395036410"
     },
     "user_tz": -540
    },
    "id": "afUV2b1jaggN"
   },
   "outputs": [],
   "source": [
    "# config.trader_set.start_datetime = \"2020-05-05 00:00:59.999\"\n",
    "config.trader_set.start_datetime = \"None\"\n",
    "\n",
    "# # 2020-05-05 00:00:59.999 <- all_in method ETH bank 의 세력 진입 분기점으로 봄, 이전 data 는 불규칙적임.\n",
    "\n",
    "\"\"\"\n",
    "p1_hhm 의 경우 out_box 를 위해 wave_itv 1 & 2 를 동일하게 설정해야함\n",
    "\"\"\"\n",
    "# # config_list[0].tr_set.wave_itv1 = 'T'\n",
    "# config_list[0].tr_set.wave_period1 = 20\n",
    "# # config_list[0].tr_set.wave_itv2 = '30T'\n",
    "# config_list[0].tr_set.wave_period2 = 20\n",
    "# # config_list[0].tr_set.tc_period = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1101,
     "status": "ok",
     "timestamp": 1666570079162,
     "user": {
      "displayName": "7th June",
      "userId": "08178289703395036410"
     },
     "user_tz": -540
    },
    "id": "WstWVNihCNH8",
    "outputId": "123f8ee6-04df-4bc8-ed0d-fc86309a9f62"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "res_df slicing : ta_lib 연산을 위해서, > double 의 data type 이 요구됨\n",
    "\"\"\"\n",
    "\n",
    "if config.trader_set.start_datetime != \"None\":\n",
    "    res_df = res_df_.astype(float).loc[pd.to_datetime(config.trader_set.start_datetime):]\n",
    "else:    \n",
    "    res_df = res_df_.astype(float)\n",
    "\n",
    "np_timeidx = np.array([intmin_np(date_) for date_ in res_df.index.to_numpy()])\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "public_indi : public 이니까, 첫번째 config 를 기준으로 작성해도 무방함.\n",
    "\"\"\"\n",
    "start_time = time.time()\n",
    "if public_override:\n",
    "    res_df = public_indi(res_df, config_list[0], np_timeidx)  # 현재 대부분의 시간은 h_candle 에서 소비되고 있음\n",
    "else:\n",
    "    res_df = bank.public.public_indi(res_df, config_list[0], np_timeidx)\n",
    "print(\"public_indi elapsed time :\", time.time() - start_time)\n",
    "\n",
    "\n",
    "# ------------ make data_list ------------ # - 반복될 이유가 없는 phase - public_indo 에 종속\n",
    "start_time = time.time()\n",
    "ohlc_cols = ['open', 'high', 'low', 'close']\n",
    "ohlc_list = [res_df[col_].to_numpy() for col_ in ohlc_cols]\n",
    "print(\"make data_list elapsed time :\", time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_df.dtypes\n",
    "# res_df.index[[0, -1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1666570030992,
     "user": {
      "displayName": "7th June",
      "userId": "08178289703395036410"
     },
     "user_tz": -540
    },
    "id": "_iYcJk8nK8Yq"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "1. p1's entry_type 잘살필 것\n",
    "\"\"\"\n",
    "\n",
    "# config_list[0].ep_set.entry_type = \"LIMIT\" # \"LIMIT\" # \"MARKET\"\n",
    "# config_list[0].ep_set.point2.entry_type = \"LIMIT\" # \"LIMIT\" # \"MARKET\"\n",
    "\n",
    "# config_list[0].tr_set.check_hlm = 0  # 0 : p1_hhm, 1: p1_hlm, 2 : p2_hlm  => p1_hlm 은 p1_hhm 의 p1_idx 를 유지하면서, hlm 을 확인하기 위함임.\n",
    "# if config_list[0].tr_set.check_hlm == 2:\n",
    "#     assert config_list[0].ep_set.entry_type == \"MARKET\"\n",
    "    \n",
    "# config_list[0].loc_set.point.candle_pattern = talib.get_function_groups()['Pattern Recognition'][51]   # \"None\" # 0.5 0.7\n",
    "config_list[0].loc_set.point1.lvrg_ceiling = 0\n",
    "# config_list[0].pos_set.short_ban = 0\n",
    "# config_list[0].pos_set.long_ban = 0\n",
    "\n",
    "# config_list[0].tr_set.wave_lesser = 3\n",
    "# config_list[0].tr_set.wave_length_min_short1 = \"None\" # 80  # \"None\" 50 45 15 110 100 \n",
    "# config_list[0].tr_set.wave_length_max_short1 = \"None\"  # \"None\" 45 15 110 100 \n",
    "# config_list[0].tr_set.wave_length_min_long1 = \"None\" # \"None\" 45 15 110 100 \n",
    "# config_list[0].tr_set.wave_length_max_long1 = \"None\"  # \"None\" 45 15 110 100 \n",
    "# config_list[0].tr_set.wave_spread1 = \"None\"  # \"None\" 15 110 100 \n",
    "# config_list[0].tr_set.wave_time_ratio1 = \"None\"  # \"None\" 6\n",
    "# config_list[0].tr_set.wave_greater1 = 0  # 0 50 \n",
    "# config_list[0].tr_set.wave_greater2 = 0  # 10\n",
    "# config_list[0].tr_set.p1_period1 = 5\n",
    "# config_list[0].tr_set.p1_period2 = 5\n",
    "# # config_list[0].tr_set.p2_period1 = 20\n",
    "# # config_list[0].tr_set.p2_period2 = 20\n",
    "\n",
    "# config_list[0].tr_set.tp_gap = -0.05 # 0.68\n",
    "# config_list[0].tr_set.ep1_gap = 0.5 # -0.8 -0.618 -0.23 -0.382 0.19 0.8 -0.12 -0.26\n",
    "# config_list[0].tr_set.ep2_gap = -0.5 # -0.618 -0.23 -0.382 0.19 \n",
    "# config_list[0].tr_set.out_gap = 1.4 # -0.48  # 0 -0.35 -0.6\n",
    "# config_list[0].tr_set.wb_tp_gap = 0.5\n",
    "# config_list[0].tr_set.wb_out_gap = -0.0\n",
    "\n",
    "# config_list[0].trader_set.limit_fee = 1e-10\n",
    "# config_list[0].trader_set.market_fee = 1e-10\n",
    "# config_list[0].trader_set.limit_fee = 0.0002\n",
    "# config_list[0].trader_set.market_fee = 0.0004\n",
    "# config_list[0].trader_set.limit_fee = 0.0005\n",
    "# config_list[0].trader_set.market_fee = 0.0005\n",
    "# config_list[0].trader_set.limit_fee = 0.00015\n",
    "# config_list[0].trader_set.market_fee = 0.00215\n",
    "# config_list[0].loc_set.point.short_wick_ratio = 0.2 # \"None\" # 2.5\n",
    "# config_list[0].loc_set.point.long_wick_ratio = 0.2 # \"None\" # 2.5\n",
    "# config_list[0].loc_set.point.crr = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 764,
     "status": "ok",
     "timestamp": 1666570079924,
     "user": {
      "displayName": "7th June",
      "userId": "08178289703395036410"
     },
     "user_tz": -540
    },
    "id": "iI39YI_5GguK",
    "outputId": "56609f9d-e6f8-44c7-cf40-a3eb8c1214ce"
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "if utils_override:   # 현재, utils_override 하는 경우 1개의 ID 만 허용함 \n",
    "    res_df = enlist_tr(res_df, config_list[0], np_timeidx, env='BANK')    # 36995.0 -> 152766.0 # 4044 np.sum(long_open_res == 1) : 4325\n",
    "else:\n",
    "    for utils_, config_ in zip(utils_list, config_list):\n",
    "        res_df = utils_.enlist_tr(res_df, config_, np_timeidx)\n",
    "        \n",
    "print(\"enlist_tr elapsed time :\", time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1666569812768,
     "user": {
      "displayName": "7th June",
      "userId": "08178289703395036410"
     },
     "user_tz": -540
    },
    "id": "gfDSOGMd91rE"
   },
   "outputs": [],
   "source": [
    "# ------ edit loc_set config ------ #\n",
    "# config_list[0].loc_set.point1.wrr_10 = \"None\" # \"None\" 0.3\n",
    "# config_list[0].loc_set.point1.wrr_21 = \"None\" # \"None\" \n",
    "config_list[0].loc_set.point1.wrr_32_min_short = \"None\" # 0.7 # \"None\" 1 0.5 0.482 0.302\n",
    "config_list[0].loc_set.point1.wrr_32_min_long = \"None\" # 0.7 # \"None\" 1 0.5 0.482 0.302\n",
    "\n",
    "config_list[0].loc_set.point1.wrr_32_max_short = 1\n",
    "config_list[0].loc_set.point1.wrr_32_max_long = 1\n",
    "# config_list[0].loc_set.point1.wrr_32_max_short = \"None\" # 0.3 # 0.7 # \"None\" 1 0.5 0.482 0.302\n",
    "# config_list[0].loc_set.point1.wrr_32_max_long = \"None\" # 0.3 # 0.7 # \"None\" 1 0.5 0.482 0.302\n",
    "\n",
    "\n",
    "config_list[0].loc_set.point1.spread_min_short = 0.025  # \"None\" # 1.02 1.05 1.0054 # 1.0054 # \"None\" # --> default 1.0054\n",
    "config_list[0].loc_set.point1.spread_min_long = 0.025  # \"None\" 1.02  1.0054\n",
    "config_list[0].loc_set.point1.spread_min_short = \"None\"\n",
    "config_list[0].loc_set.point1.spread_min_long = \"None\"\n",
    "\n",
    "config_list[0].loc_set.point1.spread_max_short = 0.15   # \"None\" # 1.0054 # 1.0054 # \"None\" # --> default 1.0054\n",
    "config_list[0].loc_set.point1.spread_max_long = 0.15 # 1.0054 \"None\" 1.05\n",
    "config_list[0].loc_set.point1.spread_max_short = \"None\"\n",
    "config_list[0].loc_set.point1.spread_max_long = \"None\"\n",
    "\n",
    "\n",
    "config_list[0].loc_set.point1.lvrg_min_short = 0 # \"None\" # 1.02 1.05 1.0054 # 1.0054 # \"None\" # --> default 1.0054\n",
    "config_list[0].loc_set.point1.lvrg_min_long = 0 # \"None\" 1.02  1.0054\n",
    "config_list[0].loc_set.point1.lvrg_min_short = \"None\"\n",
    "config_list[0].loc_set.point1.lvrg_min_long = \"None\"\n",
    "\n",
    "config_list[0].loc_set.point1.lvrg_max_short = 2   # \"None\" # 1.0054 # 1.0054 # \"None\" # --> default 1.0054\n",
    "config_list[0].loc_set.point1.lvrg_max_long = 2 # 1.0054 \"None\" 1.05\n",
    "config_list[0].loc_set.point1.lvrg_max_short = \"None\"\n",
    "config_list[0].loc_set.point1.lvrg_max_long = \"None\"\n",
    "\n",
    "\n",
    "config_list[0].loc_set.point1.tr_min_short = 0.9 # \"None\" # 1.02 1.05 1.0054 # 1.0054 # \"None\" # --> default 1.0054\n",
    "config_list[0].loc_set.point1.tr_min_long = 0.9 # \"None\" 1.02  1.0054\n",
    "# config_list[0].loc_set.point1.tr_min_short = \"None\"\n",
    "# config_list[0].loc_set.point1.tr_min_long = \"None\"\n",
    "\n",
    "config_list[0].loc_set.point1.tr_max_short = 1.5   # \"None\" # 1.0054 # 1.0054 # \"None\" # --> default 1.0054\n",
    "config_list[0].loc_set.point1.tr_max_long = 1.5 # 1.0054 \"None\" 1.05\n",
    "config_list[0].loc_set.point1.tr_max_short = \"None\"\n",
    "config_list[0].loc_set.point1.tr_max_long = \"None\"\n",
    "\n",
    "\n",
    "# config_list[0].loc_set.point1.dsc_ratio = 0.8\n",
    "# config_list[0].loc_set.point2.wrr_32 = \"None\" # \"None\" 1 0.5 0.382 0.302 0.25\n",
    "# config_list[0].loc_set.point2.csd_period = \"None\"  # \"None\" 100\n",
    "config_list[0].loc_set.zone1.use_zone = 0\n",
    "# config_list[0].loc_set.zone1.bb_trend_period = 150\n",
    "# config_list[0].loc_set.zone1.hl_loc_pct = \"None\" # \"None\" 1 0.5\n",
    "# config_list[0].loc_set.zone2.use_zone = 0\n",
    "# config_list[0].loc_set.point1.cu_es = \"None\" # \"None\" # -2\n",
    "# config_list[0].loc_set.point1.co_es = \"None\" # \"None\" # -3\n",
    "# config_list[0].loc_set.point1.cppr = 0.5   # \"None\" # 0.5 0.7\n",
    "# config_list[0].loc_set.point1.wbr = \"None\" # 0.7\n",
    "# config_list[0].loc_set.point1.dbr = \"None\"   # 0.7\n",
    "# config_list[0].loc_set.point1.dbr2 = \"None\"  # 0.7\n",
    "# config_list[0].loc_set.point1.brr = \"None\"   # 0.8\n",
    "# config_list[0].loc_set.point1.ir = \"None\" # \"None\" 0.8\n",
    "# config_list[0].loc_set.point1.wick_score_list = \"[]\"\n",
    "# config_list[0].loc_set.point1.score_itv_list = \"['H']\"\n",
    "# config_list[0].loc_set.point1.abs_ratio = \"None\"  # 0.7\n",
    "# config_list[0].loc_set.point1.short_tr_thresh = \"None\"  #  \"None\" 0.5 2 0.8 # # 0.7  5 # tr_thresh 엄청 민감함\n",
    "# config_list[0].loc_set.point1.long_tr_thresh = 5  #  \"None\" 2 0.8 ## 0.7 5 \n",
    "# # # config_list[0].loc_set.zone1.base_roll_period = 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1741,
     "status": "ok",
     "timestamp": 1666570082803,
     "user": {
      "displayName": "7th June",
      "userId": "08178289703395036410"
     },
     "user_tz": -540
    },
    "id": "9DPgykxQ92mU",
    "outputId": "36d777e8-db04-41d9-97d5-83201d40a6f4"
   },
   "outputs": [],
   "source": [
    "open_info_df1 = get_open_info_df_v2(ep_loc_p1_v3, res_df, np_timeidx, id_list, config_list, id_idx_list, open_num=1)  # --> point * dur. 관련 (loc_set) param 에 종속 (open_info 가 변경되는게 아니라면, 재실행할 필요없음)\n",
    "open_info_df2 = get_open_info_df_v2(ep_loc_p2_v3, res_df, np_timeidx, id_list, config_list, id_idx_list, open_num=2)\n",
    "open_info_df_list = [open_info_df1, open_info_df2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1666568396877,
     "user": {
      "displayName": "7th June",
      "userId": "08178289703395036410"
     },
     "user_tz": -540
    },
    "id": "KqVkg236t_f2"
   },
   "outputs": [],
   "source": [
    "# ------ edit entry & exit (ep, tp, out, ..) config ------ #\n",
    "# config_list[0].tr_set.expire_k1 = 0.0\n",
    "# config_list[0].tr_set.expire_tick = \"None\"\n",
    "# config_list[0].tr_set.p2_box_k1 = 0.0  # 0 default --> 0 ~ 1 사이 값 사용 tp_1 로부터 떨어지는 거리\n",
    "# config_list[0].tr_set.p2_box_k2 = 0.0  # 0.5 0 default --> \"None\" 불가, 0 ~ 1 사이 값 사용 tp_0 로부터 떨어지는 거리, 본디 p2_box 는 p1_box 내부에 존재해야, 정확한 hhm 이 측정가능해짐\n",
    "# config_list[0].tr_set.p1p2_low = 0.5  # 0.5 0.7 0  0 is equal to \"None\", 마찬가지로 tp_0 로부터 떨어지는 거리\n",
    "\n",
    "# config_list[0].loc_set.point2.short_tr_thresh = \"None\" #1.5 # \"None\"  #  \"None\" 0.5 2 0.8 # # 0.7 # tr_thresh 엄청 민감함\n",
    "# config_list[0].loc_set.point2.long_tr_thresh = \"None\" #1.5 # \"None\"  #  \"None\" 2 0.8 ## 0.7\n",
    "\n",
    "# config_list[0].ep_set.point2.entry_type = \"LIMIT\"\n",
    "# config_list[0].ep_set.point2.wick_score_list = str([])\n",
    "\n",
    "# config_list[0].tp_set.static_tp = 1\n",
    "# config_list[0].tp_set.non_tp = 1 # 0 1\n",
    "# config_list[0].tp_set.partial_ranges = \"[1]\"\n",
    "# config_list[0].tp_set.partial_qty_ratio = \"[1]\"\n",
    "# config_list[0].tp_set.partial_ranges = \"[0.2, 0.66, 1]\"\n",
    "# config_list[0].tp_set.partial_qty_ratio = \"[0.25, 0.25, 0.5]\"\n",
    "# config_list[0].tp_set.partial_ranges = \"[0.5, 1]\"\n",
    "# config_list[0].tp_set.partial_qty_ratio = \"[0.25, 0.75]\"\n",
    "\n",
    "# config_list[0].out_set.non_out = 0  # 손절 기능 미사용 여부.\n",
    "# config_list[0].out_set.hl_out = 1  # hl_out vs close_out 을 의미한다. (종가 기준 close 여부)\n",
    "# config_list[0].out_set.cci_exit = 0\n",
    "# config_list[0].out_set.tf_exit = \"None\" # 15 \"None\"\n",
    "# config_list[0].out_set.fisher_exit = 1\n",
    "# config_list[0].out_set.fisher_band = 1.5\n",
    "\n",
    "config_list[0].lvrg_set.static_lvrg_short = 1\n",
    "config_list[0].lvrg_set.static_lvrg_long = 1\n",
    "config_list[0].lvrg_set.leverage = 1\n",
    "config_list[0].lvrg_set.target_pct = 0.05 # 0.1 0.03\n",
    "# config_list[0].lvrg_set.allow_float = 0\n",
    "config_list[0].lvrg_set.lvrg_rejection = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 664
    },
    "executionInfo": {
     "elapsed": 2055,
     "status": "ok",
     "timestamp": 1666570084856,
     "user": {
      "displayName": "7th June",
      "userId": "08178289703395036410"
     },
     "user_tz": -540
    },
    "id": "TvGs5mBxsuBK",
    "outputId": "6669fd7c-067f-4c8f-a364-d65f8e7982e9",
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Cautions\n",
    "    1. if lvrg_rejection = 0, spread 가 큰 경우, min_lvrg 1 이기 때문에 target_pct < min_pr 가능함.\n",
    "    2. liqd = exit 까지의 min_low / high 를 의미함. (long / short 기준), 따라서 zero 일 필요가 없다는 이야기.\n",
    "    3. long & short point 의 open_side 가 일치하는 경우, p2 side_check release 해줄 것.\n",
    "    4. hlm != 1.0 인 경우는 market 으로 인해 목표가에 도달했음에도 불구하고, pr > 1 이 되지 않는 경우. hlm() function 참조.\n",
    "\"\"\"\n",
    "\n",
    "# en_ex_pairing = en_ex_pairing_v9_44\n",
    "en_ex_pairing = en_ex_pairing_v9_6  # allow trade without continuity.\n",
    "funcs1 = [expiry_p1p2, expiry_tp, lvrg_liqd_set_v2, check_entry_v6_2, check_signal_out_v4, check_hl_out_v4, check_limit_tp_exec]     # adj. expiry_tp & expiry_wave\n",
    "# funcs1 = [expiry_p1p2, expiry_tp, lvrg_liqd_set_v2, check_entry_v6_3, check_signal_out_v4, check_hl_out_v4, check_limit_tp_exec_v3]  # 보수적 검증 (체결 확률 100%) : check_entry_v6_3 & check_limit_tp_exec_v3\n",
    "# funcs1 = [expiry_p1p2, expiry_tp, lvrg_liqd_set_v4, check_entry_v6_2, check_signal_out_v4, check_hl_out_v4, check_limit_tp_exec]       # adj. lvrg_liqd_set_v4 (for DS, lvrg_needed serialized adj.)\n",
    "# funcs1 = [expiry_p1p2, expiry_tp, lvrg_liqd_set_v2, check_entry_v6_2, check_signal_out_v5_1, check_hl_out_v4, check_limit_tp_exec]   # adj. check_signal_out_v5_1 : cu / co exit added.\n",
    "\n",
    "idep_plot = idep_plot_v16_7\n",
    "funcs2 = [get_wave_bias_v6_2, get_pr_v7, get_res_info_nb_v3, plot_info_v8_2, frq_dev_plot_v5]  # get_wave_bias_v6_1 / # 여기서 입력되는 get_res_info 는 signi. mode 에 사용됨.\n",
    "\n",
    "test_ratio = 0.0\n",
    "plot_is = 1  # insample\n",
    "signi = 0\n",
    "show_detail = 0\n",
    "\n",
    "short_pr, short_obj, short_lvrg_arr, short_fee_arr, short_tpout_arr, short_tr_arr, short_bias_arr, short_net_p1_bias_tick, short_p2exec_p1_bias_tick, short_net_p1_idx_arr, short_p2_idx_arr, short_tp_1, short_tp_0, short_out_1, short_out_0, short_ep2_0, \\\n",
    "      long_pr, long_obj, long_lvrg_arr, long_fee_arr, long_tpout_arr, long_tr_arr, long_bias_arr, long_net_p1_bias_tick, long_p2exec_p1_bias_tick, long_net_p1_idx_arr, long_p2_idx_arr, long_tp_1, long_tp_0, long_out_1, long_out_0, long_ep2_0 = \\\n",
    "get_res_v2(res_df, open_info_df_list, ohlc_list, config_list, np_timeidx, en_ex_pairing, funcs1, idep_plot, funcs2, test_ratio=test_ratio, plot_is=plot_is, signi=signi, show_detail=show_detail)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### print config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------- print config ------- #\n",
    "config_sets = ['selection_id', 'trader_set', 'pos_set', 'loc_set', 'tr_set', 'ep_set', 'tp_set', 'out_set', 'lvrg_set']\n",
    "for set_i, set_ in enumerate(config_sets):\n",
    "    if set_i == len(config_sets) - 1:\n",
    "        end = '\\n'\n",
    "    else:\n",
    "        end = ', \\n'\n",
    "    print('\"{}\": {}'.format(set_, json.dumps(config_list[0][set_], indent=2)), end=end)\n",
    "# _ = [print(key_ + \":\", json.dumps(config_list[0][key_], indent=1), end=',\\n') for key_ in ['selection_id', 'trader_set', 'pos_set', 'loc_set', 'tr_set', 'ep_set', 'tp_set', 'out_set', 'lvrg_set']] #  'trader_set',"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### save trade_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # 1. public.\n",
    "# res_df_data = res_df[input_cols].to_numpy()\n",
    "input_cols = ['open', 'high', 'low', 'close', 'cci_T20', 'cu_wrr_32_T20', 'co_wrr_32_T20', 'short_open1_1', 'long_open1_1', 'short_tp_1', 'long_tp_1', 'short_ep1_1', 'long_ep1_1', 'short_out_1', 'long_out_1']\n",
    "res_df_copy = res_df[input_cols].copy()\n",
    "res_df_copy.columns = ['시가', '고가', '저가', '종가', 'CCI', 'WRR32 (SHORT)', 'WRR32 (LONG)', '거래신호 (SHORT)', '거래신호 (LONG)', '목표가 (SHORT)', '목표가 (LONG)', '진입가 (SHORT)', '진입가 (LONG)', '손절가 (SHORT)', '손절가 (LONG)']\n",
    "\n",
    "# 2. separate.\n",
    "# if data_side == \"long\":\n",
    "p1_idx_arr_long = long_obj[-1].astype(int).ravel()\n",
    "ex_idx_arr_long = long_obj[-2].astype(int).ravel()\n",
    "pr_long = long_pr.ravel()\n",
    "ts_en_long = res_df.index[p1_idx_arr_long].map(lambda x : int(datetime.timestamp(x))).to_numpy()\n",
    "ts_ex_long = res_df.index[ex_idx_arr_long].map(lambda x : int(datetime.timestamp(x))).to_numpy()\n",
    "\n",
    "p1_idx_arr_short = short_obj[-1].astype(int).ravel()\n",
    "ex_idx_arr_short = short_obj[-2].astype(int).ravel()\n",
    "pr_short = short_pr.ravel()\n",
    "ts_en_short = res_df.index[p1_idx_arr_short].map(lambda x : int(datetime.timestamp(x))).to_numpy()\n",
    "ts_ex_short = res_df.index[ex_idx_arr_short].map(lambda x : int(datetime.timestamp(x))).to_numpy()              \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "p1_idx_arr_long has duplicated index.\n",
    "\"\"\"\n",
    "\n",
    "res_df_copy['거래일시 (시작, LONG)'] = np.nan\n",
    "res_df_copy['거래일시 (종료, LONG)'] = np.nan\n",
    "res_df_copy['거래번호 (시작, LONG)'] = np.nan\n",
    "res_df_copy['거래번호 (종료, LONG)'] = np.nan\n",
    "res_df_copy['수익률 (LONG)'] = np.nan\n",
    "\n",
    "res_df_copy['거래일시 (시작, SHORT)'] = np.nan\n",
    "res_df_copy['거래일시 (종료, SHORT)'] = np.nan\n",
    "res_df_copy['거래번호 (시작, SHORT)'] = np.nan\n",
    "res_df_copy['거래번호 (종료, SHORT)'] = np.nan\n",
    "res_df_copy['수익률 (SHORT)'] = np.nan\n",
    "\n",
    "res_df_copy['거래일시 (시작, LONG)'].iloc[p1_idx_arr_long] = ts_en_long\n",
    "res_df_copy['거래일시 (종료, LONG)'].iloc[ex_idx_arr_long] = ts_ex_long\n",
    "res_df_copy['거래번호 (시작, LONG)'].iloc[p1_idx_arr_long] = np.arange(len(p1_idx_arr_long))\n",
    "res_df_copy['거래번호 (종료, LONG)'].iloc[ex_idx_arr_long] = np.arange(len(ex_idx_arr_long))\n",
    "\n",
    "res_df_copy['거래일시 (시작, SHORT)'].iloc[p1_idx_arr_short] = ts_en_short\n",
    "res_df_copy['거래일시 (종료, SHORT)'].iloc[ex_idx_arr_short] = ts_ex_short\n",
    "res_df_copy['거래번호 (시작, SHORT)'].iloc[p1_idx_arr_short] = np.arange(len(p1_idx_arr_short))\n",
    "res_df_copy['거래번호 (종료, SHORT)'].iloc[ex_idx_arr_short] = np.arange(len(ex_idx_arr_short))\n",
    "\n",
    "res_df_copy['수익률 (LONG)'].iloc[ex_idx_arr_long] = pr_long\n",
    "res_df_copy['수익률 (SHORT)'].iloc[ex_idx_arr_short] = pr_short\n",
    "\n",
    "# save_path = \"D:\\Projects\\Richer\\QuantAnalyzer\\Project\\Wave\\Data\\{}.xlsx\".format('symbol')\n",
    "# res_df_copy.to_excel(save_path)\n",
    "# print(save_path, 'saved.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_df_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# res_df_copy.iloc[p1_idx_arr_short] # ['거래일시 (시작)'] p1_idx_arr_long\n",
    "# res_df_copy\n",
    "\n",
    "save_path = \"D:\\Projects\\Richer\\QuantAnalyzer\\Project\\Wave\\Data\\{}.xlsx\".format('symbol')\n",
    "res_df_copy.to_excel(save_path)\n",
    "print(save_path, 'saved.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "symbol = data_name.split(' ')[1].split('_')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spread_short = res_df['{}_spread_{}'.format('short', config.selection_id)].to_numpy()[p1_idx_arr_short]\n",
    "spread_long = res_df['{}_spread_{}'.format('long', config.selection_id)].to_numpy()[p1_idx_arr_long]\n",
    "\n",
    "tr_short = res_df['{}_tr_{}'.format('short', config.selection_id)].to_numpy()[p1_idx_arr_short]         \n",
    "tr_long = res_df['{}_tr_{}'.format('long', config.selection_id)].to_numpy()[p1_idx_arr_long]         \n",
    "\n",
    "wrr_32_short = res_df['cu_wrr_32_{}{}'.format(config.tr_set.wave_itv1, config.tr_set.wave_period1)].to_numpy()[p1_idx_arr_short]\n",
    "wrr_32_long = res_df['co_wrr_32_{}{}'.format(config.tr_set.wave_itv1, config.tr_set.wave_period1)].to_numpy()[p1_idx_arr_long]\n",
    "\n",
    "wave_length_short = res_df['short_wave_length_fill_{}{}'.format(config.tr_set.wave_itv1, config.tr_set.wave_period1)].to_numpy()[p1_idx_arr_short]\n",
    "wave_length_long = res_df['long_wave_length_fill_{}{}'.format(config.tr_set.wave_itv1, config.tr_set.wave_period1)].to_numpy()[p1_idx_arr_long]\n",
    "\n",
    "# columns = ['symbol', 'position', 'trade_number', 'ts_en', 'ts_ex', 'spread', 'tr', 'wrr_32', 'wave_length', 'pr']\n",
    "columns = ['종목', '포지션', '거래번호', '거래일시 (시작)', '거래일시 (종료)', '스프레드', '손절 대비 수익비', 'WRR32', '파동 길이', '수익률']\n",
    "\n",
    "# print(\"ts_en_long.shape :\", ts_en_long.shape)\n",
    "# print(\"spread_long.shape :\", spread_long.shape)\n",
    "# print(\"tr_long.shape :\", tr_long.shape)\n",
    "# print(\"wrr_32_long.shape :\", wrr_32_long.shape)\n",
    "# print(\"wave_length_long.shape :\", wave_length_long.shape)\n",
    "# print(\"pr_long.shape :\", pr_long.shape)\n",
    "\n",
    "p1_idx_arr_long_len = len(p1_idx_arr_long)\n",
    "p1_idx_arr_short_len = len(p1_idx_arr_short)\n",
    "\n",
    "data_long = np.array([np.full(p1_idx_arr_long_len, symbol), np.full(p1_idx_arr_long_len, 'LONG'), np.arange(p1_idx_arr_long_len), ts_en_long, ts_ex_long, spread_long, tr_long, wrr_32_long, wave_length_long, pr_long]).T\n",
    "data_short = np.array([np.full(p1_idx_arr_short_len, symbol), np.full(p1_idx_arr_short_len, 'SHORT'), np.arange(p1_idx_arr_short_len), ts_en_short, ts_ex_short, spread_short, tr_short, wrr_32_short, wave_length_short, pr_short]).T\n",
    "# print(\"data_long.shape :\", data_long.shape)\n",
    "\n",
    "df = pd.DataFrame(np.vstack([data_long, data_short]), columns=columns)\n",
    "df.iloc[:, 2:] = df.iloc[:, 2:].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.info()\n",
    "# df.min()\n",
    "# df.max()\n",
    "\n",
    "ts_gap_max = df['거래일시 (시작)'].max()\n",
    "ts_gap_min = df['거래일시 (시작)'].min()\n",
    "ts_gap = ts_gap_max - ts_gap_min\n",
    "\n",
    "ts_ratio_train = 0.5\n",
    "ts_ratio_val = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_train_last = ts_gap_min + ts_gap * ts_ratio_train\n",
    "ts_val_last = ts_gap_min + ts_gap * ts_ratio_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['익절 여부'] = df['수익률'] > 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['데이터 타입'] = np.where(df['거래일시 (시작)'] < ts_val_last, 'VAL', 'TEST')\n",
    "df['데이터 타입'] = np.where(df['거래일시 (시작)'] < ts_train_last, 'TRAIN', df['데이터 타입'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.agg(['max', 'min'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unit_wrr32 = np.arange(0, 1.01, 0.3)\n",
    "unit_spread = np.arange(0, 0.6, 0.01)\n",
    "unit_tr = np.arange(0, 2.8, 0.1)\n",
    "unit_wave_length = np.arange(0, 250, 50)\n",
    "\n",
    "df['WRR32_category'] = pd.cut(df['WRR32'], unit_wrr32, precision=0, duplicates='drop').astype(str)\n",
    "df['spread_category'] = pd.cut(df['스프레드'], unit_spread, precision=0, duplicates='drop').astype(str)\n",
    "df['tr_category'] = pd.cut(df['손절 대비 수익비'], unit_tr, precision=0, duplicates='drop').astype(str)\n",
    "df['wave_length_category'] = pd.cut(df['파동 길이'], unit_wave_length, precision=0, duplicates='drop').astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = ['WRR32_category']\n",
    "index = ['스프레드_category']\n",
    "# index = ['wave_length_category']\n",
    "\n",
    "# index = ['WRR32_category', '데이터 타입']\n",
    "index = ['WRR32_category', '스프레드_category']\n",
    "\n",
    "# index = ['WRR32_category', '스프레드_category', '데이터 타입']\n",
    "# index = ['WRR32_category', '스프레드_category', 'tr_category', '데이터 타입']\n",
    "# index = ['WRR32_category', '스프레드_category', 'wave_length_category', '데이터 타입']\n",
    "\n",
    "df_pivot = df.pivot_table(index=index, values='익절 여부', aggfunc=['sum', 'count', 'mean'])\n",
    "df_pivot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_pivot.values # .shape\n",
    "df_pivot.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### check adj. condition result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# res_df['short_open1_{}'.format(config.selection_id)]# = short_open_res1 * (not config.pos_set.short_ban)\n",
    "# res_df['long_open1_{}'.format(config.selection_id)]# = long_open_res1 * (not config.pos_set.long_ban)\n",
    "# res_df['long_open1_{}'.format(config.selection_id)][list(open_info_df1[open_info_df1.side==1].index)]\n",
    "long_op_idxes = list(open_info_df1[open_info_df1.side==1].index)\n",
    "\n",
    "long_spread_ = res_df['long_spread_{}'.format(config.selection_id)].to_numpy()\n",
    "long_spread_op = long_spread_[long_op_idxes]\n",
    "\n",
    "long_lvrg_needed_ = res_df['long_lvrg_needed_{}'.format(config.selection_id)].to_numpy()\n",
    "long_lvrg_needed_op = long_lvrg_needed_[long_op_idxes]\n",
    "\n",
    "long_tr_ = res_df['long_tr_{}'.format(config.selection_id)].to_numpy()\n",
    "long_tr_op = long_tr_[long_op_idxes]\n",
    "\n",
    "print(len(long_spread_op))\n",
    "# print(len(long_lvrg_needed_op))\n",
    "# long_spread_op\n",
    "# long_lvrg_needed_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,3))\n",
    "plt.subplot(121)\n",
    "plt.scatter(np.arange(len(long_spread_)), long_spread_, s=5)\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.scatter(np.arange(len(long_spread_op)), long_spread_op, s=5)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10,3))\n",
    "plt.subplot(121)\n",
    "plt.scatter(np.arange(len(long_lvrg_needed_)), long_lvrg_needed_, s=5)\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.scatter(np.arange(len(long_lvrg_needed_op)), long_lvrg_needed_op, s=5)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10,3))\n",
    "plt.subplot(121)\n",
    "plt.scatter(np.arange(len(long_tr_)), long_tr_, s=5)\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.scatter(np.arange(len(long_tr_op)), long_tr_op, s=5)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_cols = ['open', 'high', 'low', 'close', 'ma_T50', 'ma_T200'] #, 'long_open1_1']\n",
    "\n",
    "data_size = 200\n",
    "cols_len = len(input_cols)\n",
    "flatten_len = data_size * cols_len\n",
    "\n",
    "# min\n",
    "# (v - v.min()) / (v.max() - v.min())\n",
    "res_df_data = res_df[input_cols].to_numpy()\n",
    "\n",
    "# data_side = \"long\" # short\n",
    "\n",
    "# if data_side == \"long\":\n",
    "#     p1_idx_arr = long_obj[-1].astype(int).ravel()\n",
    "#     y_train_1 = long_pr\n",
    "# else:\n",
    "#     p1_idx_arr = short_obj[-1].astype(int).ravel()\n",
    "#     y_train_1 = short_pr\n",
    "\n",
    "x_data_1 = []\n",
    "for p1 in long_op_idxes:\n",
    "# for p1 in short_p1_idx_arr:\n",
    "\n",
    "    # start_time = time.time()\n",
    "    # data_np = res_df[input_cols].iloc[p1 + 1 - data_size:p1 + 1].to_numpy()\n",
    "    data_np = res_df_data[p1 + 1 - data_size:p1 + 1]\n",
    "    # plt.plot(data_np[:, -2:])\n",
    "    # plt.show()\n",
    "\n",
    "    # data_np_norm = min_max_scaler(data_np)\n",
    "    # plt.plot(data_np_norm[:, -2:])\n",
    "    # plt.show()\n",
    "    x_data_1.append(data_np)\n",
    "    # print(data_np_norm.shape)\n",
    "    # break\n",
    "    # print(\"elapsed time : {}\".format(time.time() - start_time)) # 0.012034177780151367 --> 0.0009989738464355469s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "res_df_data_ml = np.array(x_data_1).reshape(-1, data_size * cols_len)\n",
    "# x_train = x_data_train # .shape\n",
    "res_df_data_ml.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "max_value = np.tile(np.max(res_df_data_ml, axis=1).reshape(-1, 1), flatten_len)\n",
    "min_value = np.tile(np.min(res_df_data_ml, axis=1).reshape(-1, 1), flatten_len)\n",
    "print(max_value.shape)\n",
    "print(min_value.shape)\n",
    "\n",
    "res_df_data_ml = (res_df_data_ml - min_value) / (max_value - min_value)\n",
    "print(res_df_data_ml.min(), res_df_data_ml.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# res_df_data_ml, key_data\n",
    "res_df_cos_sim = cosine_similarity(res_df_data_ml, key_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# np.sum(res_df_cos_sim > 0.99, axis=1)\n",
    "close_index = np.argwhere(np.sum(res_df_cos_sim > 0.99, axis=1))\n",
    "# len(np.sum(res_df_cos_sim > 0.99, axis=1)) # .shape# .ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "long_op_idxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "np.array(long_op_idxes)[close_index.ravel()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "valid_rows = []\n",
    "long_op_idxes_ml = np.array(long_op_idxes)[close_index.ravel()]\n",
    "\n",
    "for d_i, row_ in open_info_df1.iterrows():\n",
    "# for d_i, row_ in enumerate(open_info_df1):\n",
    "    # print(row_.name)\n",
    "    # print(type(row_))\n",
    "    if row_.name in long_op_idxes_ml:\n",
    "        valid_rows.append(row_)\n",
    "    # break\n",
    "    \n",
    "open_info_df1_ml = pd.DataFrame(valid_rows)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "open_info_df_list = [open_info_df1_ml, open_info_df2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8lYgsqH-rfAM",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ------ inversion ------ #\n",
    "# _ = get_res_v5(res_df, open_info_df, ohlc_list, config_list, np_timeidx, funcs, inversion=True, test_ratio=test_ratio, plot_is=1, signi=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### on multiple ticker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def idep_on_multiple_ticker(database_dir_abspath, data_list, signi=True, ml=False, excel=False):    \n",
    "    \n",
    "    input_cols = ['open', 'high', 'low', 'close'] #, 'ma_T50', 'ma_T200', 'ma_T800'] #, 'long_open1_1']    \n",
    "    cols_len = len(input_cols)\n",
    "    \n",
    "    data_side = \"long\"    \n",
    "    # data_side = \"short\"    \n",
    "    data_size = 500\n",
    "    flatten_len = data_size * cols_len    \n",
    "    \n",
    "        \n",
    "    dir_path = r\"D:\\Projects\\SystemTrading\\JnQ\\database\\binance\\ML\"    \n",
    "    dir_name = \"wrr32_03\\\\epg_epbox\\\\{}\\\\data\\\\\".format(data_side)\n",
    "    os.makedirs(os.path.join(dir_path, dir_name), exist_ok=True)\n",
    "    # dir_name = \"high_tr\\\\fisher\\\\tr_non_fix\\\\wrr32_None\\\\static_lvrg\\\\{}\\\\data\".format(data_side)    \n",
    "\n",
    "    \n",
    "    sub_title_list = ['hhm', 'hlm', 'frq', 'dpf', 'wr', 'sr', 'acc_pr', 'sum_pr', 'min_pr', 'liqd', 'acc_mdd', 'sum_mdd_prod', 'sum_mdd_sum']\n",
    "    sub_title_list_long = [col + '_long' for col in sub_title_list]\n",
    "    sub_title_list_short = [col + '_short' for col in sub_title_list]    \n",
    "    sub_title_list_len = len(sub_title_list)    \n",
    "    \n",
    "    res_list_long = []\n",
    "    res_list_short = []\n",
    "    \n",
    "    for r_, file_name in enumerate(data_list):\n",
    "\n",
    "        print(\"# ------------ rank : {}, file_name : {} ------------ #\".format(r_, file_name))\n",
    "\n",
    "        # while 1:  # memory allocation 문제로 진행했으나, while 은 해결책이 아님 (eternal loop+\n",
    "        try:                \n",
    "            start_time = time.time()\n",
    "            if idep_mode == \"CRYPTO\":\n",
    "                symbol = file_name.split(' ')[1].split('_')[0] # 2023-12-13 1INCHUSDT_1m_cls.pkl\n",
    "                data_path = os.path.join(database_dir_abspath, \"{}\".format(file_name))\n",
    "                res_df_ =pd.read_feather(data_path, columns=None, use_threads=True).set_index(\"index\")\n",
    "            else:\n",
    "                data_path = os.path.join(database_dir_abspath, \"{} {}.pkl\".format(date, file_name))\n",
    "                res_df_ = pd.read_pickle(data_path)\n",
    "                \n",
    "            print(\"symbol : {}\".format(symbol))\n",
    "            print(data_path, \"loaded !\")\n",
    "            print(\"load res_df_ elapsed time :\", time.time() - start_time)\n",
    "            \n",
    "\n",
    "            if config.trader_set.start_datetime != \"None\":\n",
    "                res_df = res_df_.astype(float).loc[pd.to_datetime(config.trader_set.start_datetime):]\n",
    "            else:    \n",
    "                res_df = res_df_.astype(float)\n",
    "            np_timeidx = np.array([intmin_np(date_) for date_ in res_df.index.to_numpy()])\n",
    "\n",
    "\n",
    "            start_time = time.time()\n",
    "            if public_override:\n",
    "                res_df = public_indi(res_df, config_list[0], np_timeidx)  # 현재 대부분의 시간은 h_candle 에서 소비되고 있음\n",
    "            else:\n",
    "                res_df = bank.public.public_indi(res_df, config_list[0], np_timeidx)\n",
    "            print(\"public_indi elapsed time :\", time.time() - start_time)\n",
    "\n",
    "\n",
    "            start_time = time.time()\n",
    "            ohlc_cols = ['open', 'high', 'low', 'close']\n",
    "            ohlc_list = [res_df[col_].to_numpy() for col_ in ohlc_cols]\n",
    "            print(\"make data_list elapsed time :\", time.time() - start_time)\n",
    "\n",
    "\n",
    "            start_time = time.time()\n",
    "            if utils_override:   # 현재, utils_override 하는 경우 1개의 ID 만 허용함 \n",
    "                  res_df = enlist_tr(res_df, config_list[0], np_timeidx)    # 36995.0 -> 152766.0 # 4044 np.sum(long_open_res == 1) : 4325\n",
    "            else:\n",
    "                for utils_, config_ in zip(utils_list, config_list):\n",
    "                    res_df = utils_.enlist_tr(res_df, config_, np_timeidx)\n",
    "            print(\"enlist_tr elapsed time :\", time.time() - start_time)\n",
    "\n",
    "\n",
    "            open_info_df1 = get_open_info_df_v2(ep_loc_p1_v3, res_df, np_timeidx, id_list, config_list, id_idx_list, open_num=1)  # --> point * dur. 관련 (loc_set) param 에 종속 (open_info 가 변경되는게 아니라면, 재실행할 필요없음)\n",
    "            open_info_df2 = get_open_info_df_v2(ep_loc_p2_v3, res_df, np_timeidx, id_list, config_list, id_idx_list, open_num=2)\n",
    "            open_info_df_list = [open_info_df1, open_info_df2]\n",
    "            \n",
    "            \n",
    "            en_ex_pairing = en_ex_pairing_v9_6\n",
    "            \n",
    "            funcs1 = [expiry_p1p2, expiry_tp, lvrg_liqd_set_v2, check_entry_v6_2, check_signal_out_v4, check_hl_out_v4, check_limit_tp_exec]     # adj. expiry_tp & expiry_wave\n",
    "            # funcs1 = [expiry_p1p2, expiry_tp, lvrg_liqd_set_v2, check_entry_v6_3, check_signal_out_v4, check_hl_out_v4, check_limit_tp_exec_v3]  # 보수적 검증 (체결 확률 100%) : check_entry_v6_3 & check_limit_tp_exec_v3\n",
    "            # funcs1 = [expiry_p1p2, expiry_tp, lvrg_liqd_set_v4, check_entry_v6_2, check_signal_out_v4, check_hl_out_v4, check_limit_tp_exec]       # adj. lvrg_liqd_set_v4 (for DS, lvrg_needed serialized adj.)\n",
    "            # funcs1 = [expiry_p1p2, expiry_tp, lvrg_liqd_set_v2, check_entry_v6_2, check_signal_out_v5_1, check_hl_out_v4, check_limit_tp_exec]   # adj. check_signal_out_v5_1 : cu / co exit added.\n",
    "\n",
    "            idep_plot = idep_plot_v16_7\n",
    "            funcs2 = [get_wave_bias_v6_2, get_pr_v7, get_res_info_nb_v3, plot_info_v8_2, frq_dev_plot_v5]  # get_wave_bias_v6_1 / # 여기서 입력되는 get_res_info 는 signi. mode 에 사용됨.\n",
    "\n",
    "            \n",
    "            test_ratio = 0.0\n",
    "            plot_is = 1  # insample\n",
    "            show_detail = 0\n",
    "\n",
    "            # if signi:\n",
    "                # short_res, long_res, both_res = get_res_v2(res_df, open_info_df_list, ohlc_list, config_list, np_timeidx, en_ex_pairing, funcs1, idep_plot, funcs2, test_ratio=test_ratio, plot_is=plot_is, signi=signi, show_detail=show_detail)\n",
    "                    \n",
    "                # res_list_long.append([np.nan] * sub_title_list_len)\n",
    "                # res_list_short.append([np.nan] * sub_title_list_len)\n",
    "                \n",
    "                # if len(long_res) != 0 and type(long_res) != float:\n",
    "                #     res_list_long.append(list(long_res))\n",
    "                # else:\n",
    "                #     res_list_long.append([np.nan] * sub_title_list_len)\n",
    "                \n",
    "                # if len(short_res) != 0 and type(short_res) != float:\n",
    "                #     res_list_short.append(list(short_res))\n",
    "                # else:\n",
    "                #     res_list_short.append([np.nan] * sub_title_list_len)\n",
    "                    \n",
    "            # else:\n",
    "            short_pr, short_obj, short_lvrg_arr, short_fee_arr, short_tpout_arr, short_tr_arr, short_bias_arr, short_net_p1_bias_tick, short_p2exec_p1_bias_tick, short_net_p1_idx_arr, short_p2_idx_arr, short_tp_1, short_tp_0, short_out_1, short_out_0, short_ep2_0, \\\n",
    "                  long_pr, long_obj, long_lvrg_arr, long_fee_arr, long_tpout_arr, long_tr_arr, long_bias_arr, long_net_p1_bias_tick, long_p2exec_p1_bias_tick, long_net_p1_idx_arr, long_p2_idx_arr, long_tp_1, long_tp_0, long_out_1, long_out_0, long_ep2_0 = \\\n",
    "            get_res_v2(res_df, open_info_df_list, ohlc_list, config_list, np_timeidx, en_ex_pairing, funcs1, idep_plot, funcs2, test_ratio=test_ratio, plot_is=plot_is, signi=signi, show_detail=show_detail)\n",
    "            \n",
    "            if ml or excel:\n",
    "     \n",
    "                # 1. public.                   \n",
    "                input_cols = ['open', 'high', 'low', 'close', 'cci_T20', 'cu_wrr_32_T20', 'co_wrr_32_T20', 'short_open1_1', 'long_open1_1', 'short_tp_1', 'long_tp_1', 'short_ep1_1', 'long_ep1_1', 'short_out_1', 'long_out_1']\n",
    "                res_df_copy = res_df[input_cols].copy()\n",
    "                res_df_copy.columns = ['시가', '고가', '저가', '종가', 'CCI', 'WRR32 (SHORT)', 'WRR32 (LONG)', '거래신호 (SHORT)', '거래신호 (LONG)', '목표가 (SHORT)', '목표가 (LONG)', '진입가 (SHORT)', '진입가 (LONG)', '손절가 (SHORT)', '손절가 (LONG)']\n",
    "\n",
    "                # res_df_data = res_df[input_cols].to_numpy()\n",
    "\n",
    "                # 2. separate.\n",
    "                # if data_side == \"long\":\n",
    "                en_arr_long = long_obj[0].ravel()\n",
    "                ex_arr_long = long_obj[1].ravel()\n",
    "                p1_idx_arr_long = long_obj[-1].astype(int).ravel()\n",
    "                ex_idx_arr_long = long_obj[-2].astype(int).ravel()\n",
    "                tp_arr_long, out_arr_long = [arr_.ravel() for arr_ in np.split(long_tpout_arr, 2, axis=1)]\n",
    "                fee_arr_long = long_fee_arr.ravel()\n",
    "                pr_long = long_pr.ravel()\n",
    "                              \n",
    "                bars_in_long = ex_idx_arr_long - p1_idx_arr_long\n",
    "                # spread_long = spread[p1_idx_arr_long] \n",
    "                # tr_long = tr[p1_idx_arr_long]\n",
    "                \n",
    "                ts_en_long = res_df.index[p1_idx_arr_long].map(lambda x : int(datetime.timestamp(x))).to_numpy()\n",
    "                ts_ex_long = res_df.index[ex_idx_arr_long].map(lambda x : int(datetime.timestamp(x))).to_numpy()\n",
    "                        \n",
    "                # else:\n",
    "                en_arr_short = short_obj[0].ravel()\n",
    "                ex_arr_short = short_obj[1].ravel()\n",
    "                p1_idx_arr_short = short_obj[-1].astype(int).ravel()\n",
    "                ex_idx_arr_short = short_obj[-2].astype(int).ravel()\n",
    "                tp_arr_short, out_arr_short = [arr_.ravel() for arr_ in np.split(short_tpout_arr, 2, axis=1)]\n",
    "                fee_arr_short = short_fee_arr.ravel()\n",
    "                pr_short = short_pr.ravel()\n",
    "            \n",
    "                bars_in_short = ex_idx_arr_short - p1_idx_arr_short                      \n",
    "                # spread_short = spread[p1_idx_arr_short]                    \n",
    "                # tr_short = tr[p1_idx_arr_short]\n",
    "                \n",
    "                ts_en_short = res_df.index[p1_idx_arr_short].map(lambda x : int(datetime.timestamp(x))).to_numpy()\n",
    "                ts_ex_short = res_df.index[ex_idx_arr_short].map(lambda x : int(datetime.timestamp(x))).to_numpy()\n",
    "                    \n",
    "\n",
    "                if excel:   \n",
    "                    \n",
    "                    \"\"\"\n",
    "                    p1_idx_arr_long has duplicated index.\n",
    "                    \"\"\"\n",
    "                    # # res_df_copy['거래일시 (시작, LONG)'] = np.nan\n",
    "                    # # res_df_copy['거래일시 (종료, LONG)'] = np.nan\n",
    "                    # res_df_copy['거래번호 (시작, LONG)'] = np.nan\n",
    "                    # res_df_copy['거래번호 (종료, LONG)'] = np.nan\n",
    "                    # res_df_copy['수익률 (LONG)'] = np.nan\n",
    "                    \n",
    "                    # # res_df_copy['거래일시 (시작, SHORT)'] = np.nan\n",
    "                    # # res_df_copy['거래일시 (종료, SHORT)'] = np.nan\n",
    "                    # res_df_copy['거래번호 (시작, SHORT)'] = np.nan\n",
    "                    # res_df_copy['거래번호 (종료, SHORT)'] = np.nan\n",
    "                    # res_df_copy['수익률 (SHORT)'] = np.nan\n",
    "                    \n",
    "                    # # res_df_copy['거래일시 (시작, LONG)'].iloc[p1_idx_arr_long] = ts_en_long\n",
    "                    # # res_df_copy['거래일시 (종료, LONG)'].iloc[ex_idx_arr_long] = ts_ex_long\n",
    "                    # res_df_copy['거래번호 (시작, LONG)'].iloc[p1_idx_arr_long] = np.arange(len(p1_idx_arr_long))\n",
    "                    # res_df_copy['거래번호 (종료, LONG)'].iloc[ex_idx_arr_long] = np.arange(len(ex_idx_arr_long))\n",
    "                    \n",
    "                    # # res_df_copy['거래일시 (시작, SHORT)'].iloc[p1_idx_arr_short] = ts_en_short\n",
    "                    # # res_df_copy['거래일시 (종료, SHORT)'].iloc[ex_idx_arr_short] = ts_ex_short\n",
    "                    # res_df_copy['거래번호 (시작, SHORT)'].iloc[p1_idx_arr_short] = np.arange(len(p1_idx_arr_short))\n",
    "                    # res_df_copy['거래번호 (종료, SHORT)'].iloc[ex_idx_arr_short] = np.arange(len(ex_idx_arr_short))\n",
    "                    \n",
    "                    # res_df_copy['수익률 (LONG)'].iloc[ex_idx_arr_long] = pr_long\n",
    "                    # res_df_copy['수익률 (SHORT)'].iloc[ex_idx_arr_short] = pr_short\n",
    "                    \n",
    "                    # save_path = \"D:\\Projects\\Richer\\QuantAnalyzer\\Project\\Wave\\Data\\Raw\\{}.xlsx\".format(symbol)\n",
    "                    # res_df_copy.head(80000).to_excel(save_path)\n",
    "                    # print(save_path, 'saved.')\n",
    "                    \n",
    "                    # 1. available features.                    \n",
    "                    spread_short = res_df['{}_spread_{}'.format('short', config.selection_id)].to_numpy()[p1_idx_arr_short]\n",
    "                    spread_long = res_df['{}_spread_{}'.format('long', config.selection_id)].to_numpy()[p1_idx_arr_long]\n",
    "                    \n",
    "                    tr_short = res_df['{}_tr_{}'.format('short', config.selection_id)].to_numpy()[p1_idx_arr_short]         \n",
    "                    tr_long = res_df['{}_tr_{}'.format('long', config.selection_id)].to_numpy()[p1_idx_arr_long]         \n",
    "                \n",
    "                    wrr_32_short = res_df['cu_wrr_32_{}{}'.format(config.tr_set.wave_itv1, config.tr_set.wave_period1)].to_numpy()[p1_idx_arr_short]\n",
    "                    wrr_32_long = res_df['co_wrr_32_{}{}'.format(config.tr_set.wave_itv1, config.tr_set.wave_period1)].to_numpy()[p1_idx_arr_long]\n",
    "                    \n",
    "                    wave_length_short = res_df['short_wave_length_fill_{}{}'.format(config.tr_set.wave_itv1, config.tr_set.wave_period1)].to_numpy()[p1_idx_arr_short]\n",
    "                    wave_length_long = res_df['long_wave_length_fill_{}{}'.format(config.tr_set.wave_itv1, config.tr_set.wave_period1)].to_numpy()[p1_idx_arr_long]\n",
    "\n",
    "                    columns = ['symbol', 'position', 'trade_number', 'ts_en', 'ts_ex', 'spread', 'tr', 'wrr_32', 'wave_length', 'pr']\n",
    "                    # columns = ['종목', '포지션', '거래번호', '거래일시 (시작)', '거래일시 (종료)', '스프레드', '손절 대비 수익비', 'WRR32', '파동 길이', '수익률']\n",
    "\n",
    "                    # print(\"ts_en_long.shape :\", ts_en_long.shape)\n",
    "                    # print(\"spread_long.shape :\", spread_long.shape)\n",
    "                    # print(\"tr_long.shape :\", tr_long.shape)\n",
    "                    # print(\"wrr_32_long.shape :\", wrr_32_long.shape)\n",
    "                    # print(\"wave_length_long.shape :\", wave_length_long.shape)\n",
    "                    # print(\"pr_long.shape :\", pr_long.shape)\n",
    "                    \n",
    "                    p1_idx_arr_long_len = len(p1_idx_arr_long)\n",
    "                    p1_idx_arr_short_len = len(p1_idx_arr_short)\n",
    "                    \n",
    "                    data_long = np.array([np.full(p1_idx_arr_long_len, symbol), np.full(p1_idx_arr_long_len, 'LONG'), np.arange(p1_idx_arr_long_len), ts_en_long, ts_ex_long, spread_long, tr_long, wrr_32_long, wave_length_long, pr_long]).T\n",
    "                    data_short = np.array([np.full(p1_idx_arr_short_len, symbol), np.full(p1_idx_arr_short_len, 'SHORT'), np.arange(p1_idx_arr_short_len), ts_en_short, ts_ex_short, spread_short, tr_short, wrr_32_short, wave_length_short, pr_short]).T\n",
    "                    # print(\"data_long.shape :\", data_long.shape)\n",
    "                    \n",
    "                    df = pd.DataFrame(np.vstack([data_long, data_short]), columns=columns)\n",
    "                    df.iloc[:, 2:] = df.iloc[:, 2:].astype(float)\n",
    "                    \n",
    "                    # save_path = \"Anal/data/{}.xlsx\".format(symbol)\n",
    "                    # save_path = \"D:\\Projects\\Richer\\QuantAnalyzer\\Project\\Wave\\Data\\Trade\\{}.xlsx\".format(symbol)\n",
    "                    # df.to_excel(save_path, index=0)\n",
    "                    save_path = r\"D:\\Projects\\SystemTrading\\JnQ\\database\\binance\\trade_data\\{}.ftr\".format(symbol)\n",
    "                    df.reset_index(drop=True).to_feather(save_path)\n",
    "                    print(save_path, 'saved.')\n",
    "\n",
    "                if ml:\n",
    "                    \n",
    "                    x_data = []\n",
    "                    valid_index = []                    \n",
    "                    for p_i, p1 in enumerate(p1_idx_arr):\n",
    "                        \n",
    "                        if p1 < data_size:\n",
    "                            continue\n",
    "\n",
    "                        start_time = time.time()\n",
    "                        # data_np = res_df[input_cols].iloc[p1 + 1 - data_size:p1 + 1].to_numpy()\n",
    "                        data_np = res_df_data[p1 + 1 - data_size:p1 + 1]\n",
    "                        # plt.plot(data_np[:, -2:])\n",
    "                        # plt.show()\n",
    "\n",
    "                        # a. min_max_scaler\n",
    "                        # data_np_norm = min_max_scaler(data_np)\n",
    "                        # b. \n",
    "                        # low_min = np.min(data_np[:, :4])\n",
    "                        # print(\"low_min :\", low_min)                        \n",
    "                        # data_np_norm = data_np / low_min\n",
    "                        \n",
    "                        if np.sum(pd.isnull(data_np)) > 0:\n",
    "                            continue\n",
    "                        \n",
    "                        # plt.plot(data_np_norm)\n",
    "                        # plt.show()\n",
    "                        # x_data.append(data_np_norm)\n",
    "                        x_data.append(data_np)\n",
    "                        # print(data_np_norm.shape)\n",
    "                        # print(\"elapsed time : {}\".format(time.time() - start_time)) # 0.012034177780151367 --> 0.0009989738464355469s\n",
    "                        \n",
    "                        valid_index.append(p_i)\n",
    "                        # break\n",
    "                    \n",
    "                    x_train = np.array(x_data) #.reshape(-1, flatten_len) --> feature_col 자유롭게 선택하기 위해 flatten 사용하지 않는다.\n",
    "                    \n",
    "                    pkl_save_path_cls = os.path.join(dir_path, dir_name, file_name.replace('.ftr', '_cls.pkl'))\n",
    "                    pkl_save_path_tr = os.path.join(dir_path, dir_name, file_name.replace('.ftr', '_tr.pkl'))\n",
    "                    pkl_save_path_spread = os.path.join(dir_path, dir_name, file_name.replace('.ftr', '_spread.pkl'))\n",
    "                    pkl_save_path_ts = os.path.join(dir_path, dir_name, file_name.replace('.ftr', '_ts.pkl'))\n",
    "                    pkl_save_path_x = os.path.join(dir_path, dir_name, file_name.replace('.ftr', '_x.pkl'))\n",
    "                    pkl_save_path_y = os.path.join(dir_path, dir_name, file_name.replace('.ftr', '_y.pkl'))\n",
    "                    pkl_save_path_bars_in = os.path.join(dir_path, dir_name, file_name.replace('.ftr', '_bars_in.pkl'))\n",
    "                    pkl_save_path_ex_idx = os.path.join(dir_path, dir_name, file_name.replace('.ftr', '_ex_idx.pkl'))\n",
    "                    pkl_save_path_en = os.path.join(dir_path, dir_name, file_name.replace('.ftr', '_en.pkl'))\n",
    "                    pkl_save_path_ex = os.path.join(dir_path, dir_name, file_name.replace('.ftr', '_ex.pkl'))\n",
    "                    pkl_save_path_tp = os.path.join(dir_path, dir_name, file_name.replace('.ftr', '_tp.pkl'))\n",
    "                    pkl_save_path_out = os.path.join(dir_path, dir_name, file_name.replace('.ftr', '_out.pkl'))\n",
    "                    pkl_save_path_fee = os.path.join(dir_path, dir_name, file_name.replace('.ftr', '_fee.pkl'))\n",
    "                        \n",
    "                    with open(pkl_save_path_cls, 'wb') as f:  \n",
    "                        # cls_ = np.full(len(p1_idx_arr), r_)\n",
    "                        cls_ = np.full(len(p1_idx_arr), symbol)\n",
    "                        # print(\"cls_.shape :\", cls_.shape)                        \n",
    "                        pickle.dump(cls_, f)\n",
    "                        print(pkl_save_path_cls, 'saved.')\n",
    "                        \n",
    "                    with open(pkl_save_path_tr, 'wb') as f:                        \n",
    "                        pickle.dump(tr_train[valid_index], f)\n",
    "                        print(pkl_save_path_tr, 'saved.')\n",
    "                        \n",
    "                    with open(pkl_save_path_spread, 'wb') as f:                        \n",
    "                        pickle.dump(spread_train[valid_index], f)\n",
    "                        print(pkl_save_path_spread, 'saved.')\n",
    "                        \n",
    "                    with open(pkl_save_path_ts, 'wb') as f:                        \n",
    "                        pickle.dump(ts[valid_index], f)\n",
    "                        print(pkl_save_path_ts, 'saved.')\n",
    "\n",
    "                    with open(pkl_save_path_x, 'wb') as f:\n",
    "                        pickle.dump(x_train, f)\n",
    "                        print(pkl_save_path_x, 'saved.')\n",
    "                          \n",
    "                    with open(pkl_save_path_y, 'wb') as f:\n",
    "                        pickle.dump(y_train[valid_index], f)\n",
    "                        print(pkl_save_path_y, 'saved.')\n",
    "                        \n",
    "                    with open(pkl_save_path_bars_in, 'wb') as f:\n",
    "                        pickle.dump(bars_in[valid_index], f)\n",
    "                        print(pkl_save_path_bars_in, 'saved.')\n",
    "\n",
    "                    with open(pkl_save_path_en, 'wb') as f:\n",
    "                        pickle.dump(en_arr[valid_index], f)\n",
    "                        print(pkl_save_path_en, 'saved.')\n",
    "\n",
    "                    with open(pkl_save_path_ex, 'wb') as f:\n",
    "                        pickle.dump(ex_arr[valid_index], f)\n",
    "                        print(pkl_save_path_ex, 'saved.')\n",
    "\n",
    "                    with open(pkl_save_path_tp, 'wb') as f:\n",
    "                        pickle.dump(tp_arr[valid_index], f)\n",
    "                        print(pkl_save_path_tp, 'saved.')\n",
    "\n",
    "                    with open(pkl_save_path_out, 'wb') as f:\n",
    "                        pickle.dump(out_arr[valid_index], f)\n",
    "                        print(pkl_save_path_out, 'saved.')\n",
    "\n",
    "                    with open(pkl_save_path_fee, 'wb') as f:\n",
    "                        pickle.dump(fee_arr[valid_index], f)\n",
    "                        print(pkl_save_path_fee, 'saved.')\n",
    "                    \n",
    "                    print(\"x_train.shape : {}\".format(x_train.shape))                    \n",
    "                    print(\"y_train[valid_index].shape : {}\".format(y_train[valid_index].shape))\n",
    "                    print(\"tp_arr[valid_index].shape : {}\".format(tp_arr[valid_index].shape))\n",
    "                    print(\"out_arr[valid_index].shape : {}\".format(out_arr[valid_index].shape))\n",
    "                    print(\"fee_arr[valid_index].shape : {}\".format(fee_arr[valid_index].shape))\n",
    "\n",
    "        except Exception as e:\n",
    "            print(\"error in get_res : {}\".format(e))\n",
    "\n",
    "    \n",
    "    # if signi:\n",
    "    #     if type(data_list) == list:\n",
    "    #         df_rank = pd.Series(data_list).to_frame()\n",
    "    #     else:        \n",
    "    #         df_rank = data_list.to_frame()\n",
    "    #     df_rank[sub_title_list_long] = np.array(res_list_long)\n",
    "    #     df_rank[sub_title_list_short] = np.array(res_list_short)\n",
    "\n",
    "    #     return df_rank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "idep_mode = \"CRYPTO\"  # CRYPTO STOCK\n",
    "\n",
    "# data_name = '2023-02-28 208370'  # 2023-01-12 ETH / 2023-02-21 FTM /2022-04-27 ETH / 2023-02-20 BTC\n",
    "# date, ticker = data_name.split(\" \")\n",
    "date = \"2023-02-21\"\n",
    "date = \"2023-10-17\"\n",
    "# date = \"2023-11-22\"\n",
    "# date = \"2023-12-13\"\n",
    "date = \"2024-02-15\"\n",
    "save_mode = 0\n",
    "\n",
    "\"\"\"\n",
    "database 는 JnQ 내부로 통일할 것.\n",
    "\"\"\"\n",
    "database_type = 'database/binance/'  # 'binance' kiwoom upbit\n",
    "database_dir_abspath = os.path.join(pkg_path, database_type, \"concat_yes\", date).replace(\"JnQ_32bit\", \"JnQ\")  # cum non_cum -> use, non_cum data for backtrade validation\n",
    "# database_dir_abspath = os.path.join(pkg_path, database_type, \"concat_no\", date).replace(\"JnQ_32bit\", \"JnQ\")  # cum non_cum -> use, non_cum data for backtrade validation\n",
    "\n",
    "\n",
    "if idep_mode == \"CRYPTO\":\n",
    "    data_list = [f_name for f_name in os.listdir(database_dir_abspath) if 'ftr' in f_name if date in f_name]\n",
    "else:\n",
    "    data_list = pd.read_pickle(os.path.join(database_dir_abspath, \"rank.pkl\"))  # [:10]    \n",
    "# data_list = [\"2023-02-21 ZRXUSDT_1m.ftr\"]   \n",
    "\n",
    "len(data_list)\n",
    "\n",
    "# with open(\"valid_data_ticker_list.pkl\", 'wb') as f:\n",
    "#     pickle.dump([ticker_.split(' ')[-1].split('_')[0] for ticker_ in data_list], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_rank = idep_on_multiple_ticker(database_dir_abspath, data_list, signi=1, ml=0, excel=1)\n",
    "# df_rank = idep_on_multiple_ticker(database_dir_abspath, data_list, signi=False, ml=False)\n",
    "\n",
    "if save_mode:\n",
    "    df_rank.to_pickle(os.path.join(database_dir_abspath, \"df_rank_wrr32_03.pkl\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### result analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(database_dir_abspath, \"df_rank_wrr32_03.pkl\"), 'rb') as f:\n",
    "    df_rank = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# df_rank\n",
    "df_rank.to_excel(r\"D:\\Projects\\E_Book\\result\\binance\\df_rank.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# print(np.mean(df_rank[df_rank.sr < 0.15]))\n",
    "# print(np.mean(df_rank[df_rank.sr > 0.15]))\n",
    "df_rank2 = df_rank[df_rank.hhm_long > 0.80]\n",
    "print(len(df_rank2))\n",
    "\n",
    "# valid_ticker = [\"2023-02-21 ICXUSDT_1m.ftr\", \"2023-02-21 RSRUSDT_1m.ftr\", \"2023-02-21 TRXUSDT_1m.ftr\"]\n",
    "# valid_ticker = [\"2023-02-21 ICXUSDT_1m.ftr\", \"2023-02-21 ALICEUSDT_1m.ftr\", \"2023-02-21 COMPUSDT_1m.ftr\", \"2023-02-21 DOTUSDT_1m.ftr\", \"2023-02-21 SNXUSDT_1m.ftr\", \"2023-02-21 SUSHIUSDT_1m.ftr\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# idep_on_multiple_ticker(database_dir_abspath, df_rank.iloc[:, 0], False)\n",
    "idep_on_multiple_ticker(database_dir_abspath, df_rank2.iloc[:, 0], False)\n",
    "# idep_on_multiple_ticker(database_dir_abspath, valid_ticker, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_dir = \"/mnt/d/Projects/Schmosslab/report/테스트/LPR_에러처리_테스트_통합_데이터\"\n",
    "\n",
    "df_list = []\n",
    "\n",
    "for p_i, path_ in enumerate(os.listdir(concat_dir)):    \n",
    "\n",
    "        save_path_res = os.path.join(concat_dir, path_)\n",
    "        \n",
    "        if os.path.isfile(save_path_res):\n",
    "            df_data = pd.read_excel(save_path_res)\n",
    "            df_list.append(df_data)\n",
    "        \n",
    "pd.concat(df_list).to_excel(os.path.join(concat_dir, \"../LPR_에러처리_테스트_통합.xlsx\"), index=False)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Excel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### concat excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_dir = \"D:\\Projects\\SystemTrading\\JnQ\\Anal\\data\\\\2024-02-15\"\n",
    "concat_dir_list = os.listdir(concat_dir)\n",
    "concat_dir_list_len = len(concat_dir_list)\n",
    "\n",
    "df_list = []\n",
    "\n",
    "chunk_size = 10\n",
    "\n",
    "for p_i, path_ in enumerate(concat_dir_list):    \n",
    "    \n",
    "    print(\"p_i :\", p_i, end='\\r')\n",
    "\n",
    "    save_path_res = os.path.join(concat_dir, path_)\n",
    "    \n",
    "    if os.path.isfile(save_path_res):\n",
    "        df_data = pd.read_excel(save_path_res)\n",
    "        df_list.append(df_data)\n",
    "\n",
    "    if p_i != 0:\n",
    "        if p_i % chunk_size == 0 or p_i == concat_dir_list_len - 1:        \n",
    "            pd.concat(df_list).to_excel(os.path.join(concat_dir, \"../2024-02-15_{}.xlsx\".format(p_i // chunk_size)), index=False)\n",
    "            df_list = []     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### concat data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "idep_mode = \"CRYPTO\"  # CRYPTO STOCK\n",
    "\n",
    "# data_name = '2023-02-28 208370'  # 2023-01-12 ETH / 2023-02-21 FTM /2022-04-27 ETH / 2023-02-20 BTC\n",
    "# date, ticker = data_name.split(\" \")\n",
    "# date = \"2023-10-17\" # \"2023-02-21\"  \"2023-03-23\"  \"2023-10-17\"\n",
    "# save_mode = 0\n",
    "\n",
    "\"\"\"\n",
    "database 는 JnQ 내부로 통일할 것.\n",
    "\"\"\"\n",
    "\n",
    "data_side = \"long\"\n",
    "# data_side = \"short\"\n",
    "dir_path = r\"D:\\Projects\\SystemTrading\\JnQ\\database\\binance\\ML\"    \n",
    "dir_name = \"wrr32_03\\\\epg_epbox\\\\{}\\\\data\\\\\".format(data_side)\n",
    "\n",
    "data_path_ml = os.path.join(dir_path, dir_name)\n",
    "# database_type = 'database\\\\binance'  # 'binance' kiwoom upbit\n",
    "# # database_dir_abspath = os.path.join(pkg_path, database_type, \"cum\", date).replace(\"JnQ_32bit\", \"JnQ\")  # cum non_cum -> use, non_cum data for backtrade validation\n",
    "# database_dir_abspath = os.path.join(pkg_path, database_type, \"ML\\\\wrr32_03\\{}\\\\train\\\\tr_fix\".format(data_side).replace(\"JnQ_32bit\", \"JnQ\"))  # cum non_cum -> use, non_cum data for backtrade validation\n",
    "# break\n",
    "\n",
    "\n",
    "if idep_mode == \"CRYPTO\":\n",
    "    data_list = [f_name for f_name in os.listdir(data_path_ml) if 'pkl' in f_name]\n",
    "else:\n",
    "    data_list = pd.read_pickle(os.path.join(data_path_ml, \"rank.pkl\"))  # [:10]        \n",
    "# list(set(pkl_.split('_')[ for data_list   \n",
    "\n",
    "\n",
    "\n",
    "concat_list_cls = []\n",
    "concat_list_tr = []\n",
    "concat_list_spread = []\n",
    "concat_list_ts = []\n",
    "concat_list_x = []\n",
    "concat_list_y = []\n",
    "concat_list_bars_in = []\n",
    "concat_list_en = []\n",
    "concat_list_ex = []\n",
    "concat_list_tp = []\n",
    "concat_list_out = []\n",
    "concat_list_fee = []\n",
    "\n",
    "for data_name in data_list:\n",
    "    \n",
    "    try:\n",
    "        # if not \"02-14\" in data_name:\n",
    "        #     continue\n",
    "        \n",
    "        if '_cls.pkl' in data_name:\n",
    "            pkl_save_path_cls = os.path.join(data_path_ml, data_name.replace('.ftr', '_cls.pkl'))\n",
    "            with open(pkl_save_path_cls, 'rb') as f:\n",
    "                concat_list_cls.append(pickle.load(f))\n",
    "                \n",
    "        elif '_tr.pkl' in data_name:\n",
    "            pkl_save_path_tr = os.path.join(data_path_ml, data_name.replace('.ftr', '_tr.pkl'))\n",
    "            with open(pkl_save_path_tr, 'rb') as f:\n",
    "                concat_list_tr.append(pickle.load(f))\n",
    "                \n",
    "        elif '_spread.pkl' in data_name:\n",
    "            pkl_save_path_spread = os.path.join(data_path_ml, data_name.replace('.ftr', '_spread.pkl'))\n",
    "            with open(pkl_save_path_spread, 'rb') as f:\n",
    "                concat_list_spread.append(pickle.load(f))\n",
    "                \n",
    "        elif '_ts.pkl' in data_name:\n",
    "            pkl_save_path_ts = os.path.join(data_path_ml, data_name.replace('.ftr', '_ts.pkl'))\n",
    "            with open(pkl_save_path_ts, 'rb') as f:\n",
    "                concat_list_ts.append(pickle.load(f))\n",
    "                \n",
    "        elif '_x.pkl' in data_name:\n",
    "            pkl_save_path_x = os.path.join(data_path_ml, data_name.replace('.ftr', '_x.pkl'))\n",
    "            with open(pkl_save_path_x, 'rb') as f:\n",
    "                # x_train_temp = pickle.load(f)\n",
    "                # print(x_train_temp.shape)\n",
    "                concat_list_x.append(pickle.load(f))\n",
    "                \n",
    "        elif '_y.pkl' in data_name:\n",
    "            pkl_save_path_y = os.path.join(data_path_ml, data_name.replace('.ftr', '_y.pkl'))\n",
    "            with open(pkl_save_path_y, 'rb') as f:\n",
    "                concat_list_y.append(pickle.load(f))\n",
    "                \n",
    "        elif '_bars_in.pkl' in data_name:\n",
    "            pkl_save_path_bars_in = os.path.join(data_path_ml, data_name.replace('.ftr', '_bars_in.pkl'))\n",
    "            with open(pkl_save_path_bars_in, 'rb') as f:\n",
    "                concat_list_bars_in.append(pickle.load(f))\n",
    "                \n",
    "        elif '_en.pkl' in data_name:\n",
    "            pkl_save_path_en = os.path.join(data_path_ml, data_name.replace('.ftr', '_en.pkl'))\n",
    "            with open(pkl_save_path_en, 'rb') as f:\n",
    "                concat_list_en.append(pickle.load(f))\n",
    "                \n",
    "        elif '_ex.pkl' in data_name:\n",
    "            pkl_save_path_ex = os.path.join(data_path_ml, data_name.replace('.ftr', '_ex.pkl'))\n",
    "            with open(pkl_save_path_ex, 'rb') as f:\n",
    "                concat_list_ex.append(pickle.load(f))\n",
    "                \n",
    "        elif '_tp.pkl' in data_name:\n",
    "            pkl_save_path_tp = os.path.join(data_path_ml, data_name.replace('.ftr', '_tp.pkl'))\n",
    "            with open(pkl_save_path_tp, 'rb') as f:\n",
    "                concat_list_tp.append(pickle.load(f))\n",
    "                \n",
    "        elif '_out.pkl' in data_name:\n",
    "            pkl_save_path_out = os.path.join(data_path_ml, data_name.replace('.ftr', '_out.pkl'))\n",
    "            with open(pkl_save_path_out, 'rb') as f:\n",
    "                concat_list_out.append(pickle.load(f))\n",
    "                \n",
    "        elif '_fee.pkl' in data_name:\n",
    "            pkl_save_path_fee = os.path.join(data_path_ml, data_name.replace('.ftr', '_fee.pkl'))\n",
    "            with open(pkl_save_path_fee, 'rb') as f:\n",
    "                concat_list_fee.append(pickle.load(f))\n",
    "        else:\n",
    "            print(\"invalid data_name : {}\".format(data_name), end='\\r')    \n",
    "            \n",
    "        # if len(concat_list_tr) > 25:\n",
    "            # break\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_cols = ['open', 'high', 'low', 'close', 'ma_T50', 'ma_T200', 'ma_T800'] #, 'long_open1_1']\n",
    "\n",
    "data_size = 500\n",
    "cols_len = len(input_cols)\n",
    "flatten_len = data_size * cols_len\n",
    "# break\n",
    "\n",
    "cls_train = np.concatenate(concat_list_cls)\n",
    "tr_train = np.concatenate(concat_list_tr)\n",
    "spread_train = np.concatenate(concat_list_spread)\n",
    "ts_train = np.concatenate(concat_list_ts)\n",
    "x_train = np.concatenate(concat_list_x).reshape(-1, flatten_len)\n",
    "y_train = np.concatenate(concat_list_y)\n",
    "bars_in_train = np.concatenate(concat_list_bars_in)\n",
    "en_train = np.concatenate(concat_list_en)\n",
    "ex_train = np.concatenate(concat_list_ex)\n",
    "tp_train = np.concatenate(concat_list_tp)\n",
    "out_train = np.concatenate(concat_list_out)\n",
    "fee_train = np.concatenate(concat_list_fee)\n",
    "\n",
    "print(cls_train.shape)\n",
    "print(tr_train.shape)\n",
    "print(spread_train.shape)\n",
    "print(ts_train.shape)\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(bars_in_train.shape)\n",
    "print(en_train.shape)\n",
    "print(ex_train.shape)\n",
    "print(tp_train.shape)\n",
    "print(out_train.shape)\n",
    "print(fee_train.shape)\n",
    "# x_train = x_train\n",
    "\n",
    "# del concat_list_spread\n",
    "# del concat_list_ts\n",
    "# del concat_list_x\n",
    "# del concat_list_y\n",
    "\n",
    "assert len(x_train.shape) == 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.scatter(x=spread_train, y=y_train.reshape(-1,), s=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_train_filtered = y_train[np.argwhere(spread_train > 1.005).ravel()]\n",
    "print(y_train_filtered[y_train_filtered > 1].min())\n",
    "print(y_train_filtered[y_train_filtered > 1].max())\n",
    "valid_idx = np.where(spread_train > 1.005)\n",
    "valid_idx[0]\n",
    "\n",
    "del y_train_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sorting_index = np.argsort(ts_train)\n",
    "target_spread = 0\n",
    "\n",
    "valid_idx = np.where(spread_train[sorting_index] > target_spread)[0] #[:80000]  # x_train's shape should 2d.\n",
    "# valid_idx = np.where((pd.isnull(x_train).sum(axis=1) == 0))[0] #[:80000]  # x_train's shape should 2d.\n",
    "print(\"valid_idx :\", valid_idx)\n",
    "\n",
    "cls_train = cls_train[sorting_index][valid_idx]\n",
    "tr_train = tr_train[sorting_index][valid_idx]\n",
    "spread_train = spread_train[sorting_index][valid_idx]\n",
    "ts_train = ts_train[sorting_index][valid_idx]\n",
    "x_train = x_train[sorting_index][valid_idx]\n",
    "y_train = y_train[sorting_index][valid_idx]\n",
    "bars_in_train = bars_in_train[sorting_index][valid_idx]\n",
    "en_train = en_train[sorting_index][valid_idx]\n",
    "ex_train = ex_train[sorting_index][valid_idx]\n",
    "tp_train = tp_train[sorting_index][valid_idx]\n",
    "out_train = out_train[sorting_index][valid_idx]\n",
    "fee_train = fee_train[sorting_index][valid_idx]\n",
    "\n",
    "print(cls_train.shape)\n",
    "print(tr_train.shape)\n",
    "print(spread_train.shape)\n",
    "print(ts_train.shape)\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(bars_in_train.shape)\n",
    "print(en_train.shape)\n",
    "print(ex_train.shape)\n",
    "print(tp_train.shape)\n",
    "print(out_train.shape)\n",
    "print(fee_train.shape)\n",
    "\n",
    "x_data_train = x_train.copy()\n",
    "x_data_train = x_data_train.reshape(-1, data_size, cols_len)\n",
    "x_data_train_copy = x_data_train.copy()  # for resizing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### data resize & rescale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# variable size check\n",
    "[print(item_) for item_ in sorted([(x, sys.getsizeof(globals().get(x))) for x in dir()], key=lambda x: x[1], reverse=1)]\n",
    "\n",
    "# del x_data_train_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_size = 200\n",
    "cols_len = 6\n",
    "flatten_len = data_size * cols_len\n",
    "\n",
    "\n",
    "x_data_train = x_data_train_copy[:, -data_size:, :cols_len]  # .shape\n",
    "x_train = x_data_train.reshape(-1, data_size * cols_len)\n",
    "\n",
    "\n",
    "# 1. scaling\n",
    "max_value = np.tile(np.max(x_train, axis=1).reshape(-1, 1), flatten_len)\n",
    "min_value = np.tile(np.min(x_train, axis=1).reshape(-1, 1), flatten_len)\n",
    "print(max_value.shape)\n",
    "print(min_value.shape)\n",
    "\n",
    "x_train = (x_train - min_value) / (max_value - min_value)\n",
    "print(x_train.min(), x_train.max())\n",
    "\n",
    "x_data_train = x_train.reshape(-1, data_size, cols_len)\n",
    "\n",
    "del max_value\n",
    "del min_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### set TVT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_ratio = 0.3\n",
    "# test_ratio = 0.9\n",
    "val_ratio = 0.3\n",
    "# val_ratio = 0.05\n",
    "\n",
    "data_len = len(x_train)\n",
    "test_len = int(data_len * test_ratio)\n",
    "val_len = int(data_len * val_ratio)\n",
    "\n",
    "# static size for profitable_index_99_100_5.pkl\n",
    "# test_len = 5000\n",
    "# val_len = 60000 # 61292\n",
    "# train_len = 61293\n",
    "train_len = data_len - (test_len + val_len)\n",
    "\n",
    "def get_tvt_data(data_):\n",
    "    return data_[:train_len], data_[train_len:train_len+val_len], data_[-test_len:]\n",
    "\n",
    "\n",
    "cls_train, cls_val, cls_test = get_tvt_data(cls_train)\n",
    "tr_train, tr_val, tr_test = get_tvt_data(tr_train)\n",
    "spread_train, spread_val, spread_test = get_tvt_data(spread_train)\n",
    "ts_train, ts_val, ts_test = get_tvt_data(ts_train)\n",
    "x_train, x_val, x_test = get_tvt_data(x_train)\n",
    "y_train, y_val, y_test = get_tvt_data(y_train)\n",
    "bars_in_train, bars_in_val, bars_in_test = get_tvt_data(bars_in_train)\n",
    "en_train, en_val, en_test = get_tvt_data(en_train)\n",
    "ex_train, ex_val, ex_test = get_tvt_data(ex_train)\n",
    "tp_train, tp_val, tp_test = get_tvt_data(tp_train)\n",
    "out_train, out_val, out_test = get_tvt_data(out_train)\n",
    "fee_train, fee_val, fee_test = get_tvt_data(fee_train)\n",
    "\n",
    "x_data_train, x_data_val, x_data_test = get_tvt_data(x_data_train)\n",
    "\n",
    "\n",
    "print(cls_train.shape, cls_val.shape, cls_test.shape)\n",
    "print(tr_train.shape, tr_val.shape, tr_test.shape)\n",
    "print(spread_train.shape, spread_val.shape, spread_test.shape)\n",
    "print(ts_train.shape, ts_val.shape, ts_test.shape)\n",
    "print(x_train.shape, x_val.shape, x_test.shape)\n",
    "print(y_train.shape, y_val.shape, y_test.shape)\n",
    "print(bars_in_train.shape, bars_in_val.shape, bars_in_test.shape)\n",
    "print(en_train.shape, en_val.shape, en_test.shape)\n",
    "print(ex_train.shape, ex_val.shape, ex_test.shape)\n",
    "print(tp_train.shape, tp_val.shape, tp_test.shape)\n",
    "print(out_train.shape, out_val.shape, out_test.shape)\n",
    "print(fee_train.shape, fee_val.shape, fee_test.shape)\n",
    "\n",
    "print(x_data_train.shape, x_data_val.shape, x_data_test.shape)\n",
    "\n",
    "\n",
    "plt.plot(np.arange(len(ts_train)), ts_train, color='g')\n",
    "plt.plot(np.arange(len(ts_val)), ts_val, color='y')\n",
    "plt.plot(np.arange(len(ts_test)), ts_test, color='r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### set key info <-- naming rule **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cluster_threshold = 0.99\n",
    "cluster_threshold_str = str(cluster_threshold).replace(\"0.\", \"\")\n",
    "robust_len = 50 # robustness 를 위한 min_len\n",
    "robust_len = 100 # robustness 를 위한 min_len\n",
    "# train_size_str = str(train_size_ratio).split('.')[-1]\n",
    "val_ratio_int = int(val_ratio * 100)\n",
    "test_ratio_int = int(test_ratio * 100)\n",
    "\n",
    "data_path_ml = os.path.join(dir_path, dir_name.replace('data', 'res'))\n",
    "pkl_save_dir_path = os.path.join(data_path_ml, \"{}_{}\\{}_{}\".format(data_size, cols_len, cluster_threshold_str, robust_len))\n",
    "\n",
    "pkl_save_path = os.path.join(pkl_save_dir_path, \"profitable_index.pkl\".format(val_ratio_int, test_ratio_int))\n",
    "# pkl_save_path = os.path.join(pkl_save_dir_path, \"profitable_index_{}_{}_5.pkl\".format(val_ratio_int, test_ratio_int))\n",
    "# pkl_save_path = os.path.join(pkl_save_dir_path, \"profitable_index_{}_{}.pkl\".format())\n",
    "# pkl_save_path = os.path.join(pkl_save_dir_path, \"profitable_index_{}_{}_backup.pkl\".format(cluster_threshold_str, robust_len))\n",
    "os.makedirs(pkl_save_dir_path, exist_ok=True)\n",
    "\n",
    "print(pkl_save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### get cluster key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def cos_sim(a: torch.Tensor, b: torch.Tensor):\n",
    "    \"\"\"\n",
    "    Computes the cosine similarity cos_sim(a[i], b[j]) for all i and j.\n",
    "    :return: Matrix with res[i][j]  = cos_sim(a[i], b[j])\n",
    "    \"\"\"\n",
    "    if not isinstance(a, torch.Tensor):\n",
    "        a = torch.tensor(a)\n",
    "\n",
    "    if not isinstance(b, torch.Tensor):\n",
    "        b = torch.tensor(b)\n",
    "\n",
    "    if len(a.shape) == 1:\n",
    "        a = a.unsqueeze(0)\n",
    "\n",
    "    if len(b.shape) == 1:\n",
    "        b = b.unsqueeze(0)\n",
    "\n",
    "    a_norm = torch.nn.functional.normalize(a, p=2, dim=1)\n",
    "    b_norm = torch.nn.functional.normalize(b, p=2, dim=1)\n",
    "    return torch.mm(a_norm, b_norm.transpose(0, 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rcParams.update({'font.size': 7})\n",
    "\n",
    "use_gpu = 1\n",
    "\n",
    "show_img = 0\n",
    "\n",
    "clear_cell_output = 1\n",
    "\n",
    "wr_threshold = 0.8\n",
    "# wr_threshold = 0.3\n",
    "\n",
    "\n",
    "profitable_index = {}\n",
    "usded_index = []\n",
    "# break                             \n",
    "\n",
    "for i_ in range(len(x_train)):\n",
    "        \n",
    "    if clear_cell_output:\n",
    "        clear_output(wait=True)\n",
    "\n",
    "    start_time = time.time()     \n",
    "\n",
    "    if i_ in usded_index:\n",
    "        # print(usded_index)\n",
    "        continue\n",
    "    else:\n",
    "        usded_index += [i_]\n",
    "    print(\"elapsed time (~usded_index) : {}\".format(time.time() - start_time))\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    1. train phase\n",
    "    \"\"\"\n",
    "    x_train_input = x_train[i_]   \n",
    "    if use_gpu:\n",
    "        similarity_ratio_train = cos_sim([x_train_input], x_train).cpu().numpy().ravel()\n",
    "    else:\n",
    "        similarity_ratio_train = cosine_similarity([x_train_input], x_train).ravel()\n",
    "    print(similarity_ratio_train)    \n",
    "    print(\"profit_index : {}\".format(i_))\n",
    "    print(\"elapsed time (~cosine_similarity) : {}\".format(time.time() - start_time))\n",
    "    \n",
    "    close_index = np.where((similarity_ratio_train > cluster_threshold) & (similarity_ratio_train < 1))[0].tolist()\n",
    "    close_index_len = len(close_index)    \n",
    "    print(\"close_index_len : {}\".format(close_index_len))\n",
    "    \n",
    "    if close_index_len < robust_len:\n",
    "        continue\n",
    "    \n",
    "    y_train_close = y_train[close_index]      \n",
    "    wr = (y_train_close > 1).sum() / close_index_len\n",
    "    pr_cumprod = np.cumprod(y_train_close)        \n",
    "    acc_pr = pr_cumprod[-1] \n",
    "    print(\"wr :\", wr)\n",
    "    print(\"acc_pr : {}\".format(acc_pr))   \n",
    "        \n",
    "        \n",
    "    usded_index += close_index\n",
    "    usded_index = list(set(usded_index))\n",
    "    # # print(usded_index)   \n",
    "    \n",
    "    \n",
    "    # if close_index_len > robust_len:\n",
    "    if wr > wr_threshold :\n",
    "\n",
    "        \n",
    "        \"\"\"\n",
    "        2. val phase\n",
    "        \"\"\"\n",
    "        if use_gpu:\n",
    "            similarity_ratio_val = cos_sim([x_train_input], x_val).cpu().numpy().ravel()\n",
    "        else:\n",
    "            similarity_ratio_val = cosine_similarity([x_train_input], x_val).ravel()            \n",
    "\n",
    "        close_index_val = np.where((similarity_ratio_val > cluster_threshold) & (similarity_ratio_val < 1))[0].tolist()        \n",
    "        close_index_val_len = len(close_index_val)        \n",
    "\n",
    "        if close_index_val_len < 1:\n",
    "            continue\n",
    "            \n",
    "        y_val_close = y_val[close_index_val]\n",
    "        wr_val = (y_val_close > 1).sum() / close_index_val_len            \n",
    "        pr_cumprod_val = np.cumprod(y_val_close)     \n",
    "        acc_pr_val = pr_cumprod_val[-1] \n",
    "        print(\"wr_val :\", wr_val)\n",
    "        print(\"acc_pr_val : {}\".format(acc_pr_val))    \n",
    "\n",
    "        if wr_val > wr_threshold :\n",
    "            \n",
    "            \"\"\"\n",
    "            3. test phase\n",
    "            \"\"\"\n",
    "            if use_gpu:\n",
    "                similarity_ratio_test = cos_sim([x_train_input], x_test).cpu().numpy().ravel()\n",
    "            else:\n",
    "                similarity_ratio_test = cosine_similarity([x_train_input], x_test).ravel()            \n",
    "\n",
    "            close_index_test = np.where((similarity_ratio_test > cluster_threshold) & (similarity_ratio_test < 1))[0].tolist()        \n",
    "            close_index_test_len = len(close_index_test)\n",
    "\n",
    "            if close_index_test_len < 1:\n",
    "                continue\n",
    "\n",
    "            y_test_close = y_test[close_index_test]\n",
    "            wr_test = (y_test_close > 1).sum() / close_index_test_len            \n",
    "            pr_cumprod_test = np.cumprod(y_test_close)     \n",
    "            acc_pr_test = pr_cumprod_test[-1] \n",
    "            print(\"wr_test :\", wr_test)\n",
    "            print(\"acc_pr_test : {}\".format(acc_pr_test))     \n",
    "            \n",
    "            \n",
    "            if wr_test > wr_threshold:  \n",
    "\n",
    "                if show_img:\n",
    "                    plt.style.use(['dark_background', 'fast'])        \n",
    "                    plt.figure(figsize=(10, 2))\n",
    "\n",
    "                    ax_ = plt.subplot(141)\n",
    "                    candle_plot_v2(ax_, x_data_train[i_][:, :4], alpha=1.0, wickwidth=1.0)\n",
    "                    plt.plot(x_data_train[i_][:, 4:])\n",
    "                    plt.title(i_, fontsize=7)\n",
    "\n",
    "                    plt.subplot(142)\n",
    "                    plt.plot(pr_cumprod)\n",
    "                    plt.title(\"wr : {:.2f}\\npr : {:.2f}\\nfrq : {}\".format(wr, pr_cumprod[-1], close_index_len), fontsize=7)\n",
    "\n",
    "                    plt.subplot(143)\n",
    "                    plt.plot(pr_cumprod_val)\n",
    "                    plt.title(\"wr : {:.2f}\\npr : {:.2f}\\nfrq : {}\".format(wr_val, pr_cumprod_val[-1], close_index_val_len), fontsize=7)\n",
    "\n",
    "                    plt.subplot(144)\n",
    "                    plt.plot(pr_cumprod_test)\n",
    "                    plt.title(\"wr : {:.2f}\\npr : {:.2f}\\nfrq : {}\".format(wr_test, pr_cumprod_test[-1], close_index_test_len), fontsize=7)\n",
    "\n",
    "                    plt.show()       \n",
    "\n",
    "                    print(\"elapsed time (~plot) : {}\".format(time.time() - start_time))\n",
    "\n",
    "\n",
    "                profitable_index[i_] = [close_index, close_index_val, close_index_test]\n",
    "                # print(profitable_index)   \n",
    "\n",
    "                with open(pkl_save_path, 'wb') as f:\n",
    "                    pickle.dump(profitable_index, f)\n",
    "                    print(pkl_save_path, 'saved.')\n",
    "            print()\n",
    "    # break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### set key info2 (for symbol by symbol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cluster_threshold = 0.97\n",
    "cluster_threshold_str = str(cluster_threshold).replace(\"0.\", \"\")\n",
    "\n",
    "robust_len_threshold = 50\n",
    "\n",
    "wr_threshold = 0.3\n",
    "wr_threshold = 0.7\n",
    "wr_threshold = 0.8\n",
    "wr_threshold_str = str(wr_threshold).replace(\"0.\", \"\")\n",
    "\n",
    "# train_size_str = str(train_size_ratio).split('.')[-1]\n",
    "val_ratio_int = int(val_ratio * 100)\n",
    "test_ratio_int = int(test_ratio * 100)\n",
    "\n",
    "data_path_ml = os.path.join(dir_path, dir_name.replace('data', 'res'))\n",
    "pkl_save_dir_path = os.path.join(data_path_ml, \"{}_{}\".format(data_size, cols_len))\n",
    "\n",
    "pkl_save_path = os.path.join(pkl_save_dir_path, \"cos_sim_data_npy\\\\test\\{}_{}_{}_{}_{}.pkl\".format(wr_threshold_str, cluster_threshold_str, robust_len_threshold, val_ratio_int, test_ratio_int))\n",
    "pkl_save_path = os.path.join(pkl_save_dir_path, \"cos_sim_data_npy\\\\val\\{}_{}_{}_{}_{}.pkl\".format(wr_threshold_str, cluster_threshold_str, robust_len_threshold, val_ratio_int, test_ratio_int))\n",
    "# pkl_save_path = os.path.join(pkl_save_dir_path, \"profitable_index_{}_{}_5.pkl\".format(cluster_threshold_str, robust_len, val_ratio_int, test_ratio_int))\n",
    "# pkl_save_path = os.path.join(pkl_save_dir_path, \"profitable_index_{}_{}.pkl\".format(cluster_threshold_str, robust_len))\n",
    "# pkl_save_path = os.path.join(pkl_save_dir_path, \"profitable_index_{}_{}_backup.pkl\".format(cluster_threshold_str, robust_len))\n",
    "os.makedirs(pkl_save_dir_path, exist_ok=True)\n",
    "\n",
    "print(pkl_save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### backtest with train DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "use_gpu = 0\n",
    "show_img = 0\n",
    "clear_cell_output = 1\n",
    "\n",
    "\n",
    "x_target, y_target = x_test, y_test\n",
    "x_target, y_target = x_val, y_val\n",
    "\n",
    "# profitable_index = {}\n",
    "# usded_index = []\n",
    "\n",
    "trade_info = []\n",
    "# break     \n",
    "\n",
    "prev_i = 0\n",
    "\n",
    "for i_ in range(len(x_target)):\n",
    "    \n",
    "    if i_ < prev_i:\n",
    "        continue\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    x_target_input = x_target[i_]    \n",
    "\n",
    "    # if i_ in usded_index:\n",
    "    #     # print(usded_index)\n",
    "    #     continue\n",
    "    # else:\n",
    "    #     usded_index += [i_]\n",
    "    # print(\"elapsed time (~usded_index) : {}\".format(time.time() - start_time))\n",
    "\n",
    "    if use_gpu:\n",
    "        similarity_ratio = cos_sim([x_target_input], x_train).cpu().numpy().ravel()\n",
    "    else:\n",
    "        similarity_ratio = cosine_similarity([x_target_input], x_train).ravel()\n",
    "    print(similarity_ratio)    \n",
    "    print(\"profit_index : {}\".format(i_))\n",
    "    print(\"elapsed time (~cosine_similarity) : {}\".format(time.time() - start_time))\n",
    "    \n",
    "    # close_index = np.where((similarity_ratio > cluster_threshold) & (similarity_ratio < 1))[0].tolist()\n",
    "    close_index = np.where(similarity_ratio > cluster_threshold)[0].tolist()\n",
    "    close_index_len = len(close_index)    \n",
    "    print(\"close_index_len : {}\".format(close_index_len))\n",
    "    \n",
    "    # usded_index += close_index\n",
    "    # usded_index = list(set(usded_index))\n",
    "    # # # print(usded_index)   \n",
    "\n",
    "    \n",
    "    if close_index_len > robust_len_threshold:\n",
    "\n",
    "        y_train_close = y_train[close_index]      \n",
    "        \n",
    "        wr = (y_train_close > 1).sum() / close_index_len\n",
    "        pr_cumprod = np.cumprod(y_train_close)        \n",
    "        acc_pr = pr_cumprod[-1] \n",
    "        print(\"wr :\", wr)\n",
    "        print(\"acc_pr : {}\".format(acc_pr))        \n",
    "\n",
    "        # if acc_pr > 1:\n",
    "        if wr > wr_threshold:       \n",
    "            \n",
    "            # trade_info.append([i_, wr, acc_pr, close_index_len])\n",
    "            trade_info.append([i_] + list(similarity_ratio))\n",
    "            # break\n",
    "            # trade_info.append([i_, wr, acc_pr, close_index_len])\n",
    "    \n",
    "#             \"\"\"\n",
    "#             2. test phase\n",
    "#             \"\"\"\n",
    "#             if use_gpu:\n",
    "#                 similarity_ratio_test = cos_sim([x_train_input], x_target).cpu().numpy().ravel()\n",
    "#             else:\n",
    "#                 similarity_ratio_test = cosine_similarity([x_train_input], x_target).ravel()            \n",
    "                \n",
    "#             close_index_test = np.where((similarity_ratio_test > cluster_threshold) & (similarity_ratio_test < 1))[0].tolist()\n",
    "            \n",
    "#             y_test_close = y_test[close_index_test]\n",
    "\n",
    "#             close_index_len_test = len(close_index_test)\n",
    "#             wr2 = (y_test_close > 1).sum() / close_index_len_test            \n",
    "#             pr_cumprod2 = np.cumprod(y_test_close)    \n",
    "            \n",
    "#             if show_img:\n",
    "#                 plt.figure(figsize=(15, 4))\n",
    "\n",
    "#                 plt.subplot(131)\n",
    "#                 plt.plot(pr_cumprod)\n",
    "#                 plt.title(\"wr : {:.2f}\\npr : {:.2f}\\nfrq : {}\".format(wr, pr_cumprod[-1], close_index_len))\n",
    "\n",
    "#                 ax2 = plt.subplot(132)\n",
    "#                 candle_plot_v2(ax2, x_data_train[i_][:, :4], alpha=1.0, wickwidth=1.0)\n",
    "#                 plt.plot(x_data_train[i_][:, 4:])\n",
    "#                 plt.title(i_)\n",
    "\n",
    "#                 print(\"elapsed time (~plot) : {}\".format(time.time() - start_time))\n",
    "            \n",
    "#                 # plt.subplot(133)\n",
    "#                 # plt.plot(pr_cumprod2)\n",
    "#                 # plt.title(\"wr : {:.2f}\\npr : {:.2f}\\nfrq : {}\".format(wr2, pr_cumprod2[-1], close_index_len_test))\n",
    "\n",
    "#                 plt.show()       \n",
    "            \n",
    "            \n",
    "            # profitable_index[i_] = [close_index, close_index_test]\n",
    "            # print(profitable_index)   \n",
    "            \n",
    "    print()\n",
    "    \n",
    "    if i_ % 100 == 0 or i_ == len(x_target) - 1:        \n",
    "        # with open(pkl_save_path, 'wb') as f:\n",
    "            # pickle.dump(trade_info, f)\n",
    "            # pickle.dump(trade_info, bz2.open(pkl_save_path.replace('.pkl', '_{}.pkl.bz2'.format(i_)), 'wb'))\n",
    "            np.save(pkl_save_path.replace('.pkl', '_{}'.format(i_)), trade_info)\n",
    "            trade_info = []\n",
    "            # bz2.open('data.pkl.bz2', 'wb')\n",
    "            print(pkl_save_path, 'saved.')\n",
    "            \n",
    "        \n",
    "    if clear_cell_output:\n",
    "        clear_output(wait=True)\n",
    "    # break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### load cos_sim_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %timeit -n1 -r5 pickle.dump(np.array(data),  open(pkl_save_path.replace('.pkl', '_test.pkl'), 'wb')) # 306 ms ± 14 ms\n",
    "%timeit -n1 -r1 pickle.dump(data,  open(pkl_save_path.replace('.pkl', '_test.pkl'), 'wb')) # 306 ms ± 14 ms\n",
    "# %timeit -n1 -r5 pickle.load( open(pkl_save_path.replace('.pkl', '_test.pkl'), 'rb')) # 7.89 ms ± 743 \n",
    "\n",
    "%timeit -n1 -r1 np.save(pkl_save_path.replace('.pkl', ''), data) # 302 ms ± 14.5 ms\n",
    "%timeit -n1 -r5 np.load(pkl_save_path.replace('.pkl', '.npy')) # 7.7 ms ± 366 µs\n",
    "\n",
    "# def decompress_pickle(file):\n",
    "#     data = bz2.BZ2File(file, 'rb')\n",
    "#     data = cPickle.load(data)\n",
    "#     return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "target_data_type = 'TEST' # 'VAL'\n",
    "target_data_type = 'VAL'\n",
    "\n",
    "cluster_threshold2 = 0.99\n",
    "\n",
    "robust_len_threshold2 = 50\n",
    "\n",
    "# wr_threshold = 0.8\n",
    "# wr_threshold_str = str(wr_threshold).replace(\"0.\", \"\")\n",
    "\n",
    "\n",
    "data_path_ml = os.path.join(dir_path, dir_name.replace('data', 'res'))\n",
    "pkl_save_dir_path = os.path.join(data_path_ml, \"{}_{}\".format(data_size, cols_len))\n",
    "\n",
    "if target_data_type == 'TEST':\n",
    "    # data_dir = os.path.join(pkl_save_dir_path, \"cos_sim_data\")\n",
    "    x_target, y_target, cls_target = x_test, y_test, cls_test    \n",
    "    data_dir = os.path.join(pkl_save_dir_path, \"cos_sim_data_npy\\\\test\")\n",
    "else:\n",
    "    x_target, y_target, cls_target = x_val, y_val, cls_val    \n",
    "    data_dir = os.path.join(pkl_save_dir_path, \"cos_sim_data_npy\\\\val\")\n",
    "\n",
    "    \n",
    "selected_idx = []\n",
    "for i_, pkl_bz2 in enumerate(os.listdir(data_dir)):\n",
    "    \n",
    "    pkl_path = os.path.join(data_dir, pkl_bz2)\n",
    "    data = np.load(pkl_path)\n",
    "    # with open(pkl_path.replace('.npy', '.pkl'), 'wb') as f:\n",
    "    # with open(pkl_path, 'rb') as f:\n",
    "    #     # pickle.dump(data, f)\n",
    "    #     data = pickle.load(f)\n",
    "    \n",
    "    if len(data) > 1:\n",
    "        # robust_idx = [np.sum(data[:, 1:] > cluster_threshold2, axis=1) > robust_len_threshold2]    \n",
    "        extracted_idx = data[:, 0][np.sum(data[:, 1:] > cluster_threshold2, axis=1) > robust_len_threshold2]    \n",
    "        \n",
    "        # close_index = data[:, 1:] > cluster_threshold2\n",
    "        # close_index_len = len(close_index) \n",
    "        # if close_index_len > robust_len_threshold2        \n",
    "        #     y_train_close = y_train[close_index]   \n",
    "        #     wr = (y_train_close > 1).sum() / close_index_len\n",
    "        \n",
    "        selected_idx += list(extracted_idx.astype(int))\n",
    "        \n",
    "    print(i_)\n",
    "    print(len(selected_idx))\n",
    "    clear_output(wait=True)\n",
    "    \n",
    "    \n",
    "    \n",
    "y_close = y_target[sorted(selected_idx)]\n",
    "close_index_len = len(y_close)\n",
    "\n",
    "wr = (y_close > 1).sum() / close_index_len\n",
    "pr_cumprod = np.cumprod(y_close)        \n",
    "acc_pr = pr_cumprod[-1]\n",
    "\n",
    "plt.plot(pr_cumprod)\n",
    "plt.title(\"wr : {:.2f}\\npr : {:.4f}\\nfrq : {}\".format(wr, pr_cumprod[-1], close_index_len))\n",
    "\n",
    "plt.savefig(r\"D:\\Projects\\SystemTrading\\JnQ\\result/Backtest_res_{}_{}.png\".format(cluster_threshold2, robust_len_threshold2), bbox_inches='tight')   \n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cls_close = cls_target[sorted(selected_idx)]\n",
    "\n",
    "cls_res = []\n",
    "for cls_i in np.unique(cls_close):\n",
    "    y_close_by_cls = y_close[cls_close == cls_i].ravel()\n",
    "    # print(\"cls_close[cls_close == cls_i] :\", cls_close[cls_close == cls_i])\n",
    "    # break\n",
    "    close_index_len = len(y_close_by_cls)\n",
    "\n",
    "    wr = (y_close_by_cls > 1).sum() / close_index_len\n",
    "    pr_cumprod = np.cumprod(y_close_by_cls)        \n",
    "    acc_pr = pr_cumprod[-1]\n",
    "    \n",
    "    cls_res.append([cls_i, wr, acc_pr, close_index_len, y_close_by_cls])\n",
    "    \n",
    "df_res_cls = pd.DataFrame(cls_res, columns=['cls_i', 'wr', 'acc_pr', 'frq', 'y_close_by_cls']).sort_values(by='acc_pr', ascending=False)\n",
    "# df_res_cls = pd.DataFrame(cls_res, columns=['cls_i', 'wr', 'acc_pr', 'frq', 'y_close_by_cls']).sort_values(by='wr', ascending=False)\n",
    "df_res_cls.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_res_cls_test = df_res_cls.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_res_cls_test.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cls_top = df_res_cls_test.head(10)['cls_i'].to_numpy()\n",
    "cls_top#.to_numpy()#.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_close_by_cls = np.hstack(df_res_cls.iloc[:10]['y_close_by_cls'].to_numpy())\n",
    "\n",
    "close_index_len = len(y_close_by_cls)\n",
    "\n",
    "wr = (y_close_by_cls > 1).sum() / close_index_len\n",
    "pr_cumprod = np.cumprod(y_close_by_cls)        \n",
    "acc_pr = pr_cumprod[-1]\n",
    "\n",
    "plt.figure(figsize=(4,3))\n",
    "plt.plot(pr_cumprod)\n",
    "plt.title(\"wr : {:.2f}\\npr : {:.4f}\\nfrq : {}\".format(wr, pr_cumprod[-1], close_index_len))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for _, row in df_res_cls.iterrows():\n",
    "    # print(row)\n",
    "    \n",
    "    if row.cls_i in cls_top:\n",
    "        pr_cumprod = np.cumprod(row.y_close_by_cls)\n",
    "        plt.figure(figsize=(4,3))\n",
    "        plt.plot(pr_cumprod)\n",
    "        plt.title(\"wr : {:.2f}\\npr : {:.4f}\\nfrq : {}\".format(row.wr, pr_cumprod[-1], row.frq))\n",
    "\n",
    "        # plt.savefig(r\"D:\\Projects\\SystemTrading\\JnQ\\result/Backtest_res_{}_{}.png\".format(cluster_threshold2, robust_len_threshold2), bbox_inches='tight')   \n",
    "\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### legacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%timeit -r1 -n5 cosine_similarity([x_train[0]], x_train)\n",
    "# %timeit -n5 cosine_similarity([x_train_2[0]], x_train_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%timeit -n1 cos_sim([x_train[0]], x_train)\n",
    "# %timeit -n1 cos_sim(x_train, x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cos_sim_res = cos_sim([x_train[0]], x_train).ravel()\n",
    "cos_sim_res_np = cos_sim_res.cpu().numpy().ravel()\n",
    "# cos_sim_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%timeit -n1 np.where(cos_sim_res > cluster_threshold)[0]\n",
    "%timeit -n1 np.where(cos_sim_res_np > cluster_threshold)[0]\n",
    "# cos_sim_res > 0.99"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load data paired key (+ get v_concated.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(pkl_save_path, 'rb') as f:\n",
    "    profitable_index2 = pickle.load(f)\n",
    "    print(pkl_save_path, \"loaded.\")\n",
    "    \n",
    "# with open(pkl_save_path, 'wb') as f:\n",
    "#     pickle.dump(profitable_index2, f)\n",
    "#     print(pkl_save_path, 'saved.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "valid_index_path = os.path.join(pkl_save_dir_path, \"valid_index_backup.txt\")  \n",
    "\n",
    "valid_key_data_path = valid_index_path.replace('.txt', '.pkl')\n",
    "robust_key_path = os.path.join(valid_key_data_path.split('.')[0], \"robust_key.txt\")  \n",
    "valid_key_res_save_path = valid_key_data_path.split('.')[0]\n",
    "robust_key_res_save_path = os.path.join(valid_key_data_path.split('.')[0], 'robust_res')\n",
    "\n",
    "print(\"valid_index_path :\", valid_index_path)\n",
    "print(\"robust_key_path :\", robust_key_path)\n",
    "print(\"valid_key_res_save_path :\", valid_key_res_save_path)\n",
    "print(\"robust_key_res_save_path :\", robust_key_res_save_path)\n",
    "os.makedirs(robust_key_res_save_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(valid_index_path, 'r') as f:\n",
    "    # valid_index = f.readlines()\n",
    "    valid_index = f.read()\n",
    "    print(valid_index_path, \"loaded.\")\n",
    "    \n",
    "valid_index = valid_index.replace(' ', '').replace('v', '').replace('?', '') #.replace(\"\\n\\n\", '')\n",
    "valid_index = valid_index.split('\\n')\n",
    "valid_index = [i_ for i_ in valid_index if i_ != '']\n",
    "valid_index_int = list(map(int, valid_index))\n",
    "\n",
    "print(\"valid_index_int :\", valid_index_int)\n",
    "\n",
    "\n",
    "v_concated = sorted(list(set(np.concatenate([profitable_index2[int(k)][0] for k in valid_index_int])))) #.ravel()\n",
    "v2_concated = sorted(list(set(np.concatenate([profitable_index2[int(k)][1] for k in valid_index_int])))) #.ravel()\n",
    "v3_concated = sorted(list(set(np.concatenate([profitable_index2[int(k)][2] for k in valid_index_int])))) #.ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### check data paired key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spread_min = 0.05\n",
    "spread_min = 0.02\n",
    "spread_min = 0.0\n",
    "\n",
    "show_img = 1\n",
    "\n",
    "for k, (v, v2, v3) in profitable_index2.items():\n",
    "    \n",
    "    if k in valid_index_int:  # on / off\n",
    "\n",
    "        v_valid = spread_train[v] > spread_min\n",
    "        y_train_close = y_train[v][v_valid]\n",
    "        \n",
    "        close_index_len = len(y_train_close)\n",
    "        \n",
    "        wr = (y_train_close > 1).sum() / close_index_len\n",
    "        pr_cumprod = np.cumprod(y_train_close)\n",
    "\n",
    "        \n",
    "        \n",
    "        # similarity_ratio_val = cosine_similarity(x_val, [x_train[k]]).ravel()        \n",
    "        # v2 = np.where(similarity_ratio_val > cluster_threshold)[0].tolist()\n",
    "        \n",
    "        v2_valid = spread_val[v2] > spread_min\n",
    "        y_val_close = y_val[v2][v2_valid]\n",
    "\n",
    "        close_index_len_val = len(y_val_close)\n",
    "        \n",
    "        wr2 = (y_val_close > 1).sum() / close_index_len_val\n",
    "        pr_cumprod2 = np.cumprod(y_val_close)\n",
    "        \n",
    "        \n",
    "        \n",
    "        # similarity_ratio_test = cosine_similarity(x_test, [x_train[k]]).ravel()        \n",
    "        # v3 = np.where(similarity_ratio_test > cluster_threshold)[0].tolist()\n",
    "        \n",
    "        v3_valid = spread_test[v3] > spread_min\n",
    "        y_test_close = y_test[v3][v3_valid]\n",
    "\n",
    "        close_index_len_test = len(y_test_close)\n",
    "        \n",
    "        wr3 = (y_test_close > 1).sum() / close_index_len_test\n",
    "        pr_cumprod3 = np.cumprod(y_test_close)\n",
    "        \n",
    "  \n",
    "        # if pr_cumprod[-1] < 1 or pr_cumprod2[-1] < 1:\n",
    "        #     continue\n",
    "\n",
    "        if show_img:        \n",
    "            try:\n",
    "                plt.style.use(['dark_background', 'fast'])        \n",
    "                plt.figure(figsize=(10, 2))\n",
    "\n",
    "                ax2 = plt.subplot(141)\n",
    "                candle_plot_v2(ax2, x_data_train[k][:, :4], alpha=1.0, wickwidth=1.0)\n",
    "                plt.plot(x_data_train[k][:, 4:])\n",
    "                plt.title(k)\n",
    "                \n",
    "                plt.subplot(142)\n",
    "                plt.plot(pr_cumprod)\n",
    "                plt.title(\"wr : {:.2f}\\npr : {:.4f}\\nfrq : {}\".format(wr, pr_cumprod[-1], close_index_len))\n",
    "                # plt.show()\n",
    "\n",
    "                plt.subplot(143)\n",
    "                plt.plot(pr_cumprod2)\n",
    "                plt.title(\"wr : {:.2f}\\npr : {:.4f}\\nfrq : {}\".format(wr2, pr_cumprod2[-1], close_index_len_val))\n",
    "\n",
    "\n",
    "                plt.subplot(144)\n",
    "                plt.plot(pr_cumprod3)\n",
    "                plt.title(\"wr : {:.2f}\\npr : {:.4f}\\nfrq : {}\".format(wr3, pr_cumprod3[-1], close_index_len_test))\n",
    "\n",
    "                plt.show()   \n",
    "                \n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### adjust_time_span on v_concated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.plot(np.cumprod(y_train_close2[abs(y_train_close2 - 1) > .001]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# v_ts_random = adjust_time_span(v_concated, ts_train, 60 * 1)\n",
    "v_ts_random = v_concated_train\n",
    "\n",
    "close_index_len2 = len(v_ts_random)\n",
    "  \n",
    "y_train_close2 = y_train[v_ts_random]  \n",
    "wr = (y_train_close2 > 1).sum() / close_index_len2\n",
    "pr_cumprod = np.cumprod(y_train_close2)\n",
    "pr_cumsum = np.cumsum(y_train_close2 - 1) + 1\n",
    "\n",
    "# plt.subplot(132)\n",
    "plt.plot(pr_cumprod)\n",
    "plt.title(\"wr : {:.2f}\\npr : {:.2f}\\nfrq : {}\".format(wr, pr_cumprod[-1], close_index_len2))\n",
    "\n",
    "\"\"\"frq_dev_plot need a ticker's whole time range (start ~ end date)\"\"\"\n",
    "# time_range = 60 * 24 * 365 * 2 # 2 years.\n",
    "time_range = (ts_train.max() - ts_train.min()) / 60\n",
    "print(\"time_range (days) : {}\".format(time_range / 1440))\n",
    "\n",
    "mode = 'CRYPTO'\n",
    "fontsize = 10\n",
    "title_msg = \"periodic_pr (acc | sum)\\n day : {:.4f} | {:.4f}\\n month : {:.2f} | {:.2f}\\n year : {:.2f} | {:.2f}\"  # \\n rev_acc_day : {:.4f}\\n month : {:.4f}\\n year : {:.4f}\"\n",
    "array_zip = np.array(list(zip(get_period_pr_v3(time_range, pr_cumprod[-1], mode=mode), get_period_pr_v3(time_range, pr_cumsum[-1], pr_type=\"SUM\", mode=mode)))).ravel()\n",
    "\n",
    "plt.suptitle(title_msg.format(*array_zip, fontsize=fontsize), y=0.3)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "v_ts_random = v_concated_test\n",
    "close_index_len2 = len(v_ts_random)\n",
    "  \n",
    "y_test_close2 = y_test[v_ts_random]  \n",
    "wr = (y_test_close2 > 1).sum() / close_index_len2\n",
    "pr_cumprod = np.cumprod(y_test_close2)\n",
    "pr_cumsum = np.cumsum(y_test_close2 - 1) + 1\n",
    "\n",
    "# plt.subplot(132)\n",
    "plt.plot(pr_cumprod)\n",
    "plt.title(\"wr : {:.2f}\\npr : {:.2f}\\nfrq : {}\".format(wr, pr_cumprod[-1], close_index_len2))\n",
    "\n",
    "\"\"\"frq_dev_plot need a ticker's whole time range (start ~ end date)\"\"\"\n",
    "# time_range = 60 * 24 * 365 * 2 # 2 years.\n",
    "time_range = (ts_test.max() - ts_test.min()) / 60\n",
    "print(\"time_range (days) : {}\".format(time_range / 1440))\n",
    "\n",
    "array_zip = np.array(list(zip(get_period_pr_v3(time_range, pr_cumprod[-1], mode=mode), get_period_pr_v3(time_range, pr_cumsum[-1], pr_type=\"SUM\", mode=mode)))).ravel()\n",
    "\n",
    "plt.suptitle(title_msg.format(*array_zip, fontsize=fontsize), y=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save key_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "valid_key_data_path = valid_index_path.replace('.txt', '_key.pkl')\n",
    "robust_key_data_path = os.path.join(robust_key_res_save_path, \"robust_key.pkl\")\n",
    "\n",
    "with open(valid_data_path, 'wb') as f:\n",
    "    key_data = x_train[valid_index_int]\n",
    "    pickle.dump(key_data, f)\n",
    "    print(valid_data_path, 'saved.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "robust_key_data_path = os.path.join(robust_key_res_save_path, \"robust_key.pkl\")\n",
    "\n",
    "with open(robust_key_data_path, 'wb') as f:\n",
    "    # key_data = x_train[valid_index_int]\n",
    "    pickle.dump(key_data[[4, 24, 29, 36]], f)\n",
    "    print(robust_key_data_path, 'saved.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### load key_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "key_data_path = valid_index_path.replace('.txt', '.pkl')\n",
    "key_data_path = robust_key_path.replace('.txt', '.pkl')\n",
    "print(\"key_data_path :\", key_data_path)\n",
    "\n",
    "with open(key_data_path, 'rb') as f:\n",
    "    key_data = pickle.load(f)\n",
    "    \n",
    "print(key_data.shape)\n",
    "# print(len(valid_index_int))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### load test_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_index_path = os.path.join(pkl_save_dir_path, \"test_index_{}_{}.txt\".format(cluster_threshold_str, robust_len))    # time_span 에 robust 한 index\n",
    "\n",
    "with open(test_index_path, 'r') as f:\n",
    "    # test_index = f.readlines()\n",
    "    test_index = f.read()\n",
    "    print(test_index_path, \"loaded.\")\n",
    "    \n",
    "test_index = test_index.replace(' ', '').replace('v', '').replace('?', '') #.replace(\"\\n\\n\", '')\n",
    "test_index = test_index.split('\\n')\n",
    "test_index = [i_ for i_ in test_index if i_ != '']\n",
    "test_index_int = list(map(int, test_index))\n",
    "\n",
    "print(\"test_index_int :\", test_index_int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### key_data on TVT : get v_concated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# use_robust_key = 1\n",
    "cluster_threshold_1 = 0.99\n",
    "\n",
    "spread_min = 0.02\n",
    "spread_min = 0.0\n",
    "\n",
    "show_img = 1\n",
    "\n",
    "save_img = 0\n",
    "\n",
    "\n",
    "# import matplotlib\n",
    "# matplotlib.rcParams.update({'font.size': 7})\n",
    "fontsize = 7\n",
    "\n",
    "\n",
    "# robust_key = []\n",
    "# if use_robust_key:\n",
    "#     try:\n",
    "#         with open(os.path.join(robust_key_save_path, \"robust_key.txt\"), 'r') as f:        \n",
    "#             robust_key = f.read()\n",
    "\n",
    "#         robust_key = robust_key.replace(' ', '').replace('v', '').replace('?', '') #.replace(\"\\n\\n\", '')\n",
    "#         robust_key = robust_key.split('\\n')\n",
    "#         robust_key = [i_ for i_ in robust_key if i_ != '']\n",
    "#         robust_key = list(map(int, robust_key))\n",
    "#     except Exception as e:\n",
    "#         print(\"error in load robust_key :\",  e)    \n",
    "#     print(\"robust_key :\", robust_key)\n",
    "  \n",
    "\n",
    "v_concated = []\n",
    "v2_concated = []  \n",
    "v3_concated = []     \n",
    "        \n",
    "for v_i, data_ in enumerate(key_data):\n",
    "    \n",
    "    # if use_robust_key and v_i not in robust_key:  # on / off\n",
    "    #     continue\n",
    "    # else:\n",
    "    \n",
    "        similarity_ratio_train = cosine_similarity(x_train, [data_]).ravel()        \n",
    "        v = np.where(similarity_ratio_train > cluster_threshold)[0].tolist()\n",
    "        \n",
    "        v_valid = spread_train[v] > spread_min\n",
    "        y_train_close = y_train[v][v_valid]\n",
    "        \n",
    "        close_index_len = len(y_train_close)\n",
    "        \n",
    "        wr = (y_train_close > 1).sum() / close_index_len\n",
    "        pr_cumprod = np.cumprod(y_train_close)\n",
    "\n",
    "        \n",
    "        \n",
    "        similarity_ratio_val = cosine_similarity(x_val, [data_]).ravel()        \n",
    "        v2 = np.where(similarity_ratio_val > cluster_threshold)[0].tolist()\n",
    "        \n",
    "        v2_valid = spread_val[v2] > spread_min\n",
    "        y_val_close = y_val[v2][v2_valid]\n",
    "\n",
    "        close_index_len_val = len(y_val_close)\n",
    "        \n",
    "        wr2 = (y_val_close > 1).sum() / close_index_len_val\n",
    "        pr_cumprod2 = np.cumprod(y_val_close)\n",
    "        \n",
    "        \n",
    "        \n",
    "        similarity_ratio_test = cosine_similarity(x_test, [data_]).ravel()        \n",
    "        v3 = np.where(similarity_ratio_test > cluster_threshold)[0].tolist()\n",
    "        \n",
    "        v3_valid = spread_test[v3] > spread_min\n",
    "        y_test_close = y_test[v3][v3_valid]\n",
    "\n",
    "        close_index_len_test = len(y_test_close)\n",
    "        \n",
    "        wr3 = (y_test_close > 1).sum() / close_index_len_test\n",
    "        pr_cumprod3 = np.cumprod(y_test_close)\n",
    "        \n",
    "        \n",
    "        v_concated += v\n",
    "        v2_concated += v2\n",
    "        v3_concated += v3\n",
    "        \n",
    "  \n",
    "        # if pr_cumprod[-1] < 1 or pr_cumprod2[-1] < 1:\n",
    "        #     continue\n",
    "\n",
    "        if show_img:        \n",
    "            try:\n",
    "                plt.style.use(['dark_background', 'fast'])        \n",
    "                plt.figure(figsize=(10, 2))\n",
    "\n",
    "                ax2 = plt.subplot(141)\n",
    "                data_reshape = data_.reshape(data_size, cols_len)\n",
    "                candle_plot_v2(ax2, data_reshape[:, :4], alpha=1.0, wickwidth=1.0)\n",
    "                plt.plot(data_reshape[:, 4:])\n",
    "                plt.title(v_i, fontsize=fontsize)\n",
    "                \n",
    "                plt.subplot(142)\n",
    "                if len(pr_cumprod) > 0:\n",
    "                    plt.plot(pr_cumprod)\n",
    "                    plt.title(\"wr : {:.2f}\\npr : {:.4f}\\nfrq : {}\".format(wr, pr_cumprod[-1], close_index_len), fontsize=fontsize)\n",
    "                    # plt.show()\n",
    "\n",
    "                plt.subplot(143)\n",
    "                if len(pr_cumprod2) > 0:\n",
    "                    plt.plot(pr_cumprod2)\n",
    "                    plt.title(\"wr : {:.2f}\\npr : {:.4f}\\nfrq : {}\".format(wr2, pr_cumprod2[-1], close_index_len_val), fontsize=fontsize)\n",
    "\n",
    "\n",
    "                plt.subplot(144)\n",
    "                if len(pr_cumprod3) > 0:\n",
    "                    plt.plot(pr_cumprod3)\n",
    "                    plt.title(\"wr : {:.2f}\\npr : {:.4f}\\nfrq : {}\".format(wr3, pr_cumprod3[-1], close_index_len_test), fontsize=fontsize)\n",
    "\n",
    "                if save_img:\n",
    "                    plt.savefig(os.path.join(robust_key_save_path, \"{}.png\".format(v_i)), bbox_inches='tight')\n",
    "                plt.show()   \n",
    "                \n",
    "            except Exception as e:\n",
    "                print(\"error in show_img : {}\".format(e))\n",
    "                pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data distribution_vx : sorting + limited workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 1. target_spread 에 도달하기 위한 lvrg selection\n",
    "use_robust_key = 1\n",
    "\n",
    "spread_max = .5\n",
    "# spread_max = .25\n",
    "# spread_max = .05\n",
    "\n",
    "spread_min = .1\n",
    "# spread_min = .075\n",
    "spread_min = .05\n",
    "# spread_min = .03\n",
    "# spread_min = .025\n",
    "spread_min = .02\n",
    "# spread_min = .015\n",
    "spread_min = .0\n",
    "\n",
    "\n",
    "lvrg_max = 1e10\n",
    "lvrg_max = 50\n",
    "# lvrg_max = 30\n",
    "# lvrg_max = 25\n",
    "# lvrg_max = 15\n",
    "lvrg_max = 2\n",
    "# lvrg_max = 1\n",
    "\n",
    "lvrg_max2 = 30\n",
    "lvrg_max2 = lvrg_max\n",
    "\n",
    "lvrg_min = 1\n",
    "# lvrg_min = 0\n",
    "\n",
    "\n",
    "\n",
    "lvrg_ceiling = 1\n",
    "# lvrg_ceiling = 0\n",
    "\n",
    "lvrg_type = 'STATIC'\n",
    "lvrg_type = 'DYNAMIC'\n",
    "\n",
    "lvrg_k = 3\n",
    "lvrg_k = 2\n",
    "lvrg_k = 1\n",
    "# lvrg_k = 0.1\n",
    "\n",
    "\n",
    "\n",
    "tr_max = 0.3\n",
    "# tr_max = 0.175\n",
    "# tr_max = 0.15\n",
    "# tr_max = 0.13\n",
    "\n",
    "# tr_min = 0.152\n",
    "tr_min = 0.1\n",
    "# tr_min = 0.075\n",
    "tr_min = 0.0\n",
    "\n",
    "\n",
    "\n",
    "workers = 200\n",
    "workers = 100\n",
    "# workers = 30\n",
    "# workers = 20\n",
    "# workers = 15\n",
    "# workers = 10\n",
    "# workers = 5\n",
    "# workers = 1\n",
    "# time_span_size = 60 * 100\n",
    "\n",
    "\n",
    "multiplier, log_base = 5000, 10\n",
    "# multiplier, log_base = 1500, 30\n",
    "multiplier, log_base = 1000, 30\n",
    "# multiplier, log_base = 0, 30\n",
    "\n",
    "\n",
    "def set_dynamic_lvrg(out_data, en_data, ex_data, fee_data, target_pct=0.05, multiplier=100, log_base=10):\n",
    "        \n",
    "    loss_expected = abs((out_data / en_data) * (1 - 0.0006) - 1)\n",
    "    # lvrg_expected = (target_pct / loss_expected).astype(int)  # flooring.\n",
    "    lvrg_expected = (loss_expected * multiplier)#.astype(int)  # flooring.\n",
    "    # lvrg_expected = (1 / loss_expected * multiplier)#.astype(int)  # flooring.\n",
    "    # plt.hist(lvrg_expected, bins=500)\n",
    "    # plt.show()        \n",
    "    print(\"lvrg_expected : {}\".format(lvrg_expected))\n",
    "    lvrg_expected =  np.array(list(map(lambda x: math.log(x, log_base) if x > 0 else 1, lvrg_expected)))\n",
    "    print(\"lvrg_expected (log adj.) : {}\".format(lvrg_expected))\n",
    "    lvrg_expected = lvrg_expected.astype(int)\n",
    "    print(\"lvrg_expected (int) : {}\".format(lvrg_expected))\n",
    "    print(\"lvrg max : {}\".format(lvrg_expected.max()))\n",
    "\n",
    "    print(\"invalid leverage percentage : {:.2%}\".format(len(loss_expected[lvrg_expected == 0]) / len(loss_expected)))\n",
    "    \n",
    "    y_data2 = (ex_data / en_data * (1 - fee_data) - 1) * lvrg_expected + 1\n",
    "    y_data2[lvrg_expected == 0] = 1  # lvrg_rejection.\n",
    "    return y_data2\n",
    "\n",
    "\n",
    "def plot_pr(y_target_final, data_period):\n",
    "    \n",
    "    y_target_final[y_target_final <= 0] = 0\n",
    "    # y_target_final_len = len(y_target_final[y_target_final != 1])\n",
    "    y_target_final_len = (y_target_final != 1).sum()\n",
    "    wr = (y_target_final > 1).sum() / y_target_final_len\n",
    "    pr_cumprod = np.cumprod(y_target_final)\n",
    "\n",
    "    plt.plot(pr_cumprod)    \n",
    "    title_msg = \"frq : {}\\nwr : {:.3f}\\npr : {:.3f}\\nmdd : {:.3f}\".format(y_target_final_len, wr, pr_cumprod[-1], mdd_v2(pr_cumprod))   \n",
    "    title_msg += \"\\nday : {:.3f}\\nmonth : {:.3f}\\nyear : {:.3f}\".format(*get_period_pr_v3(data_period, pr_cumprod[-1], pr_type=\"PROD\", mode='CRYPTO'))    \n",
    "    plt.title(title_msg, y=0.3, fontsize=10, loc='left')\n",
    "\n",
    "\n",
    "def check_distribution_v3(spread_data, y_data, out_data, en_data, ex_data, fee_data, tr_data, v_concated, ts_data, bars_in_data, multiplier, log_base, data_type='TRAIN'):\n",
    "    \n",
    "    \"\"\"\n",
    "    v2 --> v3\n",
    "        1. allow dynamic_lvrg.\n",
    "    \"\"\"\n",
    "    \n",
    "    plt.figure(figsize=(10,10))\n",
    "    # plt.figure(figsize=(9,9))\n",
    "    rows, cols = 4, 3\n",
    "    \n",
    "    if multiplier != 0:\n",
    "        y_data = set_dynamic_lvrg(out_data, en_data, ex_data, fee_data, multiplier=multiplier, log_base=log_base)\n",
    "    \n",
    "    spread_target = spread_data[v_concated]\n",
    "    y_target = y_data[v_concated]\n",
    "    tr_target = tr_data[v_concated]\n",
    "    \n",
    "    v_concated = np.array(v_concated)\n",
    "    \n",
    "    \n",
    "    data_period = (ts_data[-1] - ts_data[0]) / 60  # minute\n",
    "    # print(\"data_period :\", data_period)\n",
    "\n",
    "\n",
    "    # spread_range = spread_target - 1\n",
    "    spread_range = spread_target\n",
    "    \n",
    "    # lvrg_needed_ = (1 / spread_range_scaled * lvrg_k)\n",
    "    lvrg_needed_ = (1 / spread_range * lvrg_k)\n",
    "    if lvrg_ceiling:\n",
    "        lvrg_needed_[lvrg_needed_ > lvrg_max] = lvrg_max \n",
    "    lvrg_needed_int_ = lvrg_needed_.astype('uint32')\n",
    "    lvrg_needed_int_[lvrg_needed_int_ < 1] = 1\n",
    "       \n",
    "        \n",
    "\n",
    "    \n",
    "    v3_01 = y_target.ravel() > 1\n",
    "    v3_02 = y_target.ravel() < 1\n",
    "    \n",
    "    v3 = (spread_min <= spread_range) & (spread_range <= spread_max)    \n",
    "    \n",
    "    y_target_spread = y_target[v3].ravel()\n",
    "    v3_1 = y_target_spread > 1\n",
    "    v3_2 = y_target_spread < 1\n",
    "\n",
    "\n",
    "    plt.subplot(rows, cols, 1)\n",
    "    plt.scatter(np.arange(len(spread_range[v3_01])), spread_range[v3_01], color='g', s=5)\n",
    "    plt.scatter(np.arange(len(spread_range[v3_02])), spread_range[v3_02], color='r', s=5)\n",
    "    plt.axhline(spread_max, linestyle='--')\n",
    "    plt.axhline(spread_min, linestyle='--')\n",
    "    plt.title('spread', loc='right', fontsize=10)\n",
    "\n",
    "    plt.subplot(rows, cols, 2)\n",
    "    # plt.scatter(np.arange(len(spread_range_scaled)), spread_range_scaled, s=5)\n",
    "    \"\"\"\n",
    "    # x_range 는 짧아지는게 정상 : v3_1 / v3_2 를 분리해서 표기했기 때문에.\n",
    "    \"\"\"\n",
    "    plt.scatter(np.arange(len(spread_range[v3][v3_1])), spread_range[v3][v3_1], color='g', s=5)\n",
    "    plt.scatter(np.arange(len(spread_range[v3][v3_2])), spread_range[v3][v3_2], color='r', s=5)\n",
    "    # plt.show()\n",
    "\n",
    "    plt.subplot(rows, cols, 3)\n",
    "    \n",
    "    v_concated_final = v_concated[v3]\n",
    "    ts_valid_idx_sorted = adjust_time_span_v3(v_concated_final, ts_data, bars_in_data, workers=workers)    \n",
    "    v_concated_ts = v_concated_final[ts_valid_idx_sorted]\n",
    "    \n",
    "    y_target_spread_ts = y_data[v_concated_ts].ravel()    \n",
    "    plot_pr(y_target_spread_ts, data_period)    \n",
    "    # plt.show()\n",
    "\n",
    "    \n",
    "        \n",
    "        \n",
    "    lvrg_needed = lvrg_needed_[v3]\n",
    "    \n",
    "    # v4 = lvrg_needed < lvrg_max    \n",
    "#     v4_01 = y_target[v3].ravel() > 1\n",
    "#     v4_02 = y_target[v3].ravel() < 1    \n",
    "        \n",
    "    # 1. threshing 우선. --> loc_set.\n",
    "    # v4 = (lvrg_min < lvrg_needed)   \n",
    "    v4 = (lvrg_min <= lvrg_needed) & (lvrg_needed <= lvrg_max2)\n",
    "    \n",
    "    # 2. int화, min_value validation 나중. --> lvrg_liqd()\n",
    "    #    a. 해당 phase 를 vectorize 하기 위해 우선하게 되면, threshing 기능이 약화됨 (섬세하게 접근할 수 없게된다.)    \n",
    "    #        i. utils --> public 의 순서를 가지고 있기 때문.\n",
    "    lvrg_needed_int = lvrg_needed[v4].astype('uint32')\n",
    "    # lvrg_needed_int = lvrg_needed[v4].astype('uint8')\n",
    "    lvrg_needed_int[lvrg_needed_int < 1] = 1\n",
    "\n",
    "    y_target_lvrg = y_target[v3][v4].ravel()\n",
    "    v4_1 = y_target_lvrg > 1\n",
    "    v4_2 = y_target_lvrg < 1\n",
    "    \n",
    "    \n",
    "\n",
    "    plt.subplot(rows, cols, 4)\n",
    "\n",
    "    # plt.scatter(np.arange(len(spread_range_scaled)), lvrg_needed, s=5)\n",
    "    plt.scatter(np.arange(len(lvrg_needed[v3_1])), lvrg_needed[v3_1], color='g', s=5)\n",
    "    plt.scatter(np.arange(len(lvrg_needed[v3_2])), lvrg_needed[v3_2], color='r', s=5)\n",
    "    # plt.bar(np.arange(len(spread_range_scaled)), lvrg_needed)\n",
    "    plt.axhline(lvrg_max, alpha=.5, linestyle='--')\n",
    "    plt.axhline(lvrg_max2, alpha=1, linestyle='--')\n",
    "    plt.axhline(lvrg_min, linestyle='--')\n",
    "    plt.title('lvrg', loc='right', fontsize=10)\n",
    "    # plt.show()\n",
    "\n",
    "    plt.subplot(rows, cols, 5)\n",
    "    # plt.bar(np.arange(len(lvrg_needed_int)), lvrg_needed_int)\n",
    "    plt.scatter(np.arange(len(lvrg_needed_int[v4_1])), lvrg_needed_int[v4_1], color='g', s=5)\n",
    "    plt.scatter(np.arange(len(lvrg_needed_int[v4_2])), lvrg_needed_int[v4_2], color='r', s=5)\n",
    "    plt.title('lvrg_int', loc='right', fontsize=10)\n",
    "    # plt.show()\n",
    "\n",
    "    plt.subplot(rows, cols, 6)\n",
    "    \n",
    "    v_concated_final = v_concated[v3][v4]\n",
    "    ts_valid_idx_sorted = adjust_time_span_v3(v_concated_final, ts_data, bars_in_data, workers=workers)    \n",
    "    v_concated_ts = v_concated_final[ts_valid_idx_sorted]\n",
    "    \n",
    "    y_target_lvrg_ts = y_data[v_concated_ts].ravel()\n",
    "    plot_pr(y_target_lvrg_ts, data_period)\n",
    "    # plt.show()\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    tr_range = tr_target[v3][v4]\n",
    "\n",
    "    v5 = (tr_min <= tr_range) & (tr_range <= tr_max)\n",
    "    \n",
    "    y_target_tr = y_target[v3][v4][v5].ravel()\n",
    "    v5_1 = y_target_tr > 1\n",
    "    v5_2 = y_target_tr < 1\n",
    "    \n",
    "    \n",
    "\n",
    "    # plt.subplot(1,1,3)\n",
    "    plt.subplot(rows, cols, 7)\n",
    "    # plt.scatter(np.arange(len(tr_range)), tr_range, s=5)\n",
    "    plt.scatter(np.arange(len(tr_range[v4_1])), tr_range[v4_1], color='g', s=5)\n",
    "    plt.scatter(np.arange(len(tr_range[v4_2])), tr_range[v4_2], color='r', s=5)\n",
    "    plt.axhline(tr_max, linestyle='--')\n",
    "    plt.axhline(tr_min, linestyle='--')\n",
    "    # # plt.show()\n",
    "    plt.title('tr', loc='right', fontsize=10)\n",
    "\n",
    "    plt.subplot(rows, cols, 8)\n",
    "    # plt.scatter(np.arange(len(tr_range[v5])), tr_range[v5])\n",
    "    plt.scatter(np.arange(len(tr_range[v5][v5_1])), tr_range[v5][v5_1], color='g', s=5)\n",
    "    plt.scatter(np.arange(len(tr_range[v5][v5_2])), tr_range[v5][v5_2], color='r', s=5)\n",
    "\n",
    "    # v4 = lvrg_needed < lvrg_max\n",
    "    plt.subplot(rows, cols, 9)\n",
    "    \n",
    "    v_concated_final = v_concated[v3][v4][v5]\n",
    "    ts_valid_idx_sorted = adjust_time_span_v3(v_concated_final, ts_data, bars_in_data, workers=workers)    \n",
    "    v_concated_ts = v_concated_final[ts_valid_idx_sorted]\n",
    "    \n",
    "    y_target_tr_ts = y_data[v_concated_ts].ravel()\n",
    "    plot_pr(y_target_tr_ts, data_period)\n",
    "    \n",
    "    \n",
    "        \n",
    "    plt.subplot(rows, cols, 12) \n",
    "    \n",
    "    if lvrg_type == 'STATIC':\n",
    "        y_target_final = (y_target_tr_ts - 1) * lvrg_k + 1\n",
    "    else:\n",
    "        lvrg_needed_int_ts = lvrg_needed_int_[v3][v4][v5][ts_valid_idx_sorted]\n",
    "        y_target_final = (y_target_tr_ts - 1) * lvrg_needed_int_ts + 1\n",
    "    # print(\"np.sum(y_target_final < 0) :\", np.sum(y_target_final < 0))\n",
    "    \n",
    "    plot_pr(y_target_final, data_period)\n",
    "\n",
    "    \n",
    "    \n",
    "    plt.suptitle(\"{}\".format(data_type))\n",
    "    save_digits = ''.join(['_' + str(val) for val in [spread_max, spread_min, lvrg_max, lvrg_min, lvrg_k, tr_max, tr_min, workers, multiplier, log_base]])\n",
    "    \n",
    "    if use_robust_key:\n",
    "        plt.savefig(os.path.join(robust_key_res_save_path, \"{}_{}.png\".format(save_digits, data_type)))\n",
    "    else:\n",
    "        plt.savefig(os.path.join(valid_key_res_save_path, \"res\\\\{}_{}.png\".format(save_digits, data_type)))\n",
    "    plt.show()\n",
    "   \n",
    "\n",
    "\n",
    "# check_distribution_v2(spread_train, y_train, tr_train, v_concated, ts_train, bars_in_train, 'TRAIN_TS')\n",
    "# check_distribution_v2(spread_val, y_val, tr_val, v2_concated, ts_val, bars_in_val, 'VAL_TS')\n",
    "# check_distribution_v2(spread_test, y_test, tr_test, v3_concated, ts_test, bars_in_test, 'TEST_TS')\n",
    "\n",
    "# check_distribution_v3(spread_train, y_train, out_train, en_train, ex_train, fee_train, tr_train, v_concated, ts_train, bars_in_train, multiplier, log_base, 'TRAIN_TS')\n",
    "# check_distribution_v3(spread_val, y_val, out_val, en_val, ex_val, fee_val, tr_val, v2_concated, ts_val, bars_in_val, multiplier, log_base, 'VAL_TS')\n",
    "check_distribution_v3(spread_test, y_test, out_test, en_test, ex_test, fee_test, tr_test, v3_concated, ts_test, bars_in_test, multiplier, log_base, 'TEST_TS')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### threshold calibration on keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cluster_threshold_1 = 0.99\n",
    "\n",
    "for v_i in valid_index_int:\n",
    "    \n",
    "    similarity_ratio = cosine_similarity(x_train, [x_train[v_i]])\n",
    "    \n",
    "    ratio_row = similarity_ratio.ravel()\n",
    "    close_index = np.where((ratio_row > cluster_threshold_1) & (ratio_row < 0.999))[0].tolist()\n",
    "    close_index_len = len(close_index)   \n",
    "    \n",
    "    y_train_close = y_train[close_index]\n",
    "    \n",
    "    if close_index_len > 0:\n",
    "\n",
    "        wr = (y_train_close > 1).sum() / close_index_len\n",
    "        pr_cumprod = np.cumprod(y_train_close)\n",
    "\n",
    "        plt.figure(figsize=(10, 4))\n",
    "        plt.subplot(121)\n",
    "        plt.plot(pr_cumprod)\n",
    "        plt.title(\"wr : {:.2f}\\npr : {:.2f}\\nfrq : {}\".format(wr, pr_cumprod[-1], close_index_len))\n",
    "        # plt.show()\n",
    "\n",
    "        ax2 = plt.subplot(122)\n",
    "        candle_plot_v2(ax2, x_data_train[v_i][:, :4], alpha=1.0, wickwidth=1.0)\n",
    "        plt.plot(x_data_train[v_i][:, 4:])\n",
    "        plt.title(v_i)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### check selected index's chart_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "selected_i = 2002\n",
    "cluster_threshold = 0.99\n",
    "\n",
    "similarity_ratio = cosine_similarity([x_train[selected_i]], x_train)\n",
    "print(similarity_ratio)\n",
    "\n",
    "ratio_row = similarity_ratio.ravel()\n",
    "\n",
    "close_index = np.where((ratio_row > cluster_threshold) & (ratio_row < 1))[0] \n",
    "# close_index = np.where((ratio_row > 0.99995) & (ratio_row < 1))[0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for c_i in close_index:\n",
    "    \n",
    "    f, ax1 = plt.subplots(figsize=(6, 4))\n",
    "    \n",
    "    candle_plot_v2(ax1, x_data_train[selected_i][:, :4], alpha=1.0, wickwidth=1.0)\n",
    "    candle_plot_v2(ax1, x_data_train[c_i][:, :4], alpha=1.0, wickwidth=1.0)\n",
    "    plt.plot(x_data_train[selected_i][:, 4:])\n",
    "    plt.plot(x_data_train[c_i][:, 4:])\n",
    "\n",
    "    plt.title(ratio_row[c_i])\n",
    "    plt.show()\n",
    "    # break         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### valid_index on current dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cluster_threshold_1 = 0.985\n",
    "\n",
    "similarity_ratio = cosine_similarity(x_train_1, x_train[valid_index_int])\n",
    "similarity_ratio_max = np.max(similarity_ratio, axis=1)\n",
    "\n",
    "valid_index_1 = np.argwhere(similarity_ratio_max > cluster_threshold_1).ravel()\n",
    "\n",
    "y_train_1_valid = y_train_1[valid_index_1]\n",
    "y_train_1_valid_len = len(y_train_1_valid)\n",
    "y_train_1_len = len(y_train_1)\n",
    "\n",
    "wr = (y_train_1_valid > 1).sum() / y_train_1_valid_len\n",
    "total_pr = to_total_pr(y_train_1_len, y_train_1_valid.ravel(), valid_index_1)\n",
    "pr_cumprod = np.cumprod(total_pr)\n",
    "\n",
    "plt.subplots(2, 1, height_ratios=[10, 1])\n",
    "plt.subplot(211)\n",
    "plt.plot(pr_cumprod)\n",
    "plt.title(\"wr : {:.2f}\\npr : {:.2f}\\nfrq : {}\".format(wr, pr_cumprod[-1], y_train_1_valid_len))\n",
    "\n",
    "plt.subplot(212)\n",
    "plt.vlines(valid_index_1, ymin=0, ymax=1, color='#00ff00')\n",
    "# plt.vlines(exit_idx[~bias_arr], ymin=0, ymax=1, color='#ff00ff')\n",
    "plt.xlim(0, y_train_1_len)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "for v_i in valid_index_int:\n",
    "    \n",
    "    similarity_ratio = cosine_similarity(x_train_1, [x_train[v_i]])\n",
    "    \n",
    "    ratio_row = similarity_ratio.ravel()\n",
    "    close_index = np.where((ratio_row > cluster_threshold_1) & (ratio_row < 0.999))[0].tolist()\n",
    "    close_index_len = len(close_index)   \n",
    "    \n",
    "    y_train_1_close = y_train_1[close_index]\n",
    "    \n",
    "    if close_index_len > 0:\n",
    "\n",
    "        wr = (y_train_1_close > 1).sum() / close_index_len\n",
    "        pr_cumprod = np.cumprod(y_train_1_close)\n",
    "\n",
    "        plt.figure(figsize=(10, 4))\n",
    "        plt.subplot(121)\n",
    "        plt.plot(pr_cumprod)\n",
    "        plt.title(\"wr : {:.2f}\\npr : {:.2f}\\nfrq : {}\".format(wr, pr_cumprod[-1], close_index_len))\n",
    "        # plt.show()\n",
    "\n",
    "        ax2 = plt.subplot(122)\n",
    "        candle_plot_v2(ax2, x_data_train[v_i][:, :4], alpha=1.0, wickwidth=1.0)\n",
    "        plt.plot(x_data_train[v_i][:, 4:])\n",
    "        plt.title(v_i)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### olds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "res_df_data = res_df[input_cols].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "#       1. complete_index = -2, wave_point 는 complete_index 기준 --> res_df's last_row 는 x_train 에서 제외한다.\n",
    "p1 = -2\n",
    "data_np = res_df_data[p1 + 1 - data_size:p1 + 1]\n",
    "data_np_norm = min_max_scaler(data_np)\n",
    "# x_train = np.array([data_np_norm]).reshape(-1, flatten_len)\n",
    "x_train_one = data_np_norm.reshape(-1, flatten_len)\n",
    "print(\"x_train.shape : {}\".format(x_train_one.shape))\n",
    "\n",
    "#       2. if close data not exist, continnue.\n",
    "# cosine_similarity_res = cosine_similarity(key_data, x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# len(cosine_similarity(key_data, [x_train[0]]))\n",
    "# np.sum(cosine_similarity(key_data, [x_train[125]]) > 0.99) > 0\n",
    "# x_train[125].shape\n",
    "# similarity_ratio = cosine_similarity(key_data, x_train[[125]])\n",
    "similarity_ratio = cosine_similarity(key_data, x_train_one)\n",
    "\n",
    "ratio_row = similarity_ratio.ravel()\n",
    "close_index = np.where(ratio_row > cluster_threshold_1)[0].tolist()\n",
    "close_index_len = len(close_index)   \n",
    "\n",
    "\n",
    "#  Todo, this code would have better latency : np.sum(cosine_similarity_res > self.cluster_threshold) < 1:\n",
    "print(np.isnan(similarity_ratio).sum())\n",
    "# y_train_close = y_train[close_index]\n",
    "print(similarity_ratio)\n",
    "# close_index_len\n",
    "# if close_index_len > 0:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def viz_img(y_pred):\n",
    "    n = 10\n",
    "    fig = plt.figure(1)\n",
    "    box_index = 1\n",
    "    for cluster in range(10):\n",
    "        result = np.where(y_pred == cluster)\n",
    "        for i in np.random.choice(result[0].tolist(), n, replace=False):\n",
    "            ax = fig.add_subplot(n, n, box_index)\n",
    "            plt.imshow(x_train[i].reshape(28, 28))\n",
    "            plt.gray()\n",
    "            ax.get_xaxis().set_visible(False)\n",
    "            ax.get_yaxis().set_visible(False)\n",
    "            box_index += 1\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = TSNE(learning_rate=300)\n",
    "transformed = model.fit_transform(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = DBSCAN(eps=2.4, min_samples=100)\n",
    "predict = model.fit(transformed)\n",
    "y_pred = predict.labels_\n",
    "\n",
    "# Assign result to df\n",
    "dataset = pd.DataFrame({'Column1':transformed[:,0],'Column2':transformed[:,1]})\n",
    "dataset['cluster_num'] = pd.Series(predict.labels_)\n",
    "\n",
    "viz_img(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n6gc7lERC4VE",
    "tags": []
   },
   "source": [
    "## Statistics (case by ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tOFkzUX2imQu",
    "tags": []
   },
   "source": [
    "#### ep_loc value optimization based on hhm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HHjIdn2MC4VE"
   },
   "outputs": [],
   "source": [
    "selection_id = config.selection_id\n",
    "\n",
    "short_p1_idx = short_obj[-1].astype(int)\n",
    "long_p1_idx = long_obj[-1].astype(int)\n",
    "\n",
    "short_open_tp_1 = res_df['short_tp_1_{}'.format(selection_id)].to_numpy()[short_p1_idx]\n",
    "long_open_tp_1 = res_df['long_tp_1_{}'.format(selection_id)].to_numpy()[long_p1_idx]\n",
    "\n",
    "short_open_tp_0 = res_df['short_tp_0_{}'.format(selection_id)].to_numpy()[short_p1_idx]\n",
    "long_open_tp_0 = res_df['long_tp_0_{}'.format(selection_id)].to_numpy()[long_p1_idx]\n",
    "\n",
    "short_open_tp_gap = res_df['short_tp_gap_{}'.format(selection_id)].to_numpy()[short_p1_idx]  # use open_i\n",
    "long_open_tp_gap = res_df['long_tp_gap_{}'.format(selection_id)].to_numpy()[long_p1_idx]\n",
    "\n",
    "short_open_out_0 = res_df['short_out_0_{}'.format(selection_id)].to_numpy()[short_p1_idx]\n",
    "long_open_out_0 = res_df['long_out_0_{}'.format(selection_id)].to_numpy()[long_p1_idx]\n",
    "\n",
    "short_open_out_gap = res_df['short_out_gap_{}'.format(selection_id)].to_numpy()[short_p1_idx]  # use open_i\n",
    "long_open_out_gap = res_df['long_out_gap_{}'.format(selection_id)].to_numpy()[long_p1_idx]\n",
    "\n",
    "# ------ out case 의 max_high check (long) ------ # => tp_case 의 max_high = tp 라, 의미가 없음.\n",
    "short_max_tpg = get_max_tpg_v2(OrderSide.SELL, ohlc_list, short_pr, short_obj[:4], short_open_tp_1, short_open_tp_gap)\n",
    "long_max_tpg = get_max_tpg_v2(OrderSide.BUY, ohlc_list, long_pr, long_obj[:4], long_open_tp_1, long_open_tp_gap)\n",
    "# short_max_tpg = get_max_tpg_v2(OrderSide.SELL, ohlc_list, short_pr, short_obj[:4], short_open_tp_1, short_open_out_gap)\n",
    "# long_max_tpg = get_max_tpg_v2(OrderSide.BUY, ohlc_list, long_pr, long_obj[:4], long_open_tp_1, long_open_out_gap)\n",
    "\n",
    "# ------ true_bias 의 outg 확인 ------ # --> 추후, outg 로 tp_gap / out_gap custom 여부를 위해, 본 cell 을 지우지 않음\n",
    "short_max_outg = get_max_outg_v4(OrderSide.SELL, config, ohlc_list, short_obj, short_tpout_arr, short_open_tp_0, short_open_tp_gap)  # tp_box's mean_low 확인 위해 tp_gap 입력함\n",
    "long_max_outg = get_max_outg_v4(OrderSide.BUY, config, ohlc_list, long_obj, long_tpout_arr, long_open_tp_0, long_open_tp_gap)\n",
    "\n",
    "current_tpg = config.tr_set.tp_gap\n",
    "current_outg = config.tr_set.out_gap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gnSvYKIzC4VF"
   },
   "outputs": [],
   "source": [
    "# ------------ dependent vars. ------------ #\n",
    "# res_df 에 존재하는 col 는 모두 사용가능함\n",
    "\n",
    "wave_itv1 = 'T'\n",
    "wave_period1 = config.tr_set.wave_period1\n",
    "\n",
    "# ------ 1. 도출한 outg 와 vars. pairing 진행 (by valid_idx) ------ #\n",
    "devided_cols, public_cols = [], []\n",
    "\n",
    "devided_cols.append('tr_{}'.format(selection_id))\n",
    "\n",
    "devided_cols.append('wave_length_fill_{}{}'.format(wave_itv1, wave_period1))\n",
    "devided_cols.append('spread_{}'.format(selection_id))\n",
    "\n",
    "\n",
    "# public_cols.append('cu_wrr_21_{}{}'.format(wave_itv1, wave_period1))\n",
    "public_cols.append('cu_wrr_32_{}{}'.format(wave_itv1, wave_period1))\n",
    "# public_cols.append('co_wrr_21_{}{}'.format(wave_itv1, wave_period1))\n",
    "public_cols.append('co_wrr_32_{}{}'.format(wave_itv1, wave_period1))\n",
    "\n",
    "# public_cols.append('wave_high_terms_cnt_fill_T5')\n",
    "# public_cols.append('wave_low_terms_cnt_fill_T5')\n",
    "# public_cols.append('wave_high_loc_pct_T5')\n",
    "# public_cols.append('wave_low_loc_pct_T5')\n",
    "\n",
    "# public_cols.append('b1_co_es_15T1')\n",
    "# public_cols.append('b1_cu_es_15T1')\n",
    "# public_cols.append('b1_upper_wick_ratio_15T')\n",
    "# public_cols.append('b1_lower_wick_ratio_15T')\n",
    "\n",
    "#  'co_wrr_T5', 'cu_wrr_T5', 'b1_cppr_15T', 'b1_updbr', 'b1_lwdbr', 'b1_updbr_cppr', 'b1_lwdbr_cppr' 'abs_ratio_5T', 'rel_ratio_5T', 'body_rel_ratio_5T'\n",
    "\n",
    "# devided_cols = ['tr_{}'.format(selection_id)]  # , 'ir_5T'\n",
    "# public_cols = ['wave_high_terms_cnt_fill_T5', 'wave_low_terms_cnt_fill_T5', 'wave_high_loc_pct_T5', 'wave_low_loc_pct_T5', \n",
    "#                'b1_co_es_15T1', 'b1_cu_es_15T1', 'b1_upper_wick_ratio_15T', 'b1_lower_wick_ratio_15T']\n",
    "\n",
    "short_datas = [res_df['short_' + col].to_numpy() for col in devided_cols] + [res_df[col].to_numpy() for col in public_cols]\n",
    "long_datas = [res_df['long_' + col].to_numpy() for col in devided_cols] + [res_df[col].to_numpy() for col in public_cols]\n",
    "\n",
    "titles = devided_cols + public_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 4479,
     "status": "ok",
     "timestamp": 1658034578976,
     "user": {
      "displayName": "7th June",
      "userId": "08178289703395036410"
     },
     "user_tz": -540
    },
    "id": "Sahvc-R0jD1A",
    "outputId": "d8b458c5-24c9-40d3-cc1d-5672d1432ae9"
   },
   "outputs": [],
   "source": [
    "plt.style.use(['dark_background', 'fast'])\n",
    "fig = plt.figure(figsize=(14, len(titles) * 5), dpi=60)\n",
    "nrows, ncols = len(short_datas), 1\n",
    "gs = gridspec.GridSpec(nrows=nrows,  # row 부터 index 채우고 col 채우는 순서임 (gs_idx)\n",
    "                        ncols=ncols\n",
    "                        )\n",
    "\n",
    "num_samples = 30\n",
    "alpha = 0.8\n",
    "\n",
    "\"\"\"\n",
    "xmin / xmax 이용해서 closeup 가능함\n",
    "\"\"\"\n",
    "\n",
    "xmin = 0\n",
    "xmax = 1.1\n",
    "\n",
    "for ings_idx, (title, short_data, long_data) in enumerate(zip(titles, short_datas, long_datas)):\n",
    "  inner_gs = gs[ings_idx].subgridspec(nrows=2, ncols=2)\n",
    "\n",
    "  short_open_data = short_data[short_p1_idx]\n",
    "  long_open_data = long_data[long_p1_idx]\n",
    "\n",
    "  short_bias_ravel = short_bias_arr.ravel()\n",
    "  long_bias_ravel = long_bias_arr.ravel()\n",
    "\n",
    "  short_open_data_y = np.zeros_like(short_open_data)\n",
    "  short_open_data_y[short_bias_ravel] = 1\n",
    "\n",
    "  long_open_data_y = np.zeros_like(long_open_data)\n",
    "  long_open_data_y[long_bias_ravel] = 1\n",
    "\n",
    "  print(\"short {} corr : {}\".format(title, stats.pearsonr(short_open_data.ravel(), short_open_data_y.ravel())))\n",
    "  print(\"long {} corr : {}\".format(title, stats.pearsonr(long_open_data.ravel(), long_open_data_y.ravel())))\n",
    "\n",
    "  short_true_data = short_open_data[short_bias_ravel]    \n",
    "  # print(short_open_data)\n",
    "  # print(short_bias_arr)\n",
    "  # short_false_data = short_open_data[short_false_bias_arr.ravel()]\n",
    "  short_false_data = short_open_data[~short_bias_ravel]\n",
    "  long_true_data = long_open_data[long_bias_ravel]\n",
    "  # long_false_data = long_open_data[long_false_bias_arr.ravel()]\n",
    "  long_false_data = long_open_data[~long_bias_ravel]\n",
    "  \n",
    "  short_true_valid_idx = np.ones_like(short_true_data).astype(bool)\n",
    "  short_false_valid_idx = np.ones_like(short_false_data).astype(bool)\n",
    "  long_true_valid_idx = np.ones_like(long_true_data).astype(bool)\n",
    "  long_false_valid_idx = np.ones_like(long_false_data).astype(bool)\n",
    "\n",
    "  short_true_valid_idx *= ~np.isinf(short_true_data)\n",
    "  short_false_valid_idx *= ~np.isinf(short_false_data)\n",
    "  long_true_valid_idx *= ~np.isinf(long_true_data)\n",
    "  long_false_valid_idx *= ~np.isinf(long_false_data)\n",
    "\n",
    "  try:\n",
    "    short_true_valid_idx *= short_true_data > xmin\n",
    "    short_false_valid_idx *= short_false_data > xmin\n",
    "    long_true_valid_idx *= long_true_data > xmin\n",
    "    long_false_valid_idx *= long_false_data > xmin\n",
    "\n",
    "  except:\n",
    "    pass\n",
    "\n",
    "  try:\n",
    "    short_true_valid_idx *= short_true_data < xmax\n",
    "    short_false_valid_idx *= short_false_data < xmax\n",
    "    long_true_valid_idx *= long_true_data < xmax\n",
    "    long_false_valid_idx *= long_false_data < xmax\n",
    "    \n",
    "  except:\n",
    "    pass\n",
    "    \n",
    "  plt.subplot(inner_gs[0])\n",
    "  ns, bins, patches = plt.hist([short_true_data[short_true_valid_idx], short_false_data[short_false_valid_idx]], \n",
    "           bins=num_samples, alpha=alpha, color=['#00ff00', '#ff0000'], edgecolor='black')  \n",
    "  plt.title('short_' + title)  \n",
    "\n",
    "  plt.subplot(inner_gs[2])\n",
    "  total_ns = np.sum(ns, axis=0)\n",
    "  hist_ratio = ns[0] / total_ns\n",
    "  # valid_idx = total_ns > 1\n",
    "  valid_idx = np.full(len(hist_ratio), True)\n",
    "  valid_hist_ratio = hist_ratio[valid_idx]\n",
    "  plt.hist(bins[:-1][valid_idx], weights=valid_hist_ratio, bins=num_samples, color='#00ff00', edgecolor='black')\n",
    "  plt.ylim(0, 1)\n",
    "  \n",
    "\n",
    "  plt.subplot(inner_gs[1])\n",
    "  ns, bins, patches = plt.hist([long_true_data[long_true_valid_idx], long_false_data[long_false_valid_idx]], \n",
    "           bins=num_samples, alpha=alpha, color=['#00ff00', '#ff0000'], edgecolor='black')\n",
    "  plt.title('long_' + title)\n",
    "  \n",
    "  plt.subplot(inner_gs[3])\n",
    "  total_ns = np.sum(ns, axis=0)\n",
    "  hist_ratio = ns[0] / total_ns\n",
    "  # valid_idx = total_ns > 1\n",
    "  valid_idx = np.full(len(hist_ratio), True)\n",
    "  valid_hist_ratio = hist_ratio[valid_idx]\n",
    "  plt.hist(bins[:-1][valid_idx], weights=valid_hist_ratio, bins=num_samples, color='#00ff00', edgecolor='black')\n",
    "  plt.ylim(0, 1)\n",
    "\n",
    "  \n",
    "# plt.suptitle(title)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EtjwB7Qk-Grj",
    "tags": []
   },
   "source": [
    "#### get significance (v4 -> v5 : split short / long / both res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 54864,
     "status": "ok",
     "timestamp": 1660577567405,
     "user": {
      "displayName": "7th June",
      "userId": "08178289703395036410"
     },
     "user_tz": -540
    },
    "id": "oVifICO4-Grk",
    "outputId": "c82b56a6-e10b-4f3a-9364-41cbdffdb937",
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "1. 일단은 현재까지의 logic 은 short & long result 가 온전한 경우에 대해 정상적으로 돌아가도록 구성함.\n",
    "\"\"\"\n",
    "\n",
    "# ------------ survey param ------------ #\n",
    "# itv_num_list = [1, 3, 5, 15]\n",
    "# itv_list = ['15m', '30m', '1h', '4h']\n",
    "# itv_list = ['3m', '5m', '15m', '30m', '1h', '4h']\n",
    "\n",
    "# val_list = np.arange(0, 10, 0.5)     # prcn 1\n",
    "val_list = np.arange(-0.0, 0.5, 0.02)  # prcn 2\n",
    "# val_list = np.arange(-0.5, 0., 0.03)  # prcn 2\n",
    "# val_list = np.arange(-0.5, -0.6, -0.005)    # prcn 3\n",
    "# val_list = np.arange(0.944, 0.945, 0.0001)    # prcn 4\n",
    "# val_list = np.arange(1, 10, 1)   # prcn -1\n",
    "# val_list = np.arange(100, 120, 1)   # prcn -2\n",
    "# val_list = talib.get_function_groups()['Pattern Recognition']\n",
    "\n",
    "# ------------ get survey_res ------------ #\n",
    "short_res_list, long_res_list, both_res_list = [], [], []\n",
    "result = []\n",
    "res_shape = (3, 12)  # short, long, both x data\n",
    "config_list_copy = copy.deepcopy(config_list)\n",
    "for set_val in val_list:\n",
    "    # ------------ open 결정 이전의 인자값 ------------ #\n",
    "    # ------ point * dur. ------ #\n",
    "    # config_list_copy[0].loc_set.point1.wrr_32 = set_val\n",
    "    # config_list_copy[0].loc_set.point1.candle_pattern = set_val\n",
    "    # config_list_copy[0].loc_set.zone.degree_list = set_val\n",
    "    # config_list_copy[0].loc_set.point2.wick_score_list = str([set_val])\n",
    "    # config_list_copy[0].loc_set.zone.ir = set_val  \n",
    "    # config_list_copy[0].loc_set.zone.abs_ratio = set_val\n",
    "\n",
    "    # ------------ open 결정 이후의 인자값 ------------ #\n",
    "    # ------ utils ------ #\n",
    "    # config_list_copy[0].tr_set.tp_gap = set_val  \n",
    "    # config_list_copy[0].tr_set.ep1_gap = set_val \n",
    "    # config_list_copy[0].tr_set.ep2_gap = set_val \n",
    "    config_list_copy[0].tr_set.out_gap = set_val  \n",
    "    # config_list_copy[0].tr_set.wave_length1 = set_val  \n",
    "    # config_list_copy[0].tr_set.wave_time_ratio1 = set_val\n",
    "\n",
    "    # config_list_copy[0].tr_set.tp_gap = abs(set_val) - 0.5\n",
    "    # config_list_copy[0].tr_set.out_gap = set_val + 0.5\n",
    "    # config_list_copy[0].tr_set.wb_tp_gap = config_list_copy[0].tr_set.tp_gap\n",
    "    # config_list_copy[0].tr_set.wb_out_gap = config_list_copy[0].tr_set.out_gap\n",
    "\n",
    "    # ------ entry, exit (ep, tp, out vars.) ------ #\n",
    "    # config_list_copy[0].tr_set.expire_k = set_val\n",
    "    # config_list_copy[0].ep_set.expire_tick = set_val  \n",
    "\n",
    "    for utils_, config_ in zip(utils_list, config_list_copy):\n",
    "        enlist_tr(res_df, config_, np_timeidx)\n",
    "        \n",
    "    # open_info_df = get_open_info_df(ep_loc_v3, res_df, np_timeidx, id_list, config_list_copy, id_idx_list)   # point * mr_res 이기 때문에 utils_tr & rtc 의 영향을 충분히 받음\n",
    "    open_info_df1 = get_open_info_df_v2(ep_loc_p1_v3, res_df, np_timeidx, id_list, config_list_copy, id_idx_list, open_num=1)  # --> point * dur. 관련 (loc_set) param 에 종속 (open_info 가 변경되는게 아니라면, 재실행할 필요없음)\n",
    "    open_info_df2 = get_open_info_df_v2(ep_loc_p2_v3, res_df, np_timeidx, id_list, config_list_copy, id_idx_list, open_num=2)\n",
    "    open_info_df_list = [open_info_df1, open_info_df2]\n",
    "\n",
    "    # try:    \n",
    "    short_res, long_res, both_res = get_res_v2(res_df, open_info_df_list, ohlc_list, config_list, np_timeidx, en_ex_pairing, funcs1, idep_plot_v16_5, funcs2, test_ratio=test_ratio, plot_is=plot_is, signi=True, show_detail=show_detail)\n",
    "    short_res_list.append(short_res)\n",
    "    long_res_list.append(long_res)\n",
    "    both_res_list.append(both_res)\n",
    "    # except Exception as e\n",
    "    #   result.append(np.full(res_shape, np.nan))\n",
    "    #   print(\"error in get_res() phase : {}\".format(e))\n",
    "    # pass\n",
    "    \n",
    "# survey_res_list = [np.array(result)[:, s_i::3] for s_i in range(3)]   # 3 for s, l, b\n",
    "survey_res_list = [short_res_list, long_res_list, both_res_list]   # 3 for s, l, b\n",
    "# short_res, long_res, both_res = survey_res_list\n",
    "\n",
    "# ------------ plot survey_res ------------ #\n",
    "title_list = [\"short\", \"long\", \"both\"]\n",
    "sub_title_list = ['hhm', 'hlm', 'frq', 'dpf', 'wr', 'sr', 'acc_pr', 'sum_pr', 'min_pr', 'liqd', 'acc_mdd', 'sum_mdd_prod', 'sum_mdd_sum']\n",
    "space_ = \" \" * 120\n",
    "\n",
    "fig = plt.figure(figsize=(24, 8), dpi=70)\n",
    "plt.style.use('dark_background')\n",
    "gs = gridspec.GridSpec(nrows=1,\n",
    "                        ncols=3,\n",
    "                        # height_ratios=[1, 1, 1]\n",
    "                      )\n",
    "\n",
    "sub_rows, sub_cols, sub_height_ratio = 4, 4, [1, 1, 1, 1]\n",
    "\n",
    "for d_idx, (title_name, survey_res) in enumerate(zip(title_list, survey_res_list)):  \n",
    "    inner_gs = gs[d_idx].subgridspec(nrows=sub_rows,\n",
    "                        ncols=sub_cols,\n",
    "                        height_ratios=sub_height_ratio\n",
    "                      )\n",
    "            \n",
    "    for in_idx, (data_, sub_title) in enumerate(zip(np.array(survey_res).T, sub_title_list)):\n",
    "        plt.subplot(inner_gs[in_idx])\n",
    "        data = data_.ravel()                \n",
    "        valid_idx = ~pd.isnull(data)\n",
    "        if np.sum(valid_idx) > 0:\n",
    "            if type(val_list[0]) == str:\n",
    "                x, y = np.arange(len(val_list))[valid_idx], data[valid_idx]\n",
    "            else:\n",
    "                x, y = val_list[valid_idx], data[valid_idx]\n",
    "            plt.plot(x, y)  # 앞에서부터 len(result) 만큼만    \n",
    "            plt.title(sub_title + '_{:.2f}'.format(x[np.argmax(y)]))\n",
    "        else:\n",
    "            plt.title(sub_title)\n",
    "\n",
    "plt.suptitle(space_.join(title_list))\n",
    "plt.show()\n",
    "# print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Arnb-kXlC4VF",
    "tags": []
   },
   "source": [
    "#### tpg & outg survey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 881
    },
    "executionInfo": {
     "elapsed": 2820,
     "status": "ok",
     "timestamp": 1659074362424,
     "user": {
      "displayName": "7th June",
      "userId": "08178289703395036410"
     },
     "user_tz": -540
    },
    "id": "jHh0uFkXC4VF",
    "outputId": "cf21f56c-faf2-471d-b7e0-5c16e0ff2bdd"
   },
   "outputs": [],
   "source": [
    "# 1. outg 는 partial 을 위한 histogram 작성 진행\n",
    "# 현재, outg 내부에는 tp 한것과 out 한것이 공존하는 상태\n",
    "titles = ['outg', 'tpg']\n",
    "short_max_datas = [short_max_outg[short_bias_arr], short_max_tpg]\n",
    "long_max_datas = [long_max_outg[long_bias_arr], long_max_tpg]\n",
    "\n",
    "# titles = ['tpg']\n",
    "# short_max_datas = [short_max_tpg]\n",
    "# long_max_datas = [long_max_tpg]\n",
    "\n",
    "plt.style.use(['dark_background', 'fast'])\n",
    "fig = plt.figure(figsize=(12, 8))\n",
    "nrows, ncols = 2, 1\n",
    "gs = gridspec.GridSpec(nrows=nrows,  # row 부터 index 채우고 col 채우는 순서임 (gs_idx)\n",
    "                        ncols=ncols\n",
    "                        )\n",
    "  \n",
    "for ings_idx, (title, short_max_data, long_max_data) in enumerate(zip(titles, short_max_datas, long_max_datas)):\n",
    "\n",
    "  inner_gs = gs[ings_idx].subgridspec(nrows=1, ncols=2)\n",
    "  if ings_idx == 0:\n",
    "    axvline = current_outg\n",
    "    title_add = 'true_' + title\n",
    "  else:\n",
    "    axvline = current_tpg\n",
    "    title_add = 'false_' + title\n",
    "\n",
    "  print(len(short_max_data))\n",
    "  print(len(long_max_data))\n",
    "\n",
    "  short_plot_idx = np.ones_like(short_max_data).astype(bool)\n",
    "  long_plot_idx = np.ones_like(long_max_data).astype(bool)\n",
    "  # short_plot_idx = short_max_data <= axvline\n",
    "  # long_plot_idx = long_max_data <= axvline\n",
    "\n",
    "  print(np.sum(short_plot_idx))\n",
    "  print(np.sum(long_plot_idx))    \n",
    "\n",
    "  short_plot_idx *= ~np.isnan(short_max_data)\n",
    "  long_plot_idx *= ~np.isnan(long_max_data)  # nan 과 inf 때문에 이 방식 채택\n",
    "\n",
    "  print(np.sum(short_plot_idx))\n",
    "  print(np.sum(long_plot_idx))\n",
    "\n",
    "  short_plot_idx *= ~np.isinf(short_max_data)\n",
    "  long_plot_idx *= ~np.isinf(long_max_data)  # nan 과 inf 때문에 이 방식 채택\n",
    "  \n",
    "  print(np.sum(short_plot_idx))\n",
    "  print(np.sum(long_plot_idx))\n",
    "    \n",
    "  plt.subplot(inner_gs[0])\n",
    "  kde_plot_v2(*np.unique(short_max_data[short_plot_idx], return_counts=True))\n",
    "  plt.title('short_' + title_add)  \n",
    "  plt.axvline(axvline, color='red', linewidth=3)\n",
    "\n",
    "  plt.subplot(inner_gs[1])\n",
    "  kde_plot_v2(*np.unique(long_max_data[long_plot_idx], return_counts=True))\n",
    "  plt.title('long_' + title_add)\n",
    "  plt.axvline(axvline, color='red', linewidth=3)\n",
    "\n",
    "  print()\n",
    "\n",
    "# plt.suptitle(title)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mScdfR9hmjVu",
    "tags": []
   },
   "source": [
    "#### legacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### cci updown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "htf_df = to_htf(res_df, '30T', offset='9h')\n",
    "htf_df = cci_v2(htf_df, period=20, smooth=None, itv=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cci_updown = htf_df.cci_30T20 < htf_df.cci_30T20.shift(-1)\n",
    "\n",
    "# 1. 특정 조건에 대한 updown 확률을 구해라.\n",
    "# 2. valid_idx & incondition_idx's \"true / len(idx)\" = precision\n",
    "incondition_idx = htf_df.cci_30T20 > htf_df.cci_30T20.shift(1)\n",
    "\n",
    "valid_idx = ~(pd.isnull(htf_df.cci_30T20.shift(-1)) | pd.isnull(htf_df.cci_30T20) | pd.isnull(htf_df.cci_30T20.shift(1)))\n",
    "\n",
    "def get_odds(gt_series):\n",
    "    return np.sum(gt_series) / len(gt_series)\n",
    "\n",
    "# cci_updown[valid_idx & incondition_idx]\n",
    "print(\"original odds : {}\".format(get_odds(cci_updown[valid_idx])))\n",
    "print(\"incondition odds : {}\".format(get_odds(cci_updown[valid_idx & incondition_idx])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xivLUsSGC4VF",
    "tags": []
   },
   "source": [
    "##### outg survey for precision (eploc vars. dependency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LB28R3QIjCDc"
   },
   "outputs": [],
   "source": [
    "# ------------ dependent vars. ------------ #\n",
    "# res_df 에 존재하는 col 는 모두 사용가능함\n",
    "# ------ 1. 도출한 outg 와 vars. pairing 진행 (by valid_idx) ------ #\n",
    "devided_cols = ['tr_{}'.format(strat_version)]  # , 'ir_5T'\n",
    "public_cols = ['wave_body_ratio']  # 'abs_ratio_5T', 'rel_ratio_5T', 'body_rel_ratio_5T'\n",
    "\n",
    "short_datas = [res_df['short_' + col].to_numpy() for col in devided_cols] + [res_df[col].to_numpy() for col in public_cols]\n",
    "long_datas = [res_df['long_' + col].to_numpy() for col in devided_cols] + [res_df[col].to_numpy() for col in public_cols]\n",
    "\n",
    "titles = devided_cols + public_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nX9TpLcvFu7T"
   },
   "outputs": [],
   "source": [
    "plot_outg_range = (-1, 10)\n",
    "plot_data_range = (-10, 100)\n",
    "\n",
    "plt.style.use(['dark_background', 'fast'])\n",
    "fig = plt.figure(figsize=(12, 18))\n",
    "nrows, ncols = len(short_datas), 1\n",
    "gs = gridspec.GridSpec(nrows=nrows,  # row 부터 index 채우고 col 채우는 순서임 (gs_idx)\n",
    "                        ncols=ncols\n",
    "                        )\n",
    "\n",
    "for ings_idx, (title, short_data, long_data) in enumerate(zip(titles, short_datas, long_datas)):\n",
    "  inner_gs = gs[ings_idx].subgridspec(nrows=1, ncols=2)\n",
    "\n",
    "  print(len(long_max_outg))\n",
    "\n",
    "  short_plot_idx = (plot_outg_range[0] <= short_max_outg) & (short_max_outg <= plot_outg_range[1])\n",
    "  long_plot_idx = (plot_outg_range[0] <= long_max_outg) & (long_max_outg <= plot_outg_range[1])  # nan 과 inf 때문에 이 방식 채택\n",
    "\n",
    "  print(np.sum(long_plot_idx))\n",
    "\n",
    "  short_open_data = short_data[short_open_idx]\n",
    "  long_open_data = long_data[long_open_idx]\n",
    "  \n",
    "  short_plot_idx *= (plot_data_range[0] <= short_open_data) * (short_open_data <= plot_data_range[1])\n",
    "  long_plot_idx *= (plot_data_range[0] <= long_open_data) * (long_open_data <= plot_data_range[1]) # nan 과 inf 때문에 이 방식 채택\n",
    "\n",
    "  short_plot_idx *= ~np.isnan(short_open_data)\n",
    "  long_plot_idx *= ~np.isnan(long_open_data)  # nan 과 inf 때문에 이 방식 채택\n",
    "\n",
    "  print(np.sum(long_plot_idx))\n",
    "\n",
    "  short_plot_idx *= ~np.isinf(short_open_data)\n",
    "  long_plot_idx *= ~np.isinf(long_open_data)  # nan 과 inf 때문에 이 방식 채택\n",
    "\n",
    "  print(np.sum(long_plot_idx))\n",
    "\n",
    "  short_true_idx = short_plot_idx * short_true_open_idxth\n",
    "  long_true_idx = long_plot_idx * long_true_open_idxth\n",
    "  \n",
    "  short_false_idx = short_plot_idx * ~short_true_open_idxth\n",
    "  long_false_idx = long_plot_idx * ~long_true_open_idxth\n",
    "\n",
    "  plt.subplot(inner_gs[0])\n",
    "  # ------ true_bias ------ #\n",
    "  x, y = short_max_outg[short_true_idx].ravel(), short_open_data[short_true_idx].ravel()  \n",
    "  plt.scatter(x, y, color='white', alpha=0.5)\n",
    "  # ------ false_bias ------ #\n",
    "  x, y = short_max_outg[short_false_idx].ravel(), short_open_data[short_false_idx].ravel()\n",
    "  plt.scatter(x, y, color='fuchsia', alpha=0.3)\n",
    "  plt.axvline(current_outg, color='red', linewidth=3)\n",
    "  plt.title(\"{} coef : {:0.3f}\".format(title, np.corrcoef(-x, y)[0, 1]))\n",
    "\n",
    "  plt.subplot(inner_gs[1])    \n",
    "  # ------ true_bias ------ #\n",
    "  x, y = long_max_outg[long_true_idx].ravel(), long_open_data[long_true_idx].ravel()  \n",
    "  plt.scatter(x, y, color='white', alpha=0.5)\n",
    "  # ------ false_bias ------ #\n",
    "  x, y = long_max_outg[long_false_idx].ravel(), long_open_data[long_false_idx].ravel()\n",
    "  plt.scatter(x, y, color='fuchsia', alpha=0.3)\n",
    "  plt.axvline(current_outg, color='red', linewidth=3)\n",
    "  plt.title(\"{} coef : {:0.3f}\".format(title, np.corrcoef(-x, y)[0, 1]))\n",
    "\n",
    "  print()  \n",
    "\n",
    "# plt.suptitle(title)\n",
    "plt.show()\n",
    "# print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "983aUwM76s6X",
    "tags": []
   },
   "source": [
    "#### olds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EJ4f-3Zf4ImT",
    "tags": []
   },
   "source": [
    "## Backtrader validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BPI-9QX74ImW"
   },
   "outputs": [],
   "source": [
    "trade_log_dir_path = \"wave_cci_wrr32_spread_wave_length\"\n",
    "trade_log_name = \"1682307814.pkl\"\n",
    "\n",
    "with open(os.path.join(\"Bank/logs/trade_log\", trade_log_dir_path, trade_log_name), 'rb') as f:\n",
    "  trade_log = pickle.load(f)\n",
    "\n",
    "trade_log  # both pos_side's log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 431,
     "status": "ok",
     "timestamp": 1658222594162,
     "user": {
      "displayName": "7th June",
      "userId": "08178289703395036410"
     },
     "user_tz": -540
    },
    "id": "DiTKuq9T4ImY",
    "outputId": "a06c3f6a-311d-4a7d-c425-00563fd8980f"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "missed_data : Bank 에 missed 된 data.\n",
    "\"\"\"\n",
    "\n",
    "pos_side = \"BUY\" # BUY SELL\n",
    "val_obj = short_obj if pos_side == \"SELL\" else long_obj\n",
    "\n",
    "idep_log = []\n",
    "bank_log = []\n",
    "\n",
    "data_name = [\"open\", \"entry\", \"exit\", \"entry\", \"exit\"]\n",
    "\n",
    "# 1. idep log\n",
    "idep_log.append(list(map(lambda x : str(x), res_df.index[val_obj[4].astype(int).ravel()])))\n",
    "idep_log.append(list(map(lambda x : str(x), res_df.index[val_obj[2].astype(int).ravel()])))\n",
    "idep_log.append(list(map(lambda x : str(x), res_df.index[val_obj[3].astype(int).ravel()])))\n",
    "idep_log.append(val_obj[0].ravel())\n",
    "idep_log.append(val_obj[1].ravel())\n",
    "\n",
    "# 2. bank log\n",
    "bank_log.append([log[\"open\"][0] for log in trade_log if log[\"open\"][1] == pos_side])\n",
    "bank_log.append([log[\"entry\"][0] for log in trade_log if log[\"entry\"][1] == pos_side])\n",
    "bank_log.append([log[\"exit\"][0] for log in trade_log if log[\"exit\"][1] == pos_side])\n",
    "bank_log.append([log[\"entry\"][2] for log in trade_log if log[\"entry\"][1] == pos_side])\n",
    "bank_log.append([log[\"exit\"][2] for log in trade_log if log[\"exit\"][1] == pos_side])\n",
    "\n",
    "for name, idep_res, bank_res in zip(data_name, idep_log, bank_log):\n",
    "    \n",
    "    missed_data = [data for data in idep_res if not data in bank_res]\n",
    "    over_data = [data for data in bank_res if not data in idep_res]\n",
    "\n",
    "    print(\"{}_missed_data :\".format(name), missed_data)\n",
    "    print(\"{}_over_data :\".format(name), over_data)\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1GVZ03zDyU2N",
    "tags": []
   },
   "source": [
    "## IDEP method overriding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### get_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eiQ36_SLLE3w"
   },
   "outputs": [],
   "source": [
    "def get_res_v2(res_df, open_info_df_list, ohlc_list, config_list, np_timeidx, en_ex_pairing, funcs1, idep_plot, funcs2, inversion=False, test_ratio=0.3, plot_is=True, signi=False, show_detail=False):\n",
    "    \n",
    "    \"\"\"\n",
    "    v1 -> v2\n",
    "    1. en_ex_pairing, idep_plot 에 필요한 funcs 를 분리함, funcs1, funcs2\n",
    "    \"\"\"\n",
    "\n",
    "    # ------------ make open_info_list ------------ #\n",
    "    open_idx1, open_idx2 = [open_info_df.index.to_numpy() for open_info_df in open_info_df_list]\n",
    "    len_df = len(res_df)\n",
    "\n",
    "    sample_len = int(len_df * (1 - test_ratio))\n",
    "    sample_idx1 = (open_idx1 < sample_len) == plot_is  # in / out sample plot 여부\n",
    "    sample_open_idx1 = open_idx1[sample_idx1]\n",
    "    \n",
    "    sample_idx2 = (open_idx2 < sample_len) == plot_is  # in / out sample plot 여부\n",
    "\n",
    "    # ------------ open_info_list 기준 = p1 ------------ #\n",
    "    sample_open_info_df1, sample_open_info_df2 = [df_[idx_] for df_, idx_ in zip(open_info_df_list, [sample_idx1, sample_idx2])]\n",
    "    open_info1 = [sample_open_info_df1[col_].to_numpy() for col_ in sample_open_info_df1.columns]\n",
    "\n",
    "    if config_list[0].tr_set.check_hlm in [0, 1]:  # 여기서 open_info 자동화하더라도, utils info 는 직접 실행해주어야함\n",
    "        sample_open_idx2 = sample_open_idx1\n",
    "        open_info2 = open_info1\n",
    "    else:\n",
    "        sample_open_idx2 = open_idx2[sample_idx2]\n",
    "        open_info2 = [sample_open_info_df2[col_].to_numpy() for col_ in sample_open_info_df2.columns]\n",
    "\n",
    "    # ------------ get paired_res ------------ #\n",
    "    start_time = time.time()\n",
    "    paired_res = en_ex_pairing(res_df, [sample_open_idx1, sample_open_idx2], [open_info1, open_info2], ohlc_list, config_list, np_timeidx, funcs1, show_detail)\n",
    "    # net_p1_idx_arr, p1_idx_arr, p2_idx_arr, pair_idx_arr, pair_price_arr, lvrg_arr, fee_arr, tpout_arr, tr_arr = paired_res    \n",
    "    # print(pair_price_arr)\n",
    "    print(\"en_ex_pairing elapsed time :\", time.time() - start_time)  # 0.37 --> 0.3660471439361572 --> 0.21(lesser if)\n",
    "\n",
    "    # ------------ idep_plot ------------ #\n",
    "    start_time = time.time()\n",
    "    high, low = ohlc_list[1:3]\n",
    "    res = idep_plot(res_df, len_df, config_list[0], high, low, sample_open_info_df1, paired_res, funcs2, inversion=inversion, sample_ratio=1 - test_ratio, signi=signi)\n",
    "    print(\"idep_plot elapsed time :\", time.time() - start_time)  # 1.40452 (v6) 1.4311 (v5)\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IWO7KkqltMFt",
    "tags": []
   },
   "source": [
    "#### get_open_info_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MY1csdNRLGsk"
   },
   "outputs": [],
   "source": [
    "def get_open_info_df_v2(ep_loc_v2, res_df, np_timeidx, id_list, config_list, id_idx_list, open_num=1):\n",
    "    \n",
    "    \"\"\"\n",
    "    v1 -> v2\n",
    "        1. <U32 dtype 으로 인한 memory allocate error 에 대응하기 위해 zone, side value 를 integer 기준으로 수정함.\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    # ------ get mr_res, zone_arr ------ #\n",
    "    short_mr_res_obj = np.array([ep_loc_v2(res_df, config_, np_timeidx, show_detail=True, ep_loc_side=OrderSide.SELL) for config_ in config_list])\n",
    "    long_mr_res_obj = np.array([ep_loc_v2(res_df, config_, np_timeidx, show_detail=True, ep_loc_side=OrderSide.BUY) for config_ in config_list])\n",
    "    short_open_idx_list = [np.where(res_df['short_open{}_{}'.format(open_num, id)].to_numpy() * mr_res)[0] for id, mr_res in zip(id_list, short_mr_res_obj[:, 0])]   # \"point * mr_Res\"\n",
    "    long_open_idx_list = [np.where(res_df['long_open{}_{}'.format(open_num, id)].to_numpy() * mr_res)[0] for id, mr_res in zip(id_list, long_mr_res_obj[:, 0])]  # zip 으로 zone (str) 과 묶어서 dtype 변경됨\n",
    "    print(\"~ ep_loc_v2 elapsed time :\", time.time() - start_time)\n",
    "\n",
    "    # ------ open_info_arr ------ #\n",
    "    short_side_list = [np.full(len(list_), -1) for list_ in short_open_idx_list]\n",
    "    long_side_list = [np.full(len(list_), 1) for list_ in long_open_idx_list]\n",
    "\n",
    "    short_zone_list = [zone_res[short_open_idx] for zone_res, short_open_idx in zip(short_mr_res_obj[:, 1], short_open_idx_list)]\n",
    "    long_zone_list = [zone_res[long_open_idx] for zone_res, long_open_idx in zip(long_mr_res_obj[:, 1], long_open_idx_list)]\n",
    "\n",
    "    short_id_list = [np.full(len(list_), id) for id, list_ in zip(id_list, short_open_idx_list)]\n",
    "    long_id_list = [np.full(len(list_), id) for id, list_ in zip(id_list, long_open_idx_list)]\n",
    "\n",
    "    selected_id_idx = np.arange(len(id_idx_list))\n",
    "    short_id_idx_list = [np.full(len(list_), id) for id, list_ in zip(selected_id_idx, short_open_idx_list)]\n",
    "    long_id_idx_list = [np.full(len(list_), id) for id, list_ in zip(selected_id_idx, long_open_idx_list)]\n",
    "\n",
    "    # ------ get open_info_df ------ #\n",
    "    #   series 만들어서 short / long 끼리 합치고 둘이 합치고, 중복은 우선 순위 정해서 제거\n",
    "    short_open_df_list = [pd.DataFrame(index=index_, data=np.vstack((data_)).T, columns=['side', 'zone', 'id', 'id_idx']) for index_, data_ in zip(short_open_idx_list, zip(short_side_list, short_zone_list, short_id_list, short_id_idx_list))]\n",
    "    long_open_df_list = [pd.DataFrame(index=index_, data=np.vstack((data_)).T, columns=['side', 'zone', 'id', 'id_idx']) for index_, data_ in zip(long_open_idx_list, zip(long_side_list, long_zone_list, long_id_list, long_id_idx_list))]\n",
    "\n",
    "    open_info_df = pd.concat(short_open_df_list + long_open_df_list)\n",
    "    # ------ sorting + unique ------ #\n",
    "    open_info_df.sort_index(inplace=True)\n",
    "    # print(len(open_info_df))\n",
    "    # print(len(open_info_df))\n",
    "    # open_info_df.head()\n",
    "    print(\"~ get_open_info_df elapsed time :\", time.time() - start_time)\n",
    "    return open_info_df[~open_info_df.index.duplicated(keep='first')]  # 먼저 순서를 우선으로 지정  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EFyWTuscH8VH",
    "tags": []
   },
   "source": [
    "#### en_ex_pairing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def en_ex_pairing_v9_6(res_df, open_idx_list, open_info_list, ohlc_list, config_list, np_timeidx, funcs, show_detail=False):  # 이미 충분히 줄여놓은 idx 임\n",
    "\n",
    "    \"\"\"\n",
    "    v9_44 -> v9_6\n",
    "        1. for Data Analysis, allow trade data without continuity.\n",
    "            a. just remove \"op_idx1 < i\".\n",
    "    \"\"\"\n",
    "\n",
    "    open_info1, open_info2 = open_info_list\n",
    "    side_arr1, _, _, id_idx_arr1 = open_info1\n",
    "    side_arr2, _, _, _ = open_info2\n",
    "\n",
    "    expiry_p1p2, expiry, lvrg_set, check_entry, check_signal_out, check_hl_out, check_limit_tp_exec = funcs\n",
    "\n",
    "    net_p1_idx_list, p1_idx_list, p2_idx_list, pair_idx_list, pair_price_list, lvrg_list, fee_list, tpout_list, tr_list = [[] for li in range(9)]\n",
    "    len_df = len(res_df)\n",
    "\n",
    "    open, high, low, close = ohlc_list\n",
    "\n",
    "    open_idx1, open_idx2 = open_idx_list\n",
    "    len_open_idx1 = len(open_idx1)\n",
    "    len_open_idx2 = len(open_idx2)\n",
    "    i, open_i1, open_i2 = 0, -1, -1  # i for total_res_df indexing\n",
    "\n",
    "    while 1:\n",
    "\n",
    "        # ------------ p1 phase ------------ #\n",
    "\n",
    "        # Todo,\n",
    "        #   1. (갱신) p1's open_i + 1 과 op_idx 를 꺼내오는 건, eik1 또는 tp 체결의 경우만 해당됨,\n",
    "        #   2. out 의 경우 p2's op_idx 기준으로 retry 필요\n",
    "        #     a. 또한, p2's op_idx > p1's op_idx\n",
    "\n",
    "        # ------ 1. get p1_info ------ #\n",
    "        # if eik1 or tp_done or first loop:\n",
    "        open_i1 += 1  # 확인 끝났으면 조기 이탈(+1), 다음 open_idx 조사 진행\n",
    "        if open_i1 >= len_open_idx1:\n",
    "            break\n",
    "\n",
    "        if show_detail:\n",
    "            print(\"open_i1 : {}, side_arr1 : {}\".format(open_i1, side_arr1[open_i1]))\n",
    "\n",
    "        op_idx1 = open_idx1[open_i1]  # open_i1 는 i 와 별개로 운영\n",
    "        # if op_idx1 < i:  # i = 이전 거래 끝난후의 res_df index - \"거래 종료후 거래 시작\", '<' : 거래 종료시점 진입 가능하다는 의미\n",
    "        #     continue\n",
    "\n",
    "        # ------ 2. set loop index i ------ #\n",
    "        i = op_idx1  # + 1 --> op_idx1 = op_idx2 가능함 # open_signal 이 close_bar.shift(1) 이라고 가정하고 다음 bar 부터 체결확인한다는 의미\n",
    "        if i >= len_df:  # res_df 의 last_index 까지 돌아야함\n",
    "            break\n",
    "\n",
    "        # ------ 3. get open info ------ #\n",
    "        #            a. ID 별로 수행하기 위해 selection_id, config 호출함.\n",
    "        open_side_num = side_arr1[open_i1]\n",
    "        id_idx = id_idx_arr1.astype(int)[open_i1]  # indexing 을 위해 integer 로 변환.\n",
    "        config = config_list[id_idx]\n",
    "        selection_id = config.selection_id\n",
    "        check_hlm = config.tr_set.check_hlm\n",
    "\n",
    "        open_side = OrderSide.SELL if open_side_num == -1 else OrderSide.BUY\n",
    "        side_pos = 'short' if open_side == OrderSide.SELL else 'long'  # utils paper 접근을 위한 long / short string.\n",
    "        if show_detail:\n",
    "            print(\"------------ op_idx1 : {} {} ------------\".format(op_idx1, open_side))\n",
    "\n",
    "        # if show_detail:\n",
    "        #   print(\"check_hlm :\", check_hlm)\n",
    "\n",
    "        # ------ 4. load util paper data ------ #\n",
    "        \"\"\" \n",
    "        tr_set_idx initialize.\n",
    "            1. j 를 둔 이유는 본래 dynamic_tp / out 을 가능케 하기 위함이였음.\n",
    "                a. exec_j : open 체결 index\n",
    "                b. ep_j : entry_price 기준 index\n",
    "                c. tp_j : tp_price 기준 index\n",
    "                d. out_j : out_price 기준 index\n",
    "        \"\"\"\n",
    "        ep_j, tp_j, out_j = op_idx1, op_idx1, op_idx1  # tr_set p1, p2 에 가변적으로 기준할 수 있도록 구성함.\n",
    "        p1_tr_set_idx = (ep_j, tp_j, out_j)\n",
    "\n",
    "        tp_arr = res_df['{}_tp_{}'.format(side_pos, selection_id)].to_numpy()\n",
    "        ep1_arr = res_df['{}_ep1_{}'.format(side_pos, selection_id)].to_numpy()\n",
    "        ep2_arr = res_df['{}_ep2_{}'.format(side_pos, selection_id)].to_numpy()\n",
    "        out_arr = res_df['{}_out_{}'.format(side_pos, selection_id)].to_numpy()\n",
    "\n",
    "        tr_arr = res_df['{}_tr_{}'.format(side_pos, selection_id)].to_numpy()  # just for p1_hhm\n",
    "\n",
    "        tp_1_ = res_df['{}_tp_1_{}'.format(side_pos, selection_id)].to_numpy()[tp_j]  # for p2_box location & p1's exipiry\n",
    "        tp_0_ = res_df['{}_tp_0_{}'.format(side_pos, selection_id)].to_numpy()[tp_j]\n",
    "        tp_gap_ = res_df['{}_tp_gap_{}'.format(side_pos, selection_id)].to_numpy()[tp_j]\n",
    "\n",
    "        # if not check_net_hhm:  # this phase exist for p1 entry (net hhm sync.) in p2_platform\n",
    "        exec_j, entry_done, en_p, fee = check_entry(res_df, config, config.ep_set.entry_type, op_idx1, p1_tr_set_idx, len_df, open_side, [*ohlc_list, ep1_arr], expiry)\n",
    "\n",
    "        i = exec_j  # = entry_loop 를 돌고 나온 e_j\n",
    "        if not entry_done:\n",
    "            if show_detail:\n",
    "                print(\"p1's expiry : continue\")\n",
    "            continue\n",
    "            # else:\n",
    "        #   tp_j = op_idx1\n",
    "\n",
    "        prev_open_i2 = open_i2\n",
    "        net_p1_idx_list.append(op_idx1)\n",
    "        # if check_hlm in [0, 1]:\n",
    "        #   i = op_idx1  # allow op_idx2 = op_idx1\n",
    "        allow_exit = 1\n",
    "\n",
    "        while 1:\n",
    "            # ------------ p2 phase ------------ #\n",
    "\n",
    "            # ------ 1. get p2_info ------ #\n",
    "            if check_hlm in [1, 2]:\n",
    "                open_i2 += 1  # 확인 끝났으면 조기 이탈(+1), 다음 open_idx 조사 진행\n",
    "                if open_i2 >= len_open_idx2:  # open_i2 소진\n",
    "                    if show_detail:\n",
    "                        print(\"open_i2 >= len_open_idx2, open_i2 소진 : break\")\n",
    "                    break\n",
    "\n",
    "                # ------ check side sync. ------ #\n",
    "                if side_arr1[open_i1] != side_arr2[open_i2]:\n",
    "                    if show_detail:\n",
    "                        print(\"side check rejection, open_i2 {}, side_arr2 {}\".format(open_i2, side_arr2[open_i2]))\n",
    "                    continue\n",
    "\n",
    "                # ------ assert, op_idx2 >= exec_j ------ #\n",
    "                op_idx2 = open_idx2[open_i2]  # open_i2 는 i 와 별개로 운영\n",
    "                if check_hlm == 1 and allow_exit:\n",
    "                    if op_idx2 < op_idx1:\n",
    "                        if show_detail:\n",
    "                            print(\"check_hlm 1's allow_exit rejection, op_idx2 {} < op_idx1 {}\".format(op_idx2, op_idx1))\n",
    "                        continue\n",
    "                else:\n",
    "                    if op_idx2 < i:  # p1 execution 이후의 i 를 허용 (old, 이곳 i = op_idx1 + 1 or p2's exec_j or exit_loop's i + 1)\n",
    "                        if show_detail:\n",
    "                            print(\"op_idx2 {} < i {} : continue\".format(op_idx2, i))\n",
    "                        continue\n",
    "\n",
    "                if check_hlm == 2:\n",
    "                    i = op_idx2 + 1  # open_signal 이 close_bar.shift(1) 이라고 가정하고 다음 bar 부터 체결확인한다는 의미\n",
    "                    if i >= len_df:  # res_df 의 last_index 까지 돌아야함\n",
    "                        break\n",
    "\n",
    "                if show_detail:\n",
    "                    print(\"op_idx1 : {} op_idx2 : {}\".format(op_idx1, op_idx2))\n",
    "\n",
    "            else:\n",
    "                op_idx2 = op_idx1\n",
    "\n",
    "            # ------ 2. load util paper data for p2  ------ #\n",
    "            ep_j, tp_j, out_j = op_idx1, op_idx1, op_idx1\n",
    "            p2_tr_set_idx = (ep_j, tp_j, out_j)\n",
    "\n",
    "            ep2_ = ep2_arr[ep_j]\n",
    "            tp_ = tp_arr[tp_j]\n",
    "            out_ = out_arr[out_j]\n",
    "\n",
    "            out_1_ = res_df['{}_out_1_{}'.format(side_pos, selection_id)].to_numpy()[out_j]\n",
    "            out_0_ = res_df['{}_out_0_{}'.format(side_pos, selection_id)].to_numpy()[out_j]\n",
    "            out_gap_ = res_df['{}_out_gap_{}'.format(side_pos, selection_id)].to_numpy()[out_j]\n",
    "\n",
    "            # ------ const. for p2_wave ------ #\n",
    "            wave_itv1 = config.tr_set.wave_itv1\n",
    "            wave_period1 = config.tr_set.wave_period1\n",
    "            wave_itv2 = config.tr_set.wave_itv2\n",
    "            wave_period2 = config.tr_set.wave_period2\n",
    "\n",
    "            if check_hlm in [1, 2]:\n",
    "\n",
    "                # ------ check p1's expiry ------ # - p2_box 생성 이전의 hl_survey\n",
    "                # 1. op_idx1 ~ op_idx2 까지의 expiry check (high & low 둘다)\n",
    "                #     a. if check_hlm:  # p1_hlm, p2_hlm --> Todo, 이거를 왜 p1_hlm 에도 적용했는지 잘 모르겠음\n",
    "                if op_idx1 < op_idx2:\n",
    "                    expire, touch_idx = expiry_p1p2(res_df, config, op_idx1, op_idx2, tp_1_, tp_0_, tp_gap_, ohlc_list[1:3], open_side)\n",
    "                    if expire:  # p1's expiry\n",
    "                        if show_detail:\n",
    "                            print(\"expiry_p1p2, touch_idx = {} : break\".format(touch_idx))\n",
    "                        i = touch_idx  # + 1  --> 이거 아닌것 같음 # op_idx1 과 op_idx2 사이의 op_idx1' 을 살리기 위함, 즉 바로 다음 op_idx1 로 회귀 (건너뛰지 않고)\n",
    "                        open_i2 = prev_open_i2\n",
    "                        break  # change op_idx1\n",
    "\n",
    "                if check_hlm == 2:\n",
    "\n",
    "                    \"\"\"\n",
    "                    p2 point_validation - vectorization unavailable\n",
    "                        1. p2 로 wave_unit 을 사용할 경우만, p2 wave_validation & wave_box location 사용할 것.\n",
    "                        2. p1_loop 로 return 되는 정확한 i 를 반환하기 위해서 expiry_p1p2 뒤에 배치함\n",
    "                        3. Todo - 새로운 tp, ep, out 에 대한 처리 필요 (p1_hlm 사용시)                        \n",
    "                    \"\"\"\n",
    "\n",
    "                    # ------ p2_wave validation : 정확한 뜻을 아직 잘 모르겠음. ------ #\n",
    "                    #                     if open_side == OrderSide.SELL:\n",
    "                    #                         wave_co_post_idx = res_df['wave_co_post_idx_fill_{}{}'.format(wave_itv2, wave_period2)].to_numpy()[op_idx2]\n",
    "                    #                         if not (op_idx1 < wave_co_post_idx):\n",
    "                    #                             if show_detail:\n",
    "                    #                                 print(\"p2_wave validation : continue\")\n",
    "                    #                             continue  # change op_idx2\n",
    "\n",
    "                    #                         # --- p2_wave high validation --- #\n",
    "                    #                         # wave_high_fill1_ = res_df['wave_high_fill_{}{}'.format(wave_itv1, wave_period1)].to_numpy()[op_idx1]\n",
    "                    #                         # wave_high_fill2_ = res_df['wave_high_fill_{}{}'.format(wave_itv2, wave_period2)].to_numpy()[op_idx2]\n",
    "                    #                         # if not (wave_high_fill1_ >= wave_high_fill2_):\n",
    "                    #                         #   if show_detail:\n",
    "                    #                         #     print(\"p2_wave high validation : continue\")\n",
    "                    #                         #   continue  # change op_idx2\n",
    "\n",
    "                    #                     else:\n",
    "                    #                         wave_cu_post_idx = res_df['wave_cu_post_idx_fill_{}{}'.format(wave_itv2, wave_period2)].to_numpy()[op_idx2]\n",
    "                    #                         if not (op_idx1 < wave_cu_post_idx):\n",
    "                    #                             if show_detail:\n",
    "                    #                                 print(\"p2_wave validation : continue\")\n",
    "                    #                             continue  # change op_idx2\n",
    "\n",
    "                    #                         # --- p2_wave low validation --- #\n",
    "                    #                         # wave_low_fill1_ = res_df['wave_low_fill_{}{}'.format(wave_itv1, wave_period1)].to_numpy()[op_idx1]\n",
    "                    #                         # wave_low_fill2_ = res_df['wave_low_fill_{}{}'.format(wave_itv2, wave_period2)].to_numpy()[op_idx2]\n",
    "                    #                         # if not (wave_low_fill1_ <= wave_low_fill2_):\n",
    "                    #                         #   if show_detail:\n",
    "                    #                         #     print(\"p2_wave low validation : continue\")\n",
    "                    #                         #   continue  # change op_idx2\n",
    "\n",
    "                    #                     # ------ p2 wave_box location ------ #\n",
    "                    #                     if open_side == OrderSide.SELL:\n",
    "                    #                         if not ((tp_1_ + tp_gap_ * config.tr_set.p2_box_k1 <= out_1_) and (\n",
    "                    #                                 out_0_ <= tp_0_ - tp_gap_ * config.tr_set.p2_box_k2)):  # tp1, tp0 에 닿으면 expiry\n",
    "                    #                             # if not ((tp_1_ + tp_gap_ * config.tr_set.p2_box_k1 >= out_1_) and (out_0_ <= tp_0_ - tp_gap_ * config.tr_set.p2_box_k2)):  # tp1, tp0 에 닿으면 expiry\n",
    "                    #                             if show_detail:\n",
    "                    #                                 print(\"p2_box rejection : continue\")\n",
    "                    #                             continue\n",
    "                    #                     else:\n",
    "                    #                         if not ((tp_1_ - tp_gap_ * config.tr_set.p2_box_k1 >= out_1_) and (out_0_ >= tp_0_ + tp_gap_ * config.tr_set.p2_box_k2)):\n",
    "                    #                             # if not ((tp_1_ - tp_gap_ * config.tr_set.p2_box_k1 <= out_1_) and (out_0_ >= tp_0_ + tp_gap_ * config.tr_set.p2_box_k2)):\n",
    "                    #                             if show_detail:\n",
    "                    #                                 print(\"p2_box rejection : continue\")\n",
    "                    #                             continue\n",
    "\n",
    "                    # ------ tr_set validation & reject hl_out open_exec. ------ #\n",
    "                    if open_side == OrderSide.SELL:\n",
    "                        if not (tp_ < ep2_):\n",
    "                            break  # change op_idx1\n",
    "                        elif not (ep2_ < out_ and close[op_idx2] < out_):\n",
    "                            if show_detail:\n",
    "                                print(\"p2 tr_set validation : continue\")\n",
    "                            continue  # change op_idx2\n",
    "                    else:\n",
    "                        if not (tp_ > ep2_):\n",
    "                            break\n",
    "                        elif not (ep2_ > out_ and close[op_idx2] > out_):\n",
    "                            if show_detail:\n",
    "                                print(\"p2 tr_set validation : continue\")\n",
    "                            continue\n",
    "\n",
    "                    # ------ p1p2_low ------ #\n",
    "                    if open_side == OrderSide.SELL:\n",
    "                        if not high[op_idx1:op_idx2 + 1].max() < tp_0_ - tp_gap_ * config.tr_set.p1p2_low:\n",
    "                            if show_detail:\n",
    "                                print(\"p1p2_low rejection : continue\")\n",
    "                            continue\n",
    "                    else:\n",
    "                        if not low[op_idx1:op_idx2 + 1].min() > tp_0_ + tp_gap_ * config.tr_set.p1p2_low:\n",
    "                            if show_detail:\n",
    "                                print(\"p1p2_low rejection : continue\")\n",
    "                            continue\n",
    "\n",
    "                    # ------ check p2's expiry ------ # - 현재, op_idx2 기준의 ep2_arr 을 사용 중임.\n",
    "                    \"\"\"\n",
    "                    Caution : tr_set_idx 상황에 따라 잘 확인할 것\n",
    "                    \"\"\"\n",
    "                    exec_j, entry_done, en_p, fee = check_entry(res_df, config, config.ep_set.point2.entry_type,\n",
    "                                                                op_idx2, p2_tr_set_idx, len_df, open_side,\n",
    "                                                                [*ohlc_list, ep2_arr], expiry)\n",
    "                    i = exec_j  # = entry_loop 를 돌고 나온 e_j\n",
    "                    if not entry_done:  # p2's expiry\n",
    "                        if show_detail:\n",
    "                            print(\"expiry, i = {} at p2's : continue\".format(i))\n",
    "                        continue  # change op_idx2\n",
    "\n",
    "                    # ------ devectorized tr_calc ------ #\n",
    "                    #    1. en_p 에 대해 하는게 맞을 것으로봄\n",
    "                    #    2. tr_thresh 와 무관하게 있어야할 phase.\n",
    "                    #    Todo, fee 계산에 오류가 있는 걸로 보임 => limit_fee 를 앞에 더해주어야할 것.\n",
    "                    if open_side == OrderSide.SELL:\n",
    "                        tr_ = abs((en_p / tp_ - config.trader_set.limit_fee - 1) / (en_p / out_ - config.trader_set.market_fee - 1))\n",
    "                    else:\n",
    "                        tr_ = abs((tp_ / en_p - config.trader_set.limit_fee - 1) / (out_ / en_p - config.trader_set.market_fee - 1))\n",
    "\n",
    "                    # ------ tr_threshold ------ #\n",
    "                    if config.loc_set.point2.tr_thresh_short != \"None\":\n",
    "                        if open_side == OrderSide.SELL:\n",
    "                            if tr_ < config.loc_set.point2.tr_thresh_short:\n",
    "                                if show_detail:\n",
    "                                    print(\"tr_threshold : continue\")\n",
    "                                continue\n",
    "                        else:\n",
    "                            if tr_ < config.loc_set.point2.tr_thresh_long:\n",
    "                                if show_detail:\n",
    "                                    print(\"tr_threshold : continue\")\n",
    "                                continue\n",
    "\n",
    "            # 1. allow_exit = \"p1_hlm 의 경우, 한번 out 되면 price 가 \"wave_range 에 닿기전까지\" retrade 를 허용하지 않음\" (expiry_p1p2 을 이용해 op_idx1 을 변경할 것)\n",
    "            #     a. while phase 내부에 if not allow_exit 을 위치한 이유 : \"wave_range 에 닿기전까지\" 를 구현하기 위해서.\n",
    "            if not allow_exit:\n",
    "                if show_detail:\n",
    "                    print(\"allow_exit = {} : continue\".format(allow_exit))\n",
    "                continue\n",
    "\n",
    "            if check_hlm in [0, 1]:\n",
    "                tr_ = tr_arr[op_idx1]\n",
    "\n",
    "            # ------ leverage ------ #\n",
    "            # out = out_arr[out_j]  # lvrg_set use out on out_j (out_j shoud be based on p2)\n",
    "            leverage, liqd_p = lvrg_set(res_df, config, open_side, en_p, out_, fee)  # res_df 변수 사용됨 - 주석 처리 된 상태일뿐\n",
    "\n",
    "            if leverage is None:\n",
    "                if show_detail:\n",
    "                    print(\"leverage is None : continue\")\n",
    "                if check_hlm:\n",
    "                    continue  # change op_idx2\n",
    "                else:\n",
    "                    break  # change op_idx1\n",
    "\n",
    "            # ------------ exit phase ------------ #\n",
    "            exit_done, cross_on = 0, 0\n",
    "\n",
    "            # ------ check tpout_onexec ------ #\n",
    "            if config.ep_set.entry_type == \"LIMIT\":\n",
    "                if config.tp_set.tp_onexec:\n",
    "                    tp_j = exec_j\n",
    "                if config.out_set.out_onexec:\n",
    "                    out_j = exec_j\n",
    "\n",
    "            while 1:\n",
    "                # dynamic tp / out 을 사용하고 싶은 경우\n",
    "                if not config.tp_set.static_tp:\n",
    "                    tp_j = i\n",
    "                if not config.out_set.static_out:\n",
    "                    out_j = i\n",
    "\n",
    "                # ------------ 1. out ------------ #  # out 우선 (보수적 검증)\n",
    "                # ------ a. signal_out ------ #\n",
    "                if not exit_done:\n",
    "                    exit_done, cross_on, ex_p, fee = check_signal_out(res_df, config, open_i2, i, len_df, fee, open_side, cross_on, exit_done, [*ohlc_list, np_timeidx])\n",
    "                    # ------ b. hl_out ------ #\n",
    "                # if config.out_set.hl_out: # --> liqd_p 도입으로 hl_out 내부에서 liqd_p 조건부 수행은 필수불가결이다.\n",
    "                if not exit_done:  # and i != len_df - 1:\n",
    "                    exit_done, ex_p, fee = check_hl_out(config, i, out_j, len_df, fee, open_side, exit_done, [*ohlc_list, out_arr, liqd_p])\n",
    "\n",
    "                # ------------ 2. tp ------------ #\n",
    "                if not config.tp_set.non_tp and i != exec_j:\n",
    "                    if not exit_done:\n",
    "                        # 1. partial_tps 를 고려해 [tp_arr, ...] 형태 사용함.\n",
    "                        # 2. if config.tp_set.tp_type in ['LIMIT']:  # 'BOTH' -> 앞으로는, LIMIT 밖에 없을거라 주석처리함\n",
    "                        # 3. Todo, open_i2 는 deacy 기능을 위해 도입한 것 (추후 사용시 재확인)\n",
    "                        exit_done, ex_p, fee = check_limit_tp_exec(res_df, config, open_i2, i, tp_j, len_df, fee, open_side, exit_done, [*ohlc_list, [tp_arr]])\n",
    "\n",
    "                if exit_done:  # 이 phase 는 exit_phase 뒤에도 있어야할 것 - entry_done var. 사용은 안하겠지만\n",
    "                    # ------ 3. append dynamic result vars. ------ #\n",
    "                    p1_idx_list.append(op_idx1)  # side, zone, start_ver arr 모두 openi_list 로 접근하기 위해 open_i 를 담음\n",
    "                    p2_idx_list.append(op_idx2)\n",
    "                    pair_idx_list.append([exec_j, i])  # entry & exit (체결 기준임)\n",
    "                    pair_price_list.append([en_p, ex_p])\n",
    "                    lvrg_list.append(leverage)\n",
    "                    fee_list.append(fee)\n",
    "                    tr_list.append(tr_)  # Todo, tr vectorize 불가함, 직접 구해주어야할 건데.. (오래걸리지 않을까 --> tr_set 데이터만 모아서 vecto 계산이 나을 것)\n",
    "\n",
    "                    # for tpout_line plot_check & get_pr calc.\n",
    "                    if exit_done == 2:\n",
    "                        tpout_list.append([tp_arr[tp_j], liqd_p])\n",
    "                    else:\n",
    "                        tpout_list.append([tp_arr[tp_j], out_arr[out_j]])\n",
    "\n",
    "                    # open_i += 1  # 다음 open_idx 조사 진행\n",
    "                    break\n",
    "\n",
    "                # 1. 아래있으면, 체결 기준부터 tp, out 허용 -> tp 가 entry_idx 에 체결되는게 다소 염려되기는 함, 일단 진행 (그런 case 가 많지 않았으므로)\n",
    "                # 2. 위에있으면, entry 다음 tick 부터 exit 허용\n",
    "                i += 1\n",
    "                if i >= len_df:  # res_df 의 last_index 까지 돌아야함\n",
    "                    break\n",
    "\n",
    "            if i >= len_df:  # res_df 의 last_index 까지 돌아야함\n",
    "                break\n",
    "\n",
    "            \"\"\"\n",
    "            exit_done description            \n",
    "                1. 1 : tp_done\n",
    "                    a. check_hlm 여부와 무관하게 outer loop 의 op_idx1 을 변경 가능하도록함.\n",
    "                2. -1 : out done\n",
    "                2. 2 : liquidation done\n",
    "                3. 0 : database done                \n",
    "            \"\"\"\n",
    "\n",
    "            if exit_done == 1:\n",
    "                if show_detail:\n",
    "                    print(\"exit_done = {}, i = {} : break\".format(exit_done, i))\n",
    "                break  # change op_idx1\n",
    "            else:\n",
    "                if check_hlm in [1, 2]:\n",
    "                    if check_hlm == 1:  # exit only once in p1_hlm mode\n",
    "                        allow_exit = 0\n",
    "                    if show_detail:\n",
    "                        print(\"exit_done = {}, i = {} : continue\".format(exit_done, i))\n",
    "                    continue  # change op_idx2\n",
    "                else:\n",
    "                    if show_detail:\n",
    "                        print(\"exit_done = {}, i = {} : break\".format(exit_done, i))\n",
    "                    break  # change op_idx1\n",
    "\n",
    "        if i >= len_df:  # or open_i >= len_open_idx:  # res_df 의 last_index 까지 돌아야함\n",
    "            break\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "    return np.array(net_p1_idx_list), np.array(p1_idx_list), np.array(p2_idx_list), np.array(pair_idx_list), np.array(pair_price_list), np.array(lvrg_list), np.array(fee_list), np.array(\n",
    "        tpout_list), np.array(tr_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qfbtFVMR01UJ"
   },
   "outputs": [],
   "source": [
    "def en_ex_pairing_v9_46(res_df, open_idx_list, open_info_list, ohlc_list, config_list, np_timeidx, funcs, show_detail=False):  # 이미 충분히 줄여놓은 idx 임\n",
    "\n",
    "    \"\"\"\n",
    "    v9_44 -> v9_46\n",
    "        1. apply lvrg_needed.\n",
    "    \"\"\"\n",
    "\n",
    "    open_info1, open_info2 = open_info_list\n",
    "    side_arr1, _, _, id_idx_arr1 = open_info1\n",
    "    side_arr2, _, _, _ = open_info2\n",
    "\n",
    "    expiry_p1p2, expiry, lvrg_set, check_entry, check_signal_out, check_hl_out, check_limit_tp_exec = funcs\n",
    "\n",
    "    net_p1_idx_list, p1_idx_list, p2_idx_list, pair_idx_list, pair_price_list, lvrg_list, fee_list, tpout_list, tr_list = [[] for li in range(9)]\n",
    "    len_df = len(res_df)\n",
    "\n",
    "    open_, high, low, close = ohlc_list\n",
    "\n",
    "    open_idx1, open_idx2 = open_idx_list\n",
    "    len_open_idx1 = len(open_idx1)\n",
    "    len_open_idx2 = len(open_idx2)\n",
    "    i, open_i1, open_i2 = 0, -1, -1  # i for total_res_df indexing\n",
    "\n",
    "    while 1:\n",
    "        \n",
    "        # ------------ p1 phase ------------ #\n",
    "\n",
    "        # Todo,\n",
    "        #   1. (갱신) p1's open_i + 1 과 op_idx 를 꺼내오는 건, eik1 또는 tp 체결의 경우만 해당됨,\n",
    "        #   2. out 의 경우 p2's op_idx 기준으로 retry 필요\n",
    "        #     a. 또한, p2's op_idx > p1's op_idx\n",
    "\n",
    "        # ------ 1. get p1_info ------ #\n",
    "        # if eik1 or tp_done or first loop:\n",
    "        open_i1 += 1  # 확인 끝났으면 조기 이탈(+1), 다음 open_idx 조사 진행\n",
    "        if open_i1 >= len_open_idx1:\n",
    "            break\n",
    "\n",
    "        if show_detail:\n",
    "            print(\"open_i1 : {}, side_arr1 : {}\".format(open_i1, side_arr1[open_i1]))\n",
    "\n",
    "        op_idx1 = open_idx1[open_i1]  # open_i1 는 i 와 별개로 운영\n",
    "        if op_idx1 < i:  # i = 이전 거래 끝난후의 res_df index - \"거래 종료후 거래 시작\", '<' : 거래 종료시점 진입 가능하다는 의미\n",
    "            continue\n",
    "\n",
    "        # ------ 2. set loop index i ------ #\n",
    "        i = op_idx1  # + 1 --> op_idx1 = op_idx2 가능함 # open_signal 이 close_bar.shift(1) 이라고 가정하고 다음 bar 부터 체결확인한다는 의미\n",
    "        if i >= len_df:  # res_df 의 last_index 까지 돌아야함\n",
    "            break\n",
    "\n",
    "        # ------ 3. get open info ------ #\n",
    "        #            a. ID 별로 수행하기 위해 selection_id, config 호출함.\n",
    "        open_side_num = side_arr1[open_i1]\n",
    "        id_idx = id_idx_arr1.astype(int)[open_i1]  # indexing 을 위해 integer 로 변환.\n",
    "        \n",
    "        config = config_list[id_idx]\n",
    "        selection_id = config.selection_id\n",
    "        check_hlm = config.tr_set.check_hlm\n",
    "\n",
    "        open_side = OrderSide.SELL if open_side_num == -1 else OrderSide.BUY\n",
    "        pos_side = 'short' if open_side == OrderSide.SELL else 'long'  # utils paper 접근을 위한 long / short string.\n",
    "        if show_detail:\n",
    "            print(\"------------ op_idx1 : {} {} ------------\".format(op_idx1, open_side))\n",
    "\n",
    "        # if show_detail:\n",
    "        #   print(\"check_hlm :\", check_hlm)\n",
    "\n",
    "        \n",
    "        # ------ 4. load util paper data ------ #        \n",
    "        \"\"\" \n",
    "        tr_set_idx initialize.\n",
    "            1. j 를 둔 이유는 본래 dynamic_tp / out 을 가능케 하기 위함이였음.\n",
    "                a. exec_j : open 체결 index\n",
    "                b. ep_j : entry_price 기준 index\n",
    "                c. tp_j : tp_price 기준 index\n",
    "                d. out_j : out_price 기준 index\n",
    "        \"\"\"\n",
    "        ep_j, tp_j, out_j = op_idx1, op_idx1, op_idx1  # tr_set p1, p2 에 가변적으로 기준할 수 있도록 구성함.        \n",
    "        p1_tr_set_idx = (ep_j, tp_j, out_j)\n",
    "        \n",
    "        tp_arr = res_df['{}_tp_{}'.format(pos_side, selection_id)].to_numpy()\n",
    "        ep1_arr = res_df['{}_ep1_{}'.format(pos_side, selection_id)].to_numpy()\n",
    "        ep2_arr = res_df['{}_ep2_{}'.format(pos_side, selection_id)].to_numpy()\n",
    "        out_arr = res_df['{}_out_{}'.format(pos_side, selection_id)].to_numpy()\n",
    "\n",
    "        tr_arr = res_df['{}_tr_{}'.format(pos_side, selection_id)].to_numpy()  # just for p1_hhm\n",
    "\n",
    "        tp_1_ = res_df['{}_tp_1_{}'.format(pos_side, selection_id)].to_numpy()[tp_j]  # for p2_box location & p1's exipiry\n",
    "        tp_0_ = res_df['{}_tp_0_{}'.format(pos_side, selection_id)].to_numpy()[tp_j]\n",
    "        tp_gap_ = res_df['{}_tp_gap_{}'.format(pos_side, selection_id)].to_numpy()[tp_j]        \n",
    "        \n",
    "        lvrg_needed_ = res_df['{}_lvrg_needed_{}'.format(pos_side, selection_id)].to_numpy()[op_idx1]      \n",
    "        \n",
    "        # if not check_net_hhm:  # this phase exist for p1 entry (net hhm sync.) in p2_platform\n",
    "        exec_j, entry_done, en_p, fee = check_entry(res_df, config, config.ep_set.entry_type, op_idx1, p1_tr_set_idx, len_df, open_side, [*ohlc_list, ep1_arr], expiry)\n",
    "        \n",
    "        i = exec_j  # = entry_loop 를 돌고 나온 e_j\n",
    "        if not entry_done:\n",
    "            if show_detail:\n",
    "                print(\"p1's expiry : continue\")\n",
    "            continue\n",
    "            # else:\n",
    "        #   tp_j = op_idx1\n",
    "\n",
    "        \n",
    "        prev_open_i2 = open_i2\n",
    "        net_p1_idx_list.append(op_idx1)\n",
    "        # if check_hlm in [0, 1]:\n",
    "        #   i = op_idx1  # allow op_idx2 = op_idx1\n",
    "        allow_exit = 1\n",
    "        \n",
    "        while 1:\n",
    "            # ------------ p2 phase ------------ #\n",
    "            # ------ 1. get p2_info ------ #\n",
    "            if check_hlm in [1, 2]:\n",
    "                open_i2 += 1  # 확인 끝났으면 조기 이탈(+1), 다음 open_idx 조사 진행\n",
    "                if open_i2 >= len_open_idx2:  # open_i2 소진\n",
    "                    if show_detail:\n",
    "                        print(\"open_i2 >= len_open_idx2, open_i2 소진 : break\")\n",
    "                    break\n",
    "\n",
    "                # ------ check side sync. ------ #\n",
    "                if side_arr1[open_i1] != side_arr2[open_i2]:                \n",
    "                    if show_detail:\n",
    "                        print(\"side check rejection, open_i2 {}, side_arr2 {}\".format(open_i2, side_arr2[open_i2]))\n",
    "                    continue\n",
    "\n",
    "                # ------ assert, op_idx2 >= exec_j ------ #\n",
    "                op_idx2 = open_idx2[open_i2]  # open_i2 는 i 와 별개로 운영\n",
    "                if check_hlm == 1 and allow_exit:\n",
    "                    if op_idx2 < op_idx1:                        \n",
    "                        if show_detail:\n",
    "                            print(\"check_hlm 1's allow_exit rejection, op_idx2 {} < op_idx1 {}\".format(op_idx2, op_idx1))\n",
    "                        continue\n",
    "                else:\n",
    "                    if op_idx2 < i:  # p1 execution 이후의 i 를 허용 (old, 이곳 i = op_idx1 + 1 or p2's exec_j or exit_loop's i + 1)\n",
    "                        if show_detail:\n",
    "                            print(\"op_idx2 {} < i {} : continue\".format(op_idx2, i))\n",
    "                        continue\n",
    "\n",
    "                if check_hlm == 2:\n",
    "                    i = op_idx2 + 1  # open_signal 이 close_bar.shift(1) 이라고 가정하고 다음 bar 부터 체결확인한다는 의미\n",
    "                    if i >= len_df:  # res_df 의 last_index 까지 돌아야함\n",
    "                        break\n",
    "\n",
    "                if show_detail:\n",
    "                    print(\"op_idx1 : {} op_idx2 : {}\".format(op_idx1, op_idx2))\n",
    "\n",
    "            else:\n",
    "                op_idx2 = op_idx1\n",
    "                \n",
    "            # ------ 2. load util paper data for p2  ------ #   \n",
    "            ep_j, tp_j, out_j = op_idx1, op_idx1, op_idx1\n",
    "            p2_tr_set_idx = (ep_j, tp_j, out_j)\n",
    "            \n",
    "            ep2_ = ep2_arr[ep_j]\n",
    "            tp_ = tp_arr[tp_j]\n",
    "            out_ = out_arr[out_j]      \n",
    "        \n",
    "            out_1_ = res_df['{}_out_1_{}'.format(pos_side, selection_id)].to_numpy()[out_j]\n",
    "            out_0_ = res_df['{}_out_0_{}'.format(pos_side, selection_id)].to_numpy()[out_j]\n",
    "            out_gap_ = res_df['{}_out_gap_{}'.format(pos_side, selection_id)].to_numpy()[out_j]\n",
    "\n",
    "            # ------ const. for p2_wave ------ #\n",
    "            wave_itv1 = config.tr_set.wave_itv1\n",
    "            wave_period1 = config.tr_set.wave_period1\n",
    "            wave_itv2 = config.tr_set.wave_itv2\n",
    "            wave_period2 = config.tr_set.wave_period2\n",
    "\n",
    "            \n",
    "            if check_hlm in [1, 2]:\n",
    "                \n",
    "                # ------ check p1's expiry ------ # - p2_box 생성 이전의 hl_survey\n",
    "                # 1. op_idx1 ~ op_idx2 까지의 expiry check (high & low 둘다)\n",
    "                #     a. if check_hlm:  # p1_hlm, p2_hlm --> Todo, 이거를 왜 p1_hlm 에도 적용했는지 잘 모르겠음\n",
    "                if op_idx1 < op_idx2:\n",
    "                    expire, touch_idx = expiry_p1p2(res_df, config, op_idx1, op_idx2, tp_1_, tp_0_, tp_gap_, ohlc_list[1:3], open_side)\n",
    "                    if expire:  # p1's expiry\n",
    "                        if show_detail:\n",
    "                            print(\"expiry_p1p2, touch_idx = {} : break\".format(touch_idx))\n",
    "                        i = touch_idx  # + 1  --> 이거 아닌것 같음 # op_idx1 과 op_idx2 사이의 op_idx1' 을 살리기 위함, 즉 바로 다음 op_idx1 로 회귀 (건너뛰지 않고)\n",
    "                        open_i2 = prev_open_i2\n",
    "                        break  # change op_idx1\n",
    "\n",
    "                if check_hlm == 2:\n",
    "                    \n",
    "                    \"\"\"\n",
    "                    p2 point_validation - vectorization unavailable\n",
    "                        1. p2 로 wave_unit 을 사용할 경우만, p2 wave_validation & wave_box location 사용할 것.\n",
    "                        2. p1_loop 로 return 되는 정확한 i 를 반환하기 위해서 expiry_p1p2 뒤에 배치함\n",
    "                        3. Todo - 새로운 tp, ep, out 에 대한 처리 필요 (p1_hlm 사용시)                        \n",
    "                    \"\"\"                    \n",
    "                    \n",
    "                    # ------ p2_wave validation : 정확한 뜻을 아직 잘 모르겠음. ------ #\n",
    "#                     if open_side == OrderSide.SELL:                        \n",
    "#                         wave_co_post_idx = res_df['wave_co_post_idx_fill_{}{}'.format(wave_itv2, wave_period2)].to_numpy()[op_idx2]\n",
    "#                         if not (op_idx1 < wave_co_post_idx):\n",
    "#                             if show_detail:\n",
    "#                                 print(\"p2_wave validation : continue\")\n",
    "#                             continue  # change op_idx2\n",
    "                        \n",
    "#                         # --- p2_wave high validation --- #\n",
    "#                         # wave_high_fill1_ = res_df['wave_high_fill_{}{}'.format(wave_itv1, wave_period1)].to_numpy()[op_idx1]\n",
    "#                         # wave_high_fill2_ = res_df['wave_high_fill_{}{}'.format(wave_itv2, wave_period2)].to_numpy()[op_idx2]\n",
    "#                         # if not (wave_high_fill1_ >= wave_high_fill2_):\n",
    "#                         #   if show_detail:\n",
    "#                         #     print(\"p2_wave high validation : continue\")\n",
    "#                         #   continue  # change op_idx2\n",
    "                        \n",
    "#                     else:\n",
    "#                         wave_cu_post_idx = res_df['wave_cu_post_idx_fill_{}{}'.format(wave_itv2, wave_period2)].to_numpy()[op_idx2]\n",
    "#                         if not (op_idx1 < wave_cu_post_idx):\n",
    "#                             if show_detail:\n",
    "#                                 print(\"p2_wave validation : continue\")\n",
    "#                             continue  # change op_idx2\n",
    "                            \n",
    "#                         # --- p2_wave low validation --- #\n",
    "#                         # wave_low_fill1_ = res_df['wave_low_fill_{}{}'.format(wave_itv1, wave_period1)].to_numpy()[op_idx1]\n",
    "#                         # wave_low_fill2_ = res_df['wave_low_fill_{}{}'.format(wave_itv2, wave_period2)].to_numpy()[op_idx2]\n",
    "#                         # if not (wave_low_fill1_ <= wave_low_fill2_):\n",
    "#                         #   if show_detail:\n",
    "#                         #     print(\"p2_wave low validation : continue\")\n",
    "#                         #   continue  # change op_idx2\n",
    "                        \n",
    "#                     # ------ p2 wave_box location ------ #\n",
    "#                     if open_side == OrderSide.SELL:\n",
    "#                         if not ((tp_1_ + tp_gap_ * config.tr_set.p2_box_k1 <= out_1_) and (\n",
    "#                                 out_0_ <= tp_0_ - tp_gap_ * config.tr_set.p2_box_k2)):  # tp1, tp0 에 닿으면 expiry\n",
    "#                             # if not ((tp_1_ + tp_gap_ * config.tr_set.p2_box_k1 >= out_1_) and (out_0_ <= tp_0_ - tp_gap_ * config.tr_set.p2_box_k2)):  # tp1, tp0 에 닿으면 expiry\n",
    "#                             if show_detail:\n",
    "#                                 print(\"p2_box rejection : continue\")\n",
    "#                             continue\n",
    "#                     else:\n",
    "#                         if not ((tp_1_ - tp_gap_ * config.tr_set.p2_box_k1 >= out_1_) and (out_0_ >= tp_0_ + tp_gap_ * config.tr_set.p2_box_k2)):\n",
    "#                             # if not ((tp_1_ - tp_gap_ * config.tr_set.p2_box_k1 <= out_1_) and (out_0_ >= tp_0_ + tp_gap_ * config.tr_set.p2_box_k2)):\n",
    "#                             if show_detail:\n",
    "#                                 print(\"p2_box rejection : continue\")\n",
    "#                             continue\n",
    "                    \n",
    "                    # ------ tr_set validation & reject hl_out open_exec. ------ #\n",
    "                    if open_side == OrderSide.SELL:\n",
    "                        if not (tp_ < ep2_):\n",
    "                            break  # change op_idx1\n",
    "                        elif not (ep2_ < out_ and close[op_idx2] < out_):\n",
    "                            if show_detail:\n",
    "                                print(\"p2 tr_set validation : continue\")\n",
    "                            continue  # change op_idx2     \n",
    "                    else:\n",
    "                        if not (tp_ > ep2_):\n",
    "                            break\n",
    "                        elif not (ep2_ > out_ and close[op_idx2] > out_):\n",
    "                            if show_detail:\n",
    "                                print(\"p2 tr_set validation : continue\")\n",
    "                            continue\n",
    "                            \n",
    "                    # ------ p1p2_low ------ #\n",
    "                    if open_side == OrderSide.SELL:\n",
    "                        if not high[op_idx1:op_idx2 + 1].max() < tp_0_ - tp_gap_ * config.tr_set.p1p2_low:\n",
    "                            if show_detail:\n",
    "                                print(\"p1p2_low rejection : continue\")\n",
    "                            continue\n",
    "                    else:\n",
    "                        if not low[op_idx1:op_idx2 + 1].min() > tp_0_ + tp_gap_ * config.tr_set.p1p2_low:\n",
    "                            if show_detail:\n",
    "                                print(\"p1p2_low rejection : continue\")\n",
    "                            continue\n",
    "\n",
    "                    # ------ check p2's expiry ------ # - 현재, op_idx2 기준의 ep2_arr 을 사용 중임.\n",
    "                    \"\"\"\n",
    "                    Caution : tr_set_idx 상황에 따라 잘 확인할 것\n",
    "                    \"\"\"\n",
    "                    exec_j, entry_done, en_p, fee = check_entry(res_df, config, config.ep_set.point2.entry_type,\n",
    "                                                                                     op_idx2, p2_tr_set_idx, len_df, open_side,\n",
    "                                                                                     [*ohlc_list, ep2_arr], expiry)\n",
    "                    i = exec_j  # = entry_loop 를 돌고 나온 e_j\n",
    "                    if not entry_done:  # p2's expiry\n",
    "                        if show_detail:\n",
    "                            print(\"expiry, i = {} at p2's : continue\".format(i))\n",
    "                        continue  # change op_idx2\n",
    "\n",
    "                        \n",
    "                    # ------ devectorized tr_calc ------ #\n",
    "                    #    1. en_p 에 대해 하는게 맞을 것으로봄\n",
    "                    #    2. tr_thresh 와 무관하게 있어야할 phase.\n",
    "                    #    Todo, fee 계산에 오류가 있는 걸로 보임 => limit_fee 를 앞에 더해주어야할 것.\n",
    "                    if open_side == OrderSide.SELL:\n",
    "                        tr_ = abs((en_p / tp_ - config.trader_set.limit_fee - 1) / (en_p / out_ - config.trader_set.market_fee - 1))\n",
    "                    else:\n",
    "                        tr_ = abs((tp_ / en_p - config.trader_set.limit_fee - 1) / (out_ / en_p - config.trader_set.market_fee - 1))\n",
    "\n",
    "                        \n",
    "                    # ------ tr_threshold ------ #\n",
    "                    if config.loc_set.point2.short_tr_thresh != \"None\":\n",
    "                        if open_side == OrderSide.SELL:\n",
    "                            if tr_ < config.loc_set.point2.short_tr_thresh:\n",
    "                                if show_detail:\n",
    "                                    print(\"tr_threshold : continue\")\n",
    "                                continue\n",
    "                        else:\n",
    "                            if tr_ < config.loc_set.point2.long_tr_thresh:\n",
    "                                if show_detail:\n",
    "                                    print(\"tr_threshold : continue\")\n",
    "                                continue\n",
    "\n",
    "            # 1. allow_exit = \"p1_hlm 의 경우, 한번 out 되면 price 가 \"wave_range 에 닿기전까지\" retrade 를 허용하지 않음\" (expiry_p1p2 을 이용해 op_idx1 을 변경할 것)\n",
    "            #     a. while phase 내부에 if not allow_exit 을 위치한 이유 : \"wave_range 에 닿기전까지\" 를 구현하기 위해서.\n",
    "            if not allow_exit:\n",
    "                if show_detail:\n",
    "                    print(\"allow_exit = {} : continue\".format(allow_exit))\n",
    "                continue\n",
    "\n",
    "            if check_hlm in [0, 1]:\n",
    "                tr_ = tr_arr[op_idx1]\n",
    "\n",
    "            # ------ leverage ------ #\n",
    "            # out = out_arr[out_j]  # lvrg_set use out on out_j (out_j shoud be based on p2)\n",
    "            \n",
    "            config.lvrg_set.leverage = lvrg_needed_\n",
    "            leverage, liqd_p = lvrg_set(res_df, config, open_side, en_p, out_, fee)  # res_df 변수 사용됨 - 주석 처리 된 상태일뿐\n",
    "            \n",
    "            if leverage is None:\n",
    "                if show_detail:\n",
    "                    print(\"leverage is None : continue\")\n",
    "                if check_hlm:\n",
    "                    continue  # change op_idx2\n",
    "                else:\n",
    "                    break  # change op_idx1                \n",
    "\n",
    "            # ------------ exit phase ------------ #\n",
    "            exit_done, cross_on = 0, 0\n",
    "            \n",
    "            # ------ check tpout_onexec ------ #\n",
    "            if config.ep_set.entry_type == \"LIMIT\":\n",
    "                if config.tp_set.tp_onexec:\n",
    "                    tp_j = exec_j\n",
    "                if config.out_set.out_onexec:\n",
    "                    out_j = exec_j\n",
    "\n",
    "            while 1:                \n",
    "                # dynamic tp / out 을 사용하고 싶은 경우\n",
    "                if not config.tp_set.static_tp:\n",
    "                    tp_j = i\n",
    "                if not config.out_set.static_out:\n",
    "                    out_j = i\n",
    "\n",
    "                # ------------ 1. out ------------ #  # out 우선 (보수적 검증)\n",
    "                    # ------ a. signal_out ------ #\n",
    "                if not exit_done:\n",
    "                    exit_done, cross_on, ex_p, fee = check_signal_out(res_df, config, open_i2, i, len_df, fee, open_side, cross_on, exit_done, [*ohlc_list, np_timeidx])\n",
    "                    # ------ b. hl_out ------ #\n",
    "                if config.out_set.hl_out:\n",
    "                    if not exit_done:  # and i != len_df - 1:\n",
    "                        exit_done, ex_p, fee = check_hl_out(config, i, out_j, len_df, fee, open_side, exit_done, [*ohlc_list, out_arr, liqd_p])\n",
    "\n",
    "                # ------------ 2. tp ------------ #\n",
    "                if not config.tp_set.non_tp and i != exec_j:\n",
    "                    if not exit_done:                        \n",
    "                        # 1. partial_tps 를 고려해 [tp_arr, ...] 형태 사용함.\n",
    "                        # 2. if config.tp_set.tp_type in ['LIMIT']:  # 'BOTH' -> 앞으로는, LIMIT 밖에 없을거라 주석처리함\n",
    "                        # 3. Todo, open_i2 는 deacy 기능을 위해 도입한 것 (추후 사용시 재확인)\n",
    "                        exit_done, ex_p, fee = check_limit_tp_exec(res_df, config, open_i2, i, tp_j, len_df, fee, open_side, exit_done, [*ohlc_list, [tp_arr]])\n",
    "\n",
    "                if exit_done:  # 이 phase 는 exit_phase 뒤에도 있어야할 것 - entry_done var. 사용은 안하겠지만\n",
    "                    # ------ 3. append dynamic result vars. ------ #\n",
    "                    p1_idx_list.append(op_idx1)  # side, zone, start_ver arr 모두 openi_list 로 접근하기 위해 open_i 를 담음\n",
    "                    p2_idx_list.append(op_idx2)\n",
    "                    pair_idx_list.append([exec_j, i])  # entry & exit (체결 기준임)\n",
    "                    pair_price_list.append([en_p, ex_p])\n",
    "                    lvrg_list.append(leverage)\n",
    "                    fee_list.append(fee)\n",
    "                    tr_list.append(tr_)  # Todo, tr vectorize 불가함, 직접 구해주어야할 건데.. (오래걸리지 않을까 --> tr_set 데이터만 모아서 vecto 계산이 나을 것)\n",
    "                    \n",
    "                    # for tpout_line plot_check & get_pr calc.\n",
    "                    if exit_done == 2:\n",
    "                        tpout_list.append([tp_arr[tp_j], liqd_p])\n",
    "                    else:\n",
    "                        tpout_list.append([tp_arr[tp_j], out_arr[out_j]])\n",
    "\n",
    "                    # open_i += 1  # 다음 open_idx 조사 진행\n",
    "                    break\n",
    "\n",
    "                # 1. 아래있으면, 체결 기준부터 tp, out 허용 -> tp 가 entry_idx 에 체결되는게 다소 염려되기는 함, 일단 진행 (그런 case 가 많지 않았으므로)\n",
    "                # 2. 위에있으면, entry 다음 tick 부터 exit 허용\n",
    "                i += 1\n",
    "                if i >= len_df:  # res_df 의 last_index 까지 돌아야함\n",
    "                    break\n",
    "\n",
    "            if i >= len_df:  # res_df 의 last_index 까지 돌아야함\n",
    "                break\n",
    "                \n",
    "                \n",
    "            \"\"\"\n",
    "            exit_done description            \n",
    "                1. 1 : tp_done\n",
    "                    a. check_hlm 여부와 무관하게 outer loop 의 op_idx1 을 변경 가능하도록함.\n",
    "                2. -1 : out done\n",
    "                2. 2 : liquidation done\n",
    "                3. 0 : database done                \n",
    "            \"\"\"\n",
    "            \n",
    "            if exit_done == 1: \n",
    "                if show_detail:\n",
    "                    print(\"exit_done = {}, i = {} : break\".format(exit_done, i))\n",
    "                break  # change op_idx1\n",
    "            else:\n",
    "                if check_hlm in [1, 2]:\n",
    "                    if check_hlm == 1:  # exit only once in p1_hlm mode\n",
    "                        allow_exit = 0\n",
    "                    if show_detail:\n",
    "                        print(\"exit_done = {}, i = {} : continue\".format(exit_done, i))\n",
    "                    continue  # change op_idx2\n",
    "                else:\n",
    "                    if show_detail:\n",
    "                        print(\"exit_done = {}, i = {} : break\".format(exit_done, i))\n",
    "                    break  # change op_idx1\n",
    "\n",
    "        if i >= len_df:  # or open_i >= len_open_idx:  # res_df 의 last_index 까지 돌아야함\n",
    "            break\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "    return np.array(net_p1_idx_list), np.array(p1_idx_list), np.array(p2_idx_list), np.array(pair_idx_list), np.array(pair_price_list), np.array(lvrg_list), np.array(fee_list), np.array(tpout_list), np.array(tr_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### check_entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_entry_v6_2(res_df, config, entry_type, op_idx, tr_set_idx, len_df, open_side, np_datas, expiry):\n",
    "\n",
    "    \"\"\"\n",
    "    v6_1 -> v6_2\n",
    "        1. tr_set 을 p1, p2 에 가변적으로 기준하기 위해 ep_base_idx, tp_base_idx, out_base_idx 를 외부 참조하도록 함.\n",
    "    \"\"\"\n",
    "\n",
    "    open, high, low, close, ep_arr = np_datas\n",
    "    ep_base_idx, tp_base_idx, out_base_idx = tr_set_idx\n",
    "\n",
    "    # selection_id = config.selection_id\n",
    "    # allow_ep_in = 0 if config.ep_set.point2.use_point2 else 1\n",
    "    entry_done = 0\n",
    "    ep = None\n",
    "\n",
    "    if entry_type == \"LIMIT\":\n",
    "        fee = config.trader_set.limit_fee\n",
    "        \n",
    "        for e_j in range(op_idx + 1, len_df):\n",
    "            # ------ index setting for dynamic options ------ #\n",
    "            if not config.ep_set.static_ep:\n",
    "                ep_base_idx = e_j  # dynamic_ep 를 위한 ep_index var.\n",
    "                out_base_idx = e_j  # dynamic_out 를 위한 out_index var. - 조건식이 static_ep 와 같이 있는 이유 모름 => dynamic_lvrg 로 사료됨\n",
    "\n",
    "            if not config.tp_set.static_tp:\n",
    "                tp_base_idx = e_j\n",
    "\n",
    "            # ------ expire_k & expire_tick ------ # - limit 사용하면 default 로 expire_k 가 존재해야함\n",
    "            if expiry(res_df, config, op_idx, e_j, tp_base_idx, [high, low], open_side):\n",
    "                break\n",
    "\n",
    "            # ------ point2 ------ #\n",
    "            # if not allow_ep_in:\n",
    "            #     allow_ep_in, out_base_idx = ep_loc_point2(res_df, config, e_j, out_base_idx, side=OrderSide.SELL)\n",
    "            #     if allow_ep_in:\n",
    "            #       if config.ep_set.point2.entry_type == \"LIMIT\":\n",
    "            #         ep_base_idx = e_j\n",
    "            #         # print(\"e_j in point2 :\", e_j)\n",
    "            #         continue\n",
    "\n",
    "            # ------ check ep_exec ------ #\n",
    "            # if allow_ep_in:\n",
    "            # if config.ep_set.point2.use_point2 and config.ep_set.point2.entry_type == 'MARKET':\n",
    "            #   entry_done = 1\n",
    "            #   ep = c[e_j]\n",
    "            #   break\n",
    "            # else:\n",
    "\n",
    "            if open_side == OrderSide.SELL:\n",
    "                if high[e_j] >= ep_arr[ep_base_idx]:\n",
    "                    entry_done = 1\n",
    "                    ep = ep_arr[ep_base_idx]\n",
    "                    if open[e_j] >= ep_arr[ep_base_idx]:  # open comp 는 결국, 수익률에 얹어주는 logic (반보수) -> 사용 보류\n",
    "                        ep = open[e_j]\n",
    "                    break\n",
    "            else:\n",
    "                if low[e_j] <= ep_arr[ep_base_idx]:\n",
    "                    entry_done = 1\n",
    "                    ep = ep_arr[ep_base_idx]\n",
    "                    if open[e_j] <= ep_arr[ep_base_idx]:\n",
    "                        ep = open[e_j]\n",
    "                    break\n",
    "\n",
    "        try:\n",
    "            exec_idx = e_j\n",
    "            \n",
    "        except Exception as e:\n",
    "            exec_idx = None  # 어차피, 외부에서 entry_done = 0 로 빠지면 continue 되기 때문에 의미 없음.\n",
    "            print(\"error in check_entry e_j loop : {}\".format(e))\n",
    "\n",
    "    else:  # market entry\n",
    "        exec_idx = op_idx + 1\n",
    "        entry_done = 1\n",
    "        ep = close[op_idx]\n",
    "        fee = config.trader_set.market_fee\n",
    "\n",
    "    return exec_idx, entry_done, ep, fee"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### check_limit_tp_exec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_limit_tp_exec(res_df, config, open_i, i, tp_j, len_df, fee, open_side, exit_done, np_datas):\n",
    "\n",
    "    open, high, low, close, tps = np_datas\n",
    "    tp = None\n",
    "    selection_id = config.selection_id\n",
    "    len_tps = len(tps)\n",
    "\n",
    "    for tp_i, tp_arr in enumerate(tps):\n",
    "\n",
    "        #     decay adjustment    #\n",
    "        #     tp_j includes dynamic_j - functionalize  #\n",
    "        # try:\n",
    "        #     if config.tr_set.decay_gap != \"None\":\n",
    "        #         decay_share = (j - open_i) // config.tp_set.decay_term\n",
    "        #         decay_remain = (j - open_i) % config.tp_set.decay_term\n",
    "        #         if j != open_i and decay_remain == 0:\n",
    "        #             if open_side == OrderSide.SELL:\n",
    "        #                 tp_arr[tp_j] += res_df['short_tp_gap_{}'.format(selection_id)].iloc[open_i] * config.tr_set.decay_gap * decay_share\n",
    "        #             else:\n",
    "        #                 tp_arr[tp_j] -= res_df['long_tp_gap_{}'.format(selection_id)].iloc[open_i] * config.tr_set.decay_gap * decay_share\n",
    "        # except:\n",
    "        #     pass\n",
    "\n",
    "        if open_side == OrderSide.SELL:\n",
    "            \n",
    "            # if low[i] < tp_arr[tp_j]:  # and partial_tp_cnt == tp_i:  # we use static tp now\n",
    "            if low[i] <= tp_arr[tp_j]:  # and partial_tp_cnt == tp_i:  # we use static tp now\n",
    "                # if low[i] <= tp_arr[i] <= h[i]: --> 이건 잘못되었음\n",
    "                # partial_tp_cnt += 1 --> partial_tp 보류\n",
    "\n",
    "                # 1. dynamic tp\n",
    "                if tp_arr[i] != tp_arr[i - 1] and not config.tp_set.static_tp:\n",
    "                    # tp limit 이 불가한 경우 - open 이 이미, tp 를 넘은 경우\n",
    "                    if open[i] < tp_arr[i]:\n",
    "                        tp = open[i]\n",
    "                    # tp limit 이 가능한 경우 - open 이 아직, tp 를 넘지 않은 경우\n",
    "                    else:\n",
    "                        tp = tp_arr[i]\n",
    "\n",
    "                # 2. static tp\n",
    "                else:\n",
    "                    #   tp limit 이 불가한 경우 - open 이 이미, tp 를 넘은 경우\n",
    "                    if open[i] < tp_arr[tp_j]:  # static 해놓고 decay 사용하면 dynamic 이니까\n",
    "                        if config.tr_set.decay_gap != \"None\" and decay_remain == 0:\n",
    "                            tp = open[i]  # tp_j -> open_i 를 가리키기 때문에 decay 는 한번만 진행되는게 맞음\n",
    "                        else:\n",
    "                            tp = tp_arr[tp_j]\n",
    "                    else:\n",
    "                        tp = tp_arr[tp_j]\n",
    "\n",
    "                if tp_i == len_tps - 1:\n",
    "                    exit_done = 1  # partial 을 고려해 exit_done = 1 상태는 tp_i 가 last_index 로 체결된 경우만 해당\n",
    "\n",
    "        else:\n",
    "            # if high[i] > tp_arr[tp_j]:\n",
    "            if high[i] >= tp_arr[tp_j]:\n",
    "            \n",
    "                # 1. dynamic tp\n",
    "                if tp_arr[i] != tp_arr[i - 1] and not config.tp_set.static_tp:\n",
    "                    if open[i] > tp_arr[i]:\n",
    "                        tp = open[i]\n",
    "                    else:\n",
    "                        tp = tp_arr[i]\n",
    "\n",
    "                # 2. static tp\n",
    "                else:\n",
    "                    if open[i] > tp_arr[tp_j]:\n",
    "                        if config.tr_set.decay_gap != \"None\" and decay_remain == 0:\n",
    "                            tp = open[i]\n",
    "                        else:\n",
    "                            tp = tp_arr[tp_j]\n",
    "                    else:\n",
    "                        tp = tp_arr[tp_j]\n",
    "\n",
    "                if tp_i == len_tps - 1:\n",
    "                    exit_done = 1  # partial 을 고려해 exit_done = 1 상태는 tp_i 가 last_index 로 체결된 경우만 해당\n",
    "\n",
    "    if exit_done:\n",
    "        fee += config.trader_set.limit_fee\n",
    "\n",
    "    return exit_done, tp, fee\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### check_signal_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_signal_out_v5_1(res_df, config, open_i, i, len_df, fee, open_side, cross_on, exit_done, np_datas):\n",
    "    \"\"\"\n",
    "    v5 -> v5_1\n",
    "        1. apply to wave_unit fisher exit.\n",
    "    \"\"\"\n",
    "    _, _, _, close, np_timeidx = np_datas\n",
    "    ex_p = None\n",
    "    selection_id = config.selection_id\n",
    "\n",
    "    # 1. timestamp\n",
    "    if config.out_set.tf_exit != \"None\":\n",
    "        if np_timeidx[i] % config.out_set.tf_exit == config.out_set.tf_exit - 1 and i != open_i:\n",
    "            exit_done = -1\n",
    "\n",
    "    # 2. fisher\n",
    "    if config.out_set.fisher_exit:        \n",
    "        \n",
    "        wave_itv1 = config.tr_set.wave_itv1\n",
    "        wave_period1 = config.tr_set.wave_period1\n",
    "        wave_cu_ = res_df['wave_cu_{}{}'.format(wave_itv1, wave_period1)].to_numpy()\n",
    "        wave_co_ = res_df['wave_co_{}{}'.format(wave_itv1, wave_period1)].to_numpy()\n",
    "\n",
    "        # itv_num = itv_to_number(config.loc_set.point1.tf_entry)\n",
    "\n",
    "        # fisher_band = config.out_set.fisher_band\n",
    "        # fisher_band2 = config.out_set.fisher_band2\n",
    "\n",
    "        # if np_timeidx[i] % itv_num == itv_num - 1:\n",
    "        # fisher_ = res_df['fisher_{}{}'.format(config.loc_set.point1.tf_entry)].to_numpy()\n",
    "\n",
    "        if open_side == OrderSide.SELL:            \n",
    "            if wave_co_[i]:\n",
    "            # if (fisher_[i - itv_num] > -fisher_band) & (fisher_[i] <= -fisher_band):\n",
    "                exit_done = -1\n",
    "            # elif (fisher_[i - itv_num] < fisher_band2) & (fisher_[i] >= fisher_band2):\n",
    "            #     exit_done = -1\n",
    "        else:\n",
    "            if wave_cu_[i]:\n",
    "            # if (fisher_[i - itv_num] < fisher_band) & (fisher_[i] >= fisher_band):\n",
    "                exit_done = -1\n",
    "            # elif (fisher_[i - itv_num] > fisher_band2) & (fisher_[i] <= fisher_band2):\n",
    "            #     exit_done = -1\n",
    "\n",
    "    # 3. cci\n",
    "    if config.out_set.cci_exit:\n",
    "        cci_ = res_df['cci_T20'].to_numpy()\n",
    "\n",
    "        if open_side == OrderSide.SELL:\n",
    "            if (cci_[i - 1] >= -100) & (cci_[i] < -100):\n",
    "                # if (cci_[i - 1] <= -100) & (cci_[i] > -100):\n",
    "                exit_done = -1\n",
    "        else:\n",
    "            if (cci_[i - 1] <= 100) & (cci_[i] > 100):\n",
    "                # if (cci_[i - 1] >= 100) & (cci_[i] < 100):\n",
    "                exit_done = -1\n",
    "\n",
    "    if exit_done:\n",
    "        ex_p = close[i]\n",
    "        fee += config.trader_set.market_fee\n",
    "\n",
    "    return exit_done, cross_on, ex_p, fee"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### check_hl_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_hl_out_v4(config, i, out_j, len_df, fee, open_side, exit_done, np_datas):\n",
    "    \n",
    "    \"\"\"\n",
    "    v3 -> v4\n",
    "        1. Add non_out function.\n",
    "    \"\"\"\n",
    "\n",
    "    open, high, low, close, out_arr, liqd_p = np_datas\n",
    "    ex_p = None\n",
    "\n",
    "    # 1. liquidation default check\n",
    "    if open_side == OrderSide.SELL:\n",
    "        if high[i] >= liqd_p:\n",
    "            exit_done = 2\n",
    "    else:\n",
    "        if low[i] <= liqd_p:\n",
    "            exit_done = 2\n",
    "    \n",
    "    if not config.out_set.non_out:\n",
    "        # 2. hl_out\n",
    "        if config.out_set.hl_out:\n",
    "            if open_side == OrderSide.SELL:\n",
    "                if high[i] >= out_arr[out_j]:\n",
    "                    exit_done = -1\n",
    "            else:\n",
    "                if low[i] <= out_arr[out_j]:\n",
    "                    exit_done = -1\n",
    "        # 3. close_out\n",
    "        else:\n",
    "            if open_side == OrderSide.SELL:\n",
    "                if close[i] >= out_arr[out_j]:\n",
    "                    exit_done = -1\n",
    "            else:\n",
    "                if close[i] <= out_arr[out_j]:\n",
    "                    exit_done = -1            \n",
    "\n",
    "    if exit_done:  # exit_done should not be zero in this phase      \n",
    "        if exit_done == 2:\n",
    "            ex_p = liqd_p\n",
    "        else:\n",
    "            if config.out_set.hl_out:\n",
    "                ex_p = out_arr[out_j]\n",
    "            else:\n",
    "                ex_p = close[i]\n",
    "\n",
    "            # check open out execution\n",
    "            if open_side == OrderSide.SELL:\n",
    "                if open[i] >= out_arr[out_j]:\n",
    "                    ex_p = open[i]\n",
    "            else:\n",
    "                if open[i] <= out_arr[out_j]:\n",
    "                    ex_p = open[i]\n",
    "\n",
    "        fee += config.trader_set.market_fee\n",
    "\n",
    "    return exit_done, ex_p, fee"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### idep_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xafHpMLwESKf"
   },
   "outputs": [],
   "source": [
    "def idep_plot_v16_7(res_df, len_df, config, high, low, open_info_df1, paired_res, funcs, inversion=False, sample_ratio=0.7,\n",
    "                    title_position=(0.5, 0.5), fontsize=15, signi=False, savefig=False):  # open_idx, side_arr\n",
    "\n",
    "    \"\"\"\n",
    "    v16_6 -> v16_7\n",
    "        1. just return one type outputs, irrelevent to signi.\n",
    "    \"\"\"\n",
    "\n",
    "    get_wave_bias, get_pr, get_res_info, plot_info, frq_dev_plot = funcs\n",
    "\n",
    "    if not signi:\n",
    "        plt.style.use(['dark_background', 'fast'])\n",
    "        plt.figure(figsize=(24, 8), dpi=60)\n",
    "        gs = gridspec.GridSpec(nrows=2,  # row 몇 개\n",
    "                               ncols=3,  # col 몇 개\n",
    "                               height_ratios=[10, 1]\n",
    "                               # height_ratios=[10, 10, 1]\n",
    "                               )\n",
    "    gs_idx = 0\n",
    "    gs_idx_below = 3\n",
    "    # plt.suptitle(key)\n",
    "\n",
    "    partial_ranges, partial_qty_ratio = literal_eval(config.tp_set.partial_ranges), literal_eval(config.tp_set.partial_qty_ratio)\n",
    "    assert np.sum(partial_qty_ratio) == 1.0\n",
    "    assert len(partial_ranges) == len(partial_qty_ratio)\n",
    "\n",
    "    if sample_ratio is not None:\n",
    "        sample_len = int(len_df * sample_ratio)\n",
    "    else:\n",
    "        sample_len = len_df\n",
    "\n",
    "    # ------ short & long data preparation ------ #\n",
    "    # start_time = time.time()\n",
    "\n",
    "    net_p1_idx_arr, p1_idx_arr, p2_idx_arr, pair_idx_arr, pair_price_arr, lvrg_arr, fee_arr, tpout_arr, tr_arr = paired_res\n",
    "    assert len(p1_idx_arr) != 0, \"assert len(p1_idx_arr) != 0\"\n",
    "\n",
    "    # short_net_p1_idx_arr = net_p1_idx_arr[np.where(open_info_df1.side.loc[net_p1_idx_arr] == OrderSide.SELL)[0]]\n",
    "    # long_net_p1_idx_arr = net_p1_idx_arr[np.where(open_info_df1.side.loc[net_p1_idx_arr] == OrderSide.BUY)[0]]\n",
    "    short_net_p1_idx_arr = net_p1_idx_arr[np.where(open_info_df1.side.loc[net_p1_idx_arr] == -1)[0]]\n",
    "    long_net_p1_idx_arr = net_p1_idx_arr[np.where(open_info_df1.side.loc[net_p1_idx_arr] == 1)[0]]\n",
    "\n",
    "    short_net_p1_frq = len(short_net_p1_idx_arr)\n",
    "    long_net_p1_frq = len(long_net_p1_idx_arr)\n",
    "    # print(\"len(short_net_p1_true_bias_bool) :\", len(short_net_p1_idx_arr))\n",
    "    # print(\"len(long_net_p1_true_bias_bool) :\", len(long_net_p1_idx_arr))\n",
    "\n",
    "    # short_p1_openi_idx = np.where(open_info_df1.side.loc[p1_idx_arr] == OrderSide.SELL)[0]  # p1_idx_arr 에 대한 idx, # side_arr,\n",
    "    # long_p1_openi_idx = np.where(open_info_df1.side.loc[p1_idx_arr] == OrderSide.BUY)[0]\n",
    "    short_p1_openi_idx = np.where(open_info_df1.side.loc[p1_idx_arr] == -1)[0]  # p1_idx_arr 에 대한 idx, # side_arr,\n",
    "    long_p1_openi_idx = np.where(open_info_df1.side.loc[p1_idx_arr] == 1)[0]\n",
    "\n",
    "    # p1_idx = open_idx[p1_openi_arr].reshape(-1, 1)   # != p1_idx_arr, p1_openi_arr 은 exit_done 기준임\n",
    "\n",
    "    np_obj = np.hstack((pair_price_arr, pair_idx_arr,\n",
    "                        p1_idx_arr.reshape(-1, 1)))  # p1_idx_arr is 1d, need to be changed to 2d (for stacking)\n",
    "    short_obj = np_obj[short_p1_openi_idx]\n",
    "    long_obj = np_obj[long_p1_openi_idx]\n",
    "    both_obj = np.vstack((short_obj, long_obj))\n",
    "    # print(\"short_obj.shape :\", short_obj.shape)\n",
    "    # print(\"long_obj.shape :\", long_obj.shape)\n",
    "\n",
    "    short_obj, long_obj, both_obj = [np.split(obj_, 5, axis=1) for obj_ in [short_obj, long_obj, both_obj]]\n",
    "\n",
    "    short_p2_idx_arr, long_p2_idx_arr = [p2_idx_arr[openi_idx_].reshape(-1, 1) for openi_idx_ in [short_p1_openi_idx, long_p1_openi_idx]]\n",
    "    short_lvrg_arr, long_lvrg_arr = [lvrg_arr[openi_idx_].reshape(-1, 1) for openi_idx_ in [short_p1_openi_idx, long_p1_openi_idx]]\n",
    "    short_fee_arr, long_fee_arr = [fee_arr[openi_idx_].reshape(-1, 1) for openi_idx_ in [short_p1_openi_idx, long_p1_openi_idx]]\n",
    "    short_tpout_arr, long_tpout_arr = [tpout_arr[openi_idx_] for openi_idx_ in [short_p1_openi_idx, long_p1_openi_idx]]\n",
    "    short_tr_arr, long_tr_arr = [tr_arr[openi_idx_] for openi_idx_ in [short_p1_openi_idx, long_p1_openi_idx]]\n",
    "    # short_bias_arr, long_bias_arr = [bias_arr[openi_idx_] for openi_idx_ in [short_p1_openi_idx, long_p1_openi_idx]]\n",
    "    # print(\"long_bias_arr.shape :\", long_bias_arr.shape)\n",
    "    # print(\"short / long arr setting elapsed time :\", time.time() - start_time)\n",
    "\n",
    "    # start_time = time.time()\n",
    "\n",
    "    short_tpbox_hhm, long_tpbox_hhm, short_tpbox_p2exec_hhm, long_tpbox_p2exec_hhm, short_outbox_hhm, long_outbox_hhm, \\\n",
    "    short_net_p1_bias_tick, long_net_p1_bias_tick, short_p2exec_p1_bias_tick, long_p2exec_p1_bias_tick, short_p2_true_bias_bool, long_p2_true_bias_bool, \\\n",
    "    short_tp_1, short_tp_0, long_tp_1, long_tp_0, short_out_1, short_out_0, long_out_1, long_out_0, short_ep2_0, long_ep2_0 = \\\n",
    "        get_wave_bias(res_df, config, high, low, len_df, short_net_p1_idx_arr, long_net_p1_idx_arr, short_p2_idx_arr,\n",
    "                      long_p2_idx_arr, short_obj, long_obj)\n",
    "\n",
    "    # print(\"get_wave_bias elapsed time :\", time.time() - start_time)\n",
    "    # print(\"short_net_p1_bias_tick, long_net_p1_bias_tick, short_p2exec_p1_bias_tick, long_p2exec_p1_bias_tick :\", short_net_p1_bias_tick, long_net_p1_bias_tick, short_p2exec_p1_bias_tick, long_p2exec_p1_bias_tick)\n",
    "\n",
    "    len_short, len_long = len(short_p1_openi_idx), len(long_p1_openi_idx)\n",
    "\n",
    "    # ------ plot_data ------ #\n",
    "    try:\n",
    "        # start_time = time.time()\n",
    "        if len_short == 0:  # 0 이 아닌 경우에만 계산이 가능함\n",
    "            short_pr = []\n",
    "            short_idep_res_obj = np.nan\n",
    "        else:\n",
    "            short_tr = short_tr_arr.mean()\n",
    "            short_pr, short_liqd = get_pr(OrderSide.SELL, high, low, short_obj, short_tpout_arr, short_lvrg_arr,\n",
    "                                          short_fee_arr, partial_ranges, partial_qty_ratio, config.tp_set.non_tp, inversion)\n",
    "            short_total_pr = to_total_pr(len_df, short_pr, short_obj[-2])\n",
    "            short_acc_pr = np.cumprod(short_total_pr)\n",
    "            short_sum_pr = get_sum_pr_nb(short_total_pr)\n",
    "            short_hlm = hlm(short_pr, short_p2_true_bias_bool)\n",
    "            short_trade_ticks = np.mean(short_obj[-2] - short_obj[-1])\n",
    "            if signi:\n",
    "                short_idep_res_obj = short_tpbox_p2exec_hhm, short_hlm, *get_res_info(sample_len, short_pr,\n",
    "                                                                                        short_acc_pr,\n",
    "                                                                                        short_sum_pr, short_liqd)\n",
    "            else:\n",
    "                plot_info(gs, gs_idx, len_df, sample_len, short_tr, short_tpbox_hhm, short_tpbox_p2exec_hhm,\n",
    "                          short_outbox_hhm, short_hlm, short_trade_ticks, short_net_p1_frq, short_pr, short_total_pr,\n",
    "                          short_acc_pr, short_sum_pr, short_liqd, short_lvrg_arr.mean(), title_position, fontsize)\n",
    "\n",
    "                frq_dev_plot(gs, gs_idx_below, len_df, sample_len, short_obj[-2], short_p2_true_bias_bool, short_acc_pr[-1], short_sum_pr[-1], fontsize)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"error in short plot_data :\", e)\n",
    "\n",
    "    gs_idx += 1\n",
    "    gs_idx_below += 1\n",
    "\n",
    "    try:\n",
    "        # start_time = time.time()\n",
    "        if len_long == 0:\n",
    "            long_pr = []\n",
    "            long_idep_res_obj = np.nan\n",
    "        else:\n",
    "            long_tr = long_tr_arr.mean()\n",
    "            long_pr, long_liqd = get_pr(OrderSide.BUY, high, low, long_obj, long_tpout_arr, long_lvrg_arr,\n",
    "                                        long_fee_arr, partial_ranges, partial_qty_ratio, config.tp_set.non_tp, inversion)\n",
    "            long_total_pr = to_total_pr(len_df, long_pr, long_obj[-2])\n",
    "            long_acc_pr = np.cumprod(long_total_pr)\n",
    "            long_sum_pr = get_sum_pr_nb(long_total_pr)\n",
    "            long_hlm = hlm(long_pr, long_p2_true_bias_bool)\n",
    "            long_trade_ticks = np.mean(long_obj[-2] - long_obj[-1])\n",
    "            if signi:\n",
    "                long_idep_res_obj = long_tpbox_p2exec_hhm, long_hlm, *get_res_info(sample_len, long_pr, long_acc_pr,\n",
    "                                                                                     long_sum_pr,\n",
    "                                                                                     long_liqd)\n",
    "            else:\n",
    "                plot_info(gs, gs_idx, len_df, sample_len, long_tr, long_tpbox_hhm, long_tpbox_p2exec_hhm,\n",
    "                          long_outbox_hhm, long_hlm, long_trade_ticks, long_net_p1_frq, long_pr, long_total_pr,\n",
    "                          long_acc_pr, long_sum_pr, long_liqd, long_lvrg_arr.mean(), title_position, fontsize)\n",
    "\n",
    "                frq_dev_plot(gs, gs_idx_below, len_df, sample_len, long_obj[-2], long_p2_true_bias_bool, long_acc_pr[-1], long_sum_pr[-1], fontsize)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"error in long plot_data :\", e)\n",
    "\n",
    "    gs_idx += 1\n",
    "    gs_idx_below += 1\n",
    "\n",
    "    try:\n",
    "        # start_time = time.time()\n",
    "        if len_short * len_long == 0:\n",
    "            both_pr = []\n",
    "            both_idep_res_obj = np.nan\n",
    "        else:\n",
    "            both_tr = (short_tr + long_tr) / 2\n",
    "            both_pr = np.vstack((short_pr, long_pr))  # for 2d arr, obj 를 1d 로 만들지 않는 이상, pr 은 2d 유지될 것\n",
    "            both_total_pr = to_total_pr(len_df, both_pr, both_obj[-2])\n",
    "            both_acc_pr = np.cumprod(both_total_pr)\n",
    "            both_sum_pr = get_sum_pr_nb(both_total_pr)\n",
    "            both_liqd = min(short_liqd, long_liqd)\n",
    "\n",
    "            both_p2_true_bias_bool = np.hstack((short_p2_true_bias_bool, long_p2_true_bias_bool))  # hstack for 1d arr, vstack for 2d arr\n",
    "            both_tpbox_hhm = (short_tpbox_hhm + long_tpbox_hhm) / 2\n",
    "            both_tpbox_p2exec_hhm, both_hlm = (short_tpbox_p2exec_hhm + long_tpbox_p2exec_hhm) / 2, (short_hlm + long_hlm) / 2\n",
    "            both_outbox_hhm = (short_outbox_hhm + long_outbox_hhm) / 2\n",
    "            both_trade_ticks = np.mean(both_obj[-2] - both_obj[-1])\n",
    "            both_net_p1_frq = short_net_p1_frq + long_net_p1_frq\n",
    "            if signi:\n",
    "                both_idep_res_obj = both_tpbox_p2exec_hhm, both_hlm, *get_res_info(sample_len, both_pr, both_acc_pr,\n",
    "                                                                                     both_sum_pr,\n",
    "                                                                                     both_liqd)\n",
    "            else:\n",
    "                plot_info(gs, gs_idx, len_df, sample_len, both_tr, both_tpbox_hhm, both_tpbox_p2exec_hhm,\n",
    "                          both_outbox_hhm, both_hlm, both_trade_ticks, both_net_p1_frq, both_pr, both_total_pr,\n",
    "                          both_acc_pr, both_sum_pr, both_liqd, lvrg_arr.mean(), title_position, fontsize)\n",
    "\n",
    "                frq_dev_plot(gs, gs_idx_below, len_df, sample_len, both_obj[-2], both_p2_true_bias_bool, both_acc_pr[-1], both_sum_pr[-1], fontsize)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"error in both plot_data :\", e)\n",
    "\n",
    "    gs_idx += 1\n",
    "    gs_idx_below += 1\n",
    "\n",
    "    if not signi:\n",
    "        if savefig:\n",
    "            plt.savefig(r\"D:\\Projects\\SystemTrading\\JnQ\\result/IDEP_res_{}.png\".format(str(datetime.now().timestamp()).split('.')[0]), bbox_inches='tight')\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "\n",
    "    return short_pr, short_obj, short_lvrg_arr, short_fee_arr, short_tpout_arr, short_tr_arr, short_p2_true_bias_bool, short_net_p1_bias_tick, short_p2exec_p1_bias_tick, short_net_p1_idx_arr, short_p2_idx_arr, short_tp_1, short_tp_0, short_out_1, short_out_0, short_ep2_0, \\\n",
    "           long_pr, long_obj, long_lvrg_arr, long_fee_arr, long_tpout_arr, long_tr_arr, long_p2_true_bias_bool, long_net_p1_bias_tick, long_p2exec_p1_bias_tick, long_net_p1_idx_arr, long_p2_idx_arr, long_tp_1, long_tp_0, long_out_1, long_out_0, long_ep2_0  # long_net_p1_idx_arr long_p2_idx_arr\n",
    "    # else:\n",
    "    #     return short_idep_res_obj, long_idep_res_obj, both_idep_res_obj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### get_wave_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wave_bias_v6_2(res_df, config, high, low, len_df, short_net_p1_idx_arr, long_net_p1_idx_arr, short_p2_idx_arr, long_p2_idx_arr, short_obj, long_obj):\n",
    "\n",
    "    \"\"\"\n",
    "    v6_1 -> v6_2\n",
    "    1. p1's tr_set 을 사용하는 p2 (en_ex_pairing_v9_43) 의 올바른 out_box plotting 을 위해 ffill_line 에 p2_idx -> net_p1_idx 사용함.\n",
    "    2. hlm = 1.0 으로 오차없이 나오는 version.\n",
    "\n",
    "    Todo : p2_idx -> en2_idx 도 필요함 (outbox 를 위한 hhm)\n",
    "    \"\"\"\n",
    "\n",
    "    short_net_p1_idx = short_net_p1_idx_arr.astype(int)  # .reshape(-1, 1)\n",
    "    short_p1_idx = short_obj[-1].astype(int).ravel()\n",
    "    short_p2_idx = short_p2_idx_arr.astype(int).ravel()  # .reshape(-1, 1)\n",
    "    short_en_idx = short_obj[2].astype(int).ravel()\n",
    "\n",
    "    # tr_set 은 p1 기준이 맞음.\n",
    "    short_tp_1 = ffill_line(res_df['short_tp_1_{}'.format(config.selection_id)].to_numpy(), short_net_p1_idx)  # net_p1_idx ~ net_p1_idx' 사이에 대한 momentum 조사 (net 이유는 logic's validation)\n",
    "    short_tp_0 = ffill_line(res_df['short_tp_0_{}'.format(config.selection_id)].to_numpy(), short_net_p1_idx)\n",
    "    short_out_1 = ffill_line(res_df['short_out_1_{}'.format(config.selection_id)].to_numpy(), short_net_p1_idx)  # 체결된, p2_idx ~ p2_idx' 사이에 대한 momentum 조사\n",
    "    short_out_0 = ffill_line(res_df['short_out_0_{}'.format(config.selection_id)].to_numpy(), short_net_p1_idx)\n",
    "    short_ep2_0 = ffill_line(res_df['short_ep2_0_{}'.format(config.selection_id)].to_numpy(), short_p2_idx)\n",
    "    # short_net_wave_1 = ffill_line(res_df['short_wave_1_{}'.format(config.selection_id)].to_numpy(), short_op_idx)  # en_idx 에 sync 된 open_idx 를 사용해야함\n",
    "    # short_net_wave_0 = ffill_line(res_df['short_wave_0_{}'.format(config.selection_id)].to_numpy(), short_op_idx)\n",
    "\n",
    "    long_net_p1_idx = long_net_p1_idx_arr.astype(int)  # .reshape(-1, 1)\n",
    "    long_p1_idx = long_obj[-1].astype(int).ravel()\n",
    "    long_p2_idx = long_p2_idx_arr.astype(int).ravel()  # .reshape(-1, 1)\n",
    "    long_en_idx = long_obj[2].astype(int).ravel()\n",
    "\n",
    "    long_tp_1 = ffill_line(res_df['long_tp_1_{}'.format(config.selection_id)].to_numpy(), long_net_p1_idx)\n",
    "    long_tp_0 = ffill_line(res_df['long_tp_0_{}'.format(config.selection_id)].to_numpy(), long_net_p1_idx)\n",
    "    long_out_1 = ffill_line(res_df['long_out_1_{}'.format(config.selection_id)].to_numpy(), long_net_p1_idx)  # 체결된, p2_idx ~ p2_idx' 사이에 대한 momentum 조사\n",
    "    long_out_0 = ffill_line(res_df['long_out_0_{}'.format(config.selection_id)].to_numpy(), long_net_p1_idx)\n",
    "    long_ep2_0 = ffill_line(res_df['long_ep2_0_{}'.format(config.selection_id)].to_numpy(), long_p2_idx)\n",
    "\n",
    "    short_p2exec_p1_idx = np.unique(short_p1_idx)  # .reshape(-1, 1)   # 통일성을 위해 2d 로 설정\n",
    "    long_p2exec_p1_idx = np.unique(long_p1_idx)  # .reshape(-1, 1)\n",
    "\n",
    "    # print(\"long_net_p1_idx.shape :\", long_net_p1_idx.shape)\n",
    "    # print(\"long_en_idx.shape :\", long_en_idx.shape)\n",
    "\n",
    "    # ================== touch idx ================== #\n",
    "    # 1. min 에 초점을 맞추는 거니까, touch 없을시 len_df 로 설정\n",
    "    # 2. future_data 사용이니까, shift(-bias_info_tick) 설정 --> olds\n",
    "    # 3. entry 다음 idx 부터 -> tp & out 체결 logic 이 현재 entry_idx 부터 되어있어서 취소\n",
    "    # Todo, high 와 low 중 어디에 먼저닿느냐가 중요함을 key 로 잡고만든 logic 임\n",
    "    len_df_range = np.arange(len_df)\n",
    "    last_idx = len_df - 1  # nan 발생하면 대소 비교로 hhm 확인이 불가능해짐, np.nan <= np.nan --> false\n",
    "\n",
    "    # ------------ pair & idxs ------------ #\n",
    "    short_en_pair = list(zip(short_en_idx, np.append(short_en_idx[1:], last_idx)))  # p1's 1st & 2nd pair 위해서 last_idx 마지막에 붙여준 것\n",
    "    long_en_pair = list(zip(long_en_idx, np.append(long_en_idx[1:], last_idx)))\n",
    "\n",
    "    short_p2_pair = list(zip(short_p2_idx, np.append(short_p2_idx[1:], last_idx)))\n",
    "    long_p2_pair = list(zip(long_p2_idx, np.append(long_p2_idx[1:], last_idx)))\n",
    "\n",
    "    short_tp_1_touch_idxs = np.where(low <= short_tp_1, len_df_range, last_idx)\n",
    "    short_tp_0_touch_idxs = np.where(high >= short_tp_0, len_df_range, last_idx)\n",
    "    long_tp_1_touch_idxs = np.where(high >= long_tp_1, len_df_range, last_idx)\n",
    "    long_tp_0_touch_idxs = np.where(low <= long_tp_0, len_df_range, last_idx)\n",
    "\n",
    "    short_out_1_touch_idxs = np.where(low <= short_out_1, len_df_range, last_idx)\n",
    "    short_out_0_touch_idxs = np.where(high >= short_out_0, len_df_range, last_idx)\n",
    "    long_out_1_touch_idxs = np.where(high >= long_out_1, len_df_range, last_idx)\n",
    "    long_out_0_touch_idxs = np.where(low <= long_out_0, len_df_range, last_idx)\n",
    "\n",
    "    # ------------ min touch_idx ------------ #\n",
    "    # print(short_en_pair)\n",
    "    short_tp_1_touch_idx = get_touch_idx_fill_v2(short_tp_1_touch_idxs, short_en_pair, short_en_idx, len_df)  # pair means 구간\n",
    "    short_tp_0_touch_idx = get_touch_idx_fill_v2(short_tp_0_touch_idxs, short_en_pair, short_en_idx, len_df)\n",
    "    long_tp_1_touch_idx = get_touch_idx_fill_v2(long_tp_1_touch_idxs, long_en_pair, long_en_idx, len_df)\n",
    "    long_tp_0_touch_idx = get_touch_idx_fill_v2(long_tp_0_touch_idxs, long_en_pair, long_en_idx, len_df)\n",
    "\n",
    "    short_out_1_touch_idx = get_touch_idx_fill_v2(short_out_1_touch_idxs, short_en_pair, short_en_idx, len_df)  # pair means 구간\n",
    "    short_out_0_touch_idx = get_touch_idx_fill_v2(short_out_0_touch_idxs, short_en_pair, short_en_idx, len_df)\n",
    "    long_out_1_touch_idx = get_touch_idx_fill_v2(long_out_1_touch_idxs, long_en_pair, long_en_idx, len_df)\n",
    "    long_out_0_touch_idx = get_touch_idx_fill_v2(long_out_0_touch_idxs, long_en_pair, long_en_idx, len_df)\n",
    "\n",
    "    # ------------ point's touch_idx ------------ #\n",
    "    short_tp_1_en_touch_idx = short_tp_1_touch_idx[short_en_idx]  # for tp_box's net_hhm\n",
    "    short_tp_0_en_touch_idx = short_tp_0_touch_idx[short_en_idx]\n",
    "    long_tp_1_en_touch_idx = long_tp_1_touch_idx[long_en_idx]\n",
    "    long_tp_0_en_touch_idx = long_tp_0_touch_idx[long_en_idx]\n",
    "    # print(\"long_tp_1_en_touch_idx :\", long_tp_1_en_touch_idx)\n",
    "\n",
    "    short_tp_1_p2exec_p1_touch_idx = short_tp_1_touch_idx[short_p2exec_p1_idx]  # p2 까지 체결된 p1's hhm (p2 executed p1_hhm)\n",
    "    short_tp_0_p2exec_p1_touch_idx = short_tp_0_touch_idx[short_p2exec_p1_idx]\n",
    "    long_tp_1_p2exec_p1_touch_idx = long_tp_1_touch_idx[long_p2exec_p1_idx]\n",
    "    long_tp_0_p2exec_p1_touch_idx = long_tp_0_touch_idx[long_p2exec_p1_idx]\n",
    "\n",
    "    short_tp_1_p2_touch_idx = short_tp_1_touch_idx[short_en_idx]  # hlm 을 위한 hhm (on p2)\n",
    "    short_tp_0_p2_touch_idx = short_tp_0_touch_idx[short_en_idx]\n",
    "    long_tp_1_p2_touch_idx = long_tp_1_touch_idx[long_en_idx]\n",
    "    long_tp_0_p2_touch_idx = long_tp_0_touch_idx[long_en_idx]\n",
    "\n",
    "    short_out_1_p2_touch_idx = short_out_1_touch_idx[short_en_idx]  # for out_box's executed_hhm\n",
    "    short_out_0_p2_touch_idx = short_out_0_touch_idx[short_en_idx]\n",
    "    long_out_1_p2_touch_idx = long_out_1_touch_idx[long_en_idx]\n",
    "    long_out_0_p2_touch_idx = long_out_0_touch_idx[long_en_idx]\n",
    "\n",
    "    # ------------ get wave's bias_tick ------------ #\n",
    "    short_tp_1_en_touch_idx2 = np.where(short_tp_1_en_touch_idx == last_idx, np.nan, short_tp_1_en_touch_idx)\n",
    "    long_tp_1_en_touch_idx2 = np.where(long_tp_1_en_touch_idx == last_idx, np.nan, long_tp_1_en_touch_idx)\n",
    "\n",
    "    short_tp_1_p2exec_p1_touch_idx2 = np.where(short_tp_1_p2exec_p1_touch_idx == last_idx, np.nan, short_tp_1_p2exec_p1_touch_idx)\n",
    "    long_tp_1_p2exec_p1_touch_idx2 = np.where(long_tp_1_p2exec_p1_touch_idx == last_idx, np.nan, long_tp_1_p2exec_p1_touch_idx)\n",
    "\n",
    "    # short_net_p1_bias_tick = short_tp_1_en_touch_idx2 - short_net_p1_idx\n",
    "    # long_net_p1_bias_tick = long_tp_1_en_touch_idx2 - long_net_p1_idx\n",
    "    short_en_bias_tick = short_tp_1_en_touch_idx2 - short_en_idx\n",
    "    long_en_bias_tick = long_tp_1_en_touch_idx2 - long_en_idx\n",
    "\n",
    "    short_p2exec_p1_bias_tick = short_tp_1_p2exec_p1_touch_idx2 - short_p2exec_p1_idx\n",
    "    long_p2exec_p1_bias_tick = long_tp_1_p2exec_p1_touch_idx2 - long_p2exec_p1_idx\n",
    "\n",
    "    # ------------------ bias_bool & hhm ------------------ #\n",
    "    short_en_true_bias_bool = short_tp_1_en_touch_idx < short_tp_0_en_touch_idx  # true_bias 의 조건\n",
    "    short_en_false_bias_bool = short_tp_1_en_touch_idx >= short_tp_0_en_touch_idx  # false_bias 의 조건, ~true_bias_bool 와 같지 않음, why ..? = en_idx\n",
    "    long_en_true_bias_bool = long_tp_1_en_touch_idx < long_tp_0_en_touch_idx\n",
    "    long_en_false_bias_bool = long_tp_1_en_touch_idx >= long_tp_0_en_touch_idx\n",
    "\n",
    "    short_p2exec_p1_true_bias_bool = short_tp_1_p2exec_p1_touch_idx < short_tp_0_p2exec_p1_touch_idx  # true_bias 의 조건\n",
    "    short_p2exec_p1_false_bias_bool = short_tp_1_p2exec_p1_touch_idx >= short_tp_0_p2exec_p1_touch_idx  # false_bias 의 조건, ~true_bias_bool 와 같지 않음, why ..? = en_idx\n",
    "    long_p2exec_p1_true_bias_bool = long_tp_1_p2exec_p1_touch_idx < long_tp_0_p2exec_p1_touch_idx\n",
    "    long_p2exec_p1_false_bias_bool = long_tp_1_p2exec_p1_touch_idx >= long_tp_0_p2exec_p1_touch_idx\n",
    "\n",
    "    short_p2_true_bias_bool = short_tp_1_p2_touch_idx < short_tp_0_p2_touch_idx\n",
    "    # short_p2_false_bias_bool = short_tp_1_p2_touch_idx >= short_tp_0_p2_touch_idx\n",
    "    long_p2_true_bias_bool = long_tp_1_p2_touch_idx < long_tp_0_p2_touch_idx\n",
    "    # long_p2_false_bias_bool = long_tp_1_p2_touch_idx >= long_tp_0_p2_touch_idx\n",
    "\n",
    "    short_p2_out_true_bias_bool = short_out_1_p2_touch_idx < short_out_0_p2_touch_idx\n",
    "    short_p2_out_false_bias_bool = short_out_1_p2_touch_idx >= short_out_0_p2_touch_idx\n",
    "    long_p2_out_true_bias_bool = long_out_1_p2_touch_idx < long_out_0_p2_touch_idx\n",
    "    long_p2_out_false_bias_bool = long_out_1_p2_touch_idx >= long_out_0_p2_touch_idx\n",
    "\n",
    "    short_tpbox_hhm = hhm(short_en_true_bias_bool, short_en_false_bias_bool)\n",
    "    long_tpbox_hhm = hhm(long_en_true_bias_bool, long_en_false_bias_bool)\n",
    "\n",
    "    short_p2exec_tpbox_hhm = hhm(short_p2exec_p1_true_bias_bool, short_p2exec_p1_false_bias_bool)\n",
    "    long_p2exec_tpbox_hhm = hhm(long_p2exec_p1_true_bias_bool, long_p2exec_p1_false_bias_bool)\n",
    "\n",
    "    # short_p2_hhm = hhm(short_p2_true_bias_bool, short_p2_false_bias_bool)\n",
    "    # long_p2_hhm = hhm(long_p2_true_bias_bool, long_p2_false_bias_bool)\n",
    "\n",
    "    short_outbox_hhm = hhm(short_p2_out_true_bias_bool, short_p2_out_false_bias_bool)\n",
    "    long_outbox_hhm = hhm(long_p2_out_true_bias_bool, long_p2_out_false_bias_bool)\n",
    "\n",
    "    # print(\"short_tpbox_hhm, short_p2_hhm, short_outbox_hhm :\", short_tpbox_hhm, short_p2_hhm, short_outbox_hhm)\n",
    "    short_en_idx_2d = short_en_idx.reshape(-1, 1)\n",
    "    long_en_idx_2d = long_en_idx.reshape(-1, 1)\n",
    "\n",
    "    return short_tpbox_hhm, long_tpbox_hhm, short_p2exec_tpbox_hhm, long_p2exec_tpbox_hhm, short_outbox_hhm, long_outbox_hhm, \\\n",
    "           short_en_bias_tick, long_en_bias_tick, short_p2exec_p1_bias_tick, long_p2exec_p1_bias_tick, short_p2_true_bias_bool, long_p2_true_bias_bool, \\\n",
    "           short_tp_1[short_en_idx_2d], short_tp_0[short_en_idx_2d], long_tp_1[long_en_idx_2d], long_tp_0[long_en_idx_2d], \\\n",
    "           short_out_1[short_en_idx_2d], short_out_0[short_en_idx_2d], long_out_1[long_en_idx_2d], long_out_0[long_en_idx_2d], \\\n",
    "           short_ep2_0[short_en_idx_2d], long_ep2_0[long_en_idx_2d]  # plot_check 을 위해 en_idx 넣음\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### get_touch_idx_fill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_touch_idx_fill_v2(tp_1_touch_idxs, net_p1_pair, net_p1_idx, len_df):\n",
    "\n",
    "    \"\"\"\n",
    "    v2\n",
    "        steadly similar error occurs\n",
    "            zero-size array to reduction operation fmin which has no identity\n",
    "                from np.nanmin(tp_1_touch_idxs[iin + 1:iout]).\n",
    "                    \"iin + 1\" is considering, tp touch can be done after entry bar. (entry_idx)\n",
    "      \n",
    "    Todo : get_valid_p1_idx : p1_pair_idx...?\n",
    "        I don't know what you are talking about...\n",
    "    \n",
    "    last confirmed at, 20240322 1240\n",
    "    \"\"\"\n",
    "    \n",
    "    tp_1_touch_idx = np.full(len_df, np.nan)\n",
    "    net_p1_idx_np = np.array(net_p1_idx)\n",
    "    net_p1_pair_np = np.array(net_p1_pair) \n",
    "\n",
    "    # valid_idx = [i_ for i_, (iin, iout) in enumerate(net_p1_pair) if iin != iout if iin != len_df - 1 if iin + 1 != iout]\n",
    "    valid_idx = [i_ for i_, (iin, iout) in enumerate(net_p1_pair) if iin + 1 < iout if iin != len_df - 1]\n",
    "   \n",
    "    # tp_1_touch_idx[net_p1_idx] = [np.nanmin(tp_1_touch_idxs[iin:iout]) for iin, iout in net_p1_pair]\n",
    "    tp_1_touch_idx[net_p1_idx_np[valid_idx]] = [np.nanmin(tp_1_touch_idxs[iin + 1:iout]) for iin, iout in net_p1_pair_np[valid_idx]]\n",
    "\n",
    "    return fill_arr(tp_1_touch_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### get_pr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_pr_v7(open_side, h, l, obj, tpout, lvrg, fee, partial_ranges, partial_qty_ratio, non_tp=False, inversion=False):  # --> 여기서 사용하는 ex_p = ex_p\n",
    "\n",
    "    \"\"\"\n",
    "    v6 -> v7\n",
    "        1. partial 을 위한 수정. (np.tile(min_low, (1, len_p)) <= p_tps) # * (np.tile(max_high, (1, len_p)) <= outs)\n",
    "        2. liqd 구하는 방식도 * (1 - fee) 계산 방식 도입.\n",
    "    \"\"\"\n",
    "\n",
    "    en_p = obj[0]\n",
    "    ex_p = obj[1]\n",
    "\n",
    "    tp, out = np.split(tpout, 2, axis=1)\n",
    "    len_p = len(partial_ranges)\n",
    "    en_ps, ex_ps, tps, outs, lvrgs, fees = [np.tile(arr_, (1, len_p)) for arr_ in [en_p, ex_p, tp, out, lvrg, fee]]\n",
    "    \n",
    "    # print(partial_ranges)\n",
    "    # print(\"ex_ps (up): \", ex_ps)\n",
    "\n",
    "    np_obj = np.array(obj).T[0]\n",
    "    assert len(np_obj.shape) == 2\n",
    "\n",
    "    # iin == iout 인 경우 분리\n",
    "    en_idx = np_obj[:, 2]\n",
    "    ex_idx = np_obj[:, 3]\n",
    "    equal_idx = en_idx == ex_idx  # equal_idx 는 어차피 out 임\n",
    "\n",
    "    # 0. tp part 에 대해서는 ex_ps 를 다르게 특정한다.\n",
    "    if not non_tp:\n",
    "        min_low = np.full_like(en_p, np.nan)\n",
    "        min_low[~equal_idx] = np.array(\n",
    "            [np.min(l[int(iin + 1):int(iout + 1)]) for _, _, iin, iout in np_obj[~equal_idx, :4]]).reshape(-1, 1)  # start from iin + 1 (tp 체결을 entry_idx 부터 보지 않음)\n",
    "        max_high = np.full_like(en_p, np.nan)\n",
    "        max_high[~equal_idx] = np.array(\n",
    "            [np.max(h[int(iin + 1):int(iout + 1)]) for _, _, iin, iout in np_obj[~equal_idx, :4]]).reshape(-1, 1)\n",
    "\n",
    "        if open_side == \"SELL\":\n",
    "            p_tps = en_ps - (en_ps - tps) * partial_ranges\n",
    "            # min_low = np.full_like(en_p, np.nan)\n",
    "            # min_low[~equal_idx] = np.array([np.min(l[int(iin + 1):int(iout + 1)]) for _, _, iin, iout in np_obj[~equal_idx, :4]]).reshape(-1, 1)  # start from iin + 1 (tp 체결을 entry_idx 부터 보지 않음)\n",
    "\n",
    "            # 보수적 검증은 이곳에서 하는 것이 아니다.\n",
    "            tp_idx = (np.tile(min_low, (1, len_p)) <= p_tps) # * (np.tile(max_high, (1, len_p)) <= outs)\n",
    "        else:\n",
    "            p_tps = en_ps + (tps - en_ps) * partial_ranges\n",
    "            # max_high = np.full_like(en_p, np.nan)\n",
    "            # max_high[~equal_idx] = np.array([np.max(h[int(iin + 1):int(iout + 1)]) for _, _, iin, iout in np_obj[~equal_idx, :4]]).reshape(-1, 1)\n",
    "\n",
    "            # out_line touch 이력이 없고 partial_tp_line touch 이력이 있는 경우 => tp 체결 완료.\n",
    "            tp_idx = (np.tile(max_high, (1, len_p)) >= p_tps) # * (np.tile(min_low, (1, len_p)) >= outs)\n",
    "\n",
    "        # 1. 위 구간에서 tps 설정으로 인해, signal_out 에 대한 고려가 진행되지 않고 있음.\n",
    "        ex_ps = outs.copy()\n",
    "        ex_ps[tp_idx] = p_tps[tp_idx]\n",
    "        \n",
    "        \n",
    "        # print(\"ex_ps (2): \", ex_ps)\n",
    "        # return\n",
    "\n",
    "    # 2. get pr, liquidation\n",
    "    if open_side == \"SELL\":\n",
    "        if not inversion:\n",
    "            pr = ((en_ps / ex_ps * (1 - fees) - 1) * lvrgs * partial_qty_ratio).sum(axis=1) + 1\n",
    "\n",
    "            max_high = np.full_like(en_p, np.nan)\n",
    "            max_high[~equal_idx] = np.array([np.max(h[int(iin):int(iout)]) for _, _, iin, iout in np_obj[~equal_idx, :4]]).reshape(-1, 1)  # start from iin (liquidation 을 entry_idx 봄)\n",
    "            liqd = (en_p / max_high * (1 - fees) - 1) * lvrg + 1\n",
    "        else:\n",
    "            pr = ((ex_ps / en_ps * (1 - fees) - 1) * lvrgs * partial_qty_ratio).sum(axis=1) + 1\n",
    "\n",
    "            min_low = np.full_like(en_p, np.nan)\n",
    "            min_low[~equal_idx] = np.array([np.min(l[int(iin):int(iout)]) for _, _, iin, iout in np_obj[~equal_idx, :4]]).reshape(-1, 1)\n",
    "            liqd = (min_low / en_p * (1 - fees) - 1) * lvrg + 1\n",
    "    else:\n",
    "        if not inversion:\n",
    "            pr = ((ex_ps / en_ps * (1 - fees) - 1) * lvrgs * partial_qty_ratio).sum(axis=1) + 1\n",
    "\n",
    "            min_low = np.full_like(en_p, np.nan)\n",
    "            min_low[~equal_idx] = np.array([np.min(l[int(iin):int(iout)]) for _, _, iin, iout in np_obj[~equal_idx, :4]]).reshape(-1, 1)\n",
    "            liqd = (min_low / en_p * (1 - fees) - 1) * lvrg + 1\n",
    "        else:\n",
    "            pr = ((en_ps / ex_ps * (1 - fees) - 1) * lvrgs * partial_qty_ratio).sum(axis=1) + 1\n",
    "\n",
    "            max_high = np.full_like(en_p, np.nan)\n",
    "            max_high[~equal_idx] = np.array([np.max(h[int(iin):int(iout)]) for _, _, iin, iout in np_obj[~equal_idx, :4]]).reshape(-1, 1)\n",
    "            liqd = (en_p / max_high * (1 - fees) - 1) * lvrg + 1\n",
    "\n",
    "    pr_2d = pr.reshape(-1, 1)\n",
    "    # pr_2d[liqd <= 0] = 0  # liquidation platform 에서는 억지로 0 을 대입시킬 필요가 없어짐.\n",
    "    pr_2d[pr_2d < 0] = 0  # pr 음수 오차 수정.\n",
    "\n",
    "    return pr_2d, np.nanmin(liqd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### plot_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8hdpN7S8JJF-"
   },
   "outputs": [],
   "source": [
    "def plot_info_v8_2(gs, gs_idx, len_df, sample_len, tr, hhm, p2_hhm, out_hhm, hlm, bars_in, net_p1_frq, pr, total_pr,\n",
    "                 acc_pr, sum_pr, liqd, leverage, title_position, fontsize):\n",
    "    \n",
    "    \"\"\"\n",
    "    v8 -> v8_2\n",
    "        1. peridic sum_pr plot 을 위해 get_res_info_nb_v3, title_msg 수정함.\n",
    "        2. sum_mdd -> prod & sum mdd 둘다 plot 함.\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        plt.subplot(gs[gs_idx])\n",
    "        idep_res_obj = get_res_info_nb_v3(sample_len, pr, acc_pr, sum_pr, liqd)\n",
    "        plt.plot(acc_pr)\n",
    "        plt.plot(sum_pr, color='gold')\n",
    "        if sample_len is not None:\n",
    "            plt.axvline(sample_len, alpha=1., linestyle='--', color='#ffeb3b')\n",
    "        plt.xlim(0, len_df)\n",
    "\n",
    "        title_str = \"tr : {:.3f}\\n tpbox_hhm : {:.3f}\\n tpbox_p2exec_hhm : {:.3f}\\n outbox_hhm : {:.3f}\\n hlm : {:.3f}\\n bars_in : {:.3f}\\n net_p1_frq : {}\\n frq : {}\\n dpf : {:.3f}\\n wr : {:.3f}\\n sr : {:.3f}\\n acc_pr : {:.3f}\\n sum_pr : {:.3f}\\n\" + \\\n",
    "                    \"min_pr : {:.3f}\\n liqd : {:.3f}\\n acc_mdd : -{:.3f}\\n sum_mdd : -{:.3f} ({:.3f})\\n leverage {:.3f}\"\n",
    "        plt.title(title_str.format(tr, hhm, p2_hhm, out_hhm, hlm, bars_in, net_p1_frq, *idep_res_obj, leverage), position=title_position, fontsize=fontsize)\n",
    "    except Exception as e:\n",
    "        print(\"error in plot_info :\", e)\n",
    "\n",
    "    return gs_idx + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### sum_pr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mdd_v2(pr, mode=\"PROD\"):\n",
    "    \n",
    "    rollmax_pr = np.maximum.accumulate(pr)\n",
    "    \n",
    "    if mode == \"PROD\":\n",
    "        return np.max((rollmax_pr - pr) / rollmax_pr)\n",
    "    else:\n",
    "        return np.max(rollmax_pr - pr)\n",
    "\n",
    "\n",
    "# @jit  # almost equal\n",
    "def get_res_info_nb_v3(len_df, np_pr, acc_pr, sum_pr, liqd):\n",
    "\n",
    "    \"\"\"\n",
    "    v2 -> v3\n",
    "        1. get_sum_pr_nb 에 total_pr 을 사용함\n",
    "            a. total_pr 은 np_pr 의 거래 체결과 체결 사이에 non_trade_pr (= 1) 을 추가한 것\n",
    "    \"\"\"\n",
    "    wr = len(np_pr[np_pr > 1]) / len(np_pr[np_pr != 1])\n",
    "    sr = sharpe_ratio(np_pr)\n",
    "    min_pr = np.min(np_pr)\n",
    "\n",
    "    len_pr = len(np_pr)\n",
    "    assert len_pr != 0\n",
    "    dpf = (len_df / 1440) / len_pr  # devision zero warning\n",
    "\n",
    "    acc_mdd = mdd_v2(acc_pr)\n",
    "    sum_mdd_prod = mdd_v2(sum_pr)\n",
    "    sum_mdd_sum = mdd_v2(sum_pr, mode=\"SUM\")\n",
    "\n",
    "    return len_pr, dpf, wr, sr, acc_pr[-1], sum_pr[-1] - 1, min_pr, liqd, acc_mdd, sum_mdd_prod, sum_mdd_sum\n",
    "\n",
    "# @jit\n",
    "def get_sum_pr_nb(np_pr):\n",
    "    \n",
    "    for_sum_pr = np_pr - 1\n",
    "    for_sum_pr[0] = 1  # 1 로 시작하지 않을 경우, sum_mdd = nan 출력됨.\n",
    "    \n",
    "    # print(\"np_pr :\", np_pr[np_pr != 1])\n",
    "    # print(\"for_sum_pr :\", for_sum_pr[np_pr != 1])\n",
    "    \n",
    "    sum_pr = np.cumsum(for_sum_pr)\n",
    "    # sum_pr = np.where(sum_pr < 0, 0, sum_pr)  # sum_mdd 의 정확한 측정을 위해 주석처리함.\n",
    "\n",
    "    return sum_pr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### frq_dev_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def frq_dev_plot_v5(gs, gs_idx, len_df, sample_len, exit_idx, bias_arr, acc_pr, sum_pr, fontsize, mode='CRYPTO'):\n",
    "    \n",
    "    \"\"\"\n",
    "    v4 -> v5\n",
    "        1. add periodic sum_pr\n",
    "        2. remove return value\n",
    "        3. use get_period_pr_v3 for Stock.\n",
    "    \"\"\"\n",
    "\n",
    "    plt.subplot(gs[gs_idx])\n",
    "\n",
    "    plt.vlines(exit_idx[bias_arr], ymin=0, ymax=1, color='#00ff00')\n",
    "    # plt.vlines(exit_idx[~bias_arr], ymin=0, ymax=1, color='#ff00ff')\n",
    "    plt.xlim(0, len_df)\n",
    "\n",
    "    title_msg = \"periodic_pr (acc | sum)\\n day : {:.2f} | {:.2f}\\n month : {:.2f} | {:.2f}\\n year : {:.2f} | {:.2f}\"  # \\n rev_acc_day : {:.4f}\\n month : {:.4f}\\n year : {:.4f}\"\n",
    "    array_zip = np.array(list(zip(get_period_pr_v3(sample_len, acc_pr, mode=mode), get_period_pr_v3(sample_len, sum_pr, pr_type=\"SUM\", mode=mode)))).ravel()\n",
    "\n",
    "    plt.title(title_msg.format(*array_zip, fontsize=fontsize))\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_period_pr_v3(len_df, pr_, pr_type=\"PROD\", mode='STOCK'):\n",
    "\n",
    "    if mode == 'STOCK':\n",
    "        days = len_df / 360\n",
    "    else:\n",
    "        days = len_df / 1440\n",
    "    months = days / 30\n",
    "    years = days / 365\n",
    "\n",
    "    if pr_type == \"PROD\":\n",
    "        return [pr_ ** (1 / period) for period in [days, months, years]]\n",
    "    else:\n",
    "        return [(pr_ - 1) / period for period in [days, months, years]]  # sum_pr 은 초기 자산을 제외한 증분에 대해서만 진행함 (sum 특성상.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zQ-roiifspcX",
    "tags": []
   },
   "source": [
    "#### Archive : ep_loc.point & zone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Js5eL87VspcX"
   },
   "outputs": [],
   "source": [
    "\n",
    "    # res_df['entry_{}'.format(selection_id)] = np.where((res_df['open'] >= res_df['cloud_bline_%s' % cb_itv]) &\n",
    "    #                 # (res_df['close'].shift(config.loc_set.point.tf_entry * 1) <= res_df['cloud_bline_%s' % cb_itv]) &\n",
    "    #                 (res_df['close'] < res_df['cloud_bline_%s' % cb_itv])\n",
    "    #                 , res_df['entry_{}'.format(selection_id)] - 1, res_df['entry_{}'.format(selection_id)])\n",
    "\n",
    "    # res_df['entry_{}'.format(selection_id)] = np.where((res_df['open'] >= res_df['bb_lower_1m']) &\n",
    "    #                 # (res_df['close'].shift(config.loc_set.point.tf_entry * 1) <= res_df['bb_lower_1m']) &\n",
    "    #                 (res_df['close'] < res_df['bb_lower_1m'])\n",
    "    #                 , res_df['entry_{}'.format(selection_id)] - 1, res_df['entry_{}'.format(selection_id)])\n",
    "\n",
    "    # res_df['entry_{}'.format(selection_id)] = np.where((res_df['open'] <= res_df['cloud_bline_%s' % cb_itv]) &\n",
    "    #                   # (res_df['close'].shift(config.loc_set.point.tf_entry * 1) >= res_df['cloud_bline_%s' % cb_itv]) &\n",
    "    #                   (res_df['close'] > res_df['cloud_bline_%s' % cb_itv])\n",
    "    #                 , res_df['entry_{}'.format(selection_id)] + 1, res_df['entry_{}'.format(selection_id)])\n",
    "\n",
    "    # res_df['entry_{}'.format(selection_id)] = np.where((res_df['open'] <= res_df['bb_upper_1m']) &\n",
    "    #                   # (res_df['close'].shift(config.loc_set.point.tf_entry * 1) >= res_df['bb_upper_1m']) &\n",
    "    #                   (res_df['close'] > res_df['bb_upper_1m'])\n",
    "    #                 , res_df['entry_{}'.format(selection_id)] + 1, res_df['entry_{}'.format(selection_id)])\n",
    "\n",
    "\n",
    "    \n",
    "   # --------------- ema --------------- #   \n",
    "  # res_df['ema5_1m'] = ema(res_df['close'], 5).shift(1)\n",
    "\n",
    "  #   # --------------- cloud bline --------------- #   \n",
    "  # res_df['cloud_bline_1m'] = cloud_bline(res_df, 26).shift(1)\n",
    "  \n",
    "    #       stochastic      #\n",
    "  # res_df['stoch'] = stoch(res_df, 5, 3, 3)\n",
    "\n",
    "    #       fisher      #\n",
    "  # res_df['fisher30'] = fisher(res_df, 30)\n",
    "  # res_df['fisher60'] = fisher(res_df, 60)\n",
    "  # res_df['fisher120'] = fisher(res_df, 120)\n",
    "\n",
    "    #       cctbbo      #\n",
    "  # res_df['cctbbo'], _ = cct_bbo(res_df, 21, 13)\n",
    "\n",
    "    #       ema_roc      #\n",
    "  # res_df['ema_roc'] = ema_roc(res_df['close'], 13, 9)\n",
    "\n",
    "\n",
    "   # ------------------------------ htf data ------------------------------ #    \n",
    "\n",
    "  #             Todo              #\n",
    "  # htf_df = pd.read_excel(date_path2 + key.replace(\"_st1h_backi2\", \"\"), index_col=0)\n",
    "  # htf_df = pd.read_excel(date_path3 + key.replace(\"_st1h_backi2\", \"\"), index_col=0)\n",
    "  # # htf_df = pd.read_excel(date_path4 + key.replace(\"_st1h_backi2\", \"\"), index_col=0)\n",
    "  # # # htf_df = pd.read_excel(date_path5 + key.replace(\"_st1h_backi2\", \"\"), index_col=0)\n",
    "  # # # # # # htf_df = pd.read_excel(date_path6 + key.replace(\"_st1h_backi2\", \"\"), index_col=0)\n",
    "\n",
    "  # # ---- htf index slicing ---- #\n",
    "  # htf_df = htf_df.loc[:res_df.index[-1]]\n",
    "  \n",
    "  # print(\"res_df.index[-1] :\", res_df.index[-1])\n",
    "  # print(\"htf_df.index[-1] :\", htf_df.index[-1])\n",
    "\n",
    "  # res_df = dc_line(res_df, htf_df, '5m')\n",
    "  # res_df = dc_level(res_df, '5m', 1)\n",
    "\n",
    "\n",
    "  # # # if \"sma4\" in res_df.columns:\n",
    "  # # #   res_df.drop(\"sma4\", axis=1, inplace=1)\n",
    "\n",
    "  # # htf_df['sma'] = htf_df['close'].rolling(60).mean()\n",
    "  # # res_df = res_df.join(pd.DataFrame(index=res_df.index, data=to_lower_tf_v2(res_df, htf_df, [-1]), columns=['sma_30m']))\n",
    "  \n",
    "  # htf_df['stoch'] = stoch(htf_df, 13, 3, 3)\n",
    "  # res_df = res_df.join(pd.DataFrame(index=res_df.index, data=to_lower_tf_v2(res_df, htf_df, [-1], backing_i=-1), columns=['stoch_5m']))\n",
    "\n",
    "   \n",
    "  # fifth_df['ema'] = ema(fifth_df['close'], 5)\n",
    "  # res_df = res_df.join(pd.DataFrame(index=res_df.index, data=to_lower_tf_v2(res_df, fifth_df, [-1]), columns=['ema5']))\n",
    "\n",
    "        # ------------------------------------ short ------------------------------------ # \n",
    "\n",
    "        # --------- by sar --------- # \n",
    "        # mr_const_cnt += 1\n",
    "        # if res_df['sar_uptrend_3m'].iloc[i] == 0:\n",
    "        #   mr_score += 1\n",
    "\n",
    "        # mr_const_cnt += 1\n",
    "        # if res_df['sar_uptrend_5m'].iloc[i] == 0:\n",
    "        #   mr_score += 1          \n",
    "\n",
    "        # mr_const_cnt += 1\n",
    "        # if res_df['sar_uptrend_15m'].iloc[i] == 0:\n",
    "        #   mr_score += 1\n",
    "\n",
    "          #      dc & sar      # \n",
    "        # mr_const_cnt += 1\n",
    "        # # if res_df['dc_upper_1m'].iloc[i] <= res_df['sar_15m'].iloc[i]:\n",
    "        # if res_df['dc_upper_3m'].iloc[i] <= res_df['sar_5m'].iloc[i]:\n",
    "        # # if res_df['dc_upper_5m'].iloc[i] <= res_df['sar_15m'].iloc[i]:\n",
    "        #   mr_score += 1\n",
    "\n",
    "        # -------------- dr scheduling -------------- #\n",
    "        # if config.ep_set.entry_type == 'MARKET':\n",
    "        #   mr_const_cnt += 1\n",
    "        #   if (res_df['close'].iloc[i] - res_df['short_tp'].iloc[i]) / (res_df['short_out'].iloc[i] - res_df['close'].iloc[i]) <= config.ep_set.tr_thresh * (1 + config.ep_set.dr_error):  \n",
    "        #     mr_score += 1\n",
    "\n",
    "           \n",
    "        # ------- entry once ------- #   \n",
    "        # prev_entry_cnt = 0\n",
    "        # for back_i in range(i - 1, 0, -1):\n",
    "        #   if res_df['entry'][back_i] == 1:\n",
    "        #     break\n",
    "\n",
    "        #   elif res_df['entry'][back_i] == -1:\n",
    "        #     prev_entry_cnt += 1          \n",
    "        # # # print(\"prev_entry_cnt :\", prev_entry_cnt)\n",
    "\n",
    "        # mr_const_cnt += 1\n",
    "        # # if prev_entry_cnt <= config.ep_set.entry_incycle:\n",
    "        # # if prev_entry_cnt == config.ep_set.entry_incycle:\n",
    "        # if prev_entry_cnt >= config.ep_set.entry_incycle:\n",
    "        #   mr_score += 1\n",
    "\n",
    "        # ------- htf zoning ------- #   \n",
    "        # mr_const_cnt += 1\n",
    "        #   #       bb zone     #\n",
    "        # if res_df['close'].iloc[i] < res_df['bb_lower_%s' % bbz_interval].iloc[i]:\n",
    "        # # if res_df['close'].iloc[i] < res_df['bb_lower2_1h'].iloc[i]:\n",
    "        # # if res_df['close'].iloc[i] < res_df['bb_base_1h'].iloc[i]:\n",
    "\n",
    "        #   #       cbline zone     #\n",
    "        # # if res_df['close'].iloc[i] < res_df['cloud_bline_%s' % cb_interval].iloc[i]:\n",
    "\n",
    "        #   mr_score += 1\n",
    "\n",
    "  \n",
    "        # ------- ben ep_in's tp done ------- #   \n",
    "        # mr_const_cnt += 1\n",
    "        # if res_df['low'].iloc[i] > res_df['short_tp'].iloc[i]:\n",
    "        #   mr_score += 1\n",
    "\n",
    "\n",
    "        # -------------- feature dist const. -------------- #\n",
    "        # if initial_i < input_size:\n",
    "        #   i += 1\n",
    "        #   if i >= len(res_df):\n",
    "        #     break\n",
    "        #   continue\n",
    "\n",
    "        # entry_input_x = min_max_scale(res_df[selected_price_colname].iloc[initial_i - input_size:initial_i].values)\n",
    "       \n",
    "        # re_entry_input_x = expand_dims(entry_input_x)\n",
    "\n",
    "        # entry_vector = model.predict(re_entry_input_x, verbose=0)\n",
    "        # # print(test_result.shape)\n",
    "\n",
    "        # f_dist = vector_dist(entry_vector, selected_vector)\n",
    "        # print(\"f_dist :\", f_dist)\n",
    "\n",
    "        # if f_dist < fdist_thresh:\n",
    "          # mr_score += 1\n",
    "\n",
    "\n",
    "\n",
    "        # ------------------------------------ long ------------------------------------ # \n",
    "          \n",
    "\n",
    "        # --------- by sar --------- # \n",
    "        # mr_const_cnt += 1\n",
    "        # if res_df['sar_uptrend_3m'].iloc[i] == 1:\n",
    "        #   mr_score += 1   \n",
    "\n",
    "        # mr_const_cnt += 1\n",
    "        # if res_df['sar_uptrend_5m'].iloc[i] == 1:\n",
    "        #   mr_score += 1     \n",
    "\n",
    "        # mr_const_cnt += 1\n",
    "        # if res_df['sar_uptrend_15m'].iloc[i] == 1:\n",
    "          # mr_score += 1\n",
    "\n",
    "          #      dc & sar      # \n",
    "        # mr_const_cnt += 1\n",
    "        # # if res_df['dc_lower_1m'].iloc[i] >= res_df['sar_15m'].iloc[i]:\n",
    "        # if res_df['dc_lower_3m'].iloc[i] >= res_df['sar_5m'].iloc[i]:\n",
    "        # # if res_df['dc_lower_5m'].iloc[i] >= res_df['sar_15m'].iloc[i]:\n",
    "        #   mr_score += 1\n",
    "\n",
    "        # -------------- dr scheduling -------------- #\n",
    "        # if config.ep_set.entry_type == \"MARKET\":\n",
    "          # mr_const_cnt += 1        \n",
    "          # if (res_df['long_tp'].iloc[i] - res_df['close'].iloc[i]) / (res_df['close'].iloc[i] - res_df['long_out'].iloc[i]) <= config.ep_set.tr_thresh * (1 + config.ep_set.dr_error): # 일반적으로 dr 상에서 tp 비율이 더 커짐 (tr 보다)\n",
    "          #   mr_score += 1\n",
    "\n",
    "        # -------------- ep limit -------------- #    \n",
    "        # mr_const_cnt += 1\n",
    "        # # if (res_df['open'].iloc[i] - res_df['long_ep'].iloc[i]) / res_df['open'].iloc[i] < config.ep_set.max_eplim_pct:\n",
    "        # if config.ep_set.min_eplim_pct < (res_df['open'].iloc[i] - res_df['long_ep'].iloc[i]) / res_df['open'].iloc[i] < config.ep_set.max_eplim_pct:\n",
    "        # # if 0 < (res_df['open'].iloc[i] - res_df['long_ep'].iloc[i]) / res_df['open'].iloc[i] < config.ep_set.max_eplim_pct:\n",
    "        #   # if res_df['st_gap_15m'].iloc[i] / res_df['open'].iloc[i] < 0:\n",
    "        #   #   print(\"i, res_df['st_gap_15m'].iloc[i] :\", i, res_df['st_gap_15m'].iloc[i])\n",
    "        #   mr_score += 1\n",
    "\n",
    "\n",
    "        # -------------- entry once -------------- #    \n",
    "        # prev_entry_cnt = 0\n",
    "        # for back_i in range(i - 1, 0, -1):\n",
    "        #   if res_df['entry'][back_i] == -1:\n",
    "        #     break\n",
    "\n",
    "        #   elif res_df['entry'][back_i] == 1:\n",
    "        #     prev_entry_cnt += 1\n",
    "          \n",
    "        # mr_const_cnt += 1\n",
    "        # # if prev_entry_cnt <= config.ep_set.entry_incycle:\n",
    "        # # if prev_entry_cnt == config.ep_set.entry_incycle:\n",
    "        # if prev_entry_cnt >= config.ep_set.entry_incycle:\n",
    "        #   mr_score += 1\n",
    "\n",
    "\n",
    "        # ------- htf zoning ------- #   \n",
    "        # mr_const_cnt += 1\n",
    "          \n",
    "        #   #       bb zone     #\n",
    "        # if res_df['close'].iloc[i] > res_df['bb_upper_%s' % bbz_interval].iloc[i]:\n",
    "        # # if res_df['close'].iloc[i] > res_df['bb_upper2_1h'].iloc[i]:\n",
    "        # # if res_df['close'].iloc[i] > res_df['bb_base_1h'].iloc[i]:\n",
    "        \n",
    "        #   #       cbline zone     #\n",
    "        # # if res_df['close'].iloc[i] > res_df['cloud_bline_%s' % cb_interval].iloc[i]:\n",
    "\n",
    "        #   mr_score += 1\n",
    "\n",
    "\n",
    "        # ------- ben ep_in's tp done ------- #   \n",
    "        # mr_const_cnt += 1\n",
    "        # if res_df['high'].iloc[i] < res_df['long_tp'].iloc[i]:\n",
    "        #   mr_score += 1\n",
    "\n",
    "\n",
    "        # -------------- feature dist const. -------------- #\n",
    "        # if initial_i < input_size:\n",
    "        #   i += 1\n",
    "        #   if i >= len(res_df):\n",
    "        #     break\n",
    "        #   continue\n",
    "          \n",
    "        # entry_input_x = min_max_scale(res_df[selected_price_colname].iloc[initial_i - input_size:initial_i].values)\n",
    "       \n",
    "        # re_entry_input_x = expand_dims(entry_input_x)\n",
    "\n",
    "        # entry_vector = model.predict(re_entry_input_x, verbose=0)\n",
    "        # # print(test_result.shape)\n",
    "\n",
    "        # f_dist = vector_dist(entry_vector, selected_vector)\n",
    "        # print(\"f_dist :\", f_dist)\n",
    "\n",
    "        # if f_dist < fdist_thresh:\n",
    "          # mr_score += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wDQWK3v5xOFa"
   },
   "outputs": [],
   "source": [
    "def get_pr_v4(open_side, h, l, obj, tpout, lvrg, fee, p_ranges, p_qty_ratio, inversion=False):  # --> 여기서 사용하는 ex_p = ex_p\n",
    "\n",
    "    en_p = obj[0]\n",
    "    # ex_p = obj[1]\n",
    "    tp, out = np.split(tpout, 2, axis=1)\n",
    "    len_p = len(p_ranges)\n",
    "    en_ps, tps, outs, lvrgs, fees = [np.tile(arr_, (1, len_p)) for arr_ in [en_p, tp, out, lvrg, fee]]\n",
    "\n",
    "    np_obj = np.array(obj).T[0]\n",
    "    assert len(np_obj.shape) == 2\n",
    "\n",
    "    # iin == iout 인 경우 분리\n",
    "    en_idx = np_obj[:, 2]\n",
    "    ex_idx = np_obj[:, 3]\n",
    "    equal_idx = en_idx == ex_idx    # equal_idx 는 어차피 out 임\n",
    "    issue_idx = en_idx > ex_idx    # equal_idx 는 어차피 out 임\n",
    "\n",
    "    print('pass')\n",
    "    idx_gap_ = (ex_idx - en_idx)[~equal_idx]\n",
    "    print(\"en_idx[issue_idx] :\", en_idx[issue_idx])\n",
    "    print(\"ex_idx[issue_idx] :\", ex_idx[issue_idx])\n",
    "    print(\"idx_gap_[idx_gap_ <= 0] :\", idx_gap_[idx_gap_ <= 0])\n",
    "\n",
    "    min_low = np.full_like(en_p, np.nan)\n",
    "    min_low[~equal_idx] = np.array([np.min(l[int(iin + 1):int(iout + 1)]) for _, _, iin, iout in np_obj[~equal_idx, :4]]).reshape(-1, 1)  # start from iin + 1 (tp 체결을 entry_idx 부터 보지 않음)\n",
    "    max_high = np.full_like(en_p, np.nan)\n",
    "    max_high[~equal_idx] = np.array([np.max(h[int(iin + 1):int(iout + 1)]) for _, _, iin, iout in np_obj[~equal_idx, :4]]).reshape(-1, 1)\n",
    "\n",
    "    if open_side == \"SELL\":\n",
    "        p_tps = en_ps - (en_ps - tps) * p_ranges\n",
    "        # min_low = np.full_like(en_p, np.nan)\n",
    "        # min_low[~equal_idx] = np.array([np.min(l[int(iin + 1):int(iout + 1)]) for _, _, iin, iout in np_obj[~equal_idx, :4]]).reshape(-1, 1)  # start from iin + 1 (tp 체결을 entry_idx 부터 보지 않음)\n",
    "        tp_idx = (np.tile(min_low, (1, len_p)) <= p_tps) * (np.tile(max_high, (1, len_p)) <= outs)  # entry_idx 포함해서 out touch 금지 (보수적 검증)\n",
    "    else:\n",
    "        p_tps = en_ps + (tps - en_ps) * p_ranges\n",
    "        # max_high = np.full_like(en_p, np.nan)\n",
    "        # max_high[~equal_idx] = np.array([np.max(h[int(iin + 1):int(iout + 1)]) for _, _, iin, iout in np_obj[~equal_idx, :4]]).reshape(-1, 1)\n",
    "        tp_idx = (np.tile(max_high, (1, len_p)) >= p_tps) * (np.tile(min_low, (1, len_p)) >= outs)\n",
    "\n",
    "    ex_ps = outs.copy()\n",
    "    ex_ps[tp_idx] = p_tps[tp_idx]\n",
    "\n",
    "    if open_side == \"SELL\":\n",
    "        if not inversion:\n",
    "            pr = ((en_ps / ex_ps - fees - 1) * lvrgs * p_qty_ratio).sum(axis=1) + 1\n",
    "            # ------ liquidation ------ #\n",
    "            max_high = np.full_like(en_p, np.nan)\n",
    "            max_high[~equal_idx] = np.array([np.max(h[int(iin):int(iout)]) for _, _, iin, iout in np_obj[~equal_idx, :4]]).reshape(-1, 1)\n",
    "            liqd = np.nanmin((en_p / max_high - fee - 1) * lvrg + 1)\n",
    "        else:\n",
    "            pr = ((ex_ps / en_ps - fees - 1) * lvrgs * p_qty_ratio).sum(axis=1) + 1\n",
    "            # ------ liquidation ------ #\n",
    "            min_low = np.full_like(en_p, np.nan)\n",
    "            min_low[~equal_idx] = np.array([np.min(l[int(iin):int(iout)]) for _, _, iin, iout in np_obj[~equal_idx, :4]]).reshape(-1, 1)\n",
    "            liqd = np.nanmin((min_low / en_p - fee - 1) * lvrg + 1)\n",
    "    else:\n",
    "        if not inversion:\n",
    "            pr = ((ex_ps / en_ps - fees - 1) * lvrgs * p_qty_ratio).sum(axis=1) + 1\n",
    "            # ------ liquidation ------ #\n",
    "            min_low = np.full_like(en_p, np.nan)\n",
    "            min_low[~equal_idx] = np.array([np.min(l[int(iin):int(iout)]) for _, _, iin, iout in np_obj[~equal_idx, :4]]).reshape(-1, 1)\n",
    "            liqd = np.nanmin((min_low / en_p - fee - 1) * lvrg + 1)\n",
    "        else:\n",
    "            pr = ((en_ps / ex_ps - fees - 1) * lvrgs * p_qty_ratio).sum(axis=1) + 1\n",
    "            # ------ liquidation ------ #\n",
    "            max_high = np.full_like(en_p, np.nan)\n",
    "            max_high[~equal_idx] = np.array([np.max(h[int(iin):int(iout)]) for _, _, iin, iout in np_obj[~equal_idx, :4]]).reshape(-1, 1)\n",
    "            liqd = np.nanmin((en_p / max_high - fee - 1) * lvrg + 1)\n",
    "\n",
    "    return pr.reshape(-1, 1), liqd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oE5zkT75Beiy"
   },
   "outputs": [],
   "source": [
    "\n",
    "# ------------ plot survey_res ------------ #\n",
    "title_list = [\"short\", \"long\", \"both\"]\n",
    "sub_title_list = ['prcn', 'wb', 'len_pr', 'dpf', 'wr', 'sr', 'acc_pr', 'sum_pr', 'min_pr', 'liqd', 'acc_mdd', 'sum_mdd']\n",
    "space_ = \" \" * 120\n",
    "\n",
    "fig = plt.figure(figsize=(24, 8))\n",
    "plt.style.use('dark_background')\n",
    "gs = gridspec.GridSpec(nrows=1,\n",
    "                        ncols=3,\n",
    "                        # height_ratios=[1, 1, 1]\n",
    "                      )\n",
    "# nrows, ncols, h_r = 3, 3, [1, 1, 1]\n",
    "nrows, ncols, h_r = 3, 4, [1, 1, 1]\n",
    "# nrows, ncols, h_r = 4, 3, [1, 1, 1, 1]\n",
    "# if d_idx == 0:\n",
    "# else:\n",
    "  # nrows, ncols, h_r = 2, 2, [1, 1]\n",
    "\n",
    "for d_idx, (title_name, survey_res) in enumerate(zip(title_list, survey_res_list)):  \n",
    "  inner_gs = gs[d_idx].subgridspec(nrows=nrows,\n",
    "                        ncols=ncols,\n",
    "                        height_ratios=h_r\n",
    "                      )\n",
    "  for in_idx, (data_, sub_title) in enumerate(zip(survey_res.T, sub_title_list)):\n",
    "    plt.subplot(inner_gs[in_idx])\n",
    "    data = data_.ravel()\n",
    "    valid_idx = ~np.isnan(data)\n",
    "    if np.sum(valid_idx) > 0:\n",
    "      if type(val_list[0]) == str:\n",
    "        x, y = np.arange(len(val_list))[valid_idx], data[valid_idx]\n",
    "      else:\n",
    "        x, y = val_list[valid_idx], data[valid_idx]\n",
    "      plt.plot(x, y)  # 앞에서부터 len(result) 만큼만    \n",
    "      plt.title(sub_title + '_{:.2f}'.format(x[np.argmax(y)]))\n",
    "    else:\n",
    "      plt.title(sub_title)\n",
    "\n",
    "plt.suptitle(space_.join(title_list))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pbYUlJl34ImU"
   },
   "outputs": [],
   "source": [
    "# ------ open validation ------ #\n",
    "pos_side = \"SELL\" # SELL BUY\n",
    "\n",
    "if pos_side == \"SELL\":\n",
    "  open_ = res_df['short_open1_{}'.format(config.selection_id)].to_numpy()\n",
    "  open_ts = list(map(lambda x : str(x), res_df.index[open_ == 1]))  \n",
    "else:\n",
    "  open_ = res_df['long_open1_{}'.format(config.selection_id)].to_numpy()\n",
    "  open_ts = list(map(lambda x : str(x), res_df.index[open_ == 1]))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wyYMYcxx4ImV"
   },
   "outputs": [],
   "source": [
    "pos_index = open_info_df1.side == pos_side\n",
    "for ts in res_df.index[open_info_df1.index[pos_index]]:\n",
    "  print(ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6YcqQQzsl6Ys"
   },
   "outputs": [],
   "source": [
    "\n",
    "def hlm(pr_list, true_bool):   # true_pr in true_bias / true_bias\n",
    "  true_bias_pr = pr_list[true_bool].ravel()\n",
    "  print(\"len(pr_list) :\", len(pr_list))\n",
    "  print(\"len(true_bias_pr) :\", len(true_bias_pr))\n",
    "  print(\"np.sum(pr_list > 1) :\", np.sum(pr_list > 1))\n",
    "  print(\"np.sum(true_bias_pr > 1) :\", np.sum(true_bias_pr > 1))\n",
    "  return np.sum(true_bias_pr > 1) / len(true_bias_pr)  # 차원을 고려한 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dsI-R8Zz7ls1"
   },
   "outputs": [],
   "source": [
    "\n",
    "        # tr_arr = res_df['{}_tr_{}'.format(pos_side, selection_id)].to_numpy()\n",
    "\n",
    "        # ------ point1 & 2's tp_j ------ #\n",
    "        # point_idxgap = point_idxgap_arr[op_idx]\n",
    "        # if np.isnan(point_idxgap):\n",
    "        #     continue\n",
    "        # else:\n",
    "        #     # ------ allow point2 only next to point1 ------ #\n",
    "        #     open_arr = res_df['{}_open_{}'.format(pos_side, selection_id)].to_numpy()\n",
    "        #     tp_j = int(op_idx - point_idxgap)\n",
    "        #     if np.sum(open_arr[tp_j:op_idx]) != 0:\n",
    "        #         continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 157
    },
    "executionInfo": {
     "elapsed": 350,
     "status": "ok",
     "timestamp": 1652751452213,
     "user": {
      "displayName": "7th June",
      "userId": "08178289703395036410"
     },
     "user_tz": -540
    },
    "id": "gMur2u8WeQ2K",
    "outputId": "7b506c38-7a8f-4bd1-a021-8a065d009882"
   },
   "outputs": [],
   "source": [
    "# ------ bias frquency ------ #\n",
    "len_df = len(res_df)\n",
    "\n",
    "plt.figure(figsize=(16, 2))\n",
    "\n",
    "ax1 = plt.subplot(121)\n",
    "plt.fill_between(short_obj[-2].ravel(), 0, 1, where=short_bias_arr.ravel() > 0,\n",
    "                facecolor='#00ff00', alpha=1, transform=ax1.get_xaxis_transform())   # 00ff00\n",
    "# plt.fill_between(short_obj[-2].ravel(), 0, 1, where=short_bias_arr.ravel() < 1,\n",
    "#                 facecolor='#ff00ff', alpha=1, transform=ax1.get_xaxis_transform())\n",
    "\n",
    "ax2 = plt.subplot(122)\n",
    "plt.fill_between(long_obj[-2].ravel(), 0, 1, where=long_bias_arr.ravel() > 0,\n",
    "                facecolor='#00ff00', alpha=1, transform=ax2.get_xaxis_transform())\n",
    "plt.fill_between(long_obj[-2].ravel(), 0, 1, where=long_bias_arr.ravel() < 1,\n",
    "                facecolor='#ff00ff', alpha=1, transform=ax2.get_xaxis_transform())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 157
    },
    "executionInfo": {
     "elapsed": 872,
     "status": "ok",
     "timestamp": 1652756329304,
     "user": {
      "displayName": "7th June",
      "userId": "08178289703395036410"
     },
     "user_tz": -540
    },
    "id": "eoXMxRm3qdz2",
    "outputId": "98c90b55-4c14-402c-e2d4-d7b88f9e2e62"
   },
   "outputs": [],
   "source": [
    "# ------ bias frquency ------ #\n",
    "len_df = len(res_df)\n",
    "\n",
    "plt.figure(figsize=(16, 2))\n",
    "\n",
    "ax1 = plt.subplot(121)\n",
    "plt.fill_between(short_obj[-2].ravel(), 0, 1, where=short_bias_arr.ravel() > 0,\n",
    "                facecolor='#00ff00', alpha=1, transform=ax1.get_xaxis_transform())   # 00ff00\n",
    "# plt.fill_between(short_obj[-2].ravel(), 0, 1, where=short_bias_arr.ravel() < 1,\n",
    "#                 facecolor='#ff00ff', alpha=1, transform=ax1.get_xaxis_transform())\n",
    "\n",
    "ax2 = plt.subplot(122)\n",
    "plt.vlines(long_obj[-2][long_bias_arr], ymin=0, ymax=1, color='#00ff00')\n",
    "plt.vlines(long_obj[-2][~long_bias_arr], ymin=0, ymax=1, color='#ff00ff')\n",
    "# [plt.axvline(x_, color='#ff00ff') for x_, bias_ in zip(long_obj[-2], long_bias_arr) if not bias_]\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dm7ZqzS9vqhm"
   },
   "outputs": [],
   "source": [
    "%timeit -n1 -r10 plt.vlines(long_obj[-2][long_bias_arr], ymin=0, ymax=1, color='#00ff00')  # 528 ms per loop --> 8.71 ms per loop\n",
    "%timeit -n1 -r10 [plt.axvline(x_, color='#00ff00') for x_, bias_ in zip(long_obj[-2], long_bias_arr) if bias_]\n",
    "\n",
    "# np.sum(long_bias_arr == ~long_bias_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qOE2YSzntx8v"
   },
   "outputs": [],
   "source": [
    "# %timeit -n1 -r10 [plt.axvline(x_) for x_, bias_ in zip(long_obj[-2].ravel(), long_bias_arr.ravel()) if bias_]\n",
    "%timeit -n1 -r10 plt.fill_between(long_obj[-2].ravel(), 0, 1, where=long_bias_arr.ravel() > 0, facecolor='#00ff00', alpha=1, transform=ax2.get_xaxis_transform())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dHFkv6Ar1ojU"
   },
   "source": [
    "## Plot_check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 338,
     "status": "ok",
     "timestamp": 1666568170706,
     "user": {
      "displayName": "7th June",
      "userId": "08178289703395036410"
     },
     "user_tz": -540
    },
    "id": "rMIwv1Nr1ojX"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "해당 order_side data 가 없을시, pr.ravel() 불가해짐\n",
    "\"\"\"\n",
    "\n",
    "# ------------------ plot_config ------------------ #\n",
    "save_mode = 0\n",
    "\n",
    "front_plot = 0    # 0 : p1_tick, 1 : p2_tick\n",
    "back_plot = 4     # 0 : post_plot_size, 1 : open, 2 : p2_tick, 3 : ep_tick, 4 : tp_tick\n",
    "x_max = 50       # back_plot : 0 사용시, custom x_max 반영됨\n",
    "\n",
    "bias_plot = 0     # 1 : true_bias only, -1 : false_bias only, 0 : both\n",
    "\n",
    "pr_descend = 1    # 1 : 큰 pr 부터, -1 : 작은 pr 부터, 0 : 순서대로\n",
    "\n",
    "position = -1      # -1 : short, 0 & 1 : long\n",
    "\n",
    "x_margin_mult = 1/30\n",
    "y_margin_mult = 1/30  # 0 \n",
    "\n",
    "prev_plotsize = 100 #  150 100 20 500 1000\n",
    "post_plotsize = 200 #\n",
    "\n",
    "inversion = 0\n",
    "hedge = 0\n",
    "\n",
    "# ------ show or save ------ #\n",
    "if save_mode:\n",
    "  plot_check_dir = pkg_path + \"plot_check/\" +  key.replace(\".ftr\", \"\")\n",
    "  shutil.rmtree(plot_check_dir, ignore_errors=True)  # remove existing dir\n",
    "  os.makedirs(plot_check_dir)\n",
    "  print(plot_check_dir)\n",
    "else:\n",
    "  plot_check_dir = None\n",
    "\n",
    "# ------------ 한 방향에 대해 plot_check 함 (by position var.) ------------ #\n",
    "#   obj by position  \n",
    "if position == -1:\n",
    "  pos_str = \"SELL\"\n",
    "  pr_, obj_ = short_pr, short_obj\n",
    "  arr_list = [short_p2_idx_arr, short_lvrg_arr, short_fee_arr, short_tpout_arr, short_bias_arr, short_net_p1_bias_tick, short_tp_1, short_tp_0, short_out_1, short_out_0, short_ep2_0]\n",
    "else:   # both option currently not supported\n",
    "  pos_str = \"BUY\"\n",
    "  pr_, obj_ = long_pr, long_obj\n",
    "  arr_list = [long_p2_idx_arr, long_lvrg_arr, long_fee_arr, long_tpout_arr, long_bias_arr, long_net_p1_bias_tick, long_tp_1, long_tp_0, long_out_1, long_out_0, long_ep2_0]\n",
    "\n",
    "if pr_descend:\n",
    "  if pr_descend == -1:\n",
    "    pr_descend = 0\n",
    "  pr, obj, [p2_idx_arr, lvrg_arr, fee_arr, tpout_arr, bias_arr, bias_tick, tp_1, tp_0, out_1, out_0, ep2_0] = sort_bypr_v4(pr_, obj_, arr_list, descending=pr_descend)  # --> pr_descend 의 의미가 사라짐.. (false -> true plot 으로 이동한 것뿐)\n",
    "else:\n",
    "  pr, obj, [p2_idx_arr, lvrg_arr, fee_arr, tpout_arr, bias_arr, bias_tick, tp_1, tp_0, out_1, out_0, ep2_0] = pr_, obj_, arr_list\n",
    "\n",
    "pr_msg = \"%s\\n {} ~ {} -> {:.5f}\\n lvrg : {}\\n fee : {:.4f}\" % (pos_str)  # = data_window, pos_str 으로 이곳에서 정의함\n",
    "\n",
    "try:   # wave_range 단독 실행의 경우 tr_arr 이 존재하지 않기 때문에 try 처리함\n",
    "  res_df['short_tr_{}'.format(selection_id)].iloc[short_obj[-1].astype(int).ravel()] = short_tr_arr\n",
    "  res_df['long_tr_{}'.format(selection_id)].iloc[long_obj[-1].astype(int).ravel()] = long_tr_arr\n",
    "except:\n",
    "  pass\n",
    "\n",
    "if front_plot == 0:\n",
    "  front_idx = obj[4]      # left_margin 기준 - open_idx\n",
    "else:\n",
    "  front_idx = p2_idx_arr  # left_margin 기준 - p2_idx\n",
    "\n",
    "left_end_idx = front_idx - prev_plotsize  \n",
    "right_end_idx = obj[3] + post_plotsize\n",
    "invalid_left_end = np.sum(left_end_idx < 0)\n",
    "\n",
    "np_plot_params = np.hstack((left_end_idx, right_end_idx, pr, *obj, p2_idx_arr, lvrg_arr, fee_arr, tpout_arr, tp_1, tp_0, out_1, out_0, ep2_0))[invalid_left_end:]  # all arr should have same dimension\n",
    "# plot_idx = np.full(len(np_plot_params), True)\n",
    "\n",
    "if bias_plot:\n",
    "  if bias_plot == 1:\n",
    "    bias_idx = bias_arr[invalid_left_end:].ravel()  # true_bias 만 plot\n",
    "  else:\n",
    "    bias_idx = ~bias_arr[invalid_left_end:].ravel()  # false_bias 만 plot\n",
    "  \n",
    "  # trendy_idx = bias_tick[invalid_left_end:] < config.tr_set.bias_tick  # temp location\n",
    "\n",
    "  np_plot_params = np_plot_params[bias_idx] #  * trendy_idx]  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-4CXGqEN1ojY"
   },
   "source": [
    "### Plot indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 397,
     "status": "ok",
     "timestamp": 1666568173244,
     "user": {
      "displayName": "7th June",
      "userId": "08178289703395036410"
     },
     "user_tz": -540
    },
    "id": "2bjxKCIh1ojZ",
    "outputId": "ee095203-adee-4f09-decc-f7447de637a2"
   },
   "outputs": [],
   "source": [
    "selection_id = config.selection_id\n",
    "\n",
    "# ============ make col_idx_dict config ============ #\n",
    "nonstep_col_list = []\n",
    "step_col_list = []\n",
    "step_col_list2 = []\n",
    "stepmark_col_list = []\n",
    "data_window_p1_col_list = []\n",
    "data_window_p2_col_list = []\n",
    "\n",
    "# ============ nonstep_col_list - add info(col, alpha, color, linewidth) ============ #\n",
    "# nonstep_col_list.append([['close'], 1, '#ffffff', 2])\n",
    "\n",
    "# ============ step_col_list - add info(col, alpha, color, linewidth) ============ #\n",
    "# ------ htf_candle ------ #\n",
    "# hc_tf1 = '5T'\n",
    "hc_tf2 = '30T'\n",
    "hc_tf3 = '4H'\n",
    "\n",
    "# step_col_list.append([['open_{}'.format(hc_tf1), 'close_{}'.format(hc_tf1)], 1, '#ffffff', 1])\n",
    "# step_col_list.append([['open_{}'.format(hc_tf2), 'close_{}'.format(hc_tf2)], 1, '#ffffff', 2])\n",
    "# step_col_list.append([['open_{}'.format(hc_tf3), 'close_{}'.format(hc_tf3)], 1, '#ffffff', 3])\n",
    "\n",
    "# ------ resi_sup ------ #\n",
    "# rs_tf = 'T'\n",
    "# step_col_list.append([['resi_{}'.format(rs_tf), 'sup_{}'.format(rs_tf)], 1, '#ffeb3b', 1])\n",
    "# step_col_list.append([['resi_out_{}'.format(rs_tf), 'sup_out_{}'.format(rs_tf)], 1, 'dodgerblue', 2])\n",
    "\n",
    "# ------ wave_base ------ #\n",
    "wave_itv1 = config.tr_set.wave_itv1\n",
    "wave_itv2 = config.tr_set.wave_itv2\n",
    "wave_period1 = config.tr_set.wave_period1\n",
    "wave_period2 = config.tr_set.wave_period2\n",
    "\n",
    "# step_col_list.append([['dc_base_{}{}'.format(wave_itv1, wave_period1)], 1, '#5b9cf6', 1])\n",
    "# step_col_list.append([['dc_base_{}{}'.format(wave_itv2, wave_period2)], 1, '#5b9cf6', 5])\n",
    "\n",
    "# step_col_list.append([['wave_low_fill_{}{}'.format(wave_tf, wave_period)], 1, '#ffeb3b', 1])\n",
    "# step_col_list.append([['wave_high_fill_{}{}'.format(wave_tf, wave_period)], 1, '#ffeb3b', 1])\n",
    "# step_col_list.append([['dc_upper_{}{}'.format(wave_tf, wave_period), 'dc_lower_{}{}'.format(wave_tf, wave_period)], 1, '#ffeb3b', 1])\n",
    "\n",
    "# ------ dc ------ #\n",
    "dc_tf1 = 'T'\n",
    "dc_period1 = 20 # wave_period2  # 20\n",
    "dc_tf2 = '5T'\n",
    "dc_period2 = 20 # config.loc_set.point2.csd_period if config.loc_set.point2.csd_period != \"None\" else wave_period2 \n",
    "dc_tf3 = '15T'\n",
    "dc_period3 = 20\n",
    "dc_tf3 = 'H'\n",
    "dc_period3 = 20\n",
    "\n",
    "# step_col_list.append([['dc_upper_{}{}'.format(dc_tf1, dc_period1), 'dc_lower_{}{}'.format(dc_tf1, dc_period1)], 1, '#ff00ff', 1]),  # inner #ffeb3b\n",
    "# step_col_list.append([['dc_base_{}{}'.format(dc_tf1, dc_period1)], 1, '#5b9cf6', 1]) # ffee58 5b9cf6 \n",
    "# step_col_list.append([['dc_upper_{}{}'.format(dc_tf2, dc_period2), 'dc_lower_{}{}'.format(dc_tf2, dc_period2)], 1, '#ffee58', 2]),  # inner #ffeb3b\n",
    "# step_col_list.append([['dc_base_{}{}'.format(dc_tf2, dc_period2)], 1, '#5b9cf6', 3]) # ffee58 5b9cf6\n",
    "# step_col_list.append([['dc_base_{}{}'.format(dc_tf3, dc_period3)], 1, '#5b9cf6', 5]) # ffee58 5b9cf6\n",
    "# step_col_list.append([['dc_base_{}{}'.format(dc_tf3, dc_period3)], 1, '#5b9cf6', 7]) # ffee58 5b9cf6\n",
    "\n",
    "# ------ bb ------ #\n",
    "bb_tf1 = 'T'\n",
    "bb_period1 = 200\n",
    "\n",
    "# step_col_list.append([['bb_upper_{}{}'.format(bb_tf1, bb_period1), 'bb_lower_{}{}'.format(bb_tf1, bb_period1)], 1, '#ffffff', 1])\n",
    "# step_col_list.append([['bb_upper2_{}{}'.format(bb_tf1, bb_period1), 'bb_lower2_{}{}'.format(bb_tf1, bb_period1)], 1, '#ffffff', 1])\n",
    "# step_col_list.append([['bb_upper3_{}{}'.format(bb_tf1, bb_period1), 'bb_lower3_{}{}'.format(bb_tf1, bb_period1)], 1, '#ffffff', 1])\n",
    "# step_col_list.append([['bb_base_{}{}'.format(bb_tf1, bb_period1)], 1, '#00ff00', 1])\n",
    "\n",
    "\n",
    "# ------ ma / ema ------ #\n",
    "# step_col_list.append([['ema_5T'], 1, '#03ed30', 2])\n",
    "\n",
    "ma_period1 = 50\n",
    "ma_period2 = 200\n",
    "# step_col_list.append([['ma_T{}'.format(ma_period1)], 1, '#ffffff', 1])\n",
    "# step_col_list.append([['ma_T{}'.format(ma_period2)], 1, '#ffffff', 2])\n",
    "# step_col_list.append([['long_ma_T{}_-1'.format(ma_period)], 1, '#03ed30', 2])\n",
    "\n",
    "# ============ step_col_list2 - add info(col, alpha, color, linewidth) ============ #\n",
    "# ------ cci ------ #\n",
    "# cci_itv1 = 'T'\n",
    "cci_itv1 = wave_itv1\n",
    "cci_period1 = wave_period1\n",
    "# cci_itv2 = 'T'  # 30T 15T\n",
    "cci_period2 = 120\n",
    "\n",
    "step_col_list2.append([['cci_{}{}'.format(cci_itv1, cci_period1)], 1, '#00ff00', 1])\n",
    "# step_col_list2.append([['cci_{}{}'.format(cci_itv2, cci_period2)], 1, '#ff0000', 3])\n",
    "\n",
    "# ------ fisher ------ #\n",
    "\n",
    "# step_col_list2.append([['fisher_{}{}'.format(cci_itv1, cci_period1)], 1, '#00ff00', 1])\n",
    "# step_col_list2.append([['fisher_{}{}'.format(cci_itv1, cci_period2)], 1, '#ff0000', 1])\n",
    "\n",
    "# ------ stoch ------ #\n",
    "# step_col_list2.append([['stoch_{}{}33'.format(wave_itv1, wave_period1)], 1, '#00ff00', 3])\n",
    "\n",
    "# ------ macd ------ #\n",
    "# step_col_list2.append([['macd_T535'], 1, '#00ff00', 1])\n",
    "\n",
    "\n",
    "# ============ stepmark_col_list - add info(col, alpha, color, linewidth, marker_style) ============ #\n",
    "# stepmark_col_list.append([['sar_T'], 1, 'dodgerblue', 7])\n",
    "\n",
    "# ------ st_level ------ #\n",
    "st_period1 = '15T'\n",
    "st_period2 = '4H'\n",
    "\n",
    "# stepmark_col_list.append([['st_base_{}'.format(st_period1)], 1, '#ffffff', 2, '*'])\n",
    "# stepmark_col_list.append([['st_upper_{}'.format(st_period1)], 1, '#ffffff', 1, '*'])\n",
    "# stepmark_col_list.append([['st_upper2_{}'.format(st_period1)], 1, '#ffffff', 1, '*'])\n",
    "# stepmark_col_list.append([['st_upper3_{}'.format(st_period1)], 1, '#ffffff', 1, '*'])\n",
    "# stepmark_col_list.append([['st_lower_{}'.format(st_period1)], 1, '#ffffff', 1, '*'])\n",
    "# stepmark_col_list.append([['st_lower2_{}'.format(st_period1)], 1, '#ffffff', 1, '*'])\n",
    "# stepmark_col_list.append([['st_lower3_{}'.format(st_period1)], 1, '#ffffff', 1, '*'])\n",
    "\n",
    "# stepmark_col_list.append([['st_base_{}'.format(st_period2)], 1, '#ffeb3b', 4, '*'])\n",
    "# stepmark_col_list.append([['st_upper_{}'.format(st_period2)], 1, '#ffeb3b', 3, '*'])\n",
    "# stepmark_col_list.append([['st_upper2_{}'.format(st_period2)], 1, '#ffeb3b', 3, '*'])\n",
    "# stepmark_col_list.append([['st_upper3_{}'.format(st_period2)], 1, '#ffeb3b', 3, '*'])\n",
    "# stepmark_col_list.append([['st_lower_{}'.format(st_period2)], 1, '#ffeb3b', 3, '*'])\n",
    "# stepmark_col_list.append([['st_lower2_{}'.format(st_period2)], 1, '#ffeb3b', 3, '*'])\n",
    "# stepmark_col_list.append([['st_lower3_{}'.format(st_period2)], 1, '#ffeb3b', 3, '*'])\n",
    "\n",
    "# ------ wave_range ------ #\n",
    "stepmark_col_list.append([['wave_low_fill_{}{}'.format(wave_itv1, wave_period1)], 1, '#e91e63', 10, '*'])\n",
    "stepmark_col_list.append([['wave_high_fill_{}{}'.format(wave_itv1, wave_period1)], 1, '#2962ff', 10, '*'])\n",
    "\n",
    "# stepmark_col_list.append([['wave_low_fill_{}{}'.format(wave_itv2, wave_period2)], 1, '#ff00ff', 7, '*'])\n",
    "# stepmark_col_list.append([['wave_high_fill_{}{}'.format(wave_itv2, wave_period2)], 1, '#00ff00', 7, '*'])\n",
    "  \n",
    "# stepmark_col_list.append([['wave_co_marker_{}{}'.format(wave_itv1, wave_period1)], 1, '#00ff00', 3, 'o'])\n",
    "# stepmark_col_list.append([['wave_cu_marker_{}{}'.format(wave_itv1, wave_period1)], 1, '#ff00ff', 3, 'o'])\n",
    "\n",
    "# ============ data_window_col_list ============ #\n",
    "# ------ wrr ------ #\n",
    "# data_window_col_list.append([['cu_wrr_21_{}{}'.format(wave_itv1, wave_period1)], 'cu_wrr_21_{}{}'.format(wave_itv1, wave_period1)])\n",
    "# data_window_col_list.append([['co_wrr_21_{}{}'.format(wave_itv1, wave_period1)], 'co_wrr_21_{}{}'.format(wave_itv1, wave_period1)])\n",
    "# data_window_p1_col_list.append([['cu_wrr_32_{}{}'.format(wave_itv1, wave_period1)], 'cu_wrr_32_{}{}'.format(wave_itv1, wave_period1)])\n",
    "# data_window_p1_col_list.append([['co_wrr_32_{}{}'.format(wave_itv1, wave_period1)], 'co_wrr_32_{}{}'.format(wave_itv1, wave_period1)])\n",
    "# data_window_p2_col_list.append([['cu_wrr_32_{}{}'.format(wave_itv2, wave_period2)], 'cu_wrr_32_{}{}'.format(wave_itv2, wave_period2)])\n",
    "# data_window_p2_col_list.append([['co_wrr_32_{}{}'.format(wave_itv2, wave_period2)], 'co_wrr_32_{}{}'.format(wave_itv2, wave_period2)])\n",
    "\n",
    "data_window_p1_col_list.append([['short_tr_{}'.format(selection_id)], 'short_tr_{}'.format(selection_id)])\n",
    "data_window_p1_col_list.append([['long_tr_{}'.format(selection_id)], 'long_tr_{}'.format(selection_id)])\n",
    "\n",
    "\n",
    "data_window_p1_col_list.append([['short_spread_{}'.format(selection_id)], 'short_spread_{}'.format(selection_id)])\n",
    "data_window_p1_col_list.append([['long_spread_{}'.format(selection_id)], 'long_spread_{}'.format(selection_id)])\n",
    "\n",
    "\n",
    "# ====== str to numbcol ====== #\n",
    "nonstep_col_arr = strcol_tonumb(res_df, nonstep_col_list)\n",
    "step_col_arr = strcol_tonumb(res_df, step_col_list)\n",
    "step_col_arr2 = strcol_tonumb(res_df, step_col_list2)\n",
    "stepmark_col_arr = strcol_tonumb(res_df, stepmark_col_list)\n",
    "data_window_p1_col_arr = strcol_tonumb(res_df, data_window_p1_col_list)\n",
    "data_window_p2_col_arr = strcol_tonumb(res_df, data_window_p2_col_list)\n",
    "\n",
    "col_idx_dict = \\\n",
    "{\n",
    "  \"ohlc_col_idxs\": get_col_idxs(res_df, ['open', 'high', 'low', 'close']),\n",
    "  \"vp_col_idxs\": get_col_idxs(res_df, ['close', 'volume']),\n",
    "  # \"post_cu_idx\": get_col_idxs(res_df, ['wave_cu_post_idx_fill_{}{}'.format(wave_itv1, wave_period1)]),\n",
    "  # \"post_co_idx\": get_col_idxs(res_df, ['wave_co_post_idx_fill_{}{}'.format(wave_itv1, wave_period1)]),\n",
    "  \"post_cu_idx\": get_col_idxs(res_df, ['wave_cu_prime_idx_fill_{}{}'.format(wave_itv1, wave_period1)]), # 위에 거랑 차이를 설명하긴 어려운데, 일단은 prime_idx 사용이 wave_range 내의 volume 을 모두 설명함\n",
    "  \"post_co_idx\": get_col_idxs(res_df, ['wave_co_prime_idx_fill_{}{}'.format(wave_itv1, wave_period1)]),\n",
    "  # \"ohlc_col_idxs\": get_col_idxs(res_df, ['haopen', 'hahigh', 'halow', 'haclose']),  # heikin-ashi ver.\n",
    "  \"nonstep_col_info\": nonstep_col_arr,\n",
    "  \"step_col_info\": step_col_arr,\n",
    "  \"step_col_info2\": step_col_arr2,\n",
    "  \"stepmark_col_info\": stepmark_col_arr,\n",
    "  \"data_window_p1_col_info\": data_window_p1_col_arr,\n",
    "  \"data_window_p2_col_info\": data_window_p2_col_arr,\n",
    "  \"ylim_col_idxs\": get_col_idxs(res_df, ['short_tp_1_{}'.format(selection_id), 'long_tp_1_{}'.format(selection_id), 'short_tp_0_{}'.format(selection_id), 'long_tp_0_{}'.format(selection_id)])  \n",
    "  # \"ylim_col_idxs\": get_col_idxs(res_df, ['high', 'low'])  \n",
    "}   \n",
    "# 'short_tp_1_{}'.format(selection_id), 'long_tp_1_{}'.format(selection_id), 'short_tp_0_{}'.format(selection_id), 'long_tp_0_{}'.format(selection_id)\n",
    "#   'wave_low_fill_{}{}'.format(wave_itv2, wave_period2), 'wave_high_fill_{}{}'.format(wave_itv2, wave_period2)\n",
    "#   'dc_lower_H', 'dc_lower_H', 'dc_upper_15T', 'dc_lower_15T', 'short_out_{}'.format(selection_id), 'long_out_{}'.format(selection_id)\n",
    "#   'wave_1_{}'.format(wave_itv2), 'wave_0_{}'.format(wave_itv2), 'dc_upper_15T', 'dc_lower_15T'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XfvH5ngyieS9",
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "#### Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 17981,
     "status": "error",
     "timestamp": 1666568193368,
     "user": {
      "displayName": "7th June",
      "userId": "08178289703395036410"
     },
     "user_tz": -540
    },
    "id": "OCLMABZT1ojb",
    "outputId": "494558ba-b164-4030-fc54-eef7c34fd810",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# p2_hlm 의 경우, tr 확인을 우해 session_plot 우선 실행 필요함\n",
    "_ = [plot_check_v9(res_df, config, param_zip, pr_msg, x_max, x_margin_mult, y_margin_mult, back_plot, plot_check_dir, **col_idx_dict) for param_zip in zip(np_plot_params, np_plot_params[::-1])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iTW2ZuX61ojg",
    "tags": []
   },
   "source": [
    "### plot method override"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### plot_check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_check_v9(res_df, config, param_zip, pr_msg, x_max, x_margin_mult, y_margin_mult, back_plot, plot_check_dir=None, **col_idx_dict):\n",
    "    # start_time = time.time()\n",
    "    plt.style.use(['dark_background', 'fast'])\n",
    "    fig = plt.figure(figsize=(30, 18), dpi=60)\n",
    "    nrows, ncols = 2, 2\n",
    "    gs = gridspec.GridSpec(nrows=nrows,  # row 부터 index 채우고 col 채우는 순서임 (gs_idx)\n",
    "                           ncols=ncols,\n",
    "                           height_ratios=[3, 1]\n",
    "                           )\n",
    "    for gs_idx, params in enumerate(param_zip):\n",
    "\n",
    "        iin, iout, pr, en_p, ex_p, entry_idx, exit_idx, p1_idx, p2_idx, lvrg, fee, tp_line, out_line, tp_1, tp_0, out_1, out_0, ep2_0 = params\n",
    "\n",
    "        # print(\"en_p, ex_p :\", en_p, ex_p)\n",
    "        # print(\"tp_line, out_line, ep2_0 :\", tp_line, out_line, ep2_0)\n",
    "        # print(\"tp_1 :\", tp_1)\n",
    "\n",
    "        # temporary\n",
    "        # if exit_idx - p1_idx < 50:\n",
    "        # if exit_idx != entry_idx:\n",
    "        # print(\"p1_idx :\", p1_idx)\n",
    "        # if p1_idx != 370259:\n",
    "        #   break\n",
    "\n",
    "        # 1. define ax1 & ax2\n",
    "        ax1 = fig.add_subplot(gs[gs_idx])\n",
    "        ax2 = fig.add_subplot(gs[gs_idx + 2])\n",
    "\n",
    "        # 2. data range\n",
    "        #    a. hvline phase 에서 ylime 을 data[:x_max 로 정하기 때문에 iin + x_max 사용한다.\n",
    "        # if back_plot == 0:\n",
    "        #     iout = iin + x_max\n",
    "            # iout = iout + x_max\n",
    "            # print(\"iin, iout :\", iin, iout)\n",
    "\n",
    "        a_data = res_df.iloc[int(iin):int(iout + 1)].to_numpy()\n",
    "        # a_data = data[iin:iout]\n",
    "\n",
    "        # 3. add_col section\n",
    "        #     a. candles\n",
    "        candle_plot_v2(ax1, a_data[:, col_idx_dict['ohlc_col_idxs']], alpha=1.0, wickwidth=1.0)\n",
    "\n",
    "        #     b. add cols\n",
    "        [nonstep_col_plot_v2(ax1, a_data[:, params_[0]], *params_[1:]) for params_ in col_idx_dict['nonstep_col_info']]\n",
    "        [step_col_plot_v2(ax1, a_data[:, params_[0]], *params_[1:]) for params_ in col_idx_dict['step_col_info']]\n",
    "        [stepmark_col_plot_v2(ax1, a_data[:, params_[0]], *params_[1:]) for params_ in col_idx_dict['stepmark_col_info']]\n",
    "\n",
    "        [step_col_plot_v2(ax2, a_data[:, params_[0]], *params_[1:]) for params_ in col_idx_dict['step_col_info2']]\n",
    "\n",
    "        #     c. get vp_info\n",
    "        kde_factor = 0.1  # 커질 수록 전체적인 bars_shape 이 곡선이됨, 커질수록 latency 좋아짐 (0.00003s 정도)\n",
    "        num_samples = 100  # plot 되는 volume bars (y_axis) 와 비례관계\n",
    "\n",
    "        #         i. get vp_infovp by lookback\n",
    "        # vp_lookback = 500\n",
    "        # vp_data = res_df.iloc[int(p1_idx - 500):int(p1_idx), col_idx_dict['vp_col_idxs']].to_numpy().T\n",
    "\n",
    "        #         ii. vp by wave_point\n",
    "        #     if tp_1 < out_0:  # SELL order\n",
    "        #       post_co_idx = res_df.iloc[int(p1_idx), col_idx_dict['post_co_idx']]\n",
    "        #       # vp_iin = res_df.iloc[int(p1_idx) - 1, col_idx_dict['post_cu_idx']].to_numpy()  # Todo, co_idx 와 co_post_idx 의 차별을 위해서 -1 해줌 <-- 중요 point\n",
    "        #       vp_iin = res_df.iloc[post_co_idx, col_idx_dict['post_cu_idx']].to_numpy() # post_co_idx 에 있는 post_cu_idx ?\n",
    "        #     else:\n",
    "        #       post_cu_idx = res_df.iloc[int(p1_idx), col_idx_dict['post_cu_idx']]\n",
    "        #       # vp_iin = res_df.iloc[int(p1_idx) - 1, col_idx_dict['post_co_idx']].to_numpy()\n",
    "        #       vp_iin = res_df.iloc[int(post_cu_idx), col_idx_dict['post_co_idx']].to_numpy()\n",
    "        #       # print(\"post_cu_idx, vp_iin :\", post_cu_idx, vp_iin)\n",
    "\n",
    "        #     vp_data = res_df.iloc[int(vp_iin):int(p1_idx), col_idx_dict['vp_col_idxs']].to_numpy().T   # vp : ~ post_cu / co_idx 까지\n",
    "\n",
    "        #     vp_info = [*vp_data, kde_factor, num_samples]\n",
    "        vp_info = []\n",
    "\n",
    "        #     d. ep, tp + xlim\n",
    "        try:\n",
    "            eptp_hvline_v10(ax1, ax2, config, *params, back_plot, x_max, x_margin_mult, y_margin_mult, a_data, vp_info, **col_idx_dict)\n",
    "        except Exception as e:\n",
    "            print(\"error in eptp_hvline :\", e)\n",
    "\n",
    "        \"\"\" Todo \"\"\"\n",
    "        #     e. outer_price plot 일 경우, gs_idx + nrows 하면 됨\n",
    "\n",
    "        # 4. trade_info\n",
    "        data_msg_list = [\"\\n {} : {:.3f}\".format(*params_[1:], *res_df.iloc[int(p1_idx), params_[0]]) for params_ in\n",
    "                         col_idx_dict['data_window_p1_col_info']]  # * for unsupported format for arr\n",
    "        data_msg_list += [\"\\n {} : {:.3f}\".format(*params_[1:], *res_df.iloc[int(p2_idx), params_[0]]) for params_ in\n",
    "                          col_idx_dict['data_window_p2_col_info']]\n",
    "        ps_msg_expand = pr_msg.format(p1_idx, exit_idx, pr, lvrg, fee) + ''.join(data_msg_list)\n",
    "\n",
    "        ax1.set_title(ps_msg_expand)  # set_title on ax1\n",
    "\n",
    "    if plot_check_dir is None:\n",
    "        plt.show()\n",
    "        print()\n",
    "    else:\n",
    "        fig_name = plot_check_dir + \"/{}.png\".format(int(entry_idx))\n",
    "        plt.savefig(fig_name)\n",
    "        print(fig_name, \"saved !\")\n",
    "    plt.close()\n",
    "    # print(\"elapsed time :\", time.time() - start_time)\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### eptp_hvline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0l6joTK_1ojh"
   },
   "outputs": [],
   "source": [
    "def eptp_hvline_v10(ax1, ax2, config, iin, iout, pr, en_p, ex_p, entry_idx, exit_idx, p1_idx, p2_idx, lvrg, fee, tp_level, out_level, tp_1, tp_0,\n",
    "                    out_1, out_0, ep2_0, back_plot, x_max, x_margin_mult, y_margin_mult, a_data, vp_info, **col_idx_dict):\n",
    "    \"\"\"\n",
    "    v9_1 -> v10\n",
    "        1. back_plot = 0's x_max 개념 지움. (불필요하다고 봄), post_data_size 로 치환.\n",
    "            a. back_plot 으로 back_plot_data 의 시작점이 정해진다.\n",
    "    \"\"\"\n",
    "\n",
    "    # 1. get vertical ticks\n",
    "    entry_tick = int(entry_idx - iin)\n",
    "    exit_tick = entry_tick + int(exit_idx - entry_idx)\n",
    "    p1_tick = entry_tick - int(entry_idx - p1_idx)\n",
    "    p2_tick = p1_tick + int(p2_idx - p1_idx)\n",
    "\n",
    "    if back_plot == 1:\n",
    "        x_max = p1_tick + x_max\n",
    "    elif back_plot == 2:\n",
    "        x_max = p2_tick + x_max\n",
    "    elif back_plot == 3:\n",
    "        x_max = entry_tick + x_max\n",
    "    elif back_plot == 4:\n",
    "        x_max = exit_tick + x_max\n",
    "\n",
    "    # 2. get_xlim\n",
    "    #    a. iout - iin : 보여지는 data_size, 즉 plot_size 가 data_size 보다 작아지면 (짤리면).\n",
    "    if (iout - iin) > x_max:\n",
    "        x_margin = x_max * x_margin_mult\n",
    "        ax1.set_xlim(0 - x_margin, x_max + x_margin)\n",
    "        ax2.set_xlim(0 - x_margin, x_max + x_margin)\n",
    "    x0, x1 = ax1.get_xlim()\n",
    "\n",
    "    \"\"\" 1. Axis_1 \"\"\"\n",
    "    #     a. entry & exit\n",
    "    en_xmin = entry_tick / x1\n",
    "    ex_xmin = exit_tick / x1\n",
    "    ax1.text(x0, en_p, 'en_p :\\n {:.3f}'.format(en_p), ha='right', va='center', fontweight='bold', fontsize=15)  # en_p line label\n",
    "\n",
    "    ax1.axhline(ex_p, ex_xmin, 1, linewidth=2, linestyle='--', alpha=1, color='lime')  # ex_p line axhline (signal 도 포괄함, 존재 의미)\n",
    "    ax1.text(x1, ex_p, 'ex_p :\\n {}'.format(ex_p), ha='left', va='center', fontweight='bold', fontsize=15)  # ex_p line label\n",
    "\n",
    "    #     b. tr_set line\n",
    "    left_point = 0.1\n",
    "    right_point = 1\n",
    "    text_x_pos_left = (x0 + x1) * (left_point + 0.05)\n",
    "\n",
    "    ax1.axhline(en_p, left_point, right_point, linewidth=2, linestyle='-', alpha=1, color='#005eff')  # en_p line axhline\n",
    "\n",
    "    if config.tr_set.check_hlm in [0, 1]:\n",
    "        plot_epg_tuple = (\"epg1\", config.tr_set.ep1_gap)\n",
    "    else:\n",
    "        plot_epg_tuple = (\"epg2\", config.tr_set.ep2_gap)\n",
    "    ax1.text(text_x_pos_left, en_p, '{} {}'.format(*plot_epg_tuple), ha='right', va='bottom', fontweight='bold', fontsize=15, color='#005eff')\n",
    "\n",
    "    ax1.axhline(tp_level, left_point, right_point, linewidth=2, linestyle='-', alpha=1, color='#00ff00')  # ep 와 gap 비교 용이하기 위해 ex_xmin -> 0.1 사용\n",
    "    ax1.text(text_x_pos_left, tp_level, 'tpg {}'.format(config.tr_set.tp_gap), ha='right', va='bottom', fontweight='bold', fontsize=15, color='#00ff00')\n",
    "\n",
    "    ax1.axhline(out_level, left_point, right_point, linewidth=2, linestyle='-', alpha=1, color='#ff0000')\n",
    "    ax1.text(text_x_pos_left, out_level, 'outg {}'.format(config.tr_set.out_gap), ha='right', va='bottom', fontweight='bold', fontsize=15, color='#ff0000')\n",
    "\n",
    "    #     c. tp_box\n",
    "    left_point = 0.3\n",
    "    right_point = 1\n",
    "    text_x_pos_left = (x0 + x1) * (left_point + 0.05)\n",
    "    text_x_pos_right = (x0 + x1) * right_point\n",
    "\n",
    "    ax1.axhline(tp_1, left_point, right_point, linewidth=1, linestyle='-', alpha=1, color='#ffffff')\n",
    "    ax1.text(text_x_pos_left, tp_1, ' tp_1', ha='right', va='bottom', fontweight='bold', fontsize=15)\n",
    "    ax1.axhline(tp_0, left_point, right_point, linewidth=1, linestyle='-', alpha=1, color='#ffffff')\n",
    "    ax1.text(text_x_pos_left, tp_0, ' tp_0', ha='right', va='bottom', fontweight='bold', fontsize=15)\n",
    "\n",
    "    #     d. octa_wave_box\n",
    "    wave_gap = (tp_1 - tp_0) / 8\n",
    "    [ax1.axhline(tp_0 + wave_gap * gap_i, left_point, right_point, linewidth=1, linestyle='--', alpha=1, color='#ffffff') for gap_i in range(1, 8)]\n",
    "\n",
    "    #     e. ep_box\n",
    "    ax1.axhline(ep2_0, left_point, right_point, linewidth=1, linestyle='-', alpha=1, color='#ffffff')\n",
    "    ax1.text(text_x_pos_left, ep2_0, ' ep2_0', ha='right', va='bottom', fontweight='bold', fontsize=15)\n",
    "\n",
    "    #     f. out_box\n",
    "    ax1.axhline(out_1, left_point, right_point, linewidth=1, linestyle='-', alpha=1, color='#ffffff')\n",
    "    ax1.text(text_x_pos_right, out_1, ' out_1', ha='right', va='bottom', fontweight='bold', fontsize=15)\n",
    "    ax1.axhline(out_0, left_point, right_point, linewidth=1, linestyle='-', alpha=1, color='#ffffff')\n",
    "    ax1.text(text_x_pos_right, out_0, ' out_0', ha='right', va='bottom', fontweight='bold', fontsize=15)\n",
    "\n",
    "    #     g. volume profile\n",
    "    if len(vp_info) > 0:\n",
    "        close, volume, kde_factor, num_samples = vp_info\n",
    "        # if iin >= vp_range:\n",
    "        # start_time = time.time()\n",
    "        kde = stats.gaussian_kde(close, weights=volume, bw_method=kde_factor)\n",
    "        kdx = np.linspace(close.min(), close.max(), num_samples)\n",
    "        kdy = kde(kdx)\n",
    "        kdy_max = kdy.max()\n",
    "        # print(\"kde elapsed_time :\", time.time() - start_time)\n",
    "\n",
    "        # peaks,_ = signal.find_peaks(kdy, prominence=kdy_max * 0.3)   # get peak_entries\n",
    "        # peak_list = kdx[peaks]   # peak_list\n",
    "        # [ax1.axhline(peak, linewidth=1, linestyle='-', alpha=1, color='orange') for peak in peak_list]\n",
    "\n",
    "        kdy_ratio = p1_tick / kdy_max  # 30 / 0.0001   # max_value 가 p1_tick 까지 닿을 수 있게.\n",
    "        # print(\"kdx :\", kdx)\n",
    "        # ax1.plot(kdy * kdy_ratio, kdx, color='white')  # Todo, bars 가능 ?\n",
    "        # ax1.barh(kdy * kdy_ratio, kdx, color='white')  # Todo, bars 가능 ?\n",
    "        ax1.barh(kdx, kdy * kdy_ratio, color='#00ff00', alpha=0.5)  # Todo, bars 가능 ?\n",
    "\n",
    "    #     c. ylim - ax1 only\n",
    "    #         i. ylim by tr_set\n",
    "    y_min = min(tp_level, out_level, tp_1, tp_0, out_1, out_1)\n",
    "    y_max = max(tp_level, out_level, tp_1, tp_0, out_1, out_1)\n",
    "\n",
    "    #         ii. ylim by indicator\n",
    "    if len(col_idx_dict['ylim_col_idxs']) != 0:\n",
    "        y_lim_data = a_data[:x_max + 1, col_idx_dict['ylim_col_idxs']]  # +1 for including p1_tick\n",
    "        # if back_plot:\n",
    "        #     y_lim_data = a_data[:x_max + 1, col_idx_dict['ylim_col_idxs']]  # +1 for including p1_tick\n",
    "        # else:\n",
    "        #     y_lim_data = a_data[:, col_idx_dict['ylim_col_idxs']]\n",
    "\n",
    "        y_min = min(y_lim_data.min(), y_min)\n",
    "        y_max = max(y_lim_data.max(), y_max)\n",
    "\n",
    "    y_margin = (y_max - y_min) * y_margin_mult\n",
    "    ax1.set_ylim(y_min - y_margin, y_max + y_margin)\n",
    "\n",
    "    \"\"\" 2. Axis_2 \"\"\"\n",
    "    #     y. fisher_band\n",
    "    fisher_band = config.out_set.fisher_band\n",
    "    ax2.axhline(fisher_band, color=\"#ffffff\")\n",
    "    ax2.axhline(0, color=\"#ffffff\")\n",
    "    ax2.axhline(-fisher_band, color=\"#ffffff\")\n",
    "    \n",
    "    #     x. realtime_ud\n",
    "    # ax2.axhline(0, color=\"#ffffff\")\n",
    "\n",
    "    #     a. cci_band\n",
    "    # ax2.axhline(100, color=\"#ffffff\")\n",
    "    # ax2.axhline(0, color=\"#ffffff\")\n",
    "    # ax2.axhline(-100, color=\"#ffffff\")\n",
    "\n",
    "    #     b. stoch_band\n",
    "    # ax2.axhline(67, color=\"#ffffff\")\n",
    "    # ax2.axhline(33, color=\"#ffffff\")\n",
    "    # ax2.axhline(0, color=\"#ffffff\")\n",
    "\n",
    "    # 3. public vline (p1_tick, entry_tick, exit_tick - add p1_tick on ax2\n",
    "    y0, y1 = ax1.get_ylim()\n",
    "    low_data = a_data[:exit_tick + 1, col_idx_dict['ohlc_col_idxs'][2]]  # +1 for including exit_tick\n",
    "    p2_ymax, en_ymax, ex_ymax = [(low_data[tick_] - y0) / (y1 - y0) - .01 for tick_ in [p2_tick, entry_tick, exit_tick]]  # -.05 for margin\n",
    "    if p1_tick > 0:\n",
    "        p1_ymax = (low_data[p1_tick] - y0) / (y1 - y0) - .01\n",
    "        ax1.axvline(p1_tick, 0, p1_ymax, alpha=1, linewidth=2, linestyle='--', color='#ff0000')  # 추후, tick 별 세부 정의가 달라질 수 있음을 고려해 multi_line 작성 유지\n",
    "        ax2.axvline(p1_tick, 0, 1, alpha=1, linewidth=2, linestyle='--', color='#ff0000')\n",
    "    ax1.axvline(p2_tick, 0, p2_ymax, alpha=1, linewidth=2, linestyle='--', color='#2196f3')\n",
    "    ax1.axvline(entry_tick, 0, en_ymax, alpha=1, linewidth=2, linestyle='--', color='#ffeb3b')\n",
    "    ax1.axvline(exit_tick, 0, ex_ymax, alpha=1, linewidth=2, linestyle='--', color='#ffeb3b')\n",
    "    ax2.axvline(p2_tick, 0, 1, alpha=1, linewidth=2, linestyle='--', color='#2196f3')\n",
    "    ax2.axvline(entry_tick, 0, 1, alpha=1, linewidth=2, linestyle='--', color='#ffeb3b')\n",
    "    ax2.axvline(exit_tick, 0, 1, alpha=1, linewidth=2, linestyle='--', color='#ffeb3b')\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Whole plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### whole plot indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jjFziVVIhgSr"
   },
   "outputs": [],
   "source": [
    "s_id = config.selection_id\n",
    "\n",
    "# ------------ make col_idx_dict config ------------ #\n",
    "nonstep_col_list = []\n",
    "step_col_list = []\n",
    "stepmark_col_list = []\n",
    "\n",
    "# ============ nonstep_col_list - add info(col, alpha, color, linewidth) ============ #\n",
    "# nonstep_col_list.append([['close'], 1, '#ffffff', 2])\n",
    "\n",
    "# ============ step_col_list - add info(col, alpha, color, linewidth) ============ #\n",
    "# ------ htf_candle ------ #\n",
    "hc_tf1 = '15T'\n",
    "hc_tf2 = 'H'\n",
    "hc_tf3 = '4H'\n",
    "\n",
    "step_col_list.append([['open_{}'.format(hc_tf1), 'close_{}'.format(hc_tf1)], 1, '#ffffff', 1])\n",
    "step_col_list.append([['open_{}'.format(hc_tf2), 'close_{}'.format(hc_tf2)], 1, '#ffffff', 2])\n",
    "# step_col_list.append([['open_{}'.format(hc_tf3), 'close_{}'.format(hc_tf3)], 1, '#ffffff', 4])\n",
    "\n",
    "# ------ resi_sup ------ #\n",
    "# rs_tf = 'T'\n",
    "# step_col_list.append([['resi_{}'.format(rs_tf), 'sup_{}'.format(rs_tf)], 1, '#ffeb3b', 1])\n",
    "# step_col_list.append([['resi_out_{}'.format(rs_tf), 'sup_out_{}'.format(rs_tf)], 1, 'dodgerblue', 2])\n",
    "\n",
    "# ------ wave_range ------ #\n",
    "wave_tf1 = config_list[0].tr_set.p1_itv1\n",
    "wave_period1, wave_period2 = config_list[0].tr_set.p1_period1, config_list[0].tr_set.p1_period2\n",
    "\n",
    "# step_col_list.append([['dc_upper_{}{}'.format(wave_tf1, wave_period1), 'dc_lower_{}{}'.format(wave_tf1, wave_period1)], 1, '#ffeb3b', 1])\n",
    "# step_col_list.append([['dc_upper_{}{}'.format(wave_tf1, wave_period2), 'dc_lower_{}{}'.format(wave_tf1, wave_period2)], 1, '#ffeb3b', 1])\n",
    "\n",
    "# step_col_list.append([['dc_upper_{}{}'.format(wave_tf2, wave_period2), 'dc_lower_{}{}'.format(wave_tf2, wave_period2)], 1, '#e65100', 2])\n",
    "\n",
    "# ------ dc ------ #\n",
    "dc_tf1 = '5T'\n",
    "dc_period1 = 20\n",
    "dc_tf2 = 'H'\n",
    "dc_period2 = 20\n",
    "# step_col_list.append([['dc_upper_{}{}'.format(dc_tf1, dc_period1), 'dc_lower_{}{}'.format(dc_tf1, dc_period1)], 1, '#ffeb3b', 1]),  # inner\n",
    "# step_col_list.append([['dc_base_{}{}'.format(dc_tf1, dc_period1)], 1, '#5b9cf6', 1])\n",
    "step_col_list.append([['dc_upper_{}{}'.format(dc_tf2, dc_period2), 'dc_lower_{}{}'.format(dc_tf2, dc_period2)], 1, '#ff00ff', 4]),  # inner\n",
    "step_col_list.append([['dc_base_{}{}'.format(dc_tf2, dc_period2)], 1, '#5b9cf6', 4])\n",
    "\n",
    "# ------ bb ------ #\n",
    "bb_tf1 = 'T'\n",
    "bb_period1 = 20\n",
    "\n",
    "# step_col_list.append([['bb_upper_{}{}'.format(bb_tf1, bb_period1), 'bb_lower_{}{}'.format(bb_tf1, bb_period1)], 1, '#ffffff', 1])\n",
    "# step_col_list.append([['bb_base_{}{}'.format(bb_tf1, bb_period1)], 1, '#00ff00', 1])\n",
    "\n",
    "# step_col_list.append([['bb_upper_{}'.format(tf2), 'bb_lower_{}'.format(tf2)], 1, '#e91e63', 4])\n",
    "\n",
    "# ------ ema ------ #\n",
    "# step_col_list.append([['ema_5T'], 1, '#03ed30', 2])\n",
    "\n",
    "# ============ stepmark_col_list - add info(col, alpha, color, linewidth) ============ #\n",
    "# stepmark_col_list.append([['sar_T'], 1, 'dodgerblue', 7])\n",
    "\n",
    "\n",
    "# ============ str to numbcol ============ #\n",
    "nonstep_col_arr = strcol_tonumb(res_df, nonstep_col_list)\n",
    "step_col_arr = strcol_tonumb(res_df, step_col_list)\n",
    "stepmark_col_arr = strcol_tonumb(res_df, stepmark_col_list)\n",
    "\n",
    "col_idx_dict = \\\n",
    "{\n",
    "  \"ohlc_col_idxs\": get_col_idxs(res_df, ['open', 'high', 'low', 'close']),\n",
    "  \"vp_col_idxs\": get_col_idxs(res_df, ['close', 'volume']),\n",
    "  # \"ohlc_col_idxs\": get_col_idxs(res_df, ['haopen', 'hahigh', 'halow', 'haclose']),  # heikin-ashi ver.\n",
    "  \"nonstep_col_info\": nonstep_col_arr,\n",
    "  \"step_col_info\": step_col_arr,\n",
    "  \"stepmark_col_info\": stepmark_col_arr,\n",
    "  \"ylim_col_idxs\": get_col_idxs(res_df, ['open', 'high', 'low', 'close', 'dc_upper_15T4', 'dc_lower_15T4'])  # , 'dc_upper_H', 'dc_lower_H', 'dc_upper_15T', 'dc_lower_15T', 'short_out_{}'.format(selection_id), 'long_out_{}'.format(selection_id)\n",
    "}   # , 'wave_1_{}'.format(wave_tf2), 'wave_0_{}'.format(wave_tf2), 'dc_upper_15T', 'dc_lower_15T'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "50HXDIdJij28",
    "tags": []
   },
   "source": [
    "#### whole_plot main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-tbLsXN9eN2p"
   },
   "outputs": [],
   "source": [
    "plot_op_idx_nums = 10\n",
    "\n",
    "win_idxs = (pr_ > 1).ravel()  # [-plot_op_idx_nums:]\n",
    "selected_op_idxs = obj_[4].ravel().astype(int)  # [-plot_op_idx_nums:]\n",
    "selected_ex_idxs = obj_[3].ravel().astype(int)  # [-plot_op_idx_nums:]\n",
    "\n",
    "len_idxs = len(win_idxs)\n",
    "print(\"len_idxs :\", len_idxs)\n",
    "\n",
    "split_range = np.arange(plot_op_idx_nums, len_idxs, plot_op_idx_nums)\n",
    "win_idxs_list = np.split(win_idxs, split_range, axis=0)\n",
    "selected_op_idxs_list = np.split(selected_op_idxs, split_range, axis=0)\n",
    "selected_ex_idxs_list = np.split(selected_ex_idxs, split_range, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FtILHO-4kVlO"
   },
   "outputs": [],
   "source": [
    "_ = [whole_plot_check(np_df, a, b, c, plot_check_dir=None, **col_idx_dict) for a, b, c in zip(win_idxs_list, selected_op_idxs_list, selected_ex_idxs_list)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sZAYhcdoXnm4",
    "tags": []
   },
   "source": [
    "#### whole_plot_check override"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def whole_plot_check(data, win_idxs, selected_op_idxs, selected_ex_idxs, plot_check_dir=None, **col_idx_dict):\n",
    "  # start_time = time.time()\n",
    "  plt.style.use(['dark_background', 'fast'])\n",
    "  fig = plt.figure(figsize=(30, 12), dpi=60)\n",
    "  nrows, ncols = 1, 1\n",
    "  gs = gridspec.GridSpec(nrows=nrows,  # row 부터 index 채우고 col 채우는 순서임 (gs_idx)\n",
    "                          ncols=ncols,\n",
    "                          #height_ratios=[31, 1]\n",
    "                          )\n",
    "\n",
    "  ax = fig.add_subplot(gs[0])\n",
    "\n",
    "  # ------------ add_col section ------------ #\n",
    "  a_data = data[selected_op_idxs[0]:selected_op_idxs[-1] + 1]\n",
    "\n",
    "  plot_op_idxs = selected_op_idxs - selected_op_idxs[0]  \n",
    "  plot_win_op_idxs = plot_op_idxs[win_idxs]\n",
    "  plot_loss_op_idxs = plot_op_idxs[~win_idxs]\n",
    "\n",
    "  plot_ex_idxs = selected_ex_idxs - selected_op_idxs[0]\n",
    "  plot_win_ex_idxs = plot_ex_idxs[win_idxs]\n",
    "  plot_loss_ex_idxs = plot_ex_idxs[~win_idxs]\n",
    "\n",
    "\n",
    "  # ------ add cols ------ #\n",
    "  [nonstep_col_plot(a_data[:, params[0]], *params[1:]) for params in col_idx_dict['nonstep_col_info']]\n",
    "  [step_col_plot(a_data[:, params[0]], *params[1:]) for params in col_idx_dict['step_col_info']]\n",
    "  [stepmark_col_plot(a_data[:, params[0]], *params[1:]) for params in col_idx_dict['stepmark_col_info']]\n",
    "\n",
    "  # [plt.axvline(op_idx, color='#00ff00') for op_idx in plot_win_op_idxs]\n",
    "  # [plt.axvline(op_idx, color='#ff0000') for op_idx in plot_loss_op_idxs]\n",
    "  [plt.axvspan(op_idx, ex_idx, alpha=0.5, color='#00ff00') for op_idx, ex_idx in zip(plot_win_op_idxs, plot_win_ex_idxs)]\n",
    "  [plt.axvspan(op_idx, ex_idx, alpha=0.5, color='#ff0000') for op_idx, ex_idx in zip(plot_loss_op_idxs, plot_loss_ex_idxs)]\n",
    "  \n",
    "  plt.show()\n",
    "\n",
    "  if plot_check_dir is None:\n",
    "    plt.show()\n",
    "    print()\n",
    "  else:\n",
    "    fig_name = plot_check_dir + \"/whole_plot_{}.png\".format(selected_op_idxs[0])\n",
    "    plt.savefig(fig_name)\n",
    "    print(fig_name, \"saved !\")\n",
    "  plt.close()\n",
    "  # print(\"elapsed time :\", time.time() - start_time)\n",
    "\n",
    "  return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8soVNGFt1ojj",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### legacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ddL_BC24buq0",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### olds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zgTrEWWqbwsT",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### whole_plot thing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M4zn8wxibzAR"
   },
   "outputs": [],
   "source": [
    "fig = go.Figure(data=[go.Candlestick(x=t_df.index,\n",
    "                open=t_df.open,\n",
    "                high=t_df.high,\n",
    "                low=t_df.low,\n",
    "                close=t_df.close)])\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4IhBjPMobzAS"
   },
   "outputs": [],
   "source": [
    "cf.go_offline()\n",
    "init_notebook_mode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n9WZkE9wbzAS"
   },
   "outputs": [],
   "source": [
    "qf = cf.QuantFig(t_df, title=\"Apple's stock price in 2021\", name='AAPL')\n",
    "qf.iplot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zmYbP-Gc1ojs",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### brief np_pr survey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "82a8Km8z1ojs"
   },
   "outputs": [],
   "source": [
    "# plot_pr_list[:100]\n",
    "plt.plot(np_pr)\n",
    "plt.axhline(1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5rdQZm_71ojv",
    "tags": []
   },
   "source": [
    "#### plot indi. legacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OJqkmkpsLCYC",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### tr_tresh calc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u11r-dU91ojw"
   },
   "outputs": [],
   "source": [
    "\n",
    "  # ---------------------- ma ---------------------- #\n",
    "   # --------- ema --------- #\n",
    "  # alpha = 1\n",
    "  # for sm_i, item in enumerate(ema_list):\n",
    "  #   if sm_i > 0:\n",
    "  #     lw = 5\n",
    "  #   else:\n",
    "  #     lw = 2\n",
    "  #   plt.step(np.arange(len(plot_df)), plot_df[item].values, alpha=alpha, color='#03ed30', linewidth=lw)\n",
    "  #   alpha -= 0.2\n",
    "\n",
    "  #   # --------- sma --------- #\n",
    "  # alpha = 1\n",
    "  # for sm_i, sma in enumerate(sma_list):\n",
    "  #   if sm_i > 0:\n",
    "  #     lw = 5\n",
    "  #   else:\n",
    "  #     lw = 4\n",
    "  #   plt.step(np.arange(len(plot_df)), plot_df[sma].values, alpha=alpha, color='#e91e63', linewidth=lw)\n",
    "  #   alpha -= 0.2\n",
    "\n",
    "  \n",
    "  # ---------------------- cb ---------------------- #\n",
    "  # alpha = 1\n",
    "  # for sm_i, item in enumerate(cb_list):\n",
    "  #   if sm_i > 0:\n",
    "  #     lw = 5\n",
    "  #   else:\n",
    "  #     lw = 2\n",
    "  #   plt.step(np.arange(len(plot_df)), plot_df[item].values, alpha=alpha, color='#5b9cf6', linewidth=lw)\n",
    "  #   alpha -= 0.2\n",
    "\n",
    "\n",
    "  \n",
    "  # ---------------------- sar ---------------------- #\n",
    "  # alpha = 1\n",
    "  # markersize = 5\n",
    "  # for sar in sar_list:\n",
    "  #   plt.step(plot_df[sar].values, 'c*', alpha=alpha, markersize=markersize, color='dodgerblue')  # sar mic\n",
    "  #   markersize += 1\n",
    "  #   alpha -= 0.1\n",
    "\n",
    "  # plt.step(plot_df.values[:, [12]], 'co', alpha=1, markersize=7)  # sar mac\n",
    "\n",
    "  #               cloud               #\n",
    "  # alpha = 0.7\n",
    "  # for senkoua, senkoub in zip(senkoua_list, senkoub_list):\n",
    "  #   plt.fill_between(np.arange(len(plot_df)), plot_df[senkoua].values, plot_df[senkoub].values, # ichimoku\n",
    "  #                     where=plot_df[senkoua].values >= plot_df[senkoub].values, facecolor='g', alpha=alpha) # ichimoku\n",
    "  #   plt.fill_between(np.arange(len(plot_df)), plot_df[senkoua].values, plot_df[senkoub].values,\n",
    "  #                     where=plot_df[senkoua].values <= plot_df[senkoub].values, facecolor='r', alpha=alpha)  \n",
    "  #   alpha -= 0.05\n",
    "  \n",
    "\n",
    "\n",
    "  # ---------------------- outer price indi. ---------------------- #\n",
    "  #           macd          #\n",
    "  # plt.subplot(312)\n",
    "  # plt.subplot(gs[1])\n",
    "  # alpha = 1\n",
    "  # for macd in macd_list:\n",
    "  #   plt.step(np.arange(len(plot_df)), plot_df[macd].values, 'g', alpha=alpha)\n",
    "  #   # plt.fill_between(np.arange(len(plot_df)), 0, plot_df[macd].values, facecolor='g', alpha=alpha) \n",
    "  #   alpha -= 0.2\n",
    "\n",
    "  # plt.axvline(prev_plotsize, linestyle='--')\n",
    "  # plt.axvline(prev_plotsize + (tp_idx_list_[-1] - ep_idx_list_[0]), alpha=0.5, linestyle='--')\n",
    "  # plt.axvline(prev_plotsize - (ep_idx_list_[0] - open_idx), alpha=0.5, linestyle='--', color='lime')\n",
    "\n",
    "  # plt.axhline(0, linestyle='--')\n",
    "\n",
    "  \n",
    "  # #           trix          #  \n",
    "  # # plt.subplot(313)\n",
    "  # plt.subplot(gs[2])\n",
    "  # alpha = 1\n",
    "  # for trix in trix_list:\n",
    "  #   plt.step(np.arange(len(plot_df)), plot_df[trix].values, 'g', alpha=alpha)\n",
    "  #   # plt.fill_between(np.arange(len(plot_df)), 0, plot_df[macd].values, facecolor='g', alpha=alpha) \n",
    "  #   alpha -= 0.2\n",
    "  # plt.axvline(prev_plotsize, linestyle='--')\n",
    "  # plt.axvline(prev_plotsize + (tp_idx_list_[-1] - ep_idx_list_[0]), alpha=0.5, linestyle='--')\n",
    "  # plt.axvline(prev_plotsize - (ep_idx_list_[0] - open_idx), alpha=0.5, linestyle='--', color='lime')\n",
    "  # plt.axhline(0, linestyle='--')\n",
    "\n",
    "  \n",
    "  #           fisher          #  \n",
    "  # plt.subplot(313)\n",
    "  # plt.subplot(gs[1])\n",
    "  # alpha = 1\n",
    "  # for fisher in fisher_list:\n",
    "  #   plt.step(np.arange(len(plot_df)), plot_df[fisher].values, 'g', alpha=alpha)\n",
    "  #   # plt.fill_between(np.arange(len(plot_df)), 0, plot_df[macd].values, facecolor='g', alpha=alpha) \n",
    "  #   alpha -= 0.2\n",
    "    \n",
    "  # plt.axvline(prev_plotsize, linestyle='--')\n",
    "  # plt.axvline(prev_plotsize + (tp_idx_list_[-1] - ep_idx_list_[0]), alpha=0.5, linestyle='--')\n",
    "  # plt.axvline(prev_plotsize - (ep_idx_list_[0] - open_idx), alpha=0.5, linestyle='--', color='lime')\n",
    "\n",
    "  # plt.axhline(0, linestyle='--')\n",
    "  # plt.axhline(fisher_upper, linestyle='--')\n",
    "  # plt.axhline(fisher_lower, linestyle='--')\n",
    "\n",
    "  #           stoch          #  \n",
    "  # plt.subplot(gs[1])\n",
    "  # alpha = 1\n",
    "  # for stoch_ in stoch_list:\n",
    "  #   plt.step(np.arange(len(plot_df)), plot_df[stoch_].values, 'g', alpha=alpha)\n",
    "  #   # plt.fill_between(np.arange(len(plot_df)), 0, plot_df[macd].values, facecolor='g', alpha=alpha) \n",
    "  #   alpha -= 0.2\n",
    "  # plt.axvline(prev_plotsize, linestyle='--')\n",
    "  # plt.axhline(50, linestyle='--')\n",
    "  # plt.axhline(stoch_upper, linestyle='--')\n",
    "  # plt.axhline(stoch_lower, linestyle='--')\n",
    "  # plt.axvline(prev_plotsize + (tp_idx_list_[-1] - ep_idx_list_[0]), alpha=1, linestyle='--')\n",
    "  # plt.axvline(prev_plotsize - (ep_idx_list_[0] - open_idx), alpha=0.5, linestyle='--', color='lime')\n",
    "\n",
    "  # ---------- cctbbo ---------- #  \n",
    "  # plt.subplot(gs[1])\n",
    "  # alpha = 1\n",
    "  # for cctbbo in cctbbo_list:\n",
    "  #   plt.step(np.arange(len(plot_df)), plot_df[cctbbo].values, 'g', alpha=alpha)\n",
    "  #   # plt.fill_between(np.arange(len(plot_df)), 0, plot_df[macd].values, facecolor='g', alpha=alpha) \n",
    "  #   alpha -= 0.2\n",
    "  # plt.axvline(prev_plotsize, linestyle='--')\n",
    "  # plt.axhline(50, linestyle='--')\n",
    "  # plt.axhline(cctbbo_upper, linestyle='--')\n",
    "  # plt.axhline(cctbbo_lower, linestyle='--')\n",
    "  # plt.axvline(prev_plotsize + (tp_idx_list_[-1] - ep_idx_list_[0]), alpha=1, linestyle='--')\n",
    "  # plt.axvline(prev_plotsize - (ep_idx_list_[0] - open_idx), alpha=0.5, linestyle='--', color='lime')\n",
    "\n",
    "  # ---------- ema_roc ---------- #  \n",
    "  # plt.subplot(gs[1])\n",
    "  # alpha = 1\n",
    "  # for emaroc in emaroc_list:\n",
    "  #   plt.step(np.arange(len(plot_df)), plot_df[emaroc].values, 'g', alpha=alpha)\n",
    "  #   # plt.fill_between(np.arange(len(plot_df)), 0, plot_df[macd].values, facecolor='g', alpha=alpha) \n",
    "  #   alpha -= 0.2\n",
    "  # plt.axvline(prev_plotsize, linestyle='--')\n",
    "  # plt.axvline(prev_plotsize + (tp_idx_list_[-1] - ep_idx_list_[0]), alpha=1, linestyle='--')\n",
    "  # plt.axvline(prev_plotsize - (ep_idx_list_[0] - open_idx), alpha=0.5, linestyle='--', color='lime')\n",
    "  # plt.axhline(0, linestyle='--')\n",
    "  \n",
    "  # ---------- bbw ---------- #  \n",
    "  # plt.subplot(gs[1])\n",
    "  # alpha = 1\n",
    "  # for bbwp_ in bbwp_list:\n",
    "  #   plt.step(np.arange(len(plot_df)), plot_df[bbwp_].values, 'g', alpha=alpha)\n",
    "  #   # plt.fill_between(np.arange(len(plot_df)), 0, plot_df[macd].values, facecolor='g', alpha=alpha) \n",
    "  #   alpha -= 0.2\n",
    "  # plt.axvline(prev_plotsize, linestyle='--')\n",
    "  # plt.axvline(prev_plotsize + (tp_idx_list_[-1] - ep_idx_list_[0]), alpha=1, linestyle='--')\n",
    "  # plt.axvline(prev_plotsize - (ep_idx_list_[0] - open_idx), alpha=0.5, linestyle='--', color='lime')\n",
    "  # plt.axhline(bbwp_thresh, linestyle='--')\n",
    "\n",
    "  # plt.axvline(prev_plotsize - (ep_idx_list_[0] - open_idx), alpha=0.5, linestyle='--', color='lime')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gcpo4MGd9Wm4"
   },
   "outputs": [],
   "source": [
    "res_wr = 0.6\n",
    "# tr_thresh = 1\n",
    "# tr_thresh = ((1 - res_wr) / res_wr) ** 0.5\n",
    "tr_thresh = ((1 - res_wr) / res_wr) + 0.01\n",
    "# tr_thresh = 2.6\n",
    "print(\"res_wr :\", res_wr)\n",
    "print(\"tr_thresh :\", tr_thresh)\n",
    "\n",
    "\n",
    "#   단리    #\n",
    "trade_num = 1000\n",
    "asset = 1 # thousand USDT\n",
    "test_loss_gap = 0.95  # fee adjusted\n",
    "test_pr_gap = 1 + (1 - test_loss_gap) * tr_thresh\n",
    "\n",
    "test_loss_cnt = trade_num * (1 - res_wr)\n",
    "test_pr_cnt = trade_num * res_wr\n",
    "\n",
    "test_trade_list = [test_pr_gap] * int(test_pr_cnt) + [test_loss_gap] * int(test_loss_cnt)\n",
    "random.shuffle(test_trade_list)\n",
    "# print(\"len(test_trade_list) :\", len(test_trade_list))\n",
    "print(test_trade_list[:10])\n",
    "print()\n",
    "\n",
    "# print(\"%.5f\" % np.cumprod(test_trade_list)[-1])\n",
    "for tr_thresh_ in np.arange(1, 3, 0.2):\n",
    "  if (1 + (1 - test_loss_gap) * tr_thresh_) ** test_pr_cnt * test_loss_gap ** test_loss_cnt > 1:\n",
    "    break\n",
    "print(\"복리를 위한 tr_thresh_ :\", tr_thresh_)\n",
    "# print(\"tr_thresh :\", tr_thresh)\n",
    "print(\"np.cumprod(test_trade_list)[-1] :\", np.cumprod(test_trade_list)[-1])\n",
    "print(\"total_pr : \", np.cumprod(test_trade_list)[-1])\n",
    "print()\n",
    "#   복리 tr_thresh  #\n",
    "#   1. trade_num 에 영향 받지 않음\n",
    "#   2. loss_gap 에 비례함\n",
    "\n",
    "for tr_thresh_ in np.arange(1, 3, 0.01):\n",
    "  if ((1 - test_loss_gap) * tr_thresh_) * test_pr_cnt + (test_loss_gap - 1) * test_loss_cnt > 0:\n",
    "    break\n",
    "np_test_trade = np.array(test_trade_list) - 1\n",
    "print(np_test_trade[:10])\n",
    "# print(\"%.3f\" % )\n",
    "print(\"단리를 위한 tr_thresh_ :\", tr_thresh_)\n",
    "# print(\"tr_thresh :\", tr_thresh)\n",
    "print(\"np.cumsum(np_test_trade)[-1] :\", np.cumsum(np_test_trade)[-1])\n",
    "print(\"total_pr : \", 1 + np.cumsum(np_test_trade)[-1])\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNNrEbb2LToSahBuESvKral",
   "collapsed_sections": [
    "Ic1mfmwWCIBu",
    "E0n53hflJbnp",
    "MlFkpO1MSuzl",
    "x2yj2SwAXDLp",
    "14chOHeXh6JD",
    "Q_1wJTcRYpm8",
    "EOXQbXixiQcK",
    "RZJ6uIA_VcJs",
    "xpyP5t8Ht_pE",
    "MuD_2vY7TI_8",
    "tOFkzUX2imQu",
    "983aUwM76s6X",
    "_blyFhQJUd5X",
    "50HXDIdJij28"
   ],
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showmarkdowntxt": true,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
