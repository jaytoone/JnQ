{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python [conda env:tensorflow2_p36]","language":"python","name":"conda-env-tensorflow2_p36-py"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.5"},"colab":{"name":"Make_model_macd_osc.ipynb","provenance":[{"file_id":"1YF8_MnvES9U5ImcMOeF0v4QBeWL1Yqem","timestamp":1584115546819},{"file_id":"1W9HzvtL-1kYHCyYbDJxFsA7uKJNOCwuv","timestamp":1584004491586},{"file_id":"1WEMU-VCj-p8mZvMxqBpQAdViCsHXpo20","timestamp":1583840352407},{"file_id":"1z4z_KLPzc6RWsxo3X_dHbpByxUAgjoMJ","timestamp":1583754134002}],"collapsed_sections":[]},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"8s5fopqwFUf9","colab_type":"code","outputId":"bbccbb3d-386d-40a9-dcb7-806874a7ef4d","executionInfo":{"status":"ok","timestamp":1584115604901,"user_tz":-540,"elapsed":20530,"user":{"displayName":"J 1","photoUrl":"","userId":"08178289703395036410"}},"colab":{"base_uri":"https://localhost:8080/","height":125}},"source":["from google.colab import drive\n","\n","drive.mount('/content/gdrive')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"scrolled":false,"id":"2iYLNSeSEp7p","colab_type":"code","outputId":"86e51865-a731-4396-f7ce-e579bc2a5419","executionInfo":{"status":"ok","timestamp":1584129266969,"user_tz":-540,"elapsed":13591257,"user":{"displayName":"J 1","photoUrl":"","userId":"08178289703395036410"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["import os\n","# os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   \n","# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","\n","import pandas as pd\n","import numpy as np\n","from matplotlib import pyplot\n","import scipy.misc \n","from math import sqrt \n","import itertools\n","from IPython.display import display\n","%matplotlib inline\n","\n","# input_data_length = int(input('input_data_length : '))\n","input_data_length = 30\n","model_num = 87\n","num_classes = 4\n","\n","gdrive_path = '/content/gdrive/My Drive/Colab Notebooks/'\n","\n","Made_X = np.load(gdrive_path + 'Made_X/Made_X %s_%s.npy' % (input_data_length, model_num))\n","Made_Y = np.load(gdrive_path + 'Made_X/Made_Y %s_%s.npy' % (input_data_length, model_num)).reshape(-1, 1)\n","\n","\n","#       dataset 분리      #\n","# dataX 구성 : VOLUME, MA, CMO, OBV, RSI, MACD, MACD_SIGNAL, MACD_OSC \n","# dataX 구성 : VOLUME, EMA1, EMA2, CMO, OBV, RSI, MACD, MACD_SIGNAL, MACD_OSC \n","Made_X = Made_X[:, :, [11]]\n","print(Made_X.shape)\n","print(Made_Y.shape)\n","\n","row = Made_X.shape[1]\n","col = Made_X.shape[2]\n","\n","from sklearn.model_selection import train_test_split\n","X_train, X_val, Y_train, Y_val = train_test_split(Made_X, Made_Y, test_size=0.3,\n","                                                   shuffle=False)\n","\n","X_train = X_train.astype('float32').reshape(-1, input_data_length, col, 1)\n","X_val = X_val.astype('float32').reshape(-1, input_data_length, col, 1)\n","print(X_train.shape)\n","print(X_val.shape)\n","\n","from keras.utils import np_utils\n","from keras.preprocessing.image import ImageDataGenerator \n","\n","# Data Class Weight\n","from sklearn.utils import class_weight\n","\n","print(Y_train[:, 0])\n","class_weights = class_weight.compute_class_weight('balanced', \n","                                                  np.unique(Y_train[:, 0]),\n","                                                  Y_train[:, 0])\n","class_weights = dict(enumerate(class_weights))\n","print(class_weights)\n","# quit()\n","\n","Y_train = Y_train.astype('float32')\n","Y_val = Y_val.astype('float32')\n","Y_train = np_utils.to_categorical(Y_train, num_classes)\n","Y_val = np_utils.to_categorical(Y_val, num_classes)\n","print(Y_train.shape)\n","print(Y_val.shape)\n","\n","datagen = ImageDataGenerator( \n","#     rotation_range = 60,\n","#     zoom_range = 0.6,\n","#     shear_range = 0.6,\n","#     horizontal_flip = True,\n","#     width_shift_range=0.6,\n","#     height_shift_range=0.6,\n","    fill_mode = 'nearest'\n","    )\n","\n","testgen = ImageDataGenerator( \n","    )\n","datagen.fit(X_train)\n","batch_size = 128\n","\n","for X_batch, _ in datagen.flow(X_train, Y_train, batch_size=9):\n","    for i in range(0, 9): \n","        pyplot.axis('off') \n","        pyplot.subplot(330 + 1 + i) \n","        pyplot.imshow(X_batch[i].reshape(input_data_length, col))\n","    pyplot.axis('off') \n","    pyplot.show() \n","    break\n","    \n","    \n","train_flow = datagen.flow(X_train, Y_train, batch_size=batch_size) \n","val_flow = testgen.flow(X_val, Y_val, batch_size=batch_size) \n","\n","\n","from keras.utils import plot_model\n","import keras.backend as K\n","from keras.models import Model, Sequential\n","import keras.layers as layers\n","from keras.optimizers import Adam, SGD\n","from keras.regularizers import l1, l2\n","from sklearn.metrics import confusion_matrix\n","\n","def FER_Model(input_shape=(row, col, 1)):\n","    # first input model\n","    visible = layers.Input(shape=input_shape, name='input')\n","    \n","    net = layers.Conv2D(64, kernel_size=3, padding='same')(visible)\n","    # net = layers.Activation('relu')(net)\n","    net = layers.LeakyReLU()(net)\n","    # net = layers.MaxPool2D(pool_size=2)(net)\n","\n","    shortcut_1 = net\n","\n","    net = layers.Conv2D(128, kernel_size=3, padding='same')(net)\n","    # net = layers.Activation('relu')(net)\n","    net = layers.LeakyReLU()(net)\n","    # net = layers.MaxPool2D(pool_size=2)(net)\n","\n","    shortcut_2 = net\n","\n","#     net = layers.Conv2D(256, kernel_size=3, padding='same')(net)\n","#     # net = layers.Activation('relu')(net)\n","#     net = layers.LeakyReLU()(net)\n","#     net = layers.MaxPool2D(pool_size=2)(net)\n","\n","#     shortcut_3 = net\n","\n","#     net = layers.Conv2D(128, kernel_size=1, padding='same')(net)\n","#     # net = layers.Activation('relu')(net)\n","#     net = layers.LeakyReLU()(net)\n","#     net = layers.MaxPool2D(pool_size=2)(net)\n","\n","    net = layers.Flatten()(net)\n","    net = layers.Dense(64)(net)\n","    net = layers.LeakyReLU()(net)\n","    net = layers.Dense(num_classes, activation='softmax')(net)\n","\n","    # create model \n","    model = Model(inputs =visible, outputs = net)\n","    # summary layers\n","    print(model.summary())\n","    \n","    return model\n","\n","model = FER_Model()\n","# from keras.models import load_model\n","# model = load_model(gdrive_path + 'model/rapid_ascending %s_%s_macd_osc.hdf5' % (input_data_length, model_num))\n","opt = Adam(lr=0.0001, decay=1e-6)\n","model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n","  \n","    \n","from keras.callbacks import ModelCheckpoint, EarlyStopping, TensorBoard\n","filepath = gdrive_path + \"model/rapid_ascending %s_%s_macd_osc.hdf5\" % (input_data_length, model_num)\n","checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='auto')\n","checkpoint2 = TensorBoard(log_dir='Tensorboard_graph',\n","                          histogram_freq=0,\n","                          write_graph=True,\n","                          write_images=True)\n","checkpoint3 = EarlyStopping(monitor='val_acc', patience=200)\n","callbacks_list = [checkpoint, checkpoint2, checkpoint3]\n","\n","# keras.callbacks.Callback 로 부터 log 를 받아와 history log 를 작성할 수 있다.\n","\n","# we iterate 200 times over the entire training set\n","num_epochs = 1000\n","history = model.fit_generator(train_flow, \n","                    steps_per_epoch=len(X_train) / batch_size, \n","                    epochs=num_epochs,  \n","                    verbose=2,  \n","                    callbacks=callbacks_list,\n","                    class_weight=class_weights,\n","                    validation_data=val_flow,  \n","                    validation_steps=len(X_val) / batch_size,\n","                    shuffle=False)\n","\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["(612609, 30, 1)\n","(612609, 1)\n","(428826, 30, 1, 1)\n","(183783, 30, 1, 1)\n"],"name":"stdout"},{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"},{"output_type":"display_data","data":{"text/html":["<p style=\"color: red;\">\n","The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n","We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n","or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n","<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["[0. 0. 0. ... 1. 0. 0.]\n","{0: 0.35474760510249664, 1: 3.477117929423975, 2: 2.246856268600411, 3: 2.229984399375975}\n","(428826, 4)\n","(183783, 4)\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAPwAAADnCAYAAAA6ujs/AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nGMKCBwxhwQOG/B8RVMevSPCgwgAAAABJRU5ErkJggg==\n","text/plain":["<Figure size 432x288 with 9 Axes>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n","\n","Model: \"model_1\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","input (InputLayer)           (None, 30, 1, 1)          0         \n","_________________________________________________________________\n","conv2d_1 (Conv2D)            (None, 30, 1, 64)         640       \n","_________________________________________________________________\n","leaky_re_lu_1 (LeakyReLU)    (None, 30, 1, 64)         0         \n","_________________________________________________________________\n","conv2d_2 (Conv2D)            (None, 30, 1, 128)        73856     \n","_________________________________________________________________\n","leaky_re_lu_2 (LeakyReLU)    (None, 30, 1, 128)        0         \n","_________________________________________________________________\n","flatten_1 (Flatten)          (None, 3840)              0         \n","_________________________________________________________________\n","dense_1 (Dense)              (None, 64)                245824    \n","_________________________________________________________________\n","leaky_re_lu_3 (LeakyReLU)    (None, 64)                0         \n","_________________________________________________________________\n","dense_2 (Dense)              (None, 4)                 260       \n","=================================================================\n","Total params: 320,580\n","Trainable params: 320,580\n","Non-trainable params: 0\n","_________________________________________________________________\n","None\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.where in 2.0, which has the same broadcast rule as np.where\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/callbacks.py:1122: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/callbacks.py:1125: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n","\n","Epoch 1/1000\n"," - 47s - loss: 1.1030 - acc: 0.3069 - val_loss: 1.1937 - val_acc: 0.3520\n","\n","Epoch 00001: val_acc improved from -inf to 0.35199, saving model to /content/gdrive/My Drive/Colab Notebooks/model/rapid_ascending 30_87_macd_osc.hdf5\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/callbacks.py:1265: The name tf.Summary is deprecated. Please use tf.compat.v1.Summary instead.\n","\n","Epoch 2/1000\n"," - 40s - loss: 0.9708 - acc: 0.3768 - val_loss: 1.1383 - val_acc: 0.3849\n","\n","Epoch 00002: val_acc improved from 0.35199 to 0.38488, saving model to /content/gdrive/My Drive/Colab Notebooks/model/rapid_ascending 30_87_macd_osc.hdf5\n","Epoch 3/1000\n"," - 40s - loss: 0.9479 - acc: 0.3926 - val_loss: 1.1168 - val_acc: 0.3985\n","\n","Epoch 00003: val_acc improved from 0.38488 to 0.39845, saving model to /content/gdrive/My Drive/Colab Notebooks/model/rapid_ascending 30_87_macd_osc.hdf5\n","Epoch 4/1000\n"," - 40s - loss: 0.9373 - acc: 0.4025 - val_loss: 1.1067 - val_acc: 0.4170\n","\n","Epoch 00004: val_acc improved from 0.39845 to 0.41696, saving model to /content/gdrive/My Drive/Colab Notebooks/model/rapid_ascending 30_87_macd_osc.hdf5\n","Epoch 5/1000\n"," - 40s - loss: 0.9304 - acc: 0.4071 - val_loss: 1.1247 - val_acc: 0.4068\n","\n","Epoch 00005: val_acc did not improve from 0.41696\n","Epoch 6/1000\n"," - 40s - loss: 0.9262 - acc: 0.4113 - val_loss: 1.0928 - val_acc: 0.4171\n","\n","Epoch 00006: val_acc improved from 0.41696 to 0.41713, saving model to /content/gdrive/My Drive/Colab Notebooks/model/rapid_ascending 30_87_macd_osc.hdf5\n","Epoch 7/1000\n"," - 40s - loss: 0.9223 - acc: 0.4135 - val_loss: 1.0646 - val_acc: 0.4470\n","\n","Epoch 00007: val_acc improved from 0.41713 to 0.44704, saving model to /content/gdrive/My Drive/Colab Notebooks/model/rapid_ascending 30_87_macd_osc.hdf5\n","Epoch 8/1000\n"," - 40s - loss: 0.9190 - acc: 0.4155 - val_loss: 1.0982 - val_acc: 0.4163\n","\n","Epoch 00008: val_acc did not improve from 0.44704\n","Epoch 9/1000\n"," - 40s - loss: 0.9164 - acc: 0.4178 - val_loss: 1.0936 - val_acc: 0.4199\n","\n","Epoch 00009: val_acc did not improve from 0.44704\n","Epoch 10/1000\n"," - 40s - loss: 0.9138 - acc: 0.4191 - val_loss: 1.1236 - val_acc: 0.4085\n","\n","Epoch 00010: val_acc did not improve from 0.44704\n","Epoch 11/1000\n"," - 40s - loss: 0.9116 - acc: 0.4210 - val_loss: 1.1027 - val_acc: 0.4156\n","\n","Epoch 00011: val_acc did not improve from 0.44704\n","Epoch 12/1000\n"," - 40s - loss: 0.9098 - acc: 0.4232 - val_loss: 1.0503 - val_acc: 0.4416\n","\n","Epoch 00012: val_acc did not improve from 0.44704\n","Epoch 13/1000\n"," - 40s - loss: 0.9081 - acc: 0.4235 - val_loss: 1.1107 - val_acc: 0.4141\n","\n","Epoch 00013: val_acc did not improve from 0.44704\n","Epoch 14/1000\n"," - 40s - loss: 0.9063 - acc: 0.4254 - val_loss: 1.1421 - val_acc: 0.3976\n","\n","Epoch 00014: val_acc did not improve from 0.44704\n","Epoch 15/1000\n"," - 40s - loss: 0.9048 - acc: 0.4253 - val_loss: 1.1245 - val_acc: 0.4032\n","\n","Epoch 00015: val_acc did not improve from 0.44704\n","Epoch 16/1000\n"," - 41s - loss: 0.9033 - acc: 0.4270 - val_loss: 1.0624 - val_acc: 0.4380\n","\n","Epoch 00016: val_acc did not improve from 0.44704\n","Epoch 17/1000\n"," - 40s - loss: 0.9022 - acc: 0.4271 - val_loss: 1.1501 - val_acc: 0.4031\n","\n","Epoch 00017: val_acc did not improve from 0.44704\n","Epoch 18/1000\n"," - 40s - loss: 0.9011 - acc: 0.4290 - val_loss: 1.1015 - val_acc: 0.4193\n","\n","Epoch 00018: val_acc did not improve from 0.44704\n","Epoch 19/1000\n"," - 41s - loss: 0.9003 - acc: 0.4286 - val_loss: 1.1466 - val_acc: 0.4021\n","\n","Epoch 00019: val_acc did not improve from 0.44704\n","Epoch 20/1000\n"," - 40s - loss: 0.8990 - acc: 0.4300 - val_loss: 1.1719 - val_acc: 0.3790\n","\n","Epoch 00020: val_acc did not improve from 0.44704\n","Epoch 21/1000\n"," - 40s - loss: 0.8982 - acc: 0.4301 - val_loss: 1.1043 - val_acc: 0.4206\n","\n","Epoch 00021: val_acc did not improve from 0.44704\n","Epoch 22/1000\n"," - 40s - loss: 0.8975 - acc: 0.4315 - val_loss: 1.0955 - val_acc: 0.4233\n","\n","Epoch 00022: val_acc did not improve from 0.44704\n","Epoch 23/1000\n"," - 40s - loss: 0.8967 - acc: 0.4306 - val_loss: 1.0815 - val_acc: 0.4286\n","\n","Epoch 00023: val_acc did not improve from 0.44704\n","Epoch 24/1000\n"," - 40s - loss: 0.8955 - acc: 0.4319 - val_loss: 1.1040 - val_acc: 0.4137\n","\n","Epoch 00024: val_acc did not improve from 0.44704\n","Epoch 25/1000\n"," - 40s - loss: 0.8955 - acc: 0.4315 - val_loss: 1.1224 - val_acc: 0.4037\n","\n","Epoch 00025: val_acc did not improve from 0.44704\n","Epoch 26/1000\n"," - 41s - loss: 0.8943 - acc: 0.4328 - val_loss: 1.0414 - val_acc: 0.4578\n","\n","Epoch 00026: val_acc improved from 0.44704 to 0.45775, saving model to /content/gdrive/My Drive/Colab Notebooks/model/rapid_ascending 30_87_macd_osc.hdf5\n","Epoch 27/1000\n"," - 40s - loss: 0.8936 - acc: 0.4326 - val_loss: 1.0754 - val_acc: 0.4412\n","\n","Epoch 00027: val_acc did not improve from 0.45775\n","Epoch 28/1000\n"," - 40s - loss: 0.8931 - acc: 0.4337 - val_loss: 1.0859 - val_acc: 0.4274\n","\n","Epoch 00028: val_acc did not improve from 0.45775\n","Epoch 29/1000\n"," - 40s - loss: 0.8924 - acc: 0.4330 - val_loss: 1.1014 - val_acc: 0.4289\n","\n","Epoch 00029: val_acc did not improve from 0.45775\n","Epoch 30/1000\n"," - 40s - loss: 0.8921 - acc: 0.4339 - val_loss: 1.0788 - val_acc: 0.4348\n","\n","Epoch 00030: val_acc did not improve from 0.45775\n","Epoch 31/1000\n"," - 40s - loss: 0.8914 - acc: 0.4333 - val_loss: 1.0811 - val_acc: 0.4317\n","\n","Epoch 00031: val_acc did not improve from 0.45775\n","Epoch 32/1000\n"," - 41s - loss: 0.8912 - acc: 0.4334 - val_loss: 1.0698 - val_acc: 0.4366\n","\n","Epoch 00032: val_acc did not improve from 0.45775\n","Epoch 33/1000\n"," - 40s - loss: 0.8906 - acc: 0.4345 - val_loss: 1.0241 - val_acc: 0.4660\n","\n","Epoch 00033: val_acc improved from 0.45775 to 0.46596, saving model to /content/gdrive/My Drive/Colab Notebooks/model/rapid_ascending 30_87_macd_osc.hdf5\n","Epoch 34/1000\n"," - 41s - loss: 0.8898 - acc: 0.4348 - val_loss: 1.0686 - val_acc: 0.4368\n","\n","Epoch 00034: val_acc did not improve from 0.46596\n","Epoch 35/1000\n"," - 41s - loss: 0.8891 - acc: 0.4347 - val_loss: 1.0765 - val_acc: 0.4336\n","\n","Epoch 00035: val_acc did not improve from 0.46596\n","Epoch 36/1000\n"," - 40s - loss: 0.8888 - acc: 0.4354 - val_loss: 1.0744 - val_acc: 0.4404\n","\n","Epoch 00036: val_acc did not improve from 0.46596\n","Epoch 37/1000\n"," - 41s - loss: 0.8885 - acc: 0.4353 - val_loss: 1.0752 - val_acc: 0.4328\n","\n","Epoch 00037: val_acc did not improve from 0.46596\n","Epoch 38/1000\n"," - 41s - loss: 0.8881 - acc: 0.4356 - val_loss: 1.0930 - val_acc: 0.4321\n","\n","Epoch 00038: val_acc did not improve from 0.46596\n","Epoch 39/1000\n"," - 41s - loss: 0.8874 - acc: 0.4362 - val_loss: 1.0682 - val_acc: 0.4385\n","\n","Epoch 00039: val_acc did not improve from 0.46596\n","Epoch 40/1000\n"," - 40s - loss: 0.8869 - acc: 0.4362 - val_loss: 1.0689 - val_acc: 0.4461\n","\n","Epoch 00040: val_acc did not improve from 0.46596\n","Epoch 41/1000\n"," - 41s - loss: 0.8864 - acc: 0.4371 - val_loss: 1.0729 - val_acc: 0.4447\n","\n","Epoch 00041: val_acc did not improve from 0.46596\n","Epoch 42/1000\n"," - 41s - loss: 0.8861 - acc: 0.4377 - val_loss: 1.1273 - val_acc: 0.4022\n","\n","Epoch 00042: val_acc did not improve from 0.46596\n","Epoch 43/1000\n"," - 41s - loss: 0.8859 - acc: 0.4371 - val_loss: 1.0422 - val_acc: 0.4579\n","\n","Epoch 00043: val_acc did not improve from 0.46596\n","Epoch 44/1000\n"," - 41s - loss: 0.8851 - acc: 0.4374 - val_loss: 1.0709 - val_acc: 0.4352\n","\n","Epoch 00044: val_acc did not improve from 0.46596\n","Epoch 45/1000\n"," - 41s - loss: 0.8850 - acc: 0.4380 - val_loss: 1.0377 - val_acc: 0.4553\n","\n","Epoch 00045: val_acc did not improve from 0.46596\n","Epoch 46/1000\n"," - 41s - loss: 0.8847 - acc: 0.4373 - val_loss: 1.0577 - val_acc: 0.4477\n","\n","Epoch 00046: val_acc did not improve from 0.46596\n","Epoch 47/1000\n"," - 40s - loss: 0.8842 - acc: 0.4376 - val_loss: 1.0768 - val_acc: 0.4380\n","\n","Epoch 00047: val_acc did not improve from 0.46596\n","Epoch 48/1000\n"," - 41s - loss: 0.8836 - acc: 0.4391 - val_loss: 1.1108 - val_acc: 0.4148\n","\n","Epoch 00048: val_acc did not improve from 0.46596\n","Epoch 49/1000\n"," - 41s - loss: 0.8831 - acc: 0.4381 - val_loss: 1.0776 - val_acc: 0.4355\n","\n","Epoch 00049: val_acc did not improve from 0.46596\n","Epoch 50/1000\n"," - 41s - loss: 0.8830 - acc: 0.4392 - val_loss: 1.1497 - val_acc: 0.3916\n","\n","Epoch 00050: val_acc did not improve from 0.46596\n","Epoch 51/1000\n"," - 41s - loss: 0.8827 - acc: 0.4389 - val_loss: 1.0980 - val_acc: 0.4163\n","\n","Epoch 00051: val_acc did not improve from 0.46596\n","Epoch 52/1000\n"," - 40s - loss: 0.8824 - acc: 0.4382 - val_loss: 1.1045 - val_acc: 0.4189\n","\n","Epoch 00052: val_acc did not improve from 0.46596\n","Epoch 53/1000\n"," - 40s - loss: 0.8818 - acc: 0.4400 - val_loss: 1.0618 - val_acc: 0.4470\n","\n","Epoch 00053: val_acc did not improve from 0.46596\n","Epoch 54/1000\n"," - 40s - loss: 0.8820 - acc: 0.4402 - val_loss: 1.0849 - val_acc: 0.4332\n","\n","Epoch 00054: val_acc did not improve from 0.46596\n","Epoch 55/1000\n"," - 40s - loss: 0.8817 - acc: 0.4394 - val_loss: 1.0875 - val_acc: 0.4286\n","\n","Epoch 00055: val_acc did not improve from 0.46596\n","Epoch 56/1000\n"," - 40s - loss: 0.8812 - acc: 0.4397 - val_loss: 1.0873 - val_acc: 0.4254\n","\n","Epoch 00056: val_acc did not improve from 0.46596\n","Epoch 57/1000\n"," - 39s - loss: 0.8807 - acc: 0.4395 - val_loss: 1.0569 - val_acc: 0.4566\n","\n","Epoch 00057: val_acc did not improve from 0.46596\n","Epoch 58/1000\n"," - 40s - loss: 0.8806 - acc: 0.4400 - val_loss: 1.0643 - val_acc: 0.4454\n","\n","Epoch 00058: val_acc did not improve from 0.46596\n","Epoch 59/1000\n"," - 40s - loss: 0.8803 - acc: 0.4403 - val_loss: 1.0966 - val_acc: 0.4251\n","\n","Epoch 00059: val_acc did not improve from 0.46596\n","Epoch 60/1000\n"," - 40s - loss: 0.8799 - acc: 0.4405 - val_loss: 1.0843 - val_acc: 0.4208\n","\n","Epoch 00060: val_acc did not improve from 0.46596\n","Epoch 61/1000\n"," - 40s - loss: 0.8796 - acc: 0.4408 - val_loss: 1.0755 - val_acc: 0.4354\n","\n","Epoch 00061: val_acc did not improve from 0.46596\n","Epoch 62/1000\n"," - 40s - loss: 0.8794 - acc: 0.4413 - val_loss: 1.0786 - val_acc: 0.4323\n","\n","Epoch 00062: val_acc did not improve from 0.46596\n","Epoch 63/1000\n"," - 40s - loss: 0.8787 - acc: 0.4413 - val_loss: 1.0756 - val_acc: 0.4311\n","\n","Epoch 00063: val_acc did not improve from 0.46596\n","Epoch 64/1000\n"," - 40s - loss: 0.8787 - acc: 0.4408 - val_loss: 1.0658 - val_acc: 0.4435\n","\n","Epoch 00064: val_acc did not improve from 0.46596\n","Epoch 65/1000\n"," - 40s - loss: 0.8785 - acc: 0.4411 - val_loss: 1.0547 - val_acc: 0.4511\n","\n","Epoch 00065: val_acc did not improve from 0.46596\n","Epoch 66/1000\n"," - 40s - loss: 0.8780 - acc: 0.4413 - val_loss: 1.0648 - val_acc: 0.4447\n","\n","Epoch 00066: val_acc did not improve from 0.46596\n","Epoch 67/1000\n"," - 40s - loss: 0.8779 - acc: 0.4417 - val_loss: 1.0831 - val_acc: 0.4315\n","\n","Epoch 00067: val_acc did not improve from 0.46596\n","Epoch 68/1000\n"," - 40s - loss: 0.8776 - acc: 0.4413 - val_loss: 1.0671 - val_acc: 0.4498\n","\n","Epoch 00068: val_acc did not improve from 0.46596\n","Epoch 69/1000\n"," - 40s - loss: 0.8773 - acc: 0.4419 - val_loss: 1.0710 - val_acc: 0.4446\n","\n","Epoch 00069: val_acc did not improve from 0.46596\n","Epoch 70/1000\n"," - 40s - loss: 0.8770 - acc: 0.4427 - val_loss: 1.1082 - val_acc: 0.4239\n","\n","Epoch 00070: val_acc did not improve from 0.46596\n","Epoch 71/1000\n"," - 40s - loss: 0.8767 - acc: 0.4415 - val_loss: 1.0804 - val_acc: 0.4322\n","\n","Epoch 00071: val_acc did not improve from 0.46596\n","Epoch 72/1000\n"," - 40s - loss: 0.8765 - acc: 0.4428 - val_loss: 1.0769 - val_acc: 0.4313\n","\n","Epoch 00072: val_acc did not improve from 0.46596\n","Epoch 73/1000\n"," - 40s - loss: 0.8761 - acc: 0.4421 - val_loss: 1.1183 - val_acc: 0.4088\n","\n","Epoch 00073: val_acc did not improve from 0.46596\n","Epoch 74/1000\n"," - 40s - loss: 0.8759 - acc: 0.4423 - val_loss: 1.0844 - val_acc: 0.4362\n","\n","Epoch 00074: val_acc did not improve from 0.46596\n","Epoch 75/1000\n"," - 40s - loss: 0.8754 - acc: 0.4426 - val_loss: 1.0770 - val_acc: 0.4324\n","\n","Epoch 00075: val_acc did not improve from 0.46596\n","Epoch 76/1000\n"," - 40s - loss: 0.8757 - acc: 0.4429 - val_loss: 1.0620 - val_acc: 0.4434\n","\n","Epoch 00076: val_acc did not improve from 0.46596\n","Epoch 77/1000\n"," - 40s - loss: 0.8749 - acc: 0.4430 - val_loss: 1.0606 - val_acc: 0.4483\n","\n","Epoch 00077: val_acc did not improve from 0.46596\n","Epoch 78/1000\n"," - 40s - loss: 0.8750 - acc: 0.4425 - val_loss: 1.0625 - val_acc: 0.4517\n","\n","Epoch 00078: val_acc did not improve from 0.46596\n","Epoch 79/1000\n"," - 40s - loss: 0.8746 - acc: 0.4435 - val_loss: 1.0607 - val_acc: 0.4468\n","\n","Epoch 00079: val_acc did not improve from 0.46596\n","Epoch 80/1000\n"," - 40s - loss: 0.8746 - acc: 0.4432 - val_loss: 1.0807 - val_acc: 0.4321\n","\n","Epoch 00080: val_acc did not improve from 0.46596\n","Epoch 81/1000\n"," - 40s - loss: 0.8740 - acc: 0.4435 - val_loss: 1.0472 - val_acc: 0.4592\n","\n","Epoch 00081: val_acc did not improve from 0.46596\n","Epoch 82/1000\n"," - 40s - loss: 0.8740 - acc: 0.4437 - val_loss: 1.0328 - val_acc: 0.4658\n","\n","Epoch 00082: val_acc did not improve from 0.46596\n","Epoch 83/1000\n"," - 40s - loss: 0.8739 - acc: 0.4439 - val_loss: 1.0725 - val_acc: 0.4313\n","\n","Epoch 00083: val_acc did not improve from 0.46596\n","Epoch 84/1000\n"," - 40s - loss: 0.8733 - acc: 0.4435 - val_loss: 1.0654 - val_acc: 0.4466\n","\n","Epoch 00084: val_acc did not improve from 0.46596\n","Epoch 85/1000\n"," - 41s - loss: 0.8731 - acc: 0.4456 - val_loss: 1.0860 - val_acc: 0.4299\n","\n","Epoch 00085: val_acc did not improve from 0.46596\n","Epoch 86/1000\n"," - 40s - loss: 0.8730 - acc: 0.4440 - val_loss: 1.0872 - val_acc: 0.4202\n","\n","Epoch 00086: val_acc did not improve from 0.46596\n","Epoch 87/1000\n"," - 41s - loss: 0.8726 - acc: 0.4445 - val_loss: 1.0678 - val_acc: 0.4467\n","\n","Epoch 00087: val_acc did not improve from 0.46596\n","Epoch 88/1000\n"," - 40s - loss: 0.8726 - acc: 0.4450 - val_loss: 1.0509 - val_acc: 0.4536\n","\n","Epoch 00088: val_acc did not improve from 0.46596\n","Epoch 89/1000\n"," - 40s - loss: 0.8722 - acc: 0.4448 - val_loss: 1.0449 - val_acc: 0.4544\n","\n","Epoch 00089: val_acc did not improve from 0.46596\n","Epoch 90/1000\n"," - 40s - loss: 0.8721 - acc: 0.4446 - val_loss: 1.0570 - val_acc: 0.4505\n","\n","Epoch 00090: val_acc did not improve from 0.46596\n","Epoch 91/1000\n"," - 40s - loss: 0.8715 - acc: 0.4450 - val_loss: 1.0687 - val_acc: 0.4356\n","\n","Epoch 00091: val_acc did not improve from 0.46596\n","Epoch 92/1000\n"," - 40s - loss: 0.8717 - acc: 0.4452 - val_loss: 1.0797 - val_acc: 0.4380\n","\n","Epoch 00092: val_acc did not improve from 0.46596\n","Epoch 93/1000\n"," - 41s - loss: 0.8715 - acc: 0.4456 - val_loss: 1.1048 - val_acc: 0.4131\n","\n","Epoch 00093: val_acc did not improve from 0.46596\n","Epoch 94/1000\n"," - 40s - loss: 0.8713 - acc: 0.4452 - val_loss: 1.0914 - val_acc: 0.4257\n","\n","Epoch 00094: val_acc did not improve from 0.46596\n","Epoch 95/1000\n"," - 40s - loss: 0.8707 - acc: 0.4451 - val_loss: 1.0994 - val_acc: 0.4206\n","\n","Epoch 00095: val_acc did not improve from 0.46596\n","Epoch 96/1000\n"," - 40s - loss: 0.8709 - acc: 0.4458 - val_loss: 1.0710 - val_acc: 0.4399\n","\n","Epoch 00096: val_acc did not improve from 0.46596\n","Epoch 97/1000\n"," - 40s - loss: 0.8706 - acc: 0.4462 - val_loss: 1.1014 - val_acc: 0.4279\n","\n","Epoch 00097: val_acc did not improve from 0.46596\n","Epoch 98/1000\n"," - 40s - loss: 0.8700 - acc: 0.4464 - val_loss: 1.0941 - val_acc: 0.4252\n","\n","Epoch 00098: val_acc did not improve from 0.46596\n","Epoch 99/1000\n"," - 40s - loss: 0.8702 - acc: 0.4462 - val_loss: 1.0524 - val_acc: 0.4515\n","\n","Epoch 00099: val_acc did not improve from 0.46596\n","Epoch 100/1000\n"," - 40s - loss: 0.8698 - acc: 0.4459 - val_loss: 1.0943 - val_acc: 0.4196\n","\n","Epoch 00100: val_acc did not improve from 0.46596\n","Epoch 101/1000\n"," - 40s - loss: 0.8694 - acc: 0.4467 - val_loss: 1.1234 - val_acc: 0.4101\n","\n","Epoch 00101: val_acc did not improve from 0.46596\n","Epoch 102/1000\n"," - 40s - loss: 0.8693 - acc: 0.4462 - val_loss: 1.0719 - val_acc: 0.4435\n","\n","Epoch 00102: val_acc did not improve from 0.46596\n","Epoch 103/1000\n"," - 40s - loss: 0.8688 - acc: 0.4468 - val_loss: 1.0712 - val_acc: 0.4436\n","\n","Epoch 00103: val_acc did not improve from 0.46596\n","Epoch 104/1000\n"," - 40s - loss: 0.8690 - acc: 0.4469 - val_loss: 1.0699 - val_acc: 0.4442\n","\n","Epoch 00104: val_acc did not improve from 0.46596\n","Epoch 105/1000\n"," - 40s - loss: 0.8687 - acc: 0.4464 - val_loss: 1.0621 - val_acc: 0.4503\n","\n","Epoch 00105: val_acc did not improve from 0.46596\n","Epoch 106/1000\n"," - 40s - loss: 0.8683 - acc: 0.4479 - val_loss: 1.1113 - val_acc: 0.4128\n","\n","Epoch 00106: val_acc did not improve from 0.46596\n","Epoch 107/1000\n"," - 40s - loss: 0.8684 - acc: 0.4475 - val_loss: 1.0425 - val_acc: 0.4535\n","\n","Epoch 00107: val_acc did not improve from 0.46596\n","Epoch 108/1000\n"," - 40s - loss: 0.8678 - acc: 0.4466 - val_loss: 1.0716 - val_acc: 0.4400\n","\n","Epoch 00108: val_acc did not improve from 0.46596\n","Epoch 109/1000\n"," - 40s - loss: 0.8677 - acc: 0.4474 - val_loss: 1.0879 - val_acc: 0.4363\n","\n","Epoch 00109: val_acc did not improve from 0.46596\n","Epoch 110/1000\n"," - 40s - loss: 0.8678 - acc: 0.4471 - val_loss: 1.0785 - val_acc: 0.4335\n","\n","Epoch 00110: val_acc did not improve from 0.46596\n","Epoch 111/1000\n"," - 40s - loss: 0.8674 - acc: 0.4476 - val_loss: 1.0830 - val_acc: 0.4329\n","\n","Epoch 00111: val_acc did not improve from 0.46596\n","Epoch 112/1000\n"," - 40s - loss: 0.8670 - acc: 0.4476 - val_loss: 1.0787 - val_acc: 0.4410\n","\n","Epoch 00112: val_acc did not improve from 0.46596\n","Epoch 113/1000\n"," - 40s - loss: 0.8669 - acc: 0.4478 - val_loss: 1.0846 - val_acc: 0.4366\n","\n","Epoch 00113: val_acc did not improve from 0.46596\n","Epoch 114/1000\n"," - 41s - loss: 0.8671 - acc: 0.4474 - val_loss: 1.0742 - val_acc: 0.4420\n","\n","Epoch 00114: val_acc did not improve from 0.46596\n","Epoch 115/1000\n"," - 41s - loss: 0.8667 - acc: 0.4479 - val_loss: 1.0632 - val_acc: 0.4563\n","\n","Epoch 00115: val_acc did not improve from 0.46596\n","Epoch 116/1000\n"," - 41s - loss: 0.8663 - acc: 0.4482 - val_loss: 1.0822 - val_acc: 0.4407\n","\n","Epoch 00116: val_acc did not improve from 0.46596\n","Epoch 117/1000\n"," - 41s - loss: 0.8663 - acc: 0.4482 - val_loss: 1.0978 - val_acc: 0.4292\n","\n","Epoch 00117: val_acc did not improve from 0.46596\n","Epoch 118/1000\n"," - 41s - loss: 0.8662 - acc: 0.4481 - val_loss: 1.0740 - val_acc: 0.4467\n","\n","Epoch 00118: val_acc did not improve from 0.46596\n","Epoch 119/1000\n"," - 41s - loss: 0.8657 - acc: 0.4494 - val_loss: 1.0742 - val_acc: 0.4438\n","\n","Epoch 00119: val_acc did not improve from 0.46596\n","Epoch 120/1000\n"," - 41s - loss: 0.8659 - acc: 0.4479 - val_loss: 1.0837 - val_acc: 0.4340\n","\n","Epoch 00120: val_acc did not improve from 0.46596\n","Epoch 121/1000\n"," - 41s - loss: 0.8654 - acc: 0.4487 - val_loss: 1.0458 - val_acc: 0.4592\n","\n","Epoch 00121: val_acc did not improve from 0.46596\n","Epoch 122/1000\n"," - 41s - loss: 0.8651 - acc: 0.4495 - val_loss: 1.0618 - val_acc: 0.4447\n","\n","Epoch 00122: val_acc did not improve from 0.46596\n","Epoch 123/1000\n"," - 41s - loss: 0.8648 - acc: 0.4494 - val_loss: 1.0793 - val_acc: 0.4316\n","\n","Epoch 00123: val_acc did not improve from 0.46596\n","Epoch 124/1000\n"," - 41s - loss: 0.8649 - acc: 0.4489 - val_loss: 1.0707 - val_acc: 0.4395\n","\n","Epoch 00124: val_acc did not improve from 0.46596\n","Epoch 125/1000\n"," - 41s - loss: 0.8646 - acc: 0.4489 - val_loss: 1.0822 - val_acc: 0.4325\n","\n","Epoch 00125: val_acc did not improve from 0.46596\n","Epoch 126/1000\n"," - 41s - loss: 0.8643 - acc: 0.4489 - val_loss: 1.0706 - val_acc: 0.4422\n","\n","Epoch 00126: val_acc did not improve from 0.46596\n","Epoch 127/1000\n"," - 41s - loss: 0.8643 - acc: 0.4492 - val_loss: 1.0723 - val_acc: 0.4438\n","\n","Epoch 00127: val_acc did not improve from 0.46596\n","Epoch 128/1000\n"," - 41s - loss: 0.8639 - acc: 0.4492 - val_loss: 1.0542 - val_acc: 0.4572\n","\n","Epoch 00128: val_acc did not improve from 0.46596\n","Epoch 129/1000\n"," - 41s - loss: 0.8642 - acc: 0.4492 - val_loss: 1.0868 - val_acc: 0.4326\n","\n","Epoch 00129: val_acc did not improve from 0.46596\n","Epoch 130/1000\n"," - 41s - loss: 0.8634 - acc: 0.4498 - val_loss: 1.0695 - val_acc: 0.4433\n","\n","Epoch 00130: val_acc did not improve from 0.46596\n","Epoch 131/1000\n"," - 41s - loss: 0.8636 - acc: 0.4498 - val_loss: 1.1108 - val_acc: 0.4171\n","\n","Epoch 00131: val_acc did not improve from 0.46596\n","Epoch 132/1000\n"," - 40s - loss: 0.8633 - acc: 0.4500 - val_loss: 1.0545 - val_acc: 0.4504\n","\n","Epoch 00132: val_acc did not improve from 0.46596\n","Epoch 133/1000\n"," - 40s - loss: 0.8628 - acc: 0.4500 - val_loss: 1.0657 - val_acc: 0.4499\n","\n","Epoch 00133: val_acc did not improve from 0.46596\n","Epoch 134/1000\n"," - 40s - loss: 0.8629 - acc: 0.4500 - val_loss: 1.1042 - val_acc: 0.4280\n","\n","Epoch 00134: val_acc did not improve from 0.46596\n","Epoch 135/1000\n"," - 40s - loss: 0.8627 - acc: 0.4508 - val_loss: 1.0094 - val_acc: 0.4866\n","\n","Epoch 00135: val_acc improved from 0.46596 to 0.48661, saving model to /content/gdrive/My Drive/Colab Notebooks/model/rapid_ascending 30_87_macd_osc.hdf5\n","Epoch 136/1000\n"," - 39s - loss: 0.8625 - acc: 0.4509 - val_loss: 1.0833 - val_acc: 0.4302\n","\n","Epoch 00136: val_acc did not improve from 0.48661\n","Epoch 137/1000\n"," - 39s - loss: 0.8625 - acc: 0.4507 - val_loss: 1.0593 - val_acc: 0.4567\n","\n","Epoch 00137: val_acc did not improve from 0.48661\n","Epoch 138/1000\n"," - 39s - loss: 0.8624 - acc: 0.4505 - val_loss: 1.0817 - val_acc: 0.4400\n","\n","Epoch 00138: val_acc did not improve from 0.48661\n","Epoch 139/1000\n"," - 40s - loss: 0.8620 - acc: 0.4504 - val_loss: 1.1000 - val_acc: 0.4242\n","\n","Epoch 00139: val_acc did not improve from 0.48661\n","Epoch 140/1000\n"," - 40s - loss: 0.8622 - acc: 0.4506 - val_loss: 1.0775 - val_acc: 0.4366\n","\n","Epoch 00140: val_acc did not improve from 0.48661\n","Epoch 141/1000\n"," - 40s - loss: 0.8616 - acc: 0.4506 - val_loss: 1.0915 - val_acc: 0.4367\n","\n","Epoch 00141: val_acc did not improve from 0.48661\n","Epoch 142/1000\n"," - 40s - loss: 0.8616 - acc: 0.4507 - val_loss: 1.0846 - val_acc: 0.4264\n","\n","Epoch 00142: val_acc did not improve from 0.48661\n","Epoch 143/1000\n"," - 39s - loss: 0.8612 - acc: 0.4507 - val_loss: 1.0693 - val_acc: 0.4447\n","\n","Epoch 00143: val_acc did not improve from 0.48661\n","Epoch 144/1000\n"," - 39s - loss: 0.8608 - acc: 0.4515 - val_loss: 1.0999 - val_acc: 0.4304\n","\n","Epoch 00144: val_acc did not improve from 0.48661\n","Epoch 145/1000\n"," - 39s - loss: 0.8607 - acc: 0.4509 - val_loss: 1.0726 - val_acc: 0.4409\n","\n","Epoch 00145: val_acc did not improve from 0.48661\n","Epoch 146/1000\n"," - 39s - loss: 0.8608 - acc: 0.4509 - val_loss: 1.0540 - val_acc: 0.4515\n","\n","Epoch 00146: val_acc did not improve from 0.48661\n","Epoch 147/1000\n"," - 40s - loss: 0.8603 - acc: 0.4517 - val_loss: 1.0526 - val_acc: 0.4535\n","\n","Epoch 00147: val_acc did not improve from 0.48661\n","Epoch 148/1000\n"," - 39s - loss: 0.8600 - acc: 0.4519 - val_loss: 1.0732 - val_acc: 0.4388\n","\n","Epoch 00148: val_acc did not improve from 0.48661\n","Epoch 149/1000\n"," - 39s - loss: 0.8598 - acc: 0.4520 - val_loss: 1.0484 - val_acc: 0.4600\n","\n","Epoch 00149: val_acc did not improve from 0.48661\n","Epoch 150/1000\n"," - 39s - loss: 0.8599 - acc: 0.4518 - val_loss: 1.0970 - val_acc: 0.4222\n","\n","Epoch 00150: val_acc did not improve from 0.48661\n","Epoch 151/1000\n"," - 39s - loss: 0.8595 - acc: 0.4516 - val_loss: 1.0950 - val_acc: 0.4295\n","\n","Epoch 00151: val_acc did not improve from 0.48661\n","Epoch 152/1000\n"," - 40s - loss: 0.8595 - acc: 0.4519 - val_loss: 1.0915 - val_acc: 0.4338\n","\n","Epoch 00152: val_acc did not improve from 0.48661\n","Epoch 153/1000\n"," - 40s - loss: 0.8595 - acc: 0.4512 - val_loss: 1.0765 - val_acc: 0.4441\n","\n","Epoch 00153: val_acc did not improve from 0.48661\n","Epoch 154/1000\n"," - 40s - loss: 0.8594 - acc: 0.4524 - val_loss: 1.1042 - val_acc: 0.4202\n","\n","Epoch 00154: val_acc did not improve from 0.48661\n","Epoch 155/1000\n"," - 40s - loss: 0.8590 - acc: 0.4522 - val_loss: 1.0911 - val_acc: 0.4289\n","\n","Epoch 00155: val_acc did not improve from 0.48661\n","Epoch 156/1000\n"," - 40s - loss: 0.8588 - acc: 0.4519 - val_loss: 1.0937 - val_acc: 0.4229\n","\n","Epoch 00156: val_acc did not improve from 0.48661\n","Epoch 157/1000\n"," - 40s - loss: 0.8586 - acc: 0.4524 - val_loss: 1.0929 - val_acc: 0.4279\n","\n","Epoch 00157: val_acc did not improve from 0.48661\n","Epoch 158/1000\n"," - 40s - loss: 0.8588 - acc: 0.4521 - val_loss: 1.0725 - val_acc: 0.4481\n","\n","Epoch 00158: val_acc did not improve from 0.48661\n","Epoch 159/1000\n"," - 40s - loss: 0.8583 - acc: 0.4522 - val_loss: 1.1204 - val_acc: 0.4073\n","\n","Epoch 00159: val_acc did not improve from 0.48661\n","Epoch 160/1000\n"," - 39s - loss: 0.8580 - acc: 0.4529 - val_loss: 1.0726 - val_acc: 0.4384\n","\n","Epoch 00160: val_acc did not improve from 0.48661\n","Epoch 161/1000\n"," - 40s - loss: 0.8577 - acc: 0.4535 - val_loss: 1.1096 - val_acc: 0.4200\n","\n","Epoch 00161: val_acc did not improve from 0.48661\n","Epoch 162/1000\n"," - 41s - loss: 0.8580 - acc: 0.4526 - val_loss: 1.1439 - val_acc: 0.3951\n","\n","Epoch 00162: val_acc did not improve from 0.48661\n","Epoch 163/1000\n"," - 39s - loss: 0.8575 - acc: 0.4538 - val_loss: 1.1131 - val_acc: 0.4188\n","\n","Epoch 00163: val_acc did not improve from 0.48661\n","Epoch 164/1000\n"," - 39s - loss: 0.8574 - acc: 0.4528 - val_loss: 1.0811 - val_acc: 0.4417\n","\n","Epoch 00164: val_acc did not improve from 0.48661\n","Epoch 165/1000\n"," - 39s - loss: 0.8573 - acc: 0.4539 - val_loss: 1.0788 - val_acc: 0.4398\n","\n","Epoch 00165: val_acc did not improve from 0.48661\n","Epoch 166/1000\n"," - 40s - loss: 0.8571 - acc: 0.4534 - val_loss: 1.0674 - val_acc: 0.4451\n","\n","Epoch 00166: val_acc did not improve from 0.48661\n","Epoch 167/1000\n"," - 40s - loss: 0.8567 - acc: 0.4537 - val_loss: 1.0867 - val_acc: 0.4353\n","\n","Epoch 00167: val_acc did not improve from 0.48661\n","Epoch 168/1000\n"," - 40s - loss: 0.8568 - acc: 0.4535 - val_loss: 1.0787 - val_acc: 0.4374\n","\n","Epoch 00168: val_acc did not improve from 0.48661\n","Epoch 169/1000\n"," - 39s - loss: 0.8567 - acc: 0.4539 - val_loss: 1.1065 - val_acc: 0.4200\n","\n","Epoch 00169: val_acc did not improve from 0.48661\n","Epoch 170/1000\n"," - 40s - loss: 0.8564 - acc: 0.4538 - val_loss: 1.0850 - val_acc: 0.4260\n","\n","Epoch 00170: val_acc did not improve from 0.48661\n","Epoch 171/1000\n"," - 41s - loss: 0.8563 - acc: 0.4542 - val_loss: 1.0962 - val_acc: 0.4261\n","\n","Epoch 00171: val_acc did not improve from 0.48661\n","Epoch 172/1000\n"," - 40s - loss: 0.8561 - acc: 0.4534 - val_loss: 1.0774 - val_acc: 0.4345\n","\n","Epoch 00172: val_acc did not improve from 0.48661\n","Epoch 173/1000\n"," - 40s - loss: 0.8560 - acc: 0.4544 - val_loss: 1.0990 - val_acc: 0.4253\n","\n","Epoch 00173: val_acc did not improve from 0.48661\n","Epoch 174/1000\n"," - 39s - loss: 0.8558 - acc: 0.4541 - val_loss: 1.1058 - val_acc: 0.4230\n","\n","Epoch 00174: val_acc did not improve from 0.48661\n","Epoch 175/1000\n"," - 39s - loss: 0.8555 - acc: 0.4540 - val_loss: 1.0559 - val_acc: 0.4583\n","\n","Epoch 00175: val_acc did not improve from 0.48661\n","Epoch 176/1000\n"," - 40s - loss: 0.8557 - acc: 0.4541 - val_loss: 1.0579 - val_acc: 0.4503\n","\n","Epoch 00176: val_acc did not improve from 0.48661\n","Epoch 177/1000\n"," - 40s - loss: 0.8549 - acc: 0.4551 - val_loss: 1.0616 - val_acc: 0.4550\n","\n","Epoch 00177: val_acc did not improve from 0.48661\n","Epoch 178/1000\n"," - 40s - loss: 0.8548 - acc: 0.4553 - val_loss: 1.1063 - val_acc: 0.4216\n","\n","Epoch 00178: val_acc did not improve from 0.48661\n","Epoch 179/1000\n"," - 40s - loss: 0.8545 - acc: 0.4550 - val_loss: 1.0712 - val_acc: 0.4394\n","\n","Epoch 00179: val_acc did not improve from 0.48661\n","Epoch 180/1000\n"," - 40s - loss: 0.8548 - acc: 0.4553 - val_loss: 1.0915 - val_acc: 0.4367\n","\n","Epoch 00180: val_acc did not improve from 0.48661\n","Epoch 181/1000\n"," - 40s - loss: 0.8546 - acc: 0.4552 - val_loss: 1.0653 - val_acc: 0.4430\n","\n","Epoch 00181: val_acc did not improve from 0.48661\n","Epoch 182/1000\n"," - 40s - loss: 0.8543 - acc: 0.4551 - val_loss: 1.0645 - val_acc: 0.4504\n","\n","Epoch 00182: val_acc did not improve from 0.48661\n","Epoch 183/1000\n"," - 40s - loss: 0.8542 - acc: 0.4556 - val_loss: 1.0928 - val_acc: 0.4315\n","\n","Epoch 00183: val_acc did not improve from 0.48661\n","Epoch 184/1000\n"," - 40s - loss: 0.8536 - acc: 0.4554 - val_loss: 1.0658 - val_acc: 0.4456\n","\n","Epoch 00184: val_acc did not improve from 0.48661\n","Epoch 185/1000\n"," - 40s - loss: 0.8535 - acc: 0.4553 - val_loss: 1.0510 - val_acc: 0.4594\n","\n","Epoch 00185: val_acc did not improve from 0.48661\n","Epoch 186/1000\n"," - 40s - loss: 0.8540 - acc: 0.4554 - val_loss: 1.1009 - val_acc: 0.4246\n","\n","Epoch 00186: val_acc did not improve from 0.48661\n","Epoch 187/1000\n"," - 40s - loss: 0.8537 - acc: 0.4556 - val_loss: 1.1089 - val_acc: 0.4251\n","\n","Epoch 00187: val_acc did not improve from 0.48661\n","Epoch 188/1000\n"," - 40s - loss: 0.8533 - acc: 0.4550 - val_loss: 1.0712 - val_acc: 0.4474\n","\n","Epoch 00188: val_acc did not improve from 0.48661\n","Epoch 189/1000\n"," - 40s - loss: 0.8530 - acc: 0.4565 - val_loss: 1.0709 - val_acc: 0.4477\n","\n","Epoch 00189: val_acc did not improve from 0.48661\n","Epoch 190/1000\n"," - 40s - loss: 0.8532 - acc: 0.4556 - val_loss: 1.1168 - val_acc: 0.4223\n","\n","Epoch 00190: val_acc did not improve from 0.48661\n","Epoch 191/1000\n"," - 40s - loss: 0.8528 - acc: 0.4561 - val_loss: 1.0915 - val_acc: 0.4290\n","\n","Epoch 00191: val_acc did not improve from 0.48661\n","Epoch 192/1000\n"," - 40s - loss: 0.8530 - acc: 0.4560 - val_loss: 1.0626 - val_acc: 0.4531\n","\n","Epoch 00192: val_acc did not improve from 0.48661\n","Epoch 193/1000\n"," - 40s - loss: 0.8523 - acc: 0.4565 - val_loss: 1.0358 - val_acc: 0.4687\n","\n","Epoch 00193: val_acc did not improve from 0.48661\n","Epoch 194/1000\n"," - 40s - loss: 0.8525 - acc: 0.4566 - val_loss: 1.0674 - val_acc: 0.4479\n","\n","Epoch 00194: val_acc did not improve from 0.48661\n","Epoch 195/1000\n"," - 40s - loss: 0.8523 - acc: 0.4561 - val_loss: 1.0417 - val_acc: 0.4654\n","\n","Epoch 00195: val_acc did not improve from 0.48661\n","Epoch 196/1000\n"," - 40s - loss: 0.8520 - acc: 0.4561 - val_loss: 1.1018 - val_acc: 0.4273\n","\n","Epoch 00196: val_acc did not improve from 0.48661\n","Epoch 197/1000\n"," - 40s - loss: 0.8523 - acc: 0.4564 - val_loss: 1.0665 - val_acc: 0.4450\n","\n","Epoch 00197: val_acc did not improve from 0.48661\n","Epoch 198/1000\n"," - 40s - loss: 0.8518 - acc: 0.4566 - val_loss: 1.0829 - val_acc: 0.4354\n","\n","Epoch 00198: val_acc did not improve from 0.48661\n","Epoch 199/1000\n"," - 40s - loss: 0.8517 - acc: 0.4573 - val_loss: 1.0516 - val_acc: 0.4605\n","\n","Epoch 00199: val_acc did not improve from 0.48661\n","Epoch 200/1000\n"," - 40s - loss: 0.8515 - acc: 0.4570 - val_loss: 1.0692 - val_acc: 0.4419\n","\n","Epoch 00200: val_acc did not improve from 0.48661\n","Epoch 201/1000\n"," - 41s - loss: 0.8515 - acc: 0.4572 - val_loss: 1.1037 - val_acc: 0.4289\n","\n","Epoch 00201: val_acc did not improve from 0.48661\n","Epoch 202/1000\n"," - 41s - loss: 0.8514 - acc: 0.4569 - val_loss: 1.1045 - val_acc: 0.4233\n","\n","Epoch 00202: val_acc did not improve from 0.48661\n","Epoch 203/1000\n"," - 40s - loss: 0.8511 - acc: 0.4570 - val_loss: 1.0978 - val_acc: 0.4262\n","\n","Epoch 00203: val_acc did not improve from 0.48661\n","Epoch 204/1000\n"," - 41s - loss: 0.8507 - acc: 0.4570 - val_loss: 1.0451 - val_acc: 0.4608\n","\n","Epoch 00204: val_acc did not improve from 0.48661\n","Epoch 205/1000\n"," - 40s - loss: 0.8508 - acc: 0.4572 - val_loss: 1.0563 - val_acc: 0.4546\n","\n","Epoch 00205: val_acc did not improve from 0.48661\n","Epoch 206/1000\n"," - 40s - loss: 0.8505 - acc: 0.4565 - val_loss: 1.0922 - val_acc: 0.4364\n","\n","Epoch 00206: val_acc did not improve from 0.48661\n","Epoch 207/1000\n"," - 40s - loss: 0.8504 - acc: 0.4575 - val_loss: 1.0766 - val_acc: 0.4436\n","\n","Epoch 00207: val_acc did not improve from 0.48661\n","Epoch 208/1000\n"," - 39s - loss: 0.8504 - acc: 0.4580 - val_loss: 1.1100 - val_acc: 0.4201\n","\n","Epoch 00208: val_acc did not improve from 0.48661\n","Epoch 209/1000\n"," - 39s - loss: 0.8498 - acc: 0.4572 - val_loss: 1.1051 - val_acc: 0.4247\n","\n","Epoch 00209: val_acc did not improve from 0.48661\n","Epoch 210/1000\n"," - 40s - loss: 0.8500 - acc: 0.4569 - val_loss: 1.0554 - val_acc: 0.4571\n","\n","Epoch 00210: val_acc did not improve from 0.48661\n","Epoch 211/1000\n"," - 41s - loss: 0.8499 - acc: 0.4586 - val_loss: 1.0677 - val_acc: 0.4435\n","\n","Epoch 00211: val_acc did not improve from 0.48661\n","Epoch 212/1000\n"," - 41s - loss: 0.8496 - acc: 0.4578 - val_loss: 1.0694 - val_acc: 0.4446\n","\n","Epoch 00212: val_acc did not improve from 0.48661\n","Epoch 213/1000\n"," - 41s - loss: 0.8495 - acc: 0.4585 - val_loss: 1.0792 - val_acc: 0.4367\n","\n","Epoch 00213: val_acc did not improve from 0.48661\n","Epoch 214/1000\n"," - 41s - loss: 0.8494 - acc: 0.4583 - val_loss: 1.0968 - val_acc: 0.4354\n","\n","Epoch 00214: val_acc did not improve from 0.48661\n","Epoch 215/1000\n"," - 41s - loss: 0.8490 - acc: 0.4577 - val_loss: 1.1004 - val_acc: 0.4214\n","\n","Epoch 00215: val_acc did not improve from 0.48661\n","Epoch 216/1000\n"," - 41s - loss: 0.8489 - acc: 0.4579 - val_loss: 1.0797 - val_acc: 0.4410\n","\n","Epoch 00216: val_acc did not improve from 0.48661\n","Epoch 217/1000\n"," - 42s - loss: 0.8488 - acc: 0.4585 - val_loss: 1.0663 - val_acc: 0.4464\n","\n","Epoch 00217: val_acc did not improve from 0.48661\n","Epoch 218/1000\n"," - 41s - loss: 0.8487 - acc: 0.4584 - val_loss: 1.0957 - val_acc: 0.4285\n","\n","Epoch 00218: val_acc did not improve from 0.48661\n","Epoch 219/1000\n"," - 40s - loss: 0.8485 - acc: 0.4584 - val_loss: 1.1021 - val_acc: 0.4284\n","\n","Epoch 00219: val_acc did not improve from 0.48661\n","Epoch 220/1000\n"," - 41s - loss: 0.8481 - acc: 0.4587 - val_loss: 1.0768 - val_acc: 0.4462\n","\n","Epoch 00220: val_acc did not improve from 0.48661\n","Epoch 221/1000\n"," - 41s - loss: 0.8482 - acc: 0.4594 - val_loss: 1.0642 - val_acc: 0.4521\n","\n","Epoch 00221: val_acc did not improve from 0.48661\n","Epoch 222/1000\n"," - 40s - loss: 0.8480 - acc: 0.4580 - val_loss: 1.0823 - val_acc: 0.4424\n","\n","Epoch 00222: val_acc did not improve from 0.48661\n","Epoch 223/1000\n"," - 41s - loss: 0.8478 - acc: 0.4589 - val_loss: 1.0970 - val_acc: 0.4374\n","\n","Epoch 00223: val_acc did not improve from 0.48661\n","Epoch 224/1000\n"," - 41s - loss: 0.8480 - acc: 0.4584 - val_loss: 1.0832 - val_acc: 0.4361\n","\n","Epoch 00224: val_acc did not improve from 0.48661\n","Epoch 225/1000\n"," - 40s - loss: 0.8475 - acc: 0.4588 - val_loss: 1.0888 - val_acc: 0.4354\n","\n","Epoch 00225: val_acc did not improve from 0.48661\n","Epoch 226/1000\n"," - 40s - loss: 0.8477 - acc: 0.4588 - val_loss: 1.0633 - val_acc: 0.4500\n","\n","Epoch 00226: val_acc did not improve from 0.48661\n","Epoch 227/1000\n"," - 40s - loss: 0.8473 - acc: 0.4594 - val_loss: 1.0798 - val_acc: 0.4444\n","\n","Epoch 00227: val_acc did not improve from 0.48661\n","Epoch 228/1000\n"," - 40s - loss: 0.8470 - acc: 0.4593 - val_loss: 1.0761 - val_acc: 0.4493\n","\n","Epoch 00228: val_acc did not improve from 0.48661\n","Epoch 229/1000\n"," - 41s - loss: 0.8468 - acc: 0.4596 - val_loss: 1.1135 - val_acc: 0.4250\n","\n","Epoch 00229: val_acc did not improve from 0.48661\n","Epoch 230/1000\n"," - 41s - loss: 0.8469 - acc: 0.4588 - val_loss: 1.0842 - val_acc: 0.4396\n","\n","Epoch 00230: val_acc did not improve from 0.48661\n","Epoch 231/1000\n"," - 41s - loss: 0.8465 - acc: 0.4596 - val_loss: 1.0865 - val_acc: 0.4361\n","\n","Epoch 00231: val_acc did not improve from 0.48661\n","Epoch 232/1000\n"," - 41s - loss: 0.8467 - acc: 0.4600 - val_loss: 1.0879 - val_acc: 0.4359\n","\n","Epoch 00232: val_acc did not improve from 0.48661\n","Epoch 233/1000\n"," - 41s - loss: 0.8465 - acc: 0.4596 - val_loss: 1.0515 - val_acc: 0.4629\n","\n","Epoch 00233: val_acc did not improve from 0.48661\n","Epoch 234/1000\n"," - 41s - loss: 0.8464 - acc: 0.4594 - val_loss: 1.0412 - val_acc: 0.4694\n","\n","Epoch 00234: val_acc did not improve from 0.48661\n","Epoch 235/1000\n"," - 41s - loss: 0.8460 - acc: 0.4601 - val_loss: 1.0648 - val_acc: 0.4488\n","\n","Epoch 00235: val_acc did not improve from 0.48661\n","Epoch 236/1000\n"," - 41s - loss: 0.8459 - acc: 0.4596 - val_loss: 1.0663 - val_acc: 0.4535\n","\n","Epoch 00236: val_acc did not improve from 0.48661\n","Epoch 237/1000\n"," - 41s - loss: 0.8460 - acc: 0.4599 - val_loss: 1.0631 - val_acc: 0.4548\n","\n","Epoch 00237: val_acc did not improve from 0.48661\n","Epoch 238/1000\n"," - 41s - loss: 0.8456 - acc: 0.4601 - val_loss: 1.0693 - val_acc: 0.4518\n","\n","Epoch 00238: val_acc did not improve from 0.48661\n","Epoch 239/1000\n"," - 41s - loss: 0.8455 - acc: 0.4603 - val_loss: 1.0840 - val_acc: 0.4376\n","\n","Epoch 00239: val_acc did not improve from 0.48661\n","Epoch 240/1000\n"," - 41s - loss: 0.8451 - acc: 0.4604 - val_loss: 1.1011 - val_acc: 0.4282\n","\n","Epoch 00240: val_acc did not improve from 0.48661\n","Epoch 241/1000\n"," - 41s - loss: 0.8449 - acc: 0.4604 - val_loss: 1.1075 - val_acc: 0.4270\n","\n","Epoch 00241: val_acc did not improve from 0.48661\n","Epoch 242/1000\n"," - 41s - loss: 0.8449 - acc: 0.4605 - val_loss: 1.0563 - val_acc: 0.4571\n","\n","Epoch 00242: val_acc did not improve from 0.48661\n","Epoch 243/1000\n"," - 41s - loss: 0.8449 - acc: 0.4607 - val_loss: 1.0819 - val_acc: 0.4369\n","\n","Epoch 00243: val_acc did not improve from 0.48661\n","Epoch 244/1000\n"," - 41s - loss: 0.8443 - acc: 0.4601 - val_loss: 1.0756 - val_acc: 0.4415\n","\n","Epoch 00244: val_acc did not improve from 0.48661\n","Epoch 245/1000\n"," - 41s - loss: 0.8444 - acc: 0.4606 - val_loss: 1.0834 - val_acc: 0.4396\n","\n","Epoch 00245: val_acc did not improve from 0.48661\n","Epoch 246/1000\n"," - 41s - loss: 0.8443 - acc: 0.4606 - val_loss: 1.0732 - val_acc: 0.4460\n","\n","Epoch 00246: val_acc did not improve from 0.48661\n","Epoch 247/1000\n"," - 41s - loss: 0.8444 - acc: 0.4604 - val_loss: 1.0490 - val_acc: 0.4653\n","\n","Epoch 00247: val_acc did not improve from 0.48661\n","Epoch 248/1000\n"," - 41s - loss: 0.8442 - acc: 0.4617 - val_loss: 1.0537 - val_acc: 0.4574\n","\n","Epoch 00248: val_acc did not improve from 0.48661\n","Epoch 249/1000\n"," - 41s - loss: 0.8437 - acc: 0.4615 - val_loss: 1.0720 - val_acc: 0.4483\n","\n","Epoch 00249: val_acc did not improve from 0.48661\n","Epoch 250/1000\n"," - 41s - loss: 0.8438 - acc: 0.4611 - val_loss: 1.0864 - val_acc: 0.4377\n","\n","Epoch 00250: val_acc did not improve from 0.48661\n","Epoch 251/1000\n"," - 41s - loss: 0.8435 - acc: 0.4614 - val_loss: 1.0763 - val_acc: 0.4458\n","\n","Epoch 00251: val_acc did not improve from 0.48661\n","Epoch 252/1000\n"," - 41s - loss: 0.8433 - acc: 0.4619 - val_loss: 1.0834 - val_acc: 0.4426\n","\n","Epoch 00252: val_acc did not improve from 0.48661\n","Epoch 253/1000\n"," - 41s - loss: 0.8432 - acc: 0.4616 - val_loss: 1.0803 - val_acc: 0.4398\n","\n","Epoch 00253: val_acc did not improve from 0.48661\n","Epoch 254/1000\n"," - 41s - loss: 0.8431 - acc: 0.4617 - val_loss: 1.1036 - val_acc: 0.4295\n","\n","Epoch 00254: val_acc did not improve from 0.48661\n","Epoch 255/1000\n"," - 41s - loss: 0.8433 - acc: 0.4612 - val_loss: 1.0873 - val_acc: 0.4393\n","\n","Epoch 00255: val_acc did not improve from 0.48661\n","Epoch 256/1000\n"," - 41s - loss: 0.8430 - acc: 0.4614 - val_loss: 1.0710 - val_acc: 0.4500\n","\n","Epoch 00256: val_acc did not improve from 0.48661\n","Epoch 257/1000\n"," - 41s - loss: 0.8428 - acc: 0.4612 - val_loss: 1.0996 - val_acc: 0.4331\n","\n","Epoch 00257: val_acc did not improve from 0.48661\n","Epoch 258/1000\n"," - 41s - loss: 0.8428 - acc: 0.4618 - val_loss: 1.1001 - val_acc: 0.4269\n","\n","Epoch 00258: val_acc did not improve from 0.48661\n","Epoch 259/1000\n"," - 41s - loss: 0.8424 - acc: 0.4616 - val_loss: 1.0781 - val_acc: 0.4477\n","\n","Epoch 00259: val_acc did not improve from 0.48661\n","Epoch 260/1000\n"," - 41s - loss: 0.8423 - acc: 0.4625 - val_loss: 1.0510 - val_acc: 0.4595\n","\n","Epoch 00260: val_acc did not improve from 0.48661\n","Epoch 261/1000\n"," - 42s - loss: 0.8422 - acc: 0.4620 - val_loss: 1.0975 - val_acc: 0.4366\n","\n","Epoch 00261: val_acc did not improve from 0.48661\n","Epoch 262/1000\n"," - 42s - loss: 0.8418 - acc: 0.4615 - val_loss: 1.0426 - val_acc: 0.4676\n","\n","Epoch 00262: val_acc did not improve from 0.48661\n","Epoch 263/1000\n"," - 42s - loss: 0.8420 - acc: 0.4623 - val_loss: 1.0808 - val_acc: 0.4423\n","\n","Epoch 00263: val_acc did not improve from 0.48661\n","Epoch 264/1000\n"," - 41s - loss: 0.8420 - acc: 0.4623 - val_loss: 1.0875 - val_acc: 0.4370\n","\n","Epoch 00264: val_acc did not improve from 0.48661\n","Epoch 265/1000\n"," - 42s - loss: 0.8416 - acc: 0.4631 - val_loss: 1.1145 - val_acc: 0.4196\n","\n","Epoch 00265: val_acc did not improve from 0.48661\n","Epoch 266/1000\n"," - 42s - loss: 0.8417 - acc: 0.4624 - val_loss: 1.0864 - val_acc: 0.4370\n","\n","Epoch 00266: val_acc did not improve from 0.48661\n","Epoch 267/1000\n"," - 42s - loss: 0.8412 - acc: 0.4632 - val_loss: 1.0898 - val_acc: 0.4372\n","\n","Epoch 00267: val_acc did not improve from 0.48661\n","Epoch 268/1000\n"," - 42s - loss: 0.8414 - acc: 0.4631 - val_loss: 1.0562 - val_acc: 0.4584\n","\n","Epoch 00268: val_acc did not improve from 0.48661\n","Epoch 269/1000\n"," - 42s - loss: 0.8409 - acc: 0.4627 - val_loss: 1.0981 - val_acc: 0.4305\n","\n","Epoch 00269: val_acc did not improve from 0.48661\n","Epoch 270/1000\n"," - 42s - loss: 0.8410 - acc: 0.4626 - val_loss: 1.0871 - val_acc: 0.4395\n","\n","Epoch 00270: val_acc did not improve from 0.48661\n","Epoch 271/1000\n"," - 41s - loss: 0.8409 - acc: 0.4631 - val_loss: 1.0675 - val_acc: 0.4467\n","\n","Epoch 00271: val_acc did not improve from 0.48661\n","Epoch 272/1000\n"," - 41s - loss: 0.8403 - acc: 0.4629 - val_loss: 1.0967 - val_acc: 0.4295\n","\n","Epoch 00272: val_acc did not improve from 0.48661\n","Epoch 273/1000\n"," - 41s - loss: 0.8403 - acc: 0.4628 - val_loss: 1.0812 - val_acc: 0.4423\n","\n","Epoch 00273: val_acc did not improve from 0.48661\n","Epoch 274/1000\n"," - 41s - loss: 0.8402 - acc: 0.4634 - val_loss: 1.0941 - val_acc: 0.4379\n","\n","Epoch 00274: val_acc did not improve from 0.48661\n","Epoch 275/1000\n"," - 41s - loss: 0.8402 - acc: 0.4628 - val_loss: 1.0830 - val_acc: 0.4401\n","\n","Epoch 00275: val_acc did not improve from 0.48661\n","Epoch 276/1000\n"," - 41s - loss: 0.8401 - acc: 0.4640 - val_loss: 1.1119 - val_acc: 0.4187\n","\n","Epoch 00276: val_acc did not improve from 0.48661\n","Epoch 277/1000\n"," - 41s - loss: 0.8400 - acc: 0.4634 - val_loss: 1.0376 - val_acc: 0.4687\n","\n","Epoch 00277: val_acc did not improve from 0.48661\n","Epoch 278/1000\n"," - 41s - loss: 0.8398 - acc: 0.4632 - val_loss: 1.0887 - val_acc: 0.4364\n","\n","Epoch 00278: val_acc did not improve from 0.48661\n","Epoch 279/1000\n"," - 41s - loss: 0.8393 - acc: 0.4637 - val_loss: 1.0769 - val_acc: 0.4459\n","\n","Epoch 00279: val_acc did not improve from 0.48661\n","Epoch 280/1000\n"," - 41s - loss: 0.8393 - acc: 0.4638 - val_loss: 1.0783 - val_acc: 0.4464\n","\n","Epoch 00280: val_acc did not improve from 0.48661\n","Epoch 281/1000\n"," - 41s - loss: 0.8393 - acc: 0.4636 - val_loss: 1.0942 - val_acc: 0.4330\n","\n","Epoch 00281: val_acc did not improve from 0.48661\n","Epoch 282/1000\n"," - 41s - loss: 0.8394 - acc: 0.4633 - val_loss: 1.1048 - val_acc: 0.4352\n","\n","Epoch 00282: val_acc did not improve from 0.48661\n","Epoch 283/1000\n"," - 41s - loss: 0.8391 - acc: 0.4633 - val_loss: 1.0769 - val_acc: 0.4511\n","\n","Epoch 00283: val_acc did not improve from 0.48661\n","Epoch 284/1000\n"," - 41s - loss: 0.8389 - acc: 0.4640 - val_loss: 1.0857 - val_acc: 0.4414\n","\n","Epoch 00284: val_acc did not improve from 0.48661\n","Epoch 285/1000\n"," - 41s - loss: 0.8387 - acc: 0.4641 - val_loss: 1.0712 - val_acc: 0.4511\n","\n","Epoch 00285: val_acc did not improve from 0.48661\n","Epoch 286/1000\n"," - 41s - loss: 0.8390 - acc: 0.4641 - val_loss: 1.0758 - val_acc: 0.4459\n","\n","Epoch 00286: val_acc did not improve from 0.48661\n","Epoch 287/1000\n"," - 41s - loss: 0.8385 - acc: 0.4642 - val_loss: 1.0829 - val_acc: 0.4391\n","\n","Epoch 00287: val_acc did not improve from 0.48661\n","Epoch 288/1000\n"," - 41s - loss: 0.8383 - acc: 0.4643 - val_loss: 1.1261 - val_acc: 0.4151\n","\n","Epoch 00288: val_acc did not improve from 0.48661\n","Epoch 289/1000\n"," - 41s - loss: 0.8382 - acc: 0.4640 - val_loss: 1.0870 - val_acc: 0.4462\n","\n","Epoch 00289: val_acc did not improve from 0.48661\n","Epoch 290/1000\n"," - 41s - loss: 0.8380 - acc: 0.4641 - val_loss: 1.0807 - val_acc: 0.4417\n","\n","Epoch 00290: val_acc did not improve from 0.48661\n","Epoch 291/1000\n"," - 41s - loss: 0.8382 - acc: 0.4647 - val_loss: 1.0970 - val_acc: 0.4314\n","\n","Epoch 00291: val_acc did not improve from 0.48661\n","Epoch 292/1000\n"," - 41s - loss: 0.8380 - acc: 0.4648 - val_loss: 1.1060 - val_acc: 0.4296\n","\n","Epoch 00292: val_acc did not improve from 0.48661\n","Epoch 293/1000\n"," - 41s - loss: 0.8379 - acc: 0.4649 - val_loss: 1.0636 - val_acc: 0.4546\n","\n","Epoch 00293: val_acc did not improve from 0.48661\n","Epoch 294/1000\n"," - 41s - loss: 0.8378 - acc: 0.4645 - val_loss: 1.0473 - val_acc: 0.4660\n","\n","Epoch 00294: val_acc did not improve from 0.48661\n","Epoch 295/1000\n"," - 41s - loss: 0.8376 - acc: 0.4645 - val_loss: 1.0853 - val_acc: 0.4407\n","\n","Epoch 00295: val_acc did not improve from 0.48661\n","Epoch 296/1000\n"," - 41s - loss: 0.8375 - acc: 0.4646 - val_loss: 1.0810 - val_acc: 0.4430\n","\n","Epoch 00296: val_acc did not improve from 0.48661\n","Epoch 297/1000\n"," - 41s - loss: 0.8371 - acc: 0.4648 - val_loss: 1.0759 - val_acc: 0.4431\n","\n","Epoch 00297: val_acc did not improve from 0.48661\n","Epoch 298/1000\n"," - 41s - loss: 0.8372 - acc: 0.4656 - val_loss: 1.1020 - val_acc: 0.4271\n","\n","Epoch 00298: val_acc did not improve from 0.48661\n","Epoch 299/1000\n"," - 41s - loss: 0.8366 - acc: 0.4650 - val_loss: 1.0715 - val_acc: 0.4518\n","\n","Epoch 00299: val_acc did not improve from 0.48661\n","Epoch 300/1000\n"," - 41s - loss: 0.8368 - acc: 0.4653 - val_loss: 1.0867 - val_acc: 0.4360\n","\n","Epoch 00300: val_acc did not improve from 0.48661\n","Epoch 301/1000\n"," - 41s - loss: 0.8366 - acc: 0.4653 - val_loss: 1.0725 - val_acc: 0.4443\n","\n","Epoch 00301: val_acc did not improve from 0.48661\n","Epoch 302/1000\n"," - 41s - loss: 0.8367 - acc: 0.4647 - val_loss: 1.0889 - val_acc: 0.4392\n","\n","Epoch 00302: val_acc did not improve from 0.48661\n","Epoch 303/1000\n"," - 41s - loss: 0.8364 - acc: 0.4653 - val_loss: 1.0522 - val_acc: 0.4658\n","\n","Epoch 00303: val_acc did not improve from 0.48661\n","Epoch 304/1000\n"," - 41s - loss: 0.8361 - acc: 0.4657 - val_loss: 1.0920 - val_acc: 0.4373\n","\n","Epoch 00304: val_acc did not improve from 0.48661\n","Epoch 305/1000\n"," - 41s - loss: 0.8361 - acc: 0.4660 - val_loss: 1.1080 - val_acc: 0.4331\n","\n","Epoch 00305: val_acc did not improve from 0.48661\n","Epoch 306/1000\n"," - 41s - loss: 0.8358 - acc: 0.4657 - val_loss: 1.0537 - val_acc: 0.4646\n","\n","Epoch 00306: val_acc did not improve from 0.48661\n","Epoch 307/1000\n"," - 41s - loss: 0.8361 - acc: 0.4654 - val_loss: 1.0671 - val_acc: 0.4500\n","\n","Epoch 00307: val_acc did not improve from 0.48661\n","Epoch 308/1000\n"," - 41s - loss: 0.8356 - acc: 0.4659 - val_loss: 1.0737 - val_acc: 0.4510\n","\n","Epoch 00308: val_acc did not improve from 0.48661\n","Epoch 309/1000\n"," - 41s - loss: 0.8358 - acc: 0.4661 - val_loss: 1.0910 - val_acc: 0.4383\n","\n","Epoch 00309: val_acc did not improve from 0.48661\n","Epoch 310/1000\n"," - 41s - loss: 0.8355 - acc: 0.4658 - val_loss: 1.0480 - val_acc: 0.4631\n","\n","Epoch 00310: val_acc did not improve from 0.48661\n","Epoch 311/1000\n"," - 41s - loss: 0.8352 - acc: 0.4658 - val_loss: 1.0653 - val_acc: 0.4539\n","\n","Epoch 00311: val_acc did not improve from 0.48661\n","Epoch 312/1000\n"," - 41s - loss: 0.8352 - acc: 0.4666 - val_loss: 1.0827 - val_acc: 0.4414\n","\n","Epoch 00312: val_acc did not improve from 0.48661\n","Epoch 313/1000\n"," - 41s - loss: 0.8352 - acc: 0.4664 - val_loss: 1.1201 - val_acc: 0.4207\n","\n","Epoch 00313: val_acc did not improve from 0.48661\n","Epoch 314/1000\n"," - 41s - loss: 0.8346 - acc: 0.4662 - val_loss: 1.0846 - val_acc: 0.4430\n","\n","Epoch 00314: val_acc did not improve from 0.48661\n","Epoch 315/1000\n"," - 41s - loss: 0.8348 - acc: 0.4667 - val_loss: 1.0545 - val_acc: 0.4621\n","\n","Epoch 00315: val_acc did not improve from 0.48661\n","Epoch 316/1000\n"," - 41s - loss: 0.8349 - acc: 0.4667 - val_loss: 1.1038 - val_acc: 0.4338\n","\n","Epoch 00316: val_acc did not improve from 0.48661\n","Epoch 317/1000\n"," - 41s - loss: 0.8343 - acc: 0.4666 - val_loss: 1.0916 - val_acc: 0.4339\n","\n","Epoch 00317: val_acc did not improve from 0.48661\n","Epoch 318/1000\n"," - 41s - loss: 0.8345 - acc: 0.4664 - val_loss: 1.0847 - val_acc: 0.4446\n","\n","Epoch 00318: val_acc did not improve from 0.48661\n","Epoch 319/1000\n"," - 41s - loss: 0.8343 - acc: 0.4662 - val_loss: 1.0805 - val_acc: 0.4432\n","\n","Epoch 00319: val_acc did not improve from 0.48661\n","Epoch 320/1000\n"," - 41s - loss: 0.8344 - acc: 0.4664 - val_loss: 1.0989 - val_acc: 0.4371\n","\n","Epoch 00320: val_acc did not improve from 0.48661\n","Epoch 321/1000\n"," - 41s - loss: 0.8337 - acc: 0.4672 - val_loss: 1.0987 - val_acc: 0.4339\n","\n","Epoch 00321: val_acc did not improve from 0.48661\n","Epoch 322/1000\n"," - 41s - loss: 0.8339 - acc: 0.4668 - val_loss: 1.0753 - val_acc: 0.4515\n","\n","Epoch 00322: val_acc did not improve from 0.48661\n","Epoch 323/1000\n"," - 41s - loss: 0.8339 - acc: 0.4676 - val_loss: 1.1140 - val_acc: 0.4225\n","\n","Epoch 00323: val_acc did not improve from 0.48661\n","Epoch 324/1000\n"," - 41s - loss: 0.8337 - acc: 0.4668 - val_loss: 1.0994 - val_acc: 0.4342\n","\n","Epoch 00324: val_acc did not improve from 0.48661\n","Epoch 325/1000\n"," - 40s - loss: 0.8336 - acc: 0.4671 - val_loss: 1.0722 - val_acc: 0.4548\n","\n","Epoch 00325: val_acc did not improve from 0.48661\n","Epoch 326/1000\n"," - 41s - loss: 0.8337 - acc: 0.4674 - val_loss: 1.0743 - val_acc: 0.4539\n","\n","Epoch 00326: val_acc did not improve from 0.48661\n","Epoch 327/1000\n"," - 40s - loss: 0.8331 - acc: 0.4669 - val_loss: 1.0830 - val_acc: 0.4457\n","\n","Epoch 00327: val_acc did not improve from 0.48661\n","Epoch 328/1000\n"," - 40s - loss: 0.8334 - acc: 0.4678 - val_loss: 1.0724 - val_acc: 0.4469\n","\n","Epoch 00328: val_acc did not improve from 0.48661\n","Epoch 329/1000\n"," - 41s - loss: 0.8329 - acc: 0.4676 - val_loss: 1.0268 - val_acc: 0.4808\n","\n","Epoch 00329: val_acc did not improve from 0.48661\n","Epoch 330/1000\n"," - 41s - loss: 0.8330 - acc: 0.4669 - val_loss: 1.0834 - val_acc: 0.4416\n","\n","Epoch 00330: val_acc did not improve from 0.48661\n","Epoch 331/1000\n"," - 41s - loss: 0.8328 - acc: 0.4674 - val_loss: 1.0847 - val_acc: 0.4465\n","\n","Epoch 00331: val_acc did not improve from 0.48661\n","Epoch 332/1000\n"," - 41s - loss: 0.8326 - acc: 0.4677 - val_loss: 1.0721 - val_acc: 0.4508\n","\n","Epoch 00332: val_acc did not improve from 0.48661\n","Epoch 333/1000\n"," - 41s - loss: 0.8327 - acc: 0.4676 - val_loss: 1.0597 - val_acc: 0.4606\n","\n","Epoch 00333: val_acc did not improve from 0.48661\n","Epoch 334/1000\n"," - 41s - loss: 0.8327 - acc: 0.4672 - val_loss: 1.0675 - val_acc: 0.4513\n","\n","Epoch 00334: val_acc did not improve from 0.48661\n","Epoch 335/1000\n"," - 41s - loss: 0.8322 - acc: 0.4679 - val_loss: 1.0841 - val_acc: 0.4389\n","\n","Epoch 00335: val_acc did not improve from 0.48661\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"a01LFE7QEp70","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}