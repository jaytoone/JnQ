{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Press model num :72\n",
      "5391\n",
      "10431\n",
      "15802\n",
      "21444\n",
      "(21444, 300, 300, 3)\n",
      "(21444, 1)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"3\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.misc \n",
    "from math import sqrt \n",
    "import itertools\n",
    "from IPython.display import display\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "input_data_length = 30\n",
    "model_num = input('Press model num :')\n",
    "\n",
    "file_cnt = 1\n",
    "while True:\n",
    "    try:\n",
    "        result_x = np.load('Made_X/Made_X %s_%s %s.npy' % (input_data_length,\n",
    "                                                          69, file_cnt)).astype(np.float) / 255.\n",
    "        result_y = np.load('Made_X/Made_Y %s_%s %s.npy' % (input_data_length,\n",
    "                                                      model_num, file_cnt)).astype(np.float).reshape(-1, 1)\n",
    "        if file_cnt == 1:            \n",
    "            Made_X = result_x\n",
    "            Made_Y = result_y\n",
    "        else:            \n",
    "            Made_X = np.vstack((Made_X, result_x))\n",
    "            Made_Y = np.vstack((Made_Y, result_y))\n",
    "        print(Made_X.shape[0])\n",
    "        if len(Made_X) > 20000:\n",
    "            break\n",
    "        \n",
    "        file_cnt += 1\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        break\n",
    "\n",
    "print(Made_X.shape)\n",
    "print(Made_Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15010, 300, 300, 3)\n",
      "(6434, 300, 300, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/lab23/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/lab23/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/lab23/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/lab23/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/lab23/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/lab23/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/lab23/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/lab23/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/lab23/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/lab23/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/lab23/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/lab23/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. ... 1. 1. 1.]\n",
      "{0: 1.1040011768167108, 1: 0.9139064783244033}\n",
      "(15010, 2)\n",
      "(6434, 2)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAToAAADnCAYAAACOlZoZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8li6FKAAAL0ElEQVR4nO3dPXLqSBfG8dNvTQDRTZy4pgrCyVF+d3AdzSZmF97FrGIS3xXMAlA+IVRNkTiZCGd6A6mFACH01eru0/9flQt8AV/ZLR5Of0gyRVEIAGj2P98bAACuEXQA1CPoAKhH0AFQj6ADoN4vTx5PfUrW+N4AR2hXvWjbFlR0ANQj6ACoR9ABUI+gA6AeQQdAPYIOyzKaJzyxBGNE8nzYa54tLwGAMNQfksNX0FDRAVCPoAOgHkEHQL10g84YBsZ9OZ1EZNygMjBGukEHf15ffW8BEkPQwbvff/e9BdCOoIN3f/3lewugHUEHIHwTx9PTXTDM1c+AZKRV0THLCiQp3YoOQJQOB5HNZthr0qroLBZvAdEaGnIiqQbdbud7CwAsKM2gA5CUtIKOmVYgSbqDjoMp48KsOBzRHXSIV3XgP1BbrcqvEVhegjBx4D9unc+jX6o76BiTA+JmTPk+nvheXqbretsNWWLcjLE5AJVlgu62G+JiHdttsLFWThU+txSqJp+WmIOKazLijz8u99drEWNkvRY5HuUSbMzcqZRlvrcAi7Lv45mGn+IKun//vdz/+qpvPj8bz2FcDohTnjubbXcadGOWsdkgpzBDm6v94rbrk+dyPFYVPuKTZZdhrpkLluBmXe3v9/T3tE8gEGFlmWyrixtT2KNp2a5rW3nXVar2KAkPB+YdNGut7O1+QZr5YQfGb7pfIffC3AVdW0C1JVKPhaFdWTfmlC0AJrAD4727X/6567pmmYhM/ANUf8DMiOz3IruPj+nbBWCcmWdClxTXrCuAZdnuVOQLGZ0GXVEwfhaVxiDL7TDM3fOqGc4QfP9e3dnv63/jxDUzsQsYd7tZDsXyJYqKbr8nMJd2tz7RyvN6Bmi7Lb/1PQj97Vt1h53Eq6II9wNm/jE6exDujNh/A2DbtdEYvZrZwf7QJdKCIzzH47SZvqrd6+bP84dv5LrNHDbe/BUdexoQLzse8dtv5f2B7+fVSuTlRerX1S/3fAyfu1lXAg9tHOwXTMbPpFl9jzz324RTxjkVxRgddOMzEa4RdJhNiIPQGEDxJ05wx7oiPvb9kWUt75Uhb56W594uvi9v9b4h4QYVHTq5nhF7eXHyYxGg47Gq+j1c+IiKDt4o7inhVlHIRuyKFTenYupCRQdAPYIOSE19/YF0EHRAah4e36cXQQdAPYIOgHrMugKpSXC6e76KjmXxQLBSf3uaojvd04v+awFf7mOS9nZtHNQ99Sw9gdPariIP2nbhs2X51Nq24yq61D8eEqA45JAgKrpuWj/5n1Z0ymltVxEquhkrOpEwzqGNeSXyTkB6xgfdbldezMHDAboAhmlcNyhJdF27aS1ZaVe9aNsWLBgGoB5BB0A9gg6AegQdAPWeTUYAQPSo6ACoR9ABUI+gA6AeQQdAPYIOgHoEHQD1CDoA6hF0ANQj6ACoR9ABUI+gA6AeQQdAvWcXsE79iH+tZ6KlXfWibVtQ0QFQj6ADoB5BB0A9gg6AegQdAPX0Bp3RPLEGYAi9QYd4vL2JCJ9NcIegAxCGPG+/3/b9QG6D7nRy+uMRkapqu2LM5B0Yiux27ffbvh/IbdC9vjr98YjIz5/lrTH3+fbff142CenQ23XlerXx+PbN9xbAleYn2unU3T11SG/QAfDHziw1u5yvr+X37+/3jzlG0GFZVNr6NCuz47G87WpnG3QLIugATJNll/vbbZATTHqCjkVYYaJdEAA9QYew/fjhewuQMN1BZ9fxBVhKq2fXkLSM1axWIi8v9y9h+A6u6A46u45vwdkdPHc+i2w23c+xY9rAHHQHHcLx8THo6YGOaeOZogiysHh2zQhgWfXkBf1YzIeKDoB68Vd0pxPH1IbkeBT5/PS9FcAVf0F3OpVfU/vzNuSYsgvDdlve9mkP2gwL8Rd0r69UYtoYI3I4DK/omhMV9vXZ46f33pb9PsiBcSwv/q4r/DOmrM5shfZs7UiXzWbS69fr8vY8fgugEEEHPYyRL2Zr0SKcWdchi6ZYYAVggHCCbshYCuMuAAYIJ+gAwBHG6BA0O8/R12pV3flysjmIFEGHID06w8kz53q6lUkJXBB0CNJ5yPqQoWUfksMYHfQg7PAAQQdAPXdBx7UCAASCig7jrdecChhRcBd0+72zH92FQnJBX1/lAfgOL3zjaTeCMu6CjqMXMAN2I8yBrisA9Qg6TPfrr87/C4YkMAVBh+n+/HO5/4vEwwhBB529BvIQfdaMGnP5Qhzqdi2Ky35hG5CFwngi6KADgDkQdOjl7c33FgDjzX5QP8dXwzX2Lww1e0XnZIFnNVB3NaZ2OnFKdQC9zF7RzbnA8/LJ3fJDJ1wucb+/nOuMq+INlOeXv/3hMO2KX8BCojof3VxdFkJtgNuxiOYfj5BDJJiMAKAeQYd2N2cm+fnT47YAExF0aGfPTFJ1W5npxJ2IJgMJOsSL9PUrosFugg6AegQdruV5OdN6OET1iY2ZnU7jHgsUQYdrdoEhS0d0aq66vz2rRfP7rjWqI9ev+kTQ4SLPy4Bj7EuvZtvetrPidifocJFlvrcAMYogIJMPOoaiGiLYYTFScynI7RhbRMtExjJF986d+p6v9dSctKtetG2L5Cs6APoRdADUI+gAqEfQAVDv2WQEAESPig6AegQdAPUIOgDqEXQA1CPoAKhH0AFQj6ADoB5BB0A9gg6AegQdAPUIOgDqEXQA1PvlyeOpH/Gv9Uy0tKtetG0LKjoA6hF0ANQj6ACoR9ABUI+gA6AeQQdgEGOur3ltTMeDNw8Z0/2crv9zCoIOuqzX5RfQ8GwdnV/GiOz35f3dzu+2IA5fX763AAGKo6LLMpE8r8tXY0TkeCzL39NJRAZXwvCp6rqM6MEAo8QRdG222zIAX19F5FLwTe3LIzBVg3a169vbQtuCaMUbdIhXUVwNRbQGlS33qusOt15+uEq/nz8dbCNUIeiwDJtmjb6qzTsbVL26sXkux+P1P9kQNKb6houy4wZBh2Vl2ZiHrp603baHIvmGRwg6AOoRdAjL+3vZB12tRF5e7h8vissQHyUcego36Jg+1aklnL5/bzz0/l7eOZ9FNptFNw16hRt00KVjavTvvxfcDiSJoAOgXtiHgCE+x6PI5yeH7KWqY9z08tDwsdWpw7FhB5397Rh0jsd2W97SZgiIuq6rPQcAcxmerdcixyPtgCCEF3T2nWETa6CuHlPzTVefGwtu/PPP9aypnVoFPDBFdxdj+f6HMQ+7PfYhY0QKe1WzJ8999G/2MKKNHLuWMWiNQnftarrbJRBa21VkgfesPXta80QaATV3Wpc7tIH46PjJzabKN9ZqAeqpDToAsNIMOgbngKSEE3RLju0ENKAAwD3/6+jsyGaP8LleVkdYAehn0Yru6kyy9poP0IUTXwavWuJ4xxgpT6rQ9pw+wz0BDwnNH3T2l21cuKa+bbLXfLg5rTYAt76+yqP0WlVBd/ecyD+8xgddnpeh9ui81rtdfeEae8u5/ZUL+BMdaZs+RmfXofVYNWgXx5cLft07HFgmBzxy+5Z9VrQ9mi+svw+46puv69rjkC173rH9XsoUGnmYV1+E3DK43CBCN1/QDRhn2+2kTCHG5oB4RTSZ6H95CQAvhvY0754fUaEyqqIzRspfMuA+ORZQHUzMJBNCF86REYjWjx/VHT74ECiCDpN9fPjeAqAbQQdAPYIOgHoEHQD1CDoA6hF06KU++iGiRaKAxYJhdKsOiKzXytkzzoiwnATRoKLDMI6PTwZcIOjQS31Z1ogO+wGsJIKOYaXp7JlngBiNCrrYhmayzPcWxKfrmrhAbJKo6DDcaiXy8uJ7K4B5MOuKe3ku53M1Fhdb+Q60oKLDtTynrw91qOhwYS8iwBISKENFh3ssIYEyBB0A9Qg6AOoRdADUI+gAqJfErKudRGQy8Qn+QEk4HJ5f3L3Pc2Jiiu4FoamvFjW+N8AR2lUv2rYFXVcA6hF0ANQj6ACoR9ABUO/ZZAQARI+KDoB6BB0A9Qg6AOoRdADUI+gAqEfQAVDv/zMbCj9ZYPzzAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 9 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4070: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input (InputLayer)           (None, 300, 300, 3)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 300, 300, 64)      1792      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 300, 300, 64)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 150, 150, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 150, 150, 128)     73856     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 150, 150, 128)     0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 75, 75, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 75, 75, 256)       295168    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 75, 75, 256)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 37, 37, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 37, 37, 128)       32896     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 37, 37, 128)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 18, 18, 128)       0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 41472)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                2654272   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)    (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 130       \n",
      "=================================================================\n",
      "Total params: 3,058,114\n",
      "Trainable params: 3,058,114\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/keras/callbacks/tensorboard_v1.py:200: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/keras/callbacks/tensorboard_v1.py:203: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n",
      "\n",
      "Epoch 1/100\n",
      " - 144s - loss: 0.6047 - accuracy: 0.6660 - val_loss: 0.5930 - val_accuracy: 0.6823\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.59300, saving model to model/rapid_ascending 30_72_cross.hdf5\n",
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/keras/callbacks/tensorboard_v1.py:343: The name tf.Summary is deprecated. Please use tf.compat.v1.Summary instead.\n",
      "\n",
      "Epoch 2/100\n",
      " - 143s - loss: 0.5439 - accuracy: 0.7237 - val_loss: 0.8790 - val_accuracy: 0.6647\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.59300\n",
      "Epoch 3/100\n",
      " - 143s - loss: 0.4668 - accuracy: 0.7755 - val_loss: 0.0459 - val_accuracy: 0.6602\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.59300 to 0.04594, saving model to model/rapid_ascending 30_72_cross.hdf5\n",
      "Epoch 4/100\n",
      " - 143s - loss: 0.3514 - accuracy: 0.8451 - val_loss: 0.4362 - val_accuracy: 0.6410\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.04594\n",
      "Epoch 5/100\n",
      " - 143s - loss: 0.2198 - accuracy: 0.9119 - val_loss: 2.5660 - val_accuracy: 0.6192\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.04594\n",
      "Epoch 6/100\n",
      " - 143s - loss: 0.1236 - accuracy: 0.9541 - val_loss: 0.0711 - val_accuracy: 0.6217\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.04594\n",
      "Epoch 7/100\n",
      " - 142s - loss: 0.0650 - accuracy: 0.9775 - val_loss: 3.2197 - val_accuracy: 0.6144\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.04594\n",
      "Epoch 8/100\n",
      " - 143s - loss: 0.0435 - accuracy: 0.9838 - val_loss: 0.0819 - val_accuracy: 0.6212\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.04594\n",
      "Epoch 9/100\n",
      " - 143s - loss: 0.0329 - accuracy: 0.9887 - val_loss: 2.7872 - val_accuracy: 0.6312\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.04594\n",
      "Epoch 10/100\n",
      " - 143s - loss: 0.0373 - accuracy: 0.9869 - val_loss: 15.4072 - val_accuracy: 0.6156\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.04594\n",
      "Epoch 11/100\n",
      " - 143s - loss: 0.0191 - accuracy: 0.9936 - val_loss: 0.1631 - val_accuracy: 0.6076\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.04594\n",
      "Epoch 12/100\n",
      " - 143s - loss: 0.0217 - accuracy: 0.9924 - val_loss: 0.2073 - val_accuracy: 0.6161\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.04594\n",
      "Epoch 13/100\n",
      " - 143s - loss: 0.0244 - accuracy: 0.9916 - val_loss: 0.9733 - val_accuracy: 0.6205\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.04594\n",
      "Epoch 14/100\n"
     ]
    }
   ],
   "source": [
    "row = Made_X.shape[1]\n",
    "col = Made_X.shape[2]\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(Made_X, Made_Y, test_size=0.3,\n",
    "                                                   shuffle=False)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_val.shape)\n",
    "\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing.image import ImageDataGenerator \n",
    "\n",
    "# Data Class Weight\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "print(Y_train[:, 0])\n",
    "class_weights = class_weight.compute_class_weight('balanced', \n",
    "                                                  np.unique(Y_train[:, 0]),\n",
    "                                                  Y_train[:, 0])\n",
    "class_weights = dict(enumerate(class_weights))\n",
    "print(class_weights)\n",
    "\n",
    "num_classes = 2\n",
    "Y_train = np_utils.to_categorical(Y_train, num_classes)\n",
    "Y_val = np_utils.to_categorical(Y_val, num_classes)\n",
    "print(Y_train.shape)\n",
    "print(Y_val.shape)\n",
    "\n",
    "datagen = ImageDataGenerator( \n",
    "#     rotation_range = 60,\n",
    "#     zoom_range = 0.6,\n",
    "#     shear_range = 0.6,\n",
    "#     horizontal_flip = True,\n",
    "#     width_shift_range=0.1,\n",
    "#     height_shift_range=0.1,\n",
    "    fill_mode = 'nearest'\n",
    "    )\n",
    "\n",
    "testgen = ImageDataGenerator( \n",
    "    )\n",
    "datagen.fit(X_train)\n",
    "batch_size = 16\n",
    "\n",
    "for X_batch, _ in datagen.flow(X_train, Y_train, batch_size=9):\n",
    "    for i in range(0, 9): \n",
    "        plt.axis('off') \n",
    "        plt.subplot(330 + 1 + i) \n",
    "        plt.imshow(X_batch[i], cmap=plt.get_cmap('gray'))\n",
    "    plt.axis('off') \n",
    "    plt.show() \n",
    "    break\n",
    "    \n",
    "    \n",
    "train_flow = datagen.flow(X_train, Y_train, batch_size=batch_size) \n",
    "val_flow = testgen.flow(X_val, Y_val, batch_size=batch_size) \n",
    "\n",
    "\n",
    "from keras.utils import plot_model\n",
    "import keras.backend as K\n",
    "from keras.models import Model, Sequential\n",
    "import keras.layers as layers\n",
    "# from keras.layers import LSTM, TimeDistributed, Input, Dense, Flatten, Dropout, BatchNormalization, Conv1D, LeakyReLU\n",
    "# from keras.layers.convolutional import Conv2D\n",
    "# from keras.layers.pooling import MaxPooling2D, MaxPooling1D\n",
    "# from keras.layers.merge import concatenate\n",
    "from keras.optimizers import Adam, SGD\n",
    "from keras.regularizers import l1, l2\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def FER_Model(input_shape=(row, col, 3)):\n",
    "    # first input model\n",
    "    visible = layers.Input(shape=input_shape, name='input')\n",
    "    \n",
    "    net = layers.Conv2D(64, kernel_size=3, padding='same')(visible)\n",
    "    # net = layers.Activation('relu')(net)\n",
    "    net = layers.LeakyReLU()(net)\n",
    "    net = layers.MaxPool2D(pool_size=2)(net)\n",
    "\n",
    "    shortcut_1 = net\n",
    "\n",
    "    net = layers.Conv2D(128, kernel_size=3, padding='same')(net)\n",
    "    # net = layers.Activation('relu')(net)\n",
    "    net = layers.LeakyReLU()(net)\n",
    "    net = layers.MaxPool2D(pool_size=2)(net)\n",
    "\n",
    "    shortcut_2 = net\n",
    "\n",
    "    net = layers.Conv2D(256, kernel_size=3, padding='same')(net)\n",
    "    # net = layers.Activation('relu')(net)\n",
    "    net = layers.LeakyReLU()(net)\n",
    "    net = layers.MaxPool2D(pool_size=2)(net)\n",
    "\n",
    "    shortcut_3 = net\n",
    "\n",
    "    net = layers.Conv2D(128, kernel_size=1, padding='same')(net)\n",
    "    # net = layers.Activation('relu')(net)\n",
    "    net = layers.LeakyReLU()(net)\n",
    "    net = layers.MaxPool2D(pool_size=2)(net)\n",
    "\n",
    "    net = layers.Flatten()(net)\n",
    "    net = layers.Dense(64)(net)\n",
    "    net = layers.LeakyReLU()(net)\n",
    "    net = layers.Dense(num_classes, activation='softmax')(net)\n",
    "\n",
    "    # create model \n",
    "    model = Model(inputs =visible, outputs = net)\n",
    "    # summary layers\n",
    "    print(model.summary())\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = FER_Model()\n",
    "# from keras.models import load_model\n",
    "# model = load_model('model/rapid_ascending %s_%s_cross.hdf5' % (input_data_length, model_num))\n",
    "opt = Adam(lr=0.0001, decay=1e-6)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "       \n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, TensorBoard\n",
    "filepath=\"model/rapid_ascending %s_%s_cross.hdf5\" % (input_data_length, model_num)\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "checkpoint2 = TensorBoard(log_dir='Tensorboard_graph',\n",
    "                          histogram_freq=0,\n",
    "                          write_graph=True,\n",
    "                          write_images=True)\n",
    "checkpoint3 = EarlyStopping(monitor='val_loss', patience=30)\n",
    "callbacks_list = [checkpoint, checkpoint2, checkpoint3]\n",
    "\n",
    "# keras.callbacks.Callback 로 부터 log 를 받아와 history log 를 작성할 수 있다.\n",
    "\n",
    "# we iterate 200 times over the entire training set\n",
    "num_epochs = 100\n",
    "history = model.fit_generator(train_flow, \n",
    "                    steps_per_epoch=len(X_train) / batch_size, \n",
    "                    epochs=num_epochs,  \n",
    "                    verbose=2,  \n",
    "                    callbacks=callbacks_list,\n",
    "                    class_weight=class_weights,\n",
    "                    validation_data=val_flow,  \n",
    "                    validation_steps=len(X_val) / batch_size,\n",
    "                    shuffle=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "model = load_model('model/rapid_ascending %s_%s_cross.hdf5' % (input_data_length, model_num))\n",
    "# model = load_model('/content/gdrive/My Drive/Colab Notebooks/model/rapid_ascending %s.hdf5' % input_data_length)\n",
    "# loss = model.evaluate_generator(test_flow, steps=len(X_test) / batch_size) \n",
    "# print(\"Test Loss \" + str(loss[0]))\n",
    "# print(\"Test Acc: \" + str(loss[1]))\n",
    "\n",
    "# loss = model.evaluate(X_val, Y_val) \n",
    "# print(X_val.shape)\n",
    "# print(Y_val.shape)\n",
    "\n",
    "# print(\"Val Loss \" + str(loss[0]))\n",
    "# print(\"Val Acc: \" + str(loss[1]))\n",
    "\n",
    "#     Prediction    #\n",
    "Y_pred_ = model.predict(X_test, verbose=1)\n",
    "\n",
    "# # Y_pred = Y_pred_[:,[-1]]\n",
    "# # print(Y_pred.shape)\n",
    "# # print(Y_test.shape)\n",
    "# Y_pred = np.argmax(Y_pred_, axis=1)\n",
    "# t_te = np.argmax(Y_test, axis=1)\n",
    "\n",
    "# #     Manual processing     #\n",
    "# Y_pred_one = Y_pred_[:, [-1]]\n",
    "# print(Y_pred_)\n",
    "# max_value = np.max(Y_pred_, axis=0)\n",
    "# print(max_value)\n",
    "\n",
    "# limit_line = 0.9\n",
    "# Y_pred_one = np.where(Y_pred_one > max_value * limit_line, 1, 0)\n",
    "\n",
    "# # print(Y_pred_one)\n",
    "# Y_pred_one = Y_pred_one.reshape(-1,)\n",
    "# # print(Y_pred_1)\n",
    "# # print(Y_pred.shape)\n",
    "# # print(t_te.shape)\n",
    "\n",
    "# # fail = 0\n",
    "# # fail2 = 0\n",
    "# # for i in range(len(Y_pred)):\n",
    "# #   if Y_pred_1[i] != t_te[i]:\n",
    "# #     fail += 1\n",
    "\n",
    "# #   if Y_pred[i] != t_te[i]:\n",
    "# #     fail2 += 1\n",
    "\n",
    "# # print(1 - fail / len(Y_pred))\n",
    "# # print(1 - fail2 / len(Y_pred))\n",
    "\n",
    "# # print(np.sum(Y_pred), np.sum(t_te))\n",
    "# # print('Y_pred / Y_test :', np.sum(Y_pred) / np.sum(t_te))\n",
    "# %matplotlib inline\n",
    "# plt.figure(figsize=(100,10))\n",
    "# plt.subplot(211)\n",
    "# plt.plot(Y_test, 'purple', label='test')\n",
    "\n",
    "# plt.subplot(212)\n",
    "# plt.plot(Y_pred_one, 'y', label='pred')\n",
    "# plt.show()\n",
    "# plt.savefig('/content/gdrive/My Drive/Colab Notebooks/model/rapid_ascending %s.png' % input_data_length)\n",
    "# plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow2_p36]",
   "language": "python",
   "name": "conda-env-tensorflow2_p36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
