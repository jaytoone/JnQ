{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14751, 30, 6)\n",
      "(14751, 1)\n",
      "(10325, 30, 6, 1)\n",
      "(4426, 30, 6, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/lab23/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/lab23/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/lab23/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/lab23/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/lab23/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/lab23/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/lab23/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/lab23/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/lab23/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/lab23/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/lab23/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/lab23/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 0. ... 1. 1. 1.]\n",
      "{0: 0.7503633720930233, 1: 1.4985486211901307}\n",
      "(10325, 2)\n",
      "(4426, 2)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 9 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4070: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input (InputLayer)           (None, 30, 6, 1)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 30, 6, 64)         640       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 30, 6, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 15, 3, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 15, 3, 128)        73856     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 15, 3, 128)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 7, 1, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 896)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                57408     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 130       \n",
      "=================================================================\n",
      "Total params: 132,034\n",
      "Trainable params: 132,034\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/keras/callbacks/tensorboard_v1.py:200: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/keras/callbacks/tensorboard_v1.py:203: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n",
      "\n",
      "Epoch 1/100\n",
      " - 8s - loss: 0.6528 - accuracy: 0.6104 - val_loss: 0.8298 - val_accuracy: 0.5818\n",
      "\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.58179, saving model to model/rapid_ascending 30_73_ema.hdf5\n",
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/keras/callbacks/tensorboard_v1.py:343: The name tf.Summary is deprecated. Please use tf.compat.v1.Summary instead.\n",
      "\n",
      "Epoch 2/100\n",
      " - 6s - loss: 0.6340 - accuracy: 0.6279 - val_loss: 0.7462 - val_accuracy: 0.5779\n",
      "\n",
      "Epoch 00002: val_accuracy did not improve from 0.58179\n",
      "Epoch 3/100\n",
      " - 6s - loss: 0.6292 - accuracy: 0.6304 - val_loss: 0.7176 - val_accuracy: 0.5436\n",
      "\n",
      "Epoch 00003: val_accuracy did not improve from 0.58179\n",
      "Epoch 4/100\n",
      " - 6s - loss: 0.6254 - accuracy: 0.6320 - val_loss: 0.7035 - val_accuracy: 0.5343\n",
      "\n",
      "Epoch 00004: val_accuracy did not improve from 0.58179\n",
      "Epoch 5/100\n",
      " - 6s - loss: 0.6219 - accuracy: 0.6356 - val_loss: 0.7564 - val_accuracy: 0.6080\n",
      "\n",
      "Epoch 00005: val_accuracy improved from 0.58179 to 0.60800, saving model to model/rapid_ascending 30_73_ema.hdf5\n",
      "Epoch 6/100\n",
      " - 6s - loss: 0.6183 - accuracy: 0.6429 - val_loss: 0.4613 - val_accuracy: 0.5621\n",
      "\n",
      "Epoch 00006: val_accuracy did not improve from 0.60800\n",
      "Epoch 7/100\n",
      " - 6s - loss: 0.6167 - accuracy: 0.6368 - val_loss: 0.7186 - val_accuracy: 0.5043\n",
      "\n",
      "Epoch 00007: val_accuracy did not improve from 0.60800\n",
      "Epoch 8/100\n",
      " - 6s - loss: 0.6130 - accuracy: 0.6406 - val_loss: 0.6310 - val_accuracy: 0.5899\n",
      "\n",
      "Epoch 00008: val_accuracy did not improve from 0.60800\n",
      "Epoch 9/100\n",
      " - 6s - loss: 0.6083 - accuracy: 0.6470 - val_loss: 0.6894 - val_accuracy: 0.5642\n",
      "\n",
      "Epoch 00009: val_accuracy did not improve from 0.60800\n",
      "Epoch 10/100\n",
      " - 6s - loss: 0.6027 - accuracy: 0.6457 - val_loss: 0.9310 - val_accuracy: 0.5635\n",
      "\n",
      "Epoch 00010: val_accuracy did not improve from 0.60800\n",
      "Epoch 11/100\n",
      " - 6s - loss: 0.5981 - accuracy: 0.6527 - val_loss: 0.5759 - val_accuracy: 0.5502\n",
      "\n",
      "Epoch 00011: val_accuracy did not improve from 0.60800\n",
      "Epoch 12/100\n",
      " - 6s - loss: 0.5918 - accuracy: 0.6596 - val_loss: 0.4550 - val_accuracy: 0.5770\n",
      "\n",
      "Epoch 00012: val_accuracy did not improve from 0.60800\n",
      "Epoch 13/100\n",
      " - 6s - loss: 0.5866 - accuracy: 0.6616 - val_loss: 0.7406 - val_accuracy: 0.5508\n",
      "\n",
      "Epoch 00013: val_accuracy did not improve from 0.60800\n",
      "Epoch 14/100\n",
      " - 6s - loss: 0.5787 - accuracy: 0.6651 - val_loss: 0.6469 - val_accuracy: 0.5908\n",
      "\n",
      "Epoch 00014: val_accuracy did not improve from 0.60800\n",
      "Epoch 15/100\n",
      " - 6s - loss: 0.5713 - accuracy: 0.6749 - val_loss: 0.8119 - val_accuracy: 0.6098\n",
      "\n",
      "Epoch 00015: val_accuracy improved from 0.60800 to 0.60981, saving model to model/rapid_ascending 30_73_ema.hdf5\n",
      "Epoch 16/100\n",
      " - 6s - loss: 0.5623 - accuracy: 0.6861 - val_loss: 0.7839 - val_accuracy: 0.6039\n",
      "\n",
      "Epoch 00016: val_accuracy did not improve from 0.60981\n",
      "Epoch 17/100\n",
      " - 6s - loss: 0.5535 - accuracy: 0.6872 - val_loss: 0.8385 - val_accuracy: 0.5913\n",
      "\n",
      "Epoch 00017: val_accuracy did not improve from 0.60981\n",
      "Epoch 18/100\n",
      " - 6s - loss: 0.5434 - accuracy: 0.6977 - val_loss: 0.6570 - val_accuracy: 0.5791\n",
      "\n",
      "Epoch 00018: val_accuracy did not improve from 0.60981\n",
      "Epoch 19/100\n",
      " - 6s - loss: 0.5319 - accuracy: 0.7084 - val_loss: 1.0709 - val_accuracy: 0.6288\n",
      "\n",
      "Epoch 00019: val_accuracy improved from 0.60981 to 0.62878, saving model to model/rapid_ascending 30_73_ema.hdf5\n",
      "Epoch 20/100\n",
      " - 6s - loss: 0.5250 - accuracy: 0.7132 - val_loss: 0.2686 - val_accuracy: 0.6075\n",
      "\n",
      "Epoch 00020: val_accuracy did not improve from 0.62878\n",
      "Epoch 21/100\n",
      " - 6s - loss: 0.5134 - accuracy: 0.7232 - val_loss: 0.8808 - val_accuracy: 0.5791\n",
      "\n",
      "Epoch 00021: val_accuracy did not improve from 0.62878\n",
      "Epoch 22/100\n",
      " - 6s - loss: 0.4997 - accuracy: 0.7312 - val_loss: 0.6337 - val_accuracy: 0.6308\n",
      "\n",
      "Epoch 00022: val_accuracy improved from 0.62878 to 0.63082, saving model to model/rapid_ascending 30_73_ema.hdf5\n",
      "Epoch 23/100\n",
      " - 6s - loss: 0.4888 - accuracy: 0.7428 - val_loss: 1.1245 - val_accuracy: 0.5599\n",
      "\n",
      "Epoch 00023: val_accuracy did not improve from 0.63082\n",
      "Epoch 24/100\n",
      " - 6s - loss: 0.4791 - accuracy: 0.7458 - val_loss: 0.3817 - val_accuracy: 0.5716\n",
      "\n",
      "Epoch 00024: val_accuracy did not improve from 0.63082\n",
      "Epoch 25/100\n",
      " - 6s - loss: 0.4677 - accuracy: 0.7512 - val_loss: 0.3563 - val_accuracy: 0.6010\n",
      "\n",
      "Epoch 00025: val_accuracy did not improve from 0.63082\n",
      "Epoch 26/100\n",
      " - 6s - loss: 0.4561 - accuracy: 0.7605 - val_loss: 0.5045 - val_accuracy: 0.5574\n",
      "\n",
      "Epoch 00026: val_accuracy did not improve from 0.63082\n",
      "Epoch 27/100\n",
      " - 6s - loss: 0.4440 - accuracy: 0.7710 - val_loss: 0.7954 - val_accuracy: 0.5721\n",
      "\n",
      "Epoch 00027: val_accuracy did not improve from 0.63082\n",
      "Epoch 28/100\n",
      " - 6s - loss: 0.4301 - accuracy: 0.7804 - val_loss: 1.4157 - val_accuracy: 0.5707\n",
      "\n",
      "Epoch 00028: val_accuracy did not improve from 0.63082\n",
      "Epoch 29/100\n",
      " - 6s - loss: 0.4208 - accuracy: 0.7863 - val_loss: 0.5242 - val_accuracy: 0.5626\n",
      "\n",
      "Epoch 00029: val_accuracy did not improve from 0.63082\n",
      "Epoch 30/100\n",
      " - 6s - loss: 0.4072 - accuracy: 0.7967 - val_loss: 0.6445 - val_accuracy: 0.5920\n",
      "\n",
      "Epoch 00030: val_accuracy did not improve from 0.63082\n",
      "Epoch 31/100\n",
      " - 6s - loss: 0.3962 - accuracy: 0.8031 - val_loss: 0.5675 - val_accuracy: 0.6026\n",
      "\n",
      "Epoch 00031: val_accuracy did not improve from 0.63082\n",
      "Epoch 32/100\n",
      " - 6s - loss: 0.3844 - accuracy: 0.8110 - val_loss: 1.0478 - val_accuracy: 0.5951\n",
      "\n",
      "Epoch 00032: val_accuracy did not improve from 0.63082\n",
      "Epoch 33/100\n",
      " - 6s - loss: 0.3719 - accuracy: 0.8153 - val_loss: 0.8955 - val_accuracy: 0.5843\n",
      "\n",
      "Epoch 00033: val_accuracy did not improve from 0.63082\n",
      "Epoch 34/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 6s - loss: 0.3578 - accuracy: 0.8282 - val_loss: 0.5117 - val_accuracy: 0.6249\n",
      "\n",
      "Epoch 00034: val_accuracy did not improve from 0.63082\n",
      "Epoch 35/100\n",
      " - 6s - loss: 0.3486 - accuracy: 0.8309 - val_loss: 0.2645 - val_accuracy: 0.6003\n",
      "\n",
      "Epoch 00035: val_accuracy did not improve from 0.63082\n",
      "Epoch 36/100\n",
      " - 6s - loss: 0.3363 - accuracy: 0.8362 - val_loss: 0.9391 - val_accuracy: 0.5856\n",
      "\n",
      "Epoch 00036: val_accuracy did not improve from 0.63082\n",
      "Epoch 37/100\n",
      " - 6s - loss: 0.3241 - accuracy: 0.8467 - val_loss: 0.5973 - val_accuracy: 0.5368\n",
      "\n",
      "Epoch 00037: val_accuracy did not improve from 0.63082\n",
      "Epoch 38/100\n",
      " - 6s - loss: 0.3140 - accuracy: 0.8505 - val_loss: 0.6494 - val_accuracy: 0.6125\n",
      "\n",
      "Epoch 00038: val_accuracy did not improve from 0.63082\n",
      "Epoch 39/100\n",
      " - 6s - loss: 0.3020 - accuracy: 0.8571 - val_loss: 0.7830 - val_accuracy: 0.5682\n",
      "\n",
      "Epoch 00039: val_accuracy did not improve from 0.63082\n",
      "Epoch 40/100\n",
      " - 6s - loss: 0.2918 - accuracy: 0.8663 - val_loss: 0.9310 - val_accuracy: 0.5750\n",
      "\n",
      "Epoch 00040: val_accuracy did not improve from 0.63082\n",
      "Epoch 41/100\n",
      " - 6s - loss: 0.2849 - accuracy: 0.8679 - val_loss: 0.0705 - val_accuracy: 0.5610\n",
      "\n",
      "Epoch 00041: val_accuracy did not improve from 0.63082\n",
      "Epoch 42/100\n",
      " - 6s - loss: 0.2713 - accuracy: 0.8763 - val_loss: 0.7454 - val_accuracy: 0.5517\n",
      "\n",
      "Epoch 00042: val_accuracy did not improve from 0.63082\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot\n",
    "import scipy.misc \n",
    "from math import sqrt \n",
    "import itertools\n",
    "from IPython.display import display\n",
    "%matplotlib inline\n",
    "\n",
    "input_data_length = 30\n",
    "model_num = 73\n",
    "num_classes = 2\n",
    "\n",
    "Made_X = np.load('Made_X/Made_X %s_%s.npy' % (input_data_length, model_num))\n",
    "Made_Y = np.load('Made_X/Made_Y %s_%s.npy' % (input_data_length, model_num)).reshape(-1, 1)\n",
    "\n",
    "\n",
    "#       dataset 분리      #\n",
    "# dataX 구성 : VOLUME, MA, CMO, OBV, RSI, MACD, MACD_SIGNAL, MACD_OSC \n",
    "# dataX 구성 : VOLUME, EMA1, EMA2, CMO, OBV, RSI, MACD, MACD_SIGNAL, MACD_OSC \n",
    "Made_X = Made_X[:, :,[0,1,2,3,5,6]]\n",
    "print(Made_X.shape)\n",
    "print(Made_Y.shape)\n",
    "\n",
    "row = Made_X.shape[1]\n",
    "col = Made_X.shape[2]\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(Made_X, Made_Y, test_size=0.3,\n",
    "                                                   shuffle=False)\n",
    "\n",
    "X_train = X_train.astype('float32').reshape(-1, input_data_length, col, 1)\n",
    "X_val = X_val.astype('float32').reshape(-1, input_data_length, col, 1)\n",
    "print(X_train.shape)\n",
    "print(X_val.shape)\n",
    "\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing.image import ImageDataGenerator \n",
    "\n",
    "# Data Class Weight\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "print(Y_train[:, 0])\n",
    "class_weights = class_weight.compute_class_weight('balanced', \n",
    "                                                  np.unique(Y_train[:, 0]),\n",
    "                                                  Y_train[:, 0])\n",
    "class_weights = dict(enumerate(class_weights))\n",
    "# class_weights[1] *= 0.97\n",
    "# class_weights[2] *= 0.97\n",
    "print(class_weights)\n",
    "# quit()\n",
    "\n",
    "Y_train = Y_train.astype('float32')\n",
    "Y_val = Y_val.astype('float32')\n",
    "Y_train = np_utils.to_categorical(Y_train, num_classes)\n",
    "Y_val = np_utils.to_categorical(Y_val, num_classes)\n",
    "print(Y_train.shape)\n",
    "print(Y_val.shape)\n",
    "\n",
    "datagen = ImageDataGenerator( \n",
    "#     rotation_range = 60,\n",
    "#     zoom_range = 0.6,\n",
    "#     shear_range = 0.6,\n",
    "#     horizontal_flip = True,\n",
    "#     width_shift_range=0.6,\n",
    "#     height_shift_range=0.6,\n",
    "    fill_mode = 'nearest'\n",
    "    )\n",
    "\n",
    "testgen = ImageDataGenerator( \n",
    "    )\n",
    "datagen.fit(X_train)\n",
    "batch_size = 6\n",
    "\n",
    "for X_batch, _ in datagen.flow(X_train, Y_train, batch_size=9):\n",
    "    for i in range(0, 9): \n",
    "        pyplot.axis('off') \n",
    "        pyplot.subplot(330 + 1 + i) \n",
    "        pyplot.imshow(X_batch[i].reshape(input_data_length, col), cmap=pyplot.get_cmap('gray'))\n",
    "    pyplot.axis('off') \n",
    "    pyplot.show() \n",
    "    break\n",
    "    \n",
    "    \n",
    "train_flow = datagen.flow(X_train, Y_train, batch_size=batch_size) \n",
    "val_flow = testgen.flow(X_val, Y_val, batch_size=batch_size) \n",
    "\n",
    "\n",
    "from keras.utils import plot_model\n",
    "import keras.backend as K\n",
    "from keras.models import Model, Sequential\n",
    "import keras.layers as layers\n",
    "from keras.optimizers import Adam, SGD\n",
    "from keras.regularizers import l1, l2\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def FER_Model(input_shape=(row, col, 1)):\n",
    "    # first input model\n",
    "    visible = layers.Input(shape=input_shape, name='input')\n",
    "    \n",
    "    net = layers.Conv2D(64, kernel_size=3, padding='same')(visible)\n",
    "    # net = layers.Activation('relu')(net)\n",
    "    net = layers.LeakyReLU()(net)\n",
    "    net = layers.MaxPool2D(pool_size=2)(net)\n",
    "\n",
    "    shortcut_1 = net\n",
    "\n",
    "    net = layers.Conv2D(128, kernel_size=3, padding='same')(net)\n",
    "    # net = layers.Activation('relu')(net)\n",
    "    net = layers.LeakyReLU()(net)\n",
    "    net = layers.MaxPool2D(pool_size=2)(net)\n",
    "\n",
    "    shortcut_2 = net\n",
    "\n",
    "#     net = layers.Conv2D(256, kernel_size=3, padding='same')(net)\n",
    "#     # net = layers.Activation('relu')(net)\n",
    "#     net = layers.LeakyReLU()(net)\n",
    "#     net = layers.MaxPool2D(pool_size=2)(net)\n",
    "\n",
    "#     shortcut_3 = net\n",
    "\n",
    "#     net = layers.Conv2D(128, kernel_size=1, padding='same')(net)\n",
    "#     # net = layers.Activation('relu')(net)\n",
    "#     net = layers.LeakyReLU()(net)\n",
    "#     net = layers.MaxPool2D(pool_size=2)(net)\n",
    "\n",
    "    net = layers.Flatten()(net)\n",
    "    net = layers.Dense(64)(net)\n",
    "    net = layers.LeakyReLU()(net)\n",
    "    net = layers.Dense(num_classes, activation='softmax')(net)\n",
    "\n",
    "    # create model \n",
    "    model = Model(inputs =visible, outputs = net)\n",
    "    # summary layers\n",
    "    print(model.summary())\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = FER_Model()\n",
    "opt = Adam(lr=0.0001, decay=1e-6)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "    \n",
    "    \n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, TensorBoard\n",
    "filepath=\"model/rapid_ascending %s_%s_ema.hdf5\" % (input_data_length, model_num)\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='auto')\n",
    "checkpoint2 = TensorBoard(log_dir='Tensorboard_graph',\n",
    "                          histogram_freq=0,\n",
    "                          write_graph=True,\n",
    "                          write_images=True)\n",
    "checkpoint3 = EarlyStopping(monitor='val_accuracy', patience=20)\n",
    "callbacks_list = [checkpoint, checkpoint2, checkpoint3]\n",
    "\n",
    "# keras.callbacks.Callback 로 부터 log 를 받아와 history log 를 작성할 수 있다.\n",
    "\n",
    "# we iterate 200 times over the entire training set\n",
    "num_epochs = 100\n",
    "history = model.fit_generator(train_flow, \n",
    "                    steps_per_epoch=len(X_train) / batch_size, \n",
    "                    epochs=num_epochs,  \n",
    "                    verbose=2,  \n",
    "                    callbacks=callbacks_list,\n",
    "                    class_weight=class_weights,\n",
    "                    validation_data=val_flow,  \n",
    "                    validation_steps=len(X_val) / batch_size,\n",
    "                    shuffle=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "model = load_model('model/rapid_ascending %s_%s.hdf5' % (input_data_length, model_num))\n",
    "# model = load_model('/content/gdrive/My Drive/Colab Notebooks/model/rapid_ascending %s.hdf5' % input_data_length)\n",
    "# loss = model.evaluate_generator(test_flow, steps=len(X_test) / batch_size) \n",
    "# print(\"Test Loss \" + str(loss[0]))\n",
    "# print(\"Test Acc: \" + str(loss[1]))\n",
    "\n",
    "# loss = model.evaluate(X_val, Y_val) \n",
    "# print(X_val.shape)\n",
    "# print(Y_val.shape)\n",
    "\n",
    "# print(\"Val Loss \" + str(loss[0]))\n",
    "# print(\"Val Acc: \" + str(loss[1]))\n",
    "\n",
    "#     Prediction    #\n",
    "Y_pred_ = model.predict(X_test, verbose=1)\n",
    "\n",
    "# Y_pred = Y_pred_[:,[-1]]\n",
    "# print(Y_pred.shape)\n",
    "# print(Y_test.shape)\n",
    "Y_pred = np.argmax(Y_pred_, axis=1)\n",
    "t_te = np.argmax(Y_test, axis=1)\n",
    "\n",
    "#     Manual processing     #\n",
    "Y_pred_one = Y_pred_[:, [-1]]\n",
    "print(Y_pred_)\n",
    "max_value = np.max(Y_pred_one)\n",
    "print(max_value)\n",
    "\n",
    "limit_line = 0.9\n",
    "Y_pred_one = np.where(Y_pred_one > max_value * limit_line, 1, 0)\n",
    "\n",
    "# print(Y_pred_one)\n",
    "Y_pred_one = Y_pred_one.reshape(-1,)\n",
    "# print(Y_pred_1)\n",
    "# print(Y_pred.shape)\n",
    "# print(t_te.shape)\n",
    "\n",
    "# fail = 0\n",
    "# fail2 = 0\n",
    "# for i in range(len(Y_pred)):\n",
    "#   if Y_pred_1[i] != t_te[i]:\n",
    "#     fail += 1\n",
    "\n",
    "#   if Y_pred[i] != t_te[i]:\n",
    "#     fail2 += 1\n",
    "\n",
    "# print(1 - fail / len(Y_pred))\n",
    "# print(1 - fail2 / len(Y_pred))\n",
    "\n",
    "# print(np.sum(Y_pred), np.sum(t_te))\n",
    "# print('Y_pred / Y_test :', np.sum(Y_pred) / np.sum(t_te))\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.figure(figsize=(100,10))\n",
    "plt.subplot(211)\n",
    "plt.plot(Y_test, 'purple', label='test')\n",
    "\n",
    "plt.subplot(212)\n",
    "plt.plot(Y_pred_one, 'y', label='pred')\n",
    "plt.show()\n",
    "# plt.savefig('/content/gdrive/My Drive/Colab Notebooks/model/rapid_ascending %s.png' % input_data_length)\n",
    "# plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow2_p36]",
   "language": "python",
   "name": "conda-env-tensorflow2_p36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
