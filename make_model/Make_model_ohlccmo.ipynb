{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(397691, 30, 5)\n",
      "(397691, 1)\n",
      "(278383, 30, 5, 1)\n",
      "(119308, 30, 5, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/lab23/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/lab23/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/lab23/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/lab23/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/lab23/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/lab23/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/lab23/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/lab23/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/lab23/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/lab23/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/lab23/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/lab23/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "{0: 0.42926753296417774, 1: 3.023404578826187, 2: 2.9437958674364992}\n",
      "(278383, 3)\n",
      "(119308, 3)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 9 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4070: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input (InputLayer)           (None, 30, 5, 1)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 30, 5, 64)         640       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 30, 5, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 15, 2, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 15, 2, 128)        73856     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 15, 2, 128)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 7, 1, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 896)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                57408     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 3)                 195       \n",
      "=================================================================\n",
      "Total params: 132,099\n",
      "Trainable params: 132,099\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/keras/callbacks/tensorboard_v1.py:200: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/keras/callbacks/tensorboard_v1.py:203: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n",
      "\n",
      "Epoch 1/100\n",
      " - 61s - loss: 1.0625 - accuracy: 0.2159 - val_loss: 1.0028 - val_accuracy: 0.1804\n",
      "\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.18042, saving model to model/rapid_ascending 30_79_cmo.hdf5\n",
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/keras/callbacks/tensorboard_v1.py:343: The name tf.Summary is deprecated. Please use tf.compat.v1.Summary instead.\n",
      "\n",
      "Epoch 2/100\n",
      " - 59s - loss: 1.0537 - accuracy: 0.2330 - val_loss: 1.0507 - val_accuracy: 0.2311\n",
      "\n",
      "Epoch 00002: val_accuracy improved from 0.18042 to 0.23106, saving model to model/rapid_ascending 30_79_cmo.hdf5\n",
      "Epoch 3/100\n",
      " - 59s - loss: 1.0513 - accuracy: 0.2523 - val_loss: 1.1066 - val_accuracy: 0.2298\n",
      "\n",
      "Epoch 00003: val_accuracy did not improve from 0.23106\n",
      "Epoch 4/100\n",
      " - 59s - loss: 1.0491 - accuracy: 0.2532 - val_loss: 1.0484 - val_accuracy: 0.3019\n",
      "\n",
      "Epoch 00004: val_accuracy improved from 0.23106 to 0.30187, saving model to model/rapid_ascending 30_79_cmo.hdf5\n",
      "Epoch 5/100\n",
      " - 60s - loss: 1.0477 - accuracy: 0.2608 - val_loss: 1.0268 - val_accuracy: 0.3195\n",
      "\n",
      "Epoch 00005: val_accuracy improved from 0.30187 to 0.31951, saving model to model/rapid_ascending 30_79_cmo.hdf5\n",
      "Epoch 6/100\n",
      " - 59s - loss: 1.0461 - accuracy: 0.2639 - val_loss: 1.0931 - val_accuracy: 0.2244\n",
      "\n",
      "Epoch 00006: val_accuracy did not improve from 0.31951\n",
      "Epoch 7/100\n",
      " - 59s - loss: 1.0439 - accuracy: 0.2690 - val_loss: 1.1776 - val_accuracy: 0.2050\n",
      "\n",
      "Epoch 00007: val_accuracy did not improve from 0.31951\n",
      "Epoch 8/100\n",
      " - 60s - loss: 1.0421 - accuracy: 0.2738 - val_loss: 1.0688 - val_accuracy: 0.2697\n",
      "\n",
      "Epoch 00008: val_accuracy did not improve from 0.31951\n",
      "Epoch 9/100\n",
      " - 59s - loss: 1.0403 - accuracy: 0.2754 - val_loss: 1.1625 - val_accuracy: 0.2494\n",
      "\n",
      "Epoch 00009: val_accuracy did not improve from 0.31951\n",
      "Epoch 10/100\n",
      " - 59s - loss: 1.0379 - accuracy: 0.2854 - val_loss: 1.0664 - val_accuracy: 0.3291\n",
      "\n",
      "Epoch 00010: val_accuracy improved from 0.31951 to 0.32915, saving model to model/rapid_ascending 30_79_cmo.hdf5\n",
      "Epoch 11/100\n",
      " - 59s - loss: 1.0355 - accuracy: 0.2882 - val_loss: 1.0401 - val_accuracy: 0.2748\n",
      "\n",
      "Epoch 00011: val_accuracy did not improve from 0.32915\n",
      "Epoch 12/100\n",
      " - 59s - loss: 1.0333 - accuracy: 0.2922 - val_loss: 0.9541 - val_accuracy: 0.2936\n",
      "\n",
      "Epoch 00012: val_accuracy did not improve from 0.32915\n",
      "Epoch 13/100\n",
      " - 59s - loss: 1.0307 - accuracy: 0.2949 - val_loss: 1.1070 - val_accuracy: 0.2740\n",
      "\n",
      "Epoch 00013: val_accuracy did not improve from 0.32915\n",
      "Epoch 14/100\n",
      " - 59s - loss: 1.0274 - accuracy: 0.3016 - val_loss: 1.0588 - val_accuracy: 0.2664\n",
      "\n",
      "Epoch 00014: val_accuracy did not improve from 0.32915\n",
      "Epoch 15/100\n",
      " - 60s - loss: 1.0246 - accuracy: 0.3060 - val_loss: 1.0993 - val_accuracy: 0.2860\n",
      "\n",
      "Epoch 00015: val_accuracy did not improve from 0.32915\n",
      "Epoch 16/100\n",
      " - 59s - loss: 1.0219 - accuracy: 0.3088 - val_loss: 1.1455 - val_accuracy: 0.2916\n",
      "\n",
      "Epoch 00016: val_accuracy did not improve from 0.32915\n",
      "Epoch 17/100\n",
      " - 59s - loss: 1.0189 - accuracy: 0.3120 - val_loss: 1.1649 - val_accuracy: 0.2856\n",
      "\n",
      "Epoch 00017: val_accuracy did not improve from 0.32915\n",
      "Epoch 18/100\n",
      " - 60s - loss: 1.0156 - accuracy: 0.3187 - val_loss: 1.1789 - val_accuracy: 0.2518\n",
      "\n",
      "Epoch 00018: val_accuracy did not improve from 0.32915\n",
      "Epoch 19/100\n",
      " - 59s - loss: 1.0127 - accuracy: 0.3213 - val_loss: 1.1546 - val_accuracy: 0.2894\n",
      "\n",
      "Epoch 00019: val_accuracy did not improve from 0.32915\n",
      "Epoch 20/100\n",
      " - 59s - loss: 1.0093 - accuracy: 0.3250 - val_loss: 1.0984 - val_accuracy: 0.3063\n",
      "\n",
      "Epoch 00020: val_accuracy did not improve from 0.32915\n",
      "Epoch 21/100\n",
      " - 60s - loss: 1.0057 - accuracy: 0.3286 - val_loss: 1.0768 - val_accuracy: 0.2591\n",
      "\n",
      "Epoch 00021: val_accuracy did not improve from 0.32915\n",
      "Epoch 22/100\n",
      " - 59s - loss: 1.0025 - accuracy: 0.3338 - val_loss: 1.1108 - val_accuracy: 0.2725\n",
      "\n",
      "Epoch 00022: val_accuracy did not improve from 0.32915\n",
      "Epoch 23/100\n",
      " - 60s - loss: 0.9994 - accuracy: 0.3367 - val_loss: 1.0384 - val_accuracy: 0.2579\n",
      "\n",
      "Epoch 00023: val_accuracy did not improve from 0.32915\n",
      "Epoch 24/100\n",
      " - 60s - loss: 0.9957 - accuracy: 0.3395 - val_loss: 1.2252 - val_accuracy: 0.2757\n",
      "\n",
      "Epoch 00024: val_accuracy did not improve from 0.32915\n",
      "Epoch 25/100\n",
      " - 59s - loss: 0.9922 - accuracy: 0.3438 - val_loss: 1.1532 - val_accuracy: 0.2841\n",
      "\n",
      "Epoch 00025: val_accuracy did not improve from 0.32915\n",
      "Epoch 26/100\n",
      " - 59s - loss: 0.9891 - accuracy: 0.3462 - val_loss: 1.0083 - val_accuracy: 0.2703\n",
      "\n",
      "Epoch 00026: val_accuracy did not improve from 0.32915\n",
      "Epoch 27/100\n",
      " - 60s - loss: 0.9855 - accuracy: 0.3486 - val_loss: 1.0661 - val_accuracy: 0.2820\n",
      "\n",
      "Epoch 00027: val_accuracy did not improve from 0.32915\n",
      "Epoch 28/100\n",
      " - 59s - loss: 0.9817 - accuracy: 0.3521 - val_loss: 1.1727 - val_accuracy: 0.2558\n",
      "\n",
      "Epoch 00028: val_accuracy did not improve from 0.32915\n",
      "Epoch 29/100\n",
      " - 60s - loss: 0.9785 - accuracy: 0.3543 - val_loss: 1.2179 - val_accuracy: 0.2864\n",
      "\n",
      "Epoch 00029: val_accuracy did not improve from 0.32915\n",
      "Epoch 30/100\n",
      " - 60s - loss: 0.9754 - accuracy: 0.3581 - val_loss: 1.0592 - val_accuracy: 0.3337\n",
      "\n",
      "Epoch 00030: val_accuracy improved from 0.32915 to 0.33367, saving model to model/rapid_ascending 30_79_cmo.hdf5\n",
      "Epoch 31/100\n",
      " - 60s - loss: 0.9721 - accuracy: 0.3598 - val_loss: 1.0463 - val_accuracy: 0.2744\n",
      "\n",
      "Epoch 00031: val_accuracy did not improve from 0.33367\n",
      "Epoch 32/100\n",
      " - 59s - loss: 0.9692 - accuracy: 0.3616 - val_loss: 1.0336 - val_accuracy: 0.3883\n",
      "\n",
      "Epoch 00032: val_accuracy improved from 0.33367 to 0.38834, saving model to model/rapid_ascending 30_79_cmo.hdf5\n",
      "Epoch 33/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 60s - loss: 0.9656 - accuracy: 0.3664 - val_loss: 1.1185 - val_accuracy: 0.3421\n",
      "\n",
      "Epoch 00033: val_accuracy did not improve from 0.38834\n",
      "Epoch 34/100\n",
      " - 59s - loss: 0.9625 - accuracy: 0.3678 - val_loss: 1.0545 - val_accuracy: 0.3123\n",
      "\n",
      "Epoch 00034: val_accuracy did not improve from 0.38834\n",
      "Epoch 35/100\n",
      " - 60s - loss: 0.9589 - accuracy: 0.3698 - val_loss: 1.0389 - val_accuracy: 0.3166\n",
      "\n",
      "Epoch 00035: val_accuracy did not improve from 0.38834\n",
      "Epoch 36/100\n",
      " - 60s - loss: 0.9554 - accuracy: 0.3734 - val_loss: 1.0792 - val_accuracy: 0.3207\n",
      "\n",
      "Epoch 00036: val_accuracy did not improve from 0.38834\n",
      "Epoch 37/100\n",
      " - 60s - loss: 0.9523 - accuracy: 0.3750 - val_loss: 1.0784 - val_accuracy: 0.3429\n",
      "\n",
      "Epoch 00037: val_accuracy did not improve from 0.38834\n",
      "Epoch 38/100\n",
      " - 60s - loss: 0.9492 - accuracy: 0.3778 - val_loss: 1.0249 - val_accuracy: 0.3500\n",
      "\n",
      "Epoch 00038: val_accuracy did not improve from 0.38834\n",
      "Epoch 39/100\n",
      " - 59s - loss: 0.9465 - accuracy: 0.3810 - val_loss: 1.0131 - val_accuracy: 0.3441\n",
      "\n",
      "Epoch 00039: val_accuracy did not improve from 0.38834\n",
      "Epoch 40/100\n",
      " - 59s - loss: 0.9435 - accuracy: 0.3824 - val_loss: 1.1324 - val_accuracy: 0.3166\n",
      "\n",
      "Epoch 00040: val_accuracy did not improve from 0.38834\n",
      "Epoch 41/100\n",
      " - 58s - loss: 0.9408 - accuracy: 0.3837 - val_loss: 1.0515 - val_accuracy: 0.3484\n",
      "\n",
      "Epoch 00041: val_accuracy did not improve from 0.38834\n",
      "Epoch 42/100\n",
      " - 58s - loss: 0.9377 - accuracy: 0.3860 - val_loss: 1.0578 - val_accuracy: 0.3106\n",
      "\n",
      "Epoch 00042: val_accuracy did not improve from 0.38834\n",
      "Epoch 43/100\n",
      " - 57s - loss: 0.9346 - accuracy: 0.3878 - val_loss: 1.0802 - val_accuracy: 0.3502\n",
      "\n",
      "Epoch 00043: val_accuracy did not improve from 0.38834\n",
      "Epoch 44/100\n",
      " - 57s - loss: 0.9311 - accuracy: 0.3908 - val_loss: 0.9296 - val_accuracy: 0.3322\n",
      "\n",
      "Epoch 00044: val_accuracy did not improve from 0.38834\n",
      "Epoch 45/100\n",
      " - 57s - loss: 0.9286 - accuracy: 0.3924 - val_loss: 1.0661 - val_accuracy: 0.3040\n",
      "\n",
      "Epoch 00045: val_accuracy did not improve from 0.38834\n",
      "Epoch 46/100\n",
      " - 57s - loss: 0.9261 - accuracy: 0.3939 - val_loss: 0.9508 - val_accuracy: 0.3530\n",
      "\n",
      "Epoch 00046: val_accuracy did not improve from 0.38834\n",
      "Epoch 47/100\n",
      " - 57s - loss: 0.9231 - accuracy: 0.3958 - val_loss: 1.0873 - val_accuracy: 0.3301\n",
      "\n",
      "Epoch 00047: val_accuracy did not improve from 0.38834\n",
      "Epoch 48/100\n",
      " - 58s - loss: 0.9203 - accuracy: 0.3983 - val_loss: 1.0485 - val_accuracy: 0.3402\n",
      "\n",
      "Epoch 00048: val_accuracy did not improve from 0.38834\n",
      "Epoch 49/100\n",
      " - 58s - loss: 0.9172 - accuracy: 0.4014 - val_loss: 1.0566 - val_accuracy: 0.3199\n",
      "\n",
      "Epoch 00049: val_accuracy did not improve from 0.38834\n",
      "Epoch 50/100\n",
      " - 58s - loss: 0.9147 - accuracy: 0.4029 - val_loss: 1.2235 - val_accuracy: 0.3506\n",
      "\n",
      "Epoch 00050: val_accuracy did not improve from 0.38834\n",
      "Epoch 51/100\n",
      " - 58s - loss: 0.9121 - accuracy: 0.4055 - val_loss: 1.0913 - val_accuracy: 0.3101\n",
      "\n",
      "Epoch 00051: val_accuracy did not improve from 0.38834\n",
      "Epoch 52/100\n",
      " - 57s - loss: 0.9094 - accuracy: 0.4053 - val_loss: 1.1507 - val_accuracy: 0.3298\n",
      "\n",
      "Epoch 00052: val_accuracy did not improve from 0.38834\n",
      "Epoch 53/100\n",
      " - 57s - loss: 0.9070 - accuracy: 0.4065 - val_loss: 1.0802 - val_accuracy: 0.3058\n",
      "\n",
      "Epoch 00053: val_accuracy did not improve from 0.38834\n",
      "Epoch 54/100\n",
      " - 58s - loss: 0.9041 - accuracy: 0.4090 - val_loss: 0.9822 - val_accuracy: 0.3207\n",
      "\n",
      "Epoch 00054: val_accuracy did not improve from 0.38834\n",
      "Epoch 55/100\n",
      " - 57s - loss: 0.9014 - accuracy: 0.4117 - val_loss: 1.1659 - val_accuracy: 0.3398\n",
      "\n",
      "Epoch 00055: val_accuracy did not improve from 0.38834\n",
      "Epoch 56/100\n",
      " - 58s - loss: 0.8990 - accuracy: 0.4124 - val_loss: 1.0553 - val_accuracy: 0.3451\n",
      "\n",
      "Epoch 00056: val_accuracy did not improve from 0.38834\n",
      "Epoch 57/100\n",
      " - 59s - loss: 0.8969 - accuracy: 0.4141 - val_loss: 1.1509 - val_accuracy: 0.3116\n",
      "\n",
      "Epoch 00057: val_accuracy did not improve from 0.38834\n",
      "Epoch 58/100\n",
      " - 58s - loss: 0.8941 - accuracy: 0.4165 - val_loss: 1.2151 - val_accuracy: 0.3334\n",
      "\n",
      "Epoch 00058: val_accuracy did not improve from 0.38834\n",
      "Epoch 59/100\n",
      " - 58s - loss: 0.8917 - accuracy: 0.4169 - val_loss: 0.8959 - val_accuracy: 0.3877\n",
      "\n",
      "Epoch 00059: val_accuracy did not improve from 0.38834\n",
      "Epoch 60/100\n",
      " - 57s - loss: 0.8889 - accuracy: 0.4203 - val_loss: 1.0467 - val_accuracy: 0.3038\n",
      "\n",
      "Epoch 00060: val_accuracy did not improve from 0.38834\n",
      "Epoch 61/100\n",
      " - 58s - loss: 0.8866 - accuracy: 0.4217 - val_loss: 1.2762 - val_accuracy: 0.3368\n",
      "\n",
      "Epoch 00061: val_accuracy did not improve from 0.38834\n",
      "Epoch 62/100\n",
      " - 58s - loss: 0.8840 - accuracy: 0.4227 - val_loss: 1.4407 - val_accuracy: 0.3370\n",
      "\n",
      "Epoch 00062: val_accuracy did not improve from 0.38834\n",
      "Epoch 63/100\n",
      " - 58s - loss: 0.8823 - accuracy: 0.4237 - val_loss: 1.3272 - val_accuracy: 0.3717\n",
      "\n",
      "Epoch 00063: val_accuracy did not improve from 0.38834\n",
      "Epoch 64/100\n",
      " - 58s - loss: 0.8795 - accuracy: 0.4249 - val_loss: 1.1129 - val_accuracy: 0.3709\n",
      "\n",
      "Epoch 00064: val_accuracy did not improve from 0.38834\n",
      "Epoch 65/100\n",
      " - 57s - loss: 0.8767 - accuracy: 0.4267 - val_loss: 1.6339 - val_accuracy: 0.3402\n",
      "\n",
      "Epoch 00065: val_accuracy did not improve from 0.38834\n",
      "Epoch 66/100\n",
      " - 57s - loss: 0.8749 - accuracy: 0.4281 - val_loss: 1.0822 - val_accuracy: 0.3606\n",
      "\n",
      "Epoch 00066: val_accuracy did not improve from 0.38834\n",
      "Epoch 67/100\n",
      " - 57s - loss: 0.8724 - accuracy: 0.4296 - val_loss: 1.1801 - val_accuracy: 0.3178\n",
      "\n",
      "Epoch 00067: val_accuracy did not improve from 0.38834\n",
      "Epoch 68/100\n",
      " - 58s - loss: 0.8704 - accuracy: 0.4304 - val_loss: 1.0614 - val_accuracy: 0.3560\n",
      "\n",
      "Epoch 00068: val_accuracy did not improve from 0.38834\n",
      "Epoch 69/100\n",
      " - 58s - loss: 0.8681 - accuracy: 0.4335 - val_loss: 1.0257 - val_accuracy: 0.3640\n",
      "\n",
      "Epoch 00069: val_accuracy did not improve from 0.38834\n",
      "Epoch 70/100\n",
      " - 58s - loss: 0.8662 - accuracy: 0.4338 - val_loss: 0.8896 - val_accuracy: 0.3348\n",
      "\n",
      "Epoch 00070: val_accuracy did not improve from 0.38834\n",
      "Epoch 71/100\n",
      " - 57s - loss: 0.8639 - accuracy: 0.4351 - val_loss: 1.0358 - val_accuracy: 0.3311\n",
      "\n",
      "Epoch 00071: val_accuracy did not improve from 0.38834\n",
      "Epoch 72/100\n",
      " - 58s - loss: 0.8620 - accuracy: 0.4367 - val_loss: 1.0063 - val_accuracy: 0.3787\n",
      "\n",
      "Epoch 00072: val_accuracy did not improve from 0.38834\n",
      "Epoch 73/100\n",
      " - 57s - loss: 0.8599 - accuracy: 0.4387 - val_loss: 1.3200 - val_accuracy: 0.3228\n",
      "\n",
      "Epoch 00073: val_accuracy did not improve from 0.38834\n",
      "Epoch 74/100\n",
      " - 58s - loss: 0.8576 - accuracy: 0.4394 - val_loss: 1.0869 - val_accuracy: 0.3454\n",
      "\n",
      "Epoch 00074: val_accuracy did not improve from 0.38834\n",
      "Epoch 75/100\n",
      " - 58s - loss: 0.8552 - accuracy: 0.4414 - val_loss: 1.6340 - val_accuracy: 0.3394\n",
      "\n",
      "Epoch 00075: val_accuracy did not improve from 0.38834\n",
      "Epoch 76/100\n",
      " - 58s - loss: 0.8532 - accuracy: 0.4423 - val_loss: 0.8544 - val_accuracy: 0.3347\n",
      "\n",
      "Epoch 00076: val_accuracy did not improve from 0.38834\n",
      "Epoch 77/100\n",
      " - 58s - loss: 0.8521 - accuracy: 0.4432 - val_loss: 1.1366 - val_accuracy: 0.3511\n",
      "\n",
      "Epoch 00077: val_accuracy did not improve from 0.38834\n",
      "Epoch 78/100\n",
      " - 58s - loss: 0.8497 - accuracy: 0.4449 - val_loss: 1.3127 - val_accuracy: 0.3360\n",
      "\n",
      "Epoch 00078: val_accuracy did not improve from 0.38834\n",
      "Epoch 79/100\n",
      " - 58s - loss: 0.8476 - accuracy: 0.4456 - val_loss: 1.0539 - val_accuracy: 0.3642\n",
      "\n",
      "Epoch 00079: val_accuracy did not improve from 0.38834\n",
      "Epoch 80/100\n",
      " - 57s - loss: 0.8457 - accuracy: 0.4472 - val_loss: 1.0973 - val_accuracy: 0.3994\n",
      "\n",
      "Epoch 00080: val_accuracy improved from 0.38834 to 0.39944, saving model to model/rapid_ascending 30_79_cmo.hdf5\n",
      "Epoch 81/100\n",
      " - 57s - loss: 0.8431 - accuracy: 0.4487 - val_loss: 1.1271 - val_accuracy: 0.3180\n",
      "\n",
      "Epoch 00081: val_accuracy did not improve from 0.39944\n",
      "Epoch 82/100\n",
      " - 57s - loss: 0.8417 - accuracy: 0.4497 - val_loss: 1.0870 - val_accuracy: 0.3128\n",
      "\n",
      "Epoch 00082: val_accuracy did not improve from 0.39944\n",
      "Epoch 83/100\n",
      " - 58s - loss: 0.8391 - accuracy: 0.4513 - val_loss: 1.0760 - val_accuracy: 0.3651\n",
      "\n",
      "Epoch 00083: val_accuracy did not improve from 0.39944\n",
      "Epoch 84/100\n",
      " - 57s - loss: 0.8372 - accuracy: 0.4532 - val_loss: 1.2321 - val_accuracy: 0.3195\n",
      "\n",
      "Epoch 00084: val_accuracy did not improve from 0.39944\n",
      "Epoch 85/100\n",
      " - 57s - loss: 0.8354 - accuracy: 0.4539 - val_loss: 1.2294 - val_accuracy: 0.3641\n",
      "\n",
      "Epoch 00085: val_accuracy did not improve from 0.39944\n",
      "Epoch 86/100\n",
      " - 57s - loss: 0.8340 - accuracy: 0.4546 - val_loss: 1.1815 - val_accuracy: 0.3620\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00086: val_accuracy did not improve from 0.39944\n",
      "Epoch 87/100\n",
      " - 58s - loss: 0.8322 - accuracy: 0.4563 - val_loss: 1.1326 - val_accuracy: 0.3503\n",
      "\n",
      "Epoch 00087: val_accuracy did not improve from 0.39944\n",
      "Epoch 88/100\n",
      " - 57s - loss: 0.8300 - accuracy: 0.4580 - val_loss: 0.9299 - val_accuracy: 0.3504\n",
      "\n",
      "Epoch 00088: val_accuracy did not improve from 0.39944\n",
      "Epoch 89/100\n",
      " - 58s - loss: 0.8283 - accuracy: 0.4574 - val_loss: 1.3516 - val_accuracy: 0.3472\n",
      "\n",
      "Epoch 00089: val_accuracy did not improve from 0.39944\n",
      "Epoch 90/100\n",
      " - 58s - loss: 0.8267 - accuracy: 0.4596 - val_loss: 1.1913 - val_accuracy: 0.3460\n",
      "\n",
      "Epoch 00090: val_accuracy did not improve from 0.39944\n",
      "Epoch 91/100\n",
      " - 58s - loss: 0.8247 - accuracy: 0.4603 - val_loss: 1.3963 - val_accuracy: 0.3398\n",
      "\n",
      "Epoch 00091: val_accuracy did not improve from 0.39944\n",
      "Epoch 92/100\n",
      " - 58s - loss: 0.8228 - accuracy: 0.4614 - val_loss: 1.4255 - val_accuracy: 0.3582\n",
      "\n",
      "Epoch 00092: val_accuracy did not improve from 0.39944\n",
      "Epoch 93/100\n",
      " - 58s - loss: 0.8212 - accuracy: 0.4636 - val_loss: 1.3443 - val_accuracy: 0.3512\n",
      "\n",
      "Epoch 00093: val_accuracy did not improve from 0.39944\n",
      "Epoch 94/100\n",
      " - 57s - loss: 0.8195 - accuracy: 0.4643 - val_loss: 0.9696 - val_accuracy: 0.3462\n",
      "\n",
      "Epoch 00094: val_accuracy did not improve from 0.39944\n",
      "Epoch 95/100\n",
      " - 57s - loss: 0.8182 - accuracy: 0.4650 - val_loss: 0.9942 - val_accuracy: 0.4046\n",
      "\n",
      "Epoch 00095: val_accuracy improved from 0.39944 to 0.40461, saving model to model/rapid_ascending 30_79_cmo.hdf5\n",
      "Epoch 96/100\n",
      " - 57s - loss: 0.8155 - accuracy: 0.4659 - val_loss: 1.3214 - val_accuracy: 0.3819\n",
      "\n",
      "Epoch 00096: val_accuracy did not improve from 0.40461\n",
      "Epoch 97/100\n",
      " - 58s - loss: 0.8138 - accuracy: 0.4673 - val_loss: 1.3874 - val_accuracy: 0.3461\n",
      "\n",
      "Epoch 00097: val_accuracy did not improve from 0.40461\n",
      "Epoch 98/100\n",
      " - 58s - loss: 0.8126 - accuracy: 0.4698 - val_loss: 1.1160 - val_accuracy: 0.3321\n",
      "\n",
      "Epoch 00098: val_accuracy did not improve from 0.40461\n",
      "Epoch 99/100\n",
      " - 57s - loss: 0.8106 - accuracy: 0.4701 - val_loss: 1.0117 - val_accuracy: 0.3701\n",
      "\n",
      "Epoch 00099: val_accuracy did not improve from 0.40461\n",
      "Epoch 100/100\n",
      " - 58s - loss: 0.8088 - accuracy: 0.4701 - val_loss: 1.6272 - val_accuracy: 0.3768\n",
      "\n",
      "Epoch 00100: val_accuracy did not improve from 0.40461\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot\n",
    "import scipy.misc \n",
    "from math import sqrt \n",
    "import itertools\n",
    "from IPython.display import display\n",
    "%matplotlib inline\n",
    "\n",
    "input_data_length = 30\n",
    "model_num = 79\n",
    "num_classes = 3\n",
    "\n",
    "Made_X = np.load('Made_X/Made_X %s_%s.npy' % (input_data_length, model_num))\n",
    "Made_Y = np.load('Made_X/Made_Y %s_%s.npy' % (input_data_length, model_num)).reshape(-1, 1)\n",
    "\n",
    "\n",
    "#       dataset 분리      #\n",
    "# dataX 구성 : VOLUME, MA, CMO, OBV, RSI, MACD, MACD_SIGNAL, MACD_OSC \n",
    "# dataX 구성 : VOLUME, EMA1, EMA2, CMO, OBV, RSI, MACD, MACD_SIGNAL, MACD_OSC \n",
    "Made_X = Made_X[:, :,[0,1,2,3,6]]\n",
    "print(Made_X.shape)\n",
    "print(Made_Y.shape)\n",
    "\n",
    "row = Made_X.shape[1]\n",
    "col = Made_X.shape[2]\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(Made_X, Made_Y, test_size=0.3,\n",
    "                                                   shuffle=False)\n",
    "\n",
    "X_train = X_train.astype('float32').reshape(-1, input_data_length, col, 1)\n",
    "X_val = X_val.astype('float32').reshape(-1, input_data_length, col, 1)\n",
    "print(X_train.shape)\n",
    "print(X_val.shape)\n",
    "\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing.image import ImageDataGenerator \n",
    "\n",
    "# Data Class Weight\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "print(Y_train[:, 0])\n",
    "class_weights = class_weight.compute_class_weight('balanced', \n",
    "                                                  np.unique(Y_train[:, 0]),\n",
    "                                                  Y_train[:, 0])\n",
    "class_weights = dict(enumerate(class_weights))\n",
    "# class_weights[1] *= 0.97\n",
    "# class_weights[2] *= 0.97\n",
    "print(class_weights)\n",
    "# quit()\n",
    "\n",
    "Y_train = Y_train.astype('float32')\n",
    "Y_val = Y_val.astype('float32')\n",
    "Y_train = np_utils.to_categorical(Y_train, num_classes)\n",
    "Y_val = np_utils.to_categorical(Y_val, num_classes)\n",
    "print(Y_train.shape)\n",
    "print(Y_val.shape)\n",
    "\n",
    "datagen = ImageDataGenerator( \n",
    "#     rotation_range = 60,\n",
    "#     zoom_range = 0.6,\n",
    "#     shear_range = 0.6,\n",
    "#     horizontal_flip = True,\n",
    "#     width_shift_range=0.6,\n",
    "#     height_shift_range=0.6,\n",
    "    fill_mode = 'nearest'\n",
    "    )\n",
    "\n",
    "testgen = ImageDataGenerator( \n",
    "    )\n",
    "datagen.fit(X_train)\n",
    "batch_size = 128\n",
    "\n",
    "for X_batch, _ in datagen.flow(X_train, Y_train, batch_size=9):\n",
    "    for i in range(0, 9): \n",
    "        pyplot.axis('off') \n",
    "        pyplot.subplot(330 + 1 + i) \n",
    "        pyplot.imshow(X_batch[i].reshape(input_data_length, col), cmap=pyplot.get_cmap('gray'))\n",
    "    pyplot.axis('off') \n",
    "    pyplot.show() \n",
    "    break\n",
    "    \n",
    "    \n",
    "train_flow = datagen.flow(X_train, Y_train, batch_size=batch_size) \n",
    "val_flow = testgen.flow(X_val, Y_val, batch_size=batch_size) \n",
    "\n",
    "\n",
    "from keras.utils import plot_model\n",
    "import keras.backend as K\n",
    "from keras.models import Model, Sequential\n",
    "import keras.layers as layers\n",
    "from keras.optimizers import Adam, SGD\n",
    "from keras.regularizers import l1, l2\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def FER_Model(input_shape=(row, col, 1)):\n",
    "    # first input model\n",
    "    visible = layers.Input(shape=input_shape, name='input')\n",
    "    \n",
    "    net = layers.Conv2D(64, kernel_size=3, padding='same')(visible)\n",
    "    # net = layers.Activation('relu')(net)\n",
    "    net = layers.LeakyReLU()(net)\n",
    "    net = layers.MaxPool2D(pool_size=2)(net)\n",
    "\n",
    "    shortcut_1 = net\n",
    "\n",
    "    net = layers.Conv2D(128, kernel_size=3, padding='same')(net)\n",
    "    # net = layers.Activation('relu')(net)\n",
    "    net = layers.LeakyReLU()(net)\n",
    "    net = layers.MaxPool2D(pool_size=2)(net)\n",
    "\n",
    "    shortcut_2 = net\n",
    "\n",
    "#     net = layers.Conv2D(256, kernel_size=3, padding='same')(net)\n",
    "#     # net = layers.Activation('relu')(net)\n",
    "#     net = layers.LeakyReLU()(net)\n",
    "#     net = layers.MaxPool2D(pool_size=2)(net)\n",
    "\n",
    "#     shortcut_3 = net\n",
    "\n",
    "#     net = layers.Conv2D(128, kernel_size=1, padding='same')(net)\n",
    "#     # net = layers.Activation('relu')(net)\n",
    "#     net = layers.LeakyReLU()(net)\n",
    "#     net = layers.MaxPool2D(pool_size=2)(net)\n",
    "\n",
    "    net = layers.Flatten()(net)\n",
    "    net = layers.Dense(64)(net)\n",
    "    net = layers.LeakyReLU()(net)\n",
    "    net = layers.Dense(num_classes, activation='softmax')(net)\n",
    "\n",
    "    # create model \n",
    "    model = Model(inputs =visible, outputs = net)\n",
    "    # summary layers\n",
    "    print(model.summary())\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = FER_Model()\n",
    "opt = Adam(lr=0.0001, decay=1e-6)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "    \n",
    "    \n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, TensorBoard\n",
    "filepath=\"model/rapid_ascending %s_%s_cmo.hdf5\" % (input_data_length, model_num)\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='auto')\n",
    "checkpoint2 = TensorBoard(log_dir='Tensorboard_graph',\n",
    "                          histogram_freq=0,\n",
    "                          write_graph=True,\n",
    "                          write_images=True)\n",
    "checkpoint3 = EarlyStopping(monitor='val_accuracy', patience=50)\n",
    "callbacks_list = [checkpoint, checkpoint2, checkpoint3]\n",
    "\n",
    "# keras.callbacks.Callback 로 부터 log 를 받아와 history log 를 작성할 수 있다.\n",
    "\n",
    "# we iterate 200 times over the entire training set\n",
    "num_epochs = 100\n",
    "history = model.fit_generator(train_flow, \n",
    "                    steps_per_epoch=len(X_train) / batch_size, \n",
    "                    epochs=num_epochs,  \n",
    "                    verbose=2,  \n",
    "                    callbacks=callbacks_list,\n",
    "                    class_weight=class_weights,\n",
    "                    validation_data=val_flow,  \n",
    "                    validation_steps=len(X_val) / batch_size,\n",
    "                    shuffle=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "model = load_model('model/rapid_ascending %s_%s.hdf5' % (input_data_length, model_num))\n",
    "# model = load_model('/content/gdrive/My Drive/Colab Notebooks/model/rapid_ascending %s.hdf5' % input_data_length)\n",
    "# loss = model.evaluate_generator(test_flow, steps=len(X_test) / batch_size) \n",
    "# print(\"Test Loss \" + str(loss[0]))\n",
    "# print(\"Test Acc: \" + str(loss[1]))\n",
    "\n",
    "# loss = model.evaluate(X_val, Y_val) \n",
    "# print(X_val.shape)\n",
    "# print(Y_val.shape)\n",
    "\n",
    "# print(\"Val Loss \" + str(loss[0]))\n",
    "# print(\"Val Acc: \" + str(loss[1]))\n",
    "\n",
    "#     Prediction    #\n",
    "Y_pred_ = model.predict(X_test, verbose=1)\n",
    "\n",
    "# Y_pred = Y_pred_[:,[-1]]\n",
    "# print(Y_pred.shape)\n",
    "# print(Y_test.shape)\n",
    "Y_pred = np.argmax(Y_pred_, axis=1)\n",
    "t_te = np.argmax(Y_test, axis=1)\n",
    "\n",
    "#     Manual processing     #\n",
    "Y_pred_one = Y_pred_[:, [-1]]\n",
    "print(Y_pred_)\n",
    "max_value = np.max(Y_pred_one)\n",
    "print(max_value)\n",
    "\n",
    "limit_line = 0.9\n",
    "Y_pred_one = np.where(Y_pred_one > max_value * limit_line, 1, 0)\n",
    "\n",
    "# print(Y_pred_one)\n",
    "Y_pred_one = Y_pred_one.reshape(-1,)\n",
    "# print(Y_pred_1)\n",
    "# print(Y_pred.shape)\n",
    "# print(t_te.shape)\n",
    "\n",
    "# fail = 0\n",
    "# fail2 = 0\n",
    "# for i in range(len(Y_pred)):\n",
    "#   if Y_pred_1[i] != t_te[i]:\n",
    "#     fail += 1\n",
    "\n",
    "#   if Y_pred[i] != t_te[i]:\n",
    "#     fail2 += 1\n",
    "\n",
    "# print(1 - fail / len(Y_pred))\n",
    "# print(1 - fail2 / len(Y_pred))\n",
    "\n",
    "# print(np.sum(Y_pred), np.sum(t_te))\n",
    "# print('Y_pred / Y_test :', np.sum(Y_pred) / np.sum(t_te))\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.figure(figsize=(100,10))\n",
    "plt.subplot(211)\n",
    "plt.plot(Y_test, 'purple', label='test')\n",
    "\n",
    "plt.subplot(212)\n",
    "plt.plot(Y_pred_one, 'y', label='pred')\n",
    "plt.show()\n",
    "# plt.savefig('/content/gdrive/My Drive/Colab Notebooks/model/rapid_ascending %s.png' % input_data_length)\n",
    "# plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow2_p36]",
   "language": "python",
   "name": "conda-env-tensorflow2_p36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
